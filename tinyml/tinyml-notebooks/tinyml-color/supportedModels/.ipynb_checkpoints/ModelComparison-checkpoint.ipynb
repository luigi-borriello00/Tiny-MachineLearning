{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Comparison for TinyML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "import pickle\n",
    "from pandas import read_csv\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,  classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold,StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dense, Input, concatenate, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import m2cgen as m2c\n",
    "from micromlgen import port\n",
    "\n",
    "import warnings\n",
    "import seaborn as sbs\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/X_paper.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open('../data/y_paper.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X[:250], X[750:1000], X[1500:]), axis=0)\n",
    "y = np.concatenate((y[:250], y[750:1000], y[1500:]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Spotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 42\n",
    "scoring = 'f1_macro'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "\n",
    "#models.append(('XGB', XGBClassifier(random_state=seed)))\n",
    "models.append(('GNB', GaussianNB(var_smoothing=2e-9)))\n",
    "models.append(('LR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression(random_state=seed))])))\n",
    "models.append(('CART' , DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('SVC' , SVC(gamma=0.5, random_state=seed)))\n",
    "models.append(('RF', RandomForestClassifier(random_state=seed, n_estimators = 50)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB - 0,39 0,03\n",
      "LR - 0,34 0,03\n",
      "CART - 0,67 0,02\n",
      "SVC - 0,64 0,04\n",
      "RF - 0,66 0,04\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    # Dividere dati in n = num_folds\n",
    "    kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = np.array([])\n",
    "    for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "        X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "        X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "        model.fit(X_cross_train, y_cross_train)  \n",
    "        y_pred = model.predict(X_cross_test)\n",
    "        f1s = f1_score(y_cross_test, y_pred, average=\"weighted\")\n",
    "        cv_results = np.append(cv_results, [f1s])\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #msg = \"%s - %f - %f\" % (name, cv_results.mean(), cv_results.std())\n",
    "    msg = \"{} - {:.2f} {:.2f}\".format(name, cv_results.mean(), cv_results.std()).replace('.', ',')\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "fig.suptitle('Algorithms Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valutazione dei migliori algoritmi su test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione Inferance Rate medio (|X_test| = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = read_csv(\"InfTimeReport.csv\")\n",
    "g = sbs.barplot(x=csv['Algoritmo'], y=csv['InfTime'])\n",
    "g.set_yscale(\"log\")\n",
    "plt.ylabel(\"Inference Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoria occupata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = read_csv(\"MemOccupationReport.csv\")\n",
    "sbs.barplot(x=csv['Algoritmo'], y=csv['MemOccupata'])\n",
    "plt.ylabel(\"MemOccupata in Byte\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 700)               4200      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 7)                 4907      \n",
      "=================================================================\n",
      "Total params: 9,137\n",
      "Trainable params: 9,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "model = Sequential(name=\"Sequential-NN\")\n",
    "model.add(layers.Dense(X.shape[1], activation='relu', input_shape=(X.shape[1],)))\n",
    "model.add(layers.Dense(np.unique(y).size * n, activation='relu'))\n",
    "model.add(layers.Dense(np.unique(y).size, activation='softmax'))\n",
    "opt = Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Convert to array[int]\n",
    "y_train = np.array([int(num) for num in y_train])\n",
    "y_test = np.array([int(num) for num in y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/750\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 6.3469 - accuracy: 0.2593 - val_loss: 8.2909 - val_accuracy: 0.2593\n",
      "Epoch 2/750\n",
      "162/162 [==============================] - 0s 719us/step - loss: 5.6399 - accuracy: 0.2667 - val_loss: 3.9930 - val_accuracy: 0.2630\n",
      "Epoch 3/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 5.2621 - accuracy: 0.2704 - val_loss: 4.0758 - val_accuracy: 0.2778\n",
      "Epoch 4/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 4.0572 - accuracy: 0.2901 - val_loss: 4.9368 - val_accuracy: 0.1926\n",
      "Epoch 5/750\n",
      "162/162 [==============================] - 0s 623us/step - loss: 3.5709 - accuracy: 0.3309 - val_loss: 3.5056 - val_accuracy: 0.3222\n",
      "Epoch 6/750\n",
      "162/162 [==============================] - 0s 623us/step - loss: 4.0241 - accuracy: 0.3025 - val_loss: 6.4614 - val_accuracy: 0.3148\n",
      "Epoch 7/750\n",
      "162/162 [==============================] - 0s 741us/step - loss: 4.0560 - accuracy: 0.2901 - val_loss: 3.0759 - val_accuracy: 0.2222\n",
      "Epoch 8/750\n",
      "162/162 [==============================] - 0s 741us/step - loss: 3.3269 - accuracy: 0.2963 - val_loss: 2.7151 - val_accuracy: 0.3593\n",
      "Epoch 9/750\n",
      "162/162 [==============================] - 0s 766us/step - loss: 3.5520 - accuracy: 0.3099 - val_loss: 3.8218 - val_accuracy: 0.2593\n",
      "Epoch 10/750\n",
      "162/162 [==============================] - 0s 662us/step - loss: 3.3217 - accuracy: 0.3086 - val_loss: 2.8049 - val_accuracy: 0.3185\n",
      "Epoch 11/750\n",
      "162/162 [==============================] - 0s 821us/step - loss: 3.2665 - accuracy: 0.3123 - val_loss: 2.7771 - val_accuracy: 0.3074\n",
      "Epoch 12/750\n",
      "162/162 [==============================] - 0s 612us/step - loss: 2.8818 - accuracy: 0.3148 - val_loss: 2.8573 - val_accuracy: 0.3333\n",
      "Epoch 13/750\n",
      "162/162 [==============================] - 0s 664us/step - loss: 2.4876 - accuracy: 0.3321 - val_loss: 2.4252 - val_accuracy: 0.3444\n",
      "Epoch 14/750\n",
      "162/162 [==============================] - 0s 698us/step - loss: 2.9429 - accuracy: 0.3272 - val_loss: 1.9199 - val_accuracy: 0.3074\n",
      "Epoch 15/750\n",
      "162/162 [==============================] - 0s 745us/step - loss: 2.5319 - accuracy: 0.3222 - val_loss: 1.5330 - val_accuracy: 0.3889\n",
      "Epoch 16/750\n",
      "162/162 [==============================] - 0s 749us/step - loss: 2.2752 - accuracy: 0.3309 - val_loss: 2.2399 - val_accuracy: 0.3185\n",
      "Epoch 17/750\n",
      "162/162 [==============================] - 0s 660us/step - loss: 2.2933 - accuracy: 0.3383 - val_loss: 2.7741 - val_accuracy: 0.2481\n",
      "Epoch 18/750\n",
      "162/162 [==============================] - 0s 717us/step - loss: 2.3398 - accuracy: 0.3630 - val_loss: 1.8882 - val_accuracy: 0.4000\n",
      "Epoch 19/750\n",
      "162/162 [==============================] - 0s 693us/step - loss: 2.4161 - accuracy: 0.3136 - val_loss: 2.2786 - val_accuracy: 0.3333\n",
      "Epoch 20/750\n",
      "162/162 [==============================] - 0s 692us/step - loss: 2.0662 - accuracy: 0.3185 - val_loss: 1.6220 - val_accuracy: 0.3074\n",
      "Epoch 21/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 2.2227 - accuracy: 0.3309 - val_loss: 2.2648 - val_accuracy: 0.3407\n",
      "Epoch 22/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.9943 - accuracy: 0.3370 - val_loss: 1.6520 - val_accuracy: 0.4000\n",
      "Epoch 23/750\n",
      "162/162 [==============================] - 0s 660us/step - loss: 1.8225 - accuracy: 0.3346 - val_loss: 1.9981 - val_accuracy: 0.3630\n",
      "Epoch 24/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.8651 - accuracy: 0.3556 - val_loss: 1.9681 - val_accuracy: 0.3889\n",
      "Epoch 25/750\n",
      "162/162 [==============================] - 0s 591us/step - loss: 1.8231 - accuracy: 0.3716 - val_loss: 1.8342 - val_accuracy: 0.3630\n",
      "Epoch 26/750\n",
      "162/162 [==============================] - 0s 640us/step - loss: 1.7448 - accuracy: 0.3852 - val_loss: 1.6657 - val_accuracy: 0.3481\n",
      "Epoch 27/750\n",
      "162/162 [==============================] - 0s 736us/step - loss: 1.8086 - accuracy: 0.3605 - val_loss: 1.5472 - val_accuracy: 0.4111\n",
      "Epoch 28/750\n",
      "162/162 [==============================] - 0s 619us/step - loss: 1.7723 - accuracy: 0.3691 - val_loss: 1.6832 - val_accuracy: 0.3593\n",
      "Epoch 29/750\n",
      "162/162 [==============================] - 0s 571us/step - loss: 1.7417 - accuracy: 0.3642 - val_loss: 1.8751 - val_accuracy: 0.4111\n",
      "Epoch 30/750\n",
      "162/162 [==============================] - 0s 654us/step - loss: 1.7180 - accuracy: 0.3506 - val_loss: 1.5948 - val_accuracy: 0.4593\n",
      "Epoch 31/750\n",
      "162/162 [==============================] - 0s 733us/step - loss: 1.7828 - accuracy: 0.3506 - val_loss: 1.5403 - val_accuracy: 0.4407\n",
      "Epoch 32/750\n",
      "162/162 [==============================] - 0s 701us/step - loss: 1.6091 - accuracy: 0.4037 - val_loss: 1.6218 - val_accuracy: 0.4667\n",
      "Epoch 33/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.5931 - accuracy: 0.4012 - val_loss: 1.4369 - val_accuracy: 0.4185\n",
      "Epoch 34/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.5161 - accuracy: 0.4123 - val_loss: 1.4563 - val_accuracy: 0.3926\n",
      "Epoch 35/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.6377 - accuracy: 0.3815 - val_loss: 1.6340 - val_accuracy: 0.3148\n",
      "Epoch 36/750\n",
      "162/162 [==============================] - 0s 695us/step - loss: 1.5295 - accuracy: 0.3938 - val_loss: 1.4791 - val_accuracy: 0.4778\n",
      "Epoch 37/750\n",
      "162/162 [==============================] - 0s 606us/step - loss: 1.5122 - accuracy: 0.3926 - val_loss: 1.3931 - val_accuracy: 0.4889\n",
      "Epoch 38/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.5318 - accuracy: 0.4000 - val_loss: 1.3327 - val_accuracy: 0.5111\n",
      "Epoch 39/750\n",
      "162/162 [==============================] - 0s 590us/step - loss: 1.4499 - accuracy: 0.4333 - val_loss: 1.3483 - val_accuracy: 0.4222\n",
      "Epoch 40/750\n",
      "162/162 [==============================] - 0s 749us/step - loss: 1.4901 - accuracy: 0.4049 - val_loss: 1.4582 - val_accuracy: 0.5407\n",
      "Epoch 41/750\n",
      "162/162 [==============================] - 0s 609us/step - loss: 1.4728 - accuracy: 0.4198 - val_loss: 1.3930 - val_accuracy: 0.4741\n",
      "Epoch 42/750\n",
      "162/162 [==============================] - 0s 696us/step - loss: 1.4505 - accuracy: 0.4222 - val_loss: 1.3849 - val_accuracy: 0.4741\n",
      "Epoch 43/750\n",
      "162/162 [==============================] - 0s 561us/step - loss: 1.4081 - accuracy: 0.4469 - val_loss: 1.3555 - val_accuracy: 0.4963\n",
      "Epoch 44/750\n",
      "162/162 [==============================] - 0s 657us/step - loss: 1.4223 - accuracy: 0.4531 - val_loss: 1.4254 - val_accuracy: 0.4074\n",
      "Epoch 45/750\n",
      "162/162 [==============================] - 0s 580us/step - loss: 1.4690 - accuracy: 0.4086 - val_loss: 1.4327 - val_accuracy: 0.4593\n",
      "Epoch 46/750\n",
      "162/162 [==============================] - 0s 724us/step - loss: 1.4266 - accuracy: 0.4395 - val_loss: 1.3461 - val_accuracy: 0.4556\n",
      "Epoch 47/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.4332 - accuracy: 0.4358 - val_loss: 1.4881 - val_accuracy: 0.4000\n",
      "Epoch 48/750\n",
      "162/162 [==============================] - 0s 665us/step - loss: 1.4037 - accuracy: 0.4259 - val_loss: 1.3315 - val_accuracy: 0.4704\n",
      "Epoch 49/750\n",
      "162/162 [==============================] - 0s 731us/step - loss: 1.3594 - accuracy: 0.4494 - val_loss: 1.3414 - val_accuracy: 0.5000\n",
      "Epoch 50/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.3858 - accuracy: 0.4568 - val_loss: 1.4242 - val_accuracy: 0.4296\n",
      "Epoch 51/750\n",
      "162/162 [==============================] - 0s 713us/step - loss: 1.3514 - accuracy: 0.4741 - val_loss: 1.2719 - val_accuracy: 0.5259\n",
      "Epoch 52/750\n",
      "162/162 [==============================] - 0s 597us/step - loss: 1.3825 - accuracy: 0.4407 - val_loss: 1.3532 - val_accuracy: 0.4741\n",
      "Epoch 53/750\n",
      "162/162 [==============================] - 0s 645us/step - loss: 1.3539 - accuracy: 0.4630 - val_loss: 1.5164 - val_accuracy: 0.4259\n",
      "Epoch 54/750\n",
      "162/162 [==============================] - 0s 664us/step - loss: 1.3821 - accuracy: 0.4420 - val_loss: 1.2924 - val_accuracy: 0.4815\n",
      "Epoch 55/750\n",
      "162/162 [==============================] - 0s 578us/step - loss: 1.3196 - accuracy: 0.4802 - val_loss: 1.3881 - val_accuracy: 0.4778\n",
      "Epoch 56/750\n",
      "162/162 [==============================] - 0s 576us/step - loss: 1.3242 - accuracy: 0.4679 - val_loss: 1.2745 - val_accuracy: 0.5481\n",
      "Epoch 57/750\n",
      "162/162 [==============================] - 0s 659us/step - loss: 1.3202 - accuracy: 0.4716 - val_loss: 1.2536 - val_accuracy: 0.5370\n",
      "Epoch 58/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.2995 - accuracy: 0.4901 - val_loss: 1.3259 - val_accuracy: 0.5259\n",
      "Epoch 59/750\n",
      "162/162 [==============================] - 0s 695us/step - loss: 1.3140 - accuracy: 0.4877 - val_loss: 1.2811 - val_accuracy: 0.5407\n",
      "Epoch 60/750\n",
      "162/162 [==============================] - 0s 740us/step - loss: 1.3027 - accuracy: 0.4753 - val_loss: 1.2484 - val_accuracy: 0.5407\n",
      "Epoch 61/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.2841 - accuracy: 0.4753 - val_loss: 1.3197 - val_accuracy: 0.5259\n",
      "Epoch 62/750\n",
      "162/162 [==============================] - 0s 651us/step - loss: 1.3041 - accuracy: 0.4728 - val_loss: 1.3000 - val_accuracy: 0.4852\n",
      "Epoch 63/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.2842 - accuracy: 0.4901 - val_loss: 1.2656 - val_accuracy: 0.5481\n",
      "Epoch 64/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.2578 - accuracy: 0.5185 - val_loss: 1.2616 - val_accuracy: 0.4926\n",
      "Epoch 65/750\n",
      "162/162 [==============================] - 0s 652us/step - loss: 1.2553 - accuracy: 0.4975 - val_loss: 1.2124 - val_accuracy: 0.5630\n",
      "Epoch 66/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.2809 - accuracy: 0.4802 - val_loss: 1.2232 - val_accuracy: 0.5185\n",
      "Epoch 67/750\n",
      "162/162 [==============================] - 0s 649us/step - loss: 1.2996 - accuracy: 0.4691 - val_loss: 1.2953 - val_accuracy: 0.5111\n",
      "Epoch 68/750\n",
      "162/162 [==============================] - 0s 715us/step - loss: 1.2796 - accuracy: 0.5012 - val_loss: 1.2108 - val_accuracy: 0.5519\n",
      "Epoch 69/750\n",
      "162/162 [==============================] - 0s 641us/step - loss: 1.2651 - accuracy: 0.4963 - val_loss: 1.2646 - val_accuracy: 0.5593\n",
      "Epoch 70/750\n",
      "162/162 [==============================] - 0s 753us/step - loss: 1.2312 - accuracy: 0.5210 - val_loss: 1.2117 - val_accuracy: 0.5630\n",
      "Epoch 71/750\n",
      "162/162 [==============================] - 0s 663us/step - loss: 1.2505 - accuracy: 0.5074 - val_loss: 1.2996 - val_accuracy: 0.4889\n",
      "Epoch 72/750\n",
      "162/162 [==============================] - 0s 700us/step - loss: 1.2435 - accuracy: 0.5062 - val_loss: 1.1916 - val_accuracy: 0.5556\n",
      "Epoch 73/750\n",
      "162/162 [==============================] - 0s 663us/step - loss: 1.2696 - accuracy: 0.4741 - val_loss: 1.2939 - val_accuracy: 0.5000\n",
      "Epoch 74/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.2783 - accuracy: 0.4852 - val_loss: 1.3160 - val_accuracy: 0.4630\n",
      "Epoch 75/750\n",
      "162/162 [==============================] - 0s 649us/step - loss: 1.2344 - accuracy: 0.5198 - val_loss: 1.3211 - val_accuracy: 0.4963\n",
      "Epoch 76/750\n",
      "162/162 [==============================] - 0s 666us/step - loss: 1.2637 - accuracy: 0.4901 - val_loss: 1.1802 - val_accuracy: 0.5185\n",
      "Epoch 77/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.2122 - accuracy: 0.5284 - val_loss: 1.3623 - val_accuracy: 0.4370\n",
      "Epoch 78/750\n",
      "162/162 [==============================] - 0s 740us/step - loss: 1.2367 - accuracy: 0.5111 - val_loss: 1.2414 - val_accuracy: 0.4556\n",
      "Epoch 79/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.2123 - accuracy: 0.5222 - val_loss: 1.2250 - val_accuracy: 0.5259\n",
      "Epoch 80/750\n",
      "162/162 [==============================] - 0s 605us/step - loss: 1.2322 - accuracy: 0.5062 - val_loss: 1.1738 - val_accuracy: 0.5407\n",
      "Epoch 81/750\n",
      "162/162 [==============================] - 0s 604us/step - loss: 1.2431 - accuracy: 0.4679 - val_loss: 1.1875 - val_accuracy: 0.4556\n",
      "Epoch 82/750\n",
      "162/162 [==============================] - 0s 620us/step - loss: 1.2063 - accuracy: 0.5111 - val_loss: 1.4144 - val_accuracy: 0.5185\n",
      "Epoch 83/750\n",
      "162/162 [==============================] - 0s 733us/step - loss: 1.2306 - accuracy: 0.5173 - val_loss: 1.1263 - val_accuracy: 0.5185\n",
      "Epoch 84/750\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 1.2140 - accuracy: 0.5222 - val_loss: 1.2779 - val_accuracy: 0.5407\n",
      "Epoch 85/750\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 1.2249 - accuracy: 0.4951 - val_loss: 1.1888 - val_accuracy: 0.5444\n",
      "Epoch 86/750\n",
      "162/162 [==============================] - 0s 889us/step - loss: 1.1992 - accuracy: 0.5111 - val_loss: 1.1631 - val_accuracy: 0.5481\n",
      "Epoch 87/750\n",
      "162/162 [==============================] - 0s 938us/step - loss: 1.1848 - accuracy: 0.5185 - val_loss: 1.1350 - val_accuracy: 0.5556\n",
      "Epoch 88/750\n",
      "162/162 [==============================] - 0s 839us/step - loss: 1.2248 - accuracy: 0.4938 - val_loss: 1.2594 - val_accuracy: 0.5000\n",
      "Epoch 89/750\n",
      "162/162 [==============================] - 0s 889us/step - loss: 1.1937 - accuracy: 0.5099 - val_loss: 1.1666 - val_accuracy: 0.5741\n",
      "Epoch 90/750\n",
      "162/162 [==============================] - 0s 740us/step - loss: 1.2091 - accuracy: 0.5148 - val_loss: 1.2416 - val_accuracy: 0.4963\n",
      "Epoch 91/750\n",
      "162/162 [==============================] - 0s 790us/step - loss: 1.2051 - accuracy: 0.5296 - val_loss: 1.2270 - val_accuracy: 0.5296\n",
      "Epoch 92/750\n",
      "162/162 [==============================] - 0s 790us/step - loss: 1.2023 - accuracy: 0.5111 - val_loss: 1.1670 - val_accuracy: 0.5519\n",
      "Epoch 93/750\n",
      "162/162 [==============================] - 0s 937us/step - loss: 1.1809 - accuracy: 0.5148 - val_loss: 1.1557 - val_accuracy: 0.5444\n",
      "Epoch 94/750\n",
      "162/162 [==============================] - 0s 788us/step - loss: 1.1823 - accuracy: 0.5235 - val_loss: 1.1403 - val_accuracy: 0.5667\n",
      "Epoch 95/750\n",
      "162/162 [==============================] - 0s 640us/step - loss: 1.1918 - accuracy: 0.5160 - val_loss: 1.1531 - val_accuracy: 0.5259\n",
      "Epoch 96/750\n",
      "162/162 [==============================] - 0s 695us/step - loss: 1.1777 - accuracy: 0.5074 - val_loss: 1.2292 - val_accuracy: 0.4556\n",
      "Epoch 97/750\n",
      "162/162 [==============================] - 0s 752us/step - loss: 1.1871 - accuracy: 0.5136 - val_loss: 1.2183 - val_accuracy: 0.5185\n",
      "Epoch 98/750\n",
      "162/162 [==============================] - 0s 701us/step - loss: 1.1767 - accuracy: 0.5173 - val_loss: 1.1442 - val_accuracy: 0.5185\n",
      "Epoch 99/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.2025 - accuracy: 0.5185 - val_loss: 1.1487 - val_accuracy: 0.5667\n",
      "Epoch 100/750\n",
      "162/162 [==============================] - 0s 771us/step - loss: 1.1764 - accuracy: 0.5296 - val_loss: 1.1388 - val_accuracy: 0.5926\n",
      "Epoch 101/750\n",
      "162/162 [==============================] - 0s 869us/step - loss: 1.1742 - accuracy: 0.5160 - val_loss: 1.1381 - val_accuracy: 0.5593\n",
      "Epoch 102/750\n",
      "162/162 [==============================] - 0s 737us/step - loss: 1.1711 - accuracy: 0.5235 - val_loss: 1.1738 - val_accuracy: 0.4741\n",
      "Epoch 103/750\n",
      "162/162 [==============================] - 0s 650us/step - loss: 1.1526 - accuracy: 0.5247 - val_loss: 1.1371 - val_accuracy: 0.5185\n",
      "Epoch 104/750\n",
      "162/162 [==============================] - 0s 645us/step - loss: 1.1593 - accuracy: 0.5160 - val_loss: 1.1276 - val_accuracy: 0.5556\n",
      "Epoch 105/750\n",
      "162/162 [==============================] - 0s 745us/step - loss: 1.1914 - accuracy: 0.5259 - val_loss: 1.1463 - val_accuracy: 0.5148\n",
      "Epoch 106/750\n",
      "162/162 [==============================] - 0s 658us/step - loss: 1.1631 - accuracy: 0.4975 - val_loss: 1.1438 - val_accuracy: 0.5259\n",
      "Epoch 107/750\n",
      "162/162 [==============================] - 0s 647us/step - loss: 1.1584 - accuracy: 0.5420 - val_loss: 1.1231 - val_accuracy: 0.5296\n",
      "Epoch 108/750\n",
      "162/162 [==============================] - 0s 633us/step - loss: 1.1594 - accuracy: 0.5198 - val_loss: 1.1431 - val_accuracy: 0.5815\n",
      "Epoch 109/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.1590 - accuracy: 0.5383 - val_loss: 1.1896 - val_accuracy: 0.5519\n",
      "Epoch 110/750\n",
      "162/162 [==============================] - 0s 675us/step - loss: 1.1616 - accuracy: 0.5333 - val_loss: 1.1837 - val_accuracy: 0.5222\n",
      "Epoch 111/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.1547 - accuracy: 0.5370 - val_loss: 1.1348 - val_accuracy: 0.5296\n",
      "Epoch 112/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.1592 - accuracy: 0.5173 - val_loss: 1.1438 - val_accuracy: 0.5556\n",
      "Epoch 113/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 638us/step - loss: 1.1598 - accuracy: 0.5346 - val_loss: 1.1635 - val_accuracy: 0.5000\n",
      "Epoch 114/750\n",
      "162/162 [==============================] - 0s 647us/step - loss: 1.1395 - accuracy: 0.5358 - val_loss: 1.2689 - val_accuracy: 0.4778\n",
      "Epoch 115/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.1627 - accuracy: 0.5074 - val_loss: 1.1654 - val_accuracy: 0.5556\n",
      "Epoch 116/750\n",
      "162/162 [==============================] - 0s 652us/step - loss: 1.1789 - accuracy: 0.5099 - val_loss: 1.2329 - val_accuracy: 0.4407\n",
      "Epoch 117/750\n",
      "162/162 [==============================] - 0s 623us/step - loss: 1.1505 - accuracy: 0.5457 - val_loss: 1.1621 - val_accuracy: 0.4741\n",
      "Epoch 118/750\n",
      "162/162 [==============================] - 0s 653us/step - loss: 1.1603 - accuracy: 0.5444 - val_loss: 1.1461 - val_accuracy: 0.5259\n",
      "Epoch 119/750\n",
      "162/162 [==============================] - 0s 645us/step - loss: 1.1569 - accuracy: 0.5247 - val_loss: 1.1852 - val_accuracy: 0.4481\n",
      "Epoch 120/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.1642 - accuracy: 0.5235 - val_loss: 1.1991 - val_accuracy: 0.5481\n",
      "Epoch 121/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.1425 - accuracy: 0.5284 - val_loss: 1.1393 - val_accuracy: 0.5185\n",
      "Epoch 122/750\n",
      "162/162 [==============================] - 0s 640us/step - loss: 1.1558 - accuracy: 0.5309 - val_loss: 1.1652 - val_accuracy: 0.4333\n",
      "Epoch 123/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.1310 - accuracy: 0.5358 - val_loss: 1.1273 - val_accuracy: 0.5519\n",
      "Epoch 124/750\n",
      "162/162 [==============================] - 0s 745us/step - loss: 1.1544 - accuracy: 0.5160 - val_loss: 1.2514 - val_accuracy: 0.4667\n",
      "Epoch 125/750\n",
      "162/162 [==============================] - 0s 670us/step - loss: 1.1480 - accuracy: 0.5235 - val_loss: 1.1180 - val_accuracy: 0.5778\n",
      "Epoch 126/750\n",
      "162/162 [==============================] - 0s 621us/step - loss: 1.1291 - accuracy: 0.5358 - val_loss: 1.1334 - val_accuracy: 0.5519\n",
      "Epoch 127/750\n",
      "162/162 [==============================] - 0s 616us/step - loss: 1.1531 - accuracy: 0.5247 - val_loss: 1.1458 - val_accuracy: 0.5667\n",
      "Epoch 128/750\n",
      "162/162 [==============================] - 0s 738us/step - loss: 1.1359 - accuracy: 0.5358 - val_loss: 1.0968 - val_accuracy: 0.5593\n",
      "Epoch 129/750\n",
      "162/162 [==============================] - 0s 680us/step - loss: 1.1288 - accuracy: 0.5556 - val_loss: 1.0939 - val_accuracy: 0.5593\n",
      "Epoch 130/750\n",
      "162/162 [==============================] - 0s 636us/step - loss: 1.1548 - accuracy: 0.5222 - val_loss: 1.1479 - val_accuracy: 0.5296\n",
      "Epoch 131/750\n",
      "162/162 [==============================] - 0s 765us/step - loss: 1.1428 - accuracy: 0.5222 - val_loss: 1.1148 - val_accuracy: 0.6000\n",
      "Epoch 132/750\n",
      "162/162 [==============================] - 0s 595us/step - loss: 1.1294 - accuracy: 0.5407 - val_loss: 1.1064 - val_accuracy: 0.5630\n",
      "Epoch 133/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.1144 - accuracy: 0.5531 - val_loss: 1.0924 - val_accuracy: 0.5630\n",
      "Epoch 134/750\n",
      "162/162 [==============================] - ETA: 0s - loss: 1.1219 - accuracy: 0.53 - 0s 565us/step - loss: 1.1377 - accuracy: 0.5333 - val_loss: 1.1548 - val_accuracy: 0.5704\n",
      "Epoch 135/750\n",
      "162/162 [==============================] - 0s 579us/step - loss: 1.1650 - accuracy: 0.5000 - val_loss: 1.1323 - val_accuracy: 0.5630\n",
      "Epoch 136/750\n",
      "162/162 [==============================] - 0s 633us/step - loss: 1.1294 - accuracy: 0.5346 - val_loss: 1.1965 - val_accuracy: 0.4667\n",
      "Epoch 137/750\n",
      "162/162 [==============================] - 0s 656us/step - loss: 1.1290 - accuracy: 0.5321 - val_loss: 1.0960 - val_accuracy: 0.6074\n",
      "Epoch 138/750\n",
      "162/162 [==============================] - 0s 580us/step - loss: 1.1166 - accuracy: 0.5432 - val_loss: 1.1153 - val_accuracy: 0.5630\n",
      "Epoch 139/750\n",
      "162/162 [==============================] - 0s 620us/step - loss: 1.1196 - accuracy: 0.5333 - val_loss: 1.1750 - val_accuracy: 0.5370\n",
      "Epoch 140/750\n",
      "162/162 [==============================] - 0s 615us/step - loss: 1.1413 - accuracy: 0.5272 - val_loss: 1.1185 - val_accuracy: 0.5370\n",
      "Epoch 141/750\n",
      "162/162 [==============================] - 0s 557us/step - loss: 1.1198 - accuracy: 0.5395 - val_loss: 1.1053 - val_accuracy: 0.5593\n",
      "Epoch 142/750\n",
      "162/162 [==============================] - 0s 555us/step - loss: 1.1208 - accuracy: 0.5333 - val_loss: 1.1056 - val_accuracy: 0.5630\n",
      "Epoch 143/750\n",
      "162/162 [==============================] - 0s 653us/step - loss: 1.1303 - accuracy: 0.5222 - val_loss: 1.1260 - val_accuracy: 0.5444\n",
      "Epoch 144/750\n",
      "162/162 [==============================] - 0s 624us/step - loss: 1.1342 - accuracy: 0.5284 - val_loss: 1.1346 - val_accuracy: 0.5296\n",
      "Epoch 145/750\n",
      "162/162 [==============================] - 0s 626us/step - loss: 1.1215 - accuracy: 0.5444 - val_loss: 1.1207 - val_accuracy: 0.5519\n",
      "Epoch 146/750\n",
      "162/162 [==============================] - 0s 628us/step - loss: 1.1320 - accuracy: 0.5395 - val_loss: 1.1212 - val_accuracy: 0.5370\n",
      "Epoch 147/750\n",
      "162/162 [==============================] - 0s 748us/step - loss: 1.1187 - accuracy: 0.5481 - val_loss: 1.1169 - val_accuracy: 0.5519\n",
      "Epoch 148/750\n",
      "162/162 [==============================] - 0s 732us/step - loss: 1.1285 - accuracy: 0.5481 - val_loss: 1.0965 - val_accuracy: 0.5556\n",
      "Epoch 149/750\n",
      "162/162 [==============================] - 0s 701us/step - loss: 1.1112 - accuracy: 0.5519 - val_loss: 1.0717 - val_accuracy: 0.5556\n",
      "Epoch 150/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.1332 - accuracy: 0.5259 - val_loss: 1.1520 - val_accuracy: 0.5000\n",
      "Epoch 151/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.1026 - accuracy: 0.5444 - val_loss: 1.0907 - val_accuracy: 0.5741\n",
      "Epoch 152/750\n",
      "162/162 [==============================] - 0s 613us/step - loss: 1.1499 - accuracy: 0.5333 - val_loss: 1.1053 - val_accuracy: 0.5519\n",
      "Epoch 153/750\n",
      "162/162 [==============================] - 0s 589us/step - loss: 1.1367 - accuracy: 0.5160 - val_loss: 1.0903 - val_accuracy: 0.5704\n",
      "Epoch 154/750\n",
      "162/162 [==============================] - 0s 620us/step - loss: 1.1066 - accuracy: 0.5420 - val_loss: 1.1251 - val_accuracy: 0.5704\n",
      "Epoch 155/750\n",
      "162/162 [==============================] - 0s 617us/step - loss: 1.1112 - accuracy: 0.5444 - val_loss: 1.0817 - val_accuracy: 0.5519\n",
      "Epoch 156/750\n",
      "162/162 [==============================] - 0s 625us/step - loss: 1.1189 - accuracy: 0.5605 - val_loss: 1.1373 - val_accuracy: 0.5704\n",
      "Epoch 157/750\n",
      "162/162 [==============================] - 0s 630us/step - loss: 1.1156 - accuracy: 0.5272 - val_loss: 1.1169 - val_accuracy: 0.5741\n",
      "Epoch 158/750\n",
      "162/162 [==============================] - 0s 671us/step - loss: 1.1267 - accuracy: 0.5309 - val_loss: 1.1349 - val_accuracy: 0.5407\n",
      "Epoch 159/750\n",
      "162/162 [==============================] - 0s 583us/step - loss: 1.1310 - accuracy: 0.5568 - val_loss: 1.1414 - val_accuracy: 0.5333\n",
      "Epoch 160/750\n",
      "162/162 [==============================] - 0s 668us/step - loss: 1.1171 - accuracy: 0.5321 - val_loss: 1.1051 - val_accuracy: 0.5630\n",
      "Epoch 161/750\n",
      "162/162 [==============================] - 0s 581us/step - loss: 1.1373 - accuracy: 0.5222 - val_loss: 1.0884 - val_accuracy: 0.5704\n",
      "Epoch 162/750\n",
      "162/162 [==============================] - 0s 612us/step - loss: 1.1082 - accuracy: 0.5642 - val_loss: 1.1120 - val_accuracy: 0.6037\n",
      "Epoch 163/750\n",
      "162/162 [==============================] - 0s 615us/step - loss: 1.1031 - accuracy: 0.5679 - val_loss: 1.0812 - val_accuracy: 0.6037\n",
      "Epoch 164/750\n",
      "162/162 [==============================] - 0s 664us/step - loss: 1.1124 - accuracy: 0.5296 - val_loss: 1.1842 - val_accuracy: 0.4852\n",
      "Epoch 165/750\n",
      "162/162 [==============================] - 0s 578us/step - loss: 1.1082 - accuracy: 0.5432 - val_loss: 1.0958 - val_accuracy: 0.5778\n",
      "Epoch 166/750\n",
      "162/162 [==============================] - 0s 617us/step - loss: 1.0992 - accuracy: 0.5519 - val_loss: 1.1285 - val_accuracy: 0.5778\n",
      "Epoch 167/750\n",
      "162/162 [==============================] - 0s 618us/step - loss: 1.1450 - accuracy: 0.5494 - val_loss: 1.1294 - val_accuracy: 0.5296\n",
      "Epoch 168/750\n",
      "162/162 [==============================] - 0s 609us/step - loss: 1.1384 - accuracy: 0.5111 - val_loss: 1.1183 - val_accuracy: 0.5556\n",
      "Epoch 169/750\n",
      "162/162 [==============================] - 0s 719us/step - loss: 1.1213 - accuracy: 0.5235 - val_loss: 1.1173 - val_accuracy: 0.5407\n",
      "Epoch 170/750\n",
      "162/162 [==============================] - 0s 577us/step - loss: 1.1060 - accuracy: 0.5444 - val_loss: 1.1049 - val_accuracy: 0.5630\n",
      "Epoch 171/750\n",
      "162/162 [==============================] - 0s 616us/step - loss: 1.1110 - accuracy: 0.5654 - val_loss: 1.1630 - val_accuracy: 0.5222\n",
      "Epoch 172/750\n",
      "162/162 [==============================] - 0s 618us/step - loss: 1.1181 - accuracy: 0.5222 - val_loss: 1.0704 - val_accuracy: 0.6037\n",
      "Epoch 173/750\n",
      "162/162 [==============================] - 0s 670us/step - loss: 1.1071 - accuracy: 0.5580 - val_loss: 1.0804 - val_accuracy: 0.5556\n",
      "Epoch 174/750\n",
      "162/162 [==============================] - 0s 699us/step - loss: 1.1005 - accuracy: 0.5481 - val_loss: 1.0723 - val_accuracy: 0.6111\n",
      "Epoch 175/750\n",
      "162/162 [==============================] - 0s 611us/step - loss: 1.1101 - accuracy: 0.5333 - val_loss: 1.0975 - val_accuracy: 0.6074\n",
      "Epoch 176/750\n",
      "162/162 [==============================] - 0s 572us/step - loss: 1.1151 - accuracy: 0.5605 - val_loss: 1.1475 - val_accuracy: 0.5000\n",
      "Epoch 177/750\n",
      "162/162 [==============================] - 0s 627us/step - loss: 1.1178 - accuracy: 0.5296 - val_loss: 1.0799 - val_accuracy: 0.5889\n",
      "Epoch 178/750\n",
      "162/162 [==============================] - 0s 665us/step - loss: 1.0884 - accuracy: 0.5617 - val_loss: 1.1206 - val_accuracy: 0.5519\n",
      "Epoch 179/750\n",
      "162/162 [==============================] - 0s 582us/step - loss: 1.1016 - accuracy: 0.5630 - val_loss: 1.0901 - val_accuracy: 0.5889\n",
      "Epoch 180/750\n",
      "162/162 [==============================] - 0s 628us/step - loss: 1.1050 - accuracy: 0.5444 - val_loss: 1.0811 - val_accuracy: 0.5852\n",
      "Epoch 181/750\n",
      "162/162 [==============================] - 0s 628us/step - loss: 1.1072 - accuracy: 0.5506 - val_loss: 1.0724 - val_accuracy: 0.5667\n",
      "Epoch 182/750\n",
      "162/162 [==============================] - 0s 654us/step - loss: 1.0858 - accuracy: 0.5580 - val_loss: 1.1062 - val_accuracy: 0.5370\n",
      "Epoch 183/750\n",
      "162/162 [==============================] - 0s 632us/step - loss: 1.0972 - accuracy: 0.5630 - val_loss: 1.0943 - val_accuracy: 0.5630\n",
      "Epoch 184/750\n",
      "162/162 [==============================] - 0s 781us/step - loss: 1.1150 - accuracy: 0.5506 - val_loss: 1.0959 - val_accuracy: 0.5593\n",
      "Epoch 185/750\n",
      "162/162 [==============================] - 0s 644us/step - loss: 1.0946 - accuracy: 0.5654 - val_loss: 1.1582 - val_accuracy: 0.5370\n",
      "Epoch 186/750\n",
      "162/162 [==============================] - 0s 676us/step - loss: 1.1006 - accuracy: 0.5407 - val_loss: 1.1493 - val_accuracy: 0.5481\n",
      "Epoch 187/750\n",
      "162/162 [==============================] - 0s 737us/step - loss: 1.1018 - accuracy: 0.5370 - val_loss: 1.2255 - val_accuracy: 0.5370\n",
      "Epoch 188/750\n",
      "162/162 [==============================] - 0s 651us/step - loss: 1.1178 - accuracy: 0.5494 - val_loss: 1.3162 - val_accuracy: 0.4481\n",
      "Epoch 189/750\n",
      "162/162 [==============================] - 0s 741us/step - loss: 1.1231 - accuracy: 0.5469 - val_loss: 1.1758 - val_accuracy: 0.4519\n",
      "Epoch 190/750\n",
      "162/162 [==============================] - 0s 647us/step - loss: 1.0819 - accuracy: 0.5642 - val_loss: 1.1881 - val_accuracy: 0.5074\n",
      "Epoch 191/750\n",
      "162/162 [==============================] - 0s 647us/step - loss: 1.0852 - accuracy: 0.5370 - val_loss: 1.2667 - val_accuracy: 0.4667\n",
      "Epoch 192/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0996 - accuracy: 0.5383 - val_loss: 1.1106 - val_accuracy: 0.5704\n",
      "Epoch 193/750\n",
      "162/162 [==============================] - 0s 588us/step - loss: 1.1066 - accuracy: 0.5358 - val_loss: 1.0739 - val_accuracy: 0.5556\n",
      "Epoch 194/750\n",
      "162/162 [==============================] - 0s 670us/step - loss: 1.0913 - accuracy: 0.5568 - val_loss: 1.0776 - val_accuracy: 0.5778\n",
      "Epoch 195/750\n",
      "162/162 [==============================] - 0s 584us/step - loss: 1.0725 - accuracy: 0.5679 - val_loss: 1.1321 - val_accuracy: 0.5593\n",
      "Epoch 196/750\n",
      "162/162 [==============================] - 0s 733us/step - loss: 1.0970 - accuracy: 0.5494 - val_loss: 1.1485 - val_accuracy: 0.4704\n",
      "Epoch 197/750\n",
      "162/162 [==============================] - 0s 768us/step - loss: 1.0976 - accuracy: 0.5519 - val_loss: 1.2250 - val_accuracy: 0.4407\n",
      "Epoch 198/750\n",
      "162/162 [==============================] - 0s 761us/step - loss: 1.1316 - accuracy: 0.5259 - val_loss: 1.0762 - val_accuracy: 0.6111\n",
      "Epoch 199/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.1001 - accuracy: 0.5605 - val_loss: 1.0810 - val_accuracy: 0.6037\n",
      "Epoch 200/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0922 - accuracy: 0.5593 - val_loss: 1.1246 - val_accuracy: 0.5259\n",
      "Epoch 201/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.1069 - accuracy: 0.5469 - val_loss: 1.1502 - val_accuracy: 0.5444\n",
      "Epoch 202/750\n",
      "162/162 [==============================] - 0s 623us/step - loss: 1.0674 - accuracy: 0.5716 - val_loss: 1.0664 - val_accuracy: 0.5667\n",
      "Epoch 203/750\n",
      "162/162 [==============================] - 0s 581us/step - loss: 1.1068 - accuracy: 0.5580 - val_loss: 1.0993 - val_accuracy: 0.5593\n",
      "Epoch 204/750\n",
      "162/162 [==============================] - 0s 614us/step - loss: 1.0709 - accuracy: 0.5519 - val_loss: 1.1001 - val_accuracy: 0.5519\n",
      "Epoch 205/750\n",
      "162/162 [==============================] - 0s 657us/step - loss: 1.0881 - accuracy: 0.5494 - val_loss: 1.0725 - val_accuracy: 0.6074\n",
      "Epoch 206/750\n",
      "162/162 [==============================] - 0s 673us/step - loss: 1.1056 - accuracy: 0.5309 - val_loss: 1.0850 - val_accuracy: 0.6222\n",
      "Epoch 207/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0904 - accuracy: 0.5667 - val_loss: 1.1555 - val_accuracy: 0.5556\n",
      "Epoch 208/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0907 - accuracy: 0.5346 - val_loss: 1.1249 - val_accuracy: 0.5556\n",
      "Epoch 209/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0904 - accuracy: 0.5617 - val_loss: 1.1382 - val_accuracy: 0.4778\n",
      "Epoch 210/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0910 - accuracy: 0.5457 - val_loss: 1.0727 - val_accuracy: 0.5926\n",
      "Epoch 211/750\n",
      "162/162 [==============================] - 0s 692us/step - loss: 1.0766 - accuracy: 0.5593 - val_loss: 1.0742 - val_accuracy: 0.6000\n",
      "Epoch 212/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0950 - accuracy: 0.5395 - val_loss: 1.0877 - val_accuracy: 0.5519\n",
      "Epoch 213/750\n",
      "162/162 [==============================] - 0s 609us/step - loss: 1.0847 - accuracy: 0.5543 - val_loss: 1.1160 - val_accuracy: 0.6037\n",
      "Epoch 214/750\n",
      "162/162 [==============================] - 0s 589us/step - loss: 1.0819 - accuracy: 0.5593 - val_loss: 1.0909 - val_accuracy: 0.5667\n",
      "Epoch 215/750\n",
      "162/162 [==============================] - 0s 578us/step - loss: 1.1052 - accuracy: 0.5481 - val_loss: 1.1911 - val_accuracy: 0.5333\n",
      "Epoch 216/750\n",
      "162/162 [==============================] - 0s 612us/step - loss: 1.0987 - accuracy: 0.5481 - val_loss: 1.0650 - val_accuracy: 0.5630\n",
      "Epoch 217/750\n",
      "162/162 [==============================] - 0s 619us/step - loss: 1.0930 - accuracy: 0.5617 - val_loss: 1.1274 - val_accuracy: 0.5481\n",
      "Epoch 218/750\n",
      "162/162 [==============================] - 0s 591us/step - loss: 1.0810 - accuracy: 0.5617 - val_loss: 1.0892 - val_accuracy: 0.5444\n",
      "Epoch 219/750\n",
      "162/162 [==============================] - 0s 704us/step - loss: 1.0801 - accuracy: 0.5568 - val_loss: 1.1100 - val_accuracy: 0.5519\n",
      "Epoch 220/750\n",
      "162/162 [==============================] - 0s 588us/step - loss: 1.1024 - accuracy: 0.5444 - val_loss: 1.1065 - val_accuracy: 0.5667\n",
      "Epoch 221/750\n",
      "162/162 [==============================] - 0s 681us/step - loss: 1.1358 - accuracy: 0.5247 - val_loss: 1.1754 - val_accuracy: 0.4852\n",
      "Epoch 222/750\n",
      "162/162 [==============================] - 0s 583us/step - loss: 1.1016 - accuracy: 0.5296 - val_loss: 1.0695 - val_accuracy: 0.5889\n",
      "Epoch 223/750\n",
      "162/162 [==============================] - 0s 622us/step - loss: 1.0778 - accuracy: 0.5432 - val_loss: 1.1167 - val_accuracy: 0.5593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/750\n",
      "162/162 [==============================] - 0s 612us/step - loss: 1.1121 - accuracy: 0.5432 - val_loss: 1.1637 - val_accuracy: 0.5259\n",
      "Epoch 225/750\n",
      "162/162 [==============================] - 0s 664us/step - loss: 1.0818 - accuracy: 0.5728 - val_loss: 1.1055 - val_accuracy: 0.5407\n",
      "Epoch 226/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0873 - accuracy: 0.5617 - val_loss: 1.0720 - val_accuracy: 0.5963\n",
      "Epoch 227/750\n",
      "162/162 [==============================] - 0s 640us/step - loss: 1.0859 - accuracy: 0.5395 - val_loss: 1.1225 - val_accuracy: 0.5963\n",
      "Epoch 228/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0881 - accuracy: 0.5383 - val_loss: 1.0741 - val_accuracy: 0.5963\n",
      "Epoch 229/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.1009 - accuracy: 0.5519 - val_loss: 1.0727 - val_accuracy: 0.5778\n",
      "Epoch 230/750\n",
      "162/162 [==============================] - 0s 662us/step - loss: 1.0811 - accuracy: 0.5605 - val_loss: 1.2450 - val_accuracy: 0.4148\n",
      "Epoch 231/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0808 - accuracy: 0.5531 - val_loss: 1.0865 - val_accuracy: 0.5556\n",
      "Epoch 232/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0572 - accuracy: 0.5741 - val_loss: 1.0940 - val_accuracy: 0.5556\n",
      "Epoch 233/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0914 - accuracy: 0.5704 - val_loss: 1.0607 - val_accuracy: 0.6074\n",
      "Epoch 234/750\n",
      "162/162 [==============================] - 0s 705us/step - loss: 1.0612 - accuracy: 0.5679 - val_loss: 1.1524 - val_accuracy: 0.5296\n",
      "Epoch 235/750\n",
      "162/162 [==============================] - 0s 616us/step - loss: 1.1113 - accuracy: 0.5284 - val_loss: 1.0457 - val_accuracy: 0.6111\n",
      "Epoch 236/750\n",
      "162/162 [==============================] - 0s 699us/step - loss: 1.0796 - accuracy: 0.5642 - val_loss: 1.0693 - val_accuracy: 0.5889\n",
      "Epoch 237/750\n",
      "162/162 [==============================] - 0s 741us/step - loss: 1.0568 - accuracy: 0.5556 - val_loss: 1.0543 - val_accuracy: 0.6000\n",
      "Epoch 238/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0667 - accuracy: 0.5531 - val_loss: 1.1745 - val_accuracy: 0.5741\n",
      "Epoch 239/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.1228 - accuracy: 0.5506 - val_loss: 1.1475 - val_accuracy: 0.5593\n",
      "Epoch 240/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0989 - accuracy: 0.5457 - val_loss: 1.0988 - val_accuracy: 0.5778\n",
      "Epoch 241/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0787 - accuracy: 0.5691 - val_loss: 1.1129 - val_accuracy: 0.5667\n",
      "Epoch 242/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0660 - accuracy: 0.5753 - val_loss: 1.0634 - val_accuracy: 0.5852\n",
      "Epoch 243/750\n",
      "162/162 [==============================] - 0s 740us/step - loss: 1.0862 - accuracy: 0.5605 - val_loss: 1.1233 - val_accuracy: 0.5444\n",
      "Epoch 244/750\n",
      "162/162 [==============================] - 0s 687us/step - loss: 1.0664 - accuracy: 0.5580 - val_loss: 1.0510 - val_accuracy: 0.5778\n",
      "Epoch 245/750\n",
      "162/162 [==============================] - 0s 645us/step - loss: 1.0625 - accuracy: 0.5617 - val_loss: 1.0483 - val_accuracy: 0.5963\n",
      "Epoch 246/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0973 - accuracy: 0.5494 - val_loss: 1.0740 - val_accuracy: 0.5704\n",
      "Epoch 247/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0534 - accuracy: 0.5840 - val_loss: 1.0698 - val_accuracy: 0.5926\n",
      "Epoch 248/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0673 - accuracy: 0.5531 - val_loss: 1.0627 - val_accuracy: 0.5889\n",
      "Epoch 249/750\n",
      "162/162 [==============================] - 0s 683us/step - loss: 1.0690 - accuracy: 0.5481 - val_loss: 1.1787 - val_accuracy: 0.5444\n",
      "Epoch 250/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.1179 - accuracy: 0.5210 - val_loss: 1.0664 - val_accuracy: 0.5815\n",
      "Epoch 251/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0585 - accuracy: 0.5778 - val_loss: 1.0792 - val_accuracy: 0.6111\n",
      "Epoch 252/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0853 - accuracy: 0.5617 - val_loss: 1.1285 - val_accuracy: 0.5444\n",
      "Epoch 253/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0794 - accuracy: 0.5420 - val_loss: 1.1383 - val_accuracy: 0.5667\n",
      "Epoch 254/750\n",
      "162/162 [==============================] - 0s 675us/step - loss: 1.0488 - accuracy: 0.5679 - val_loss: 1.0966 - val_accuracy: 0.5593\n",
      "Epoch 255/750\n",
      "162/162 [==============================] - 0s 672us/step - loss: 1.0627 - accuracy: 0.5593 - val_loss: 1.0549 - val_accuracy: 0.6259\n",
      "Epoch 256/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0831 - accuracy: 0.5580 - val_loss: 1.0724 - val_accuracy: 0.6037\n",
      "Epoch 257/750\n",
      "162/162 [==============================] - ETA: 0s - loss: 1.0577 - accuracy: 0.55 - 0s 519us/step - loss: 1.0685 - accuracy: 0.5519 - val_loss: 1.1531 - val_accuracy: 0.5000\n",
      "Epoch 258/750\n",
      "162/162 [==============================] - 0s 577us/step - loss: 1.0747 - accuracy: 0.5531 - val_loss: 1.0696 - val_accuracy: 0.5667\n",
      "Epoch 259/750\n",
      "162/162 [==============================] - 0s 692us/step - loss: 1.0784 - accuracy: 0.5457 - val_loss: 1.1105 - val_accuracy: 0.6037\n",
      "Epoch 260/750\n",
      "162/162 [==============================] - 0s 594us/step - loss: 1.0938 - accuracy: 0.5580 - val_loss: 1.0724 - val_accuracy: 0.5481\n",
      "Epoch 261/750\n",
      "162/162 [==============================] - 0s 752us/step - loss: 1.0813 - accuracy: 0.5716 - val_loss: 1.0924 - val_accuracy: 0.5667\n",
      "Epoch 262/750\n",
      "162/162 [==============================] - 0s 660us/step - loss: 1.0583 - accuracy: 0.5741 - val_loss: 1.0390 - val_accuracy: 0.5852\n",
      "Epoch 263/750\n",
      "162/162 [==============================] - 0s 744us/step - loss: 1.0651 - accuracy: 0.5580 - val_loss: 1.0653 - val_accuracy: 0.6037\n",
      "Epoch 264/750\n",
      "162/162 [==============================] - 0s 670us/step - loss: 1.0557 - accuracy: 0.5815 - val_loss: 1.1561 - val_accuracy: 0.4889\n",
      "Epoch 265/750\n",
      "162/162 [==============================] - 0s 655us/step - loss: 1.0888 - accuracy: 0.5605 - val_loss: 1.0497 - val_accuracy: 0.6074\n",
      "Epoch 266/750\n",
      "162/162 [==============================] - 0s 663us/step - loss: 1.0754 - accuracy: 0.5667 - val_loss: 1.1668 - val_accuracy: 0.5259\n",
      "Epoch 267/750\n",
      "162/162 [==============================] - 0s 616us/step - loss: 1.1070 - accuracy: 0.5333 - val_loss: 1.0562 - val_accuracy: 0.6074\n",
      "Epoch 268/750\n",
      "162/162 [==============================] - 0s 598us/step - loss: 1.0567 - accuracy: 0.5864 - val_loss: 1.0890 - val_accuracy: 0.5926\n",
      "Epoch 269/750\n",
      "162/162 [==============================] - 0s 614us/step - loss: 1.0612 - accuracy: 0.5765 - val_loss: 1.0940 - val_accuracy: 0.5556\n",
      "Epoch 270/750\n",
      "162/162 [==============================] - 0s 627us/step - loss: 1.0920 - accuracy: 0.5506 - val_loss: 1.1007 - val_accuracy: 0.5519\n",
      "Epoch 271/750\n",
      "162/162 [==============================] - 0s 699us/step - loss: 1.0441 - accuracy: 0.5716 - val_loss: 1.1829 - val_accuracy: 0.5407\n",
      "Epoch 272/750\n",
      "162/162 [==============================] - 0s 689us/step - loss: 1.0798 - accuracy: 0.5642 - val_loss: 1.0613 - val_accuracy: 0.6037\n",
      "Epoch 273/750\n",
      "162/162 [==============================] - 0s 601us/step - loss: 1.0604 - accuracy: 0.5469 - val_loss: 1.0811 - val_accuracy: 0.6148\n",
      "Epoch 274/750\n",
      "162/162 [==============================] - 0s 666us/step - loss: 1.0682 - accuracy: 0.5815 - val_loss: 1.2253 - val_accuracy: 0.5185\n",
      "Epoch 275/750\n",
      "162/162 [==============================] - 0s 651us/step - loss: 1.0681 - accuracy: 0.5642 - val_loss: 1.0669 - val_accuracy: 0.6111\n",
      "Epoch 276/750\n",
      "162/162 [==============================] - 0s 630us/step - loss: 1.0712 - accuracy: 0.5654 - val_loss: 1.1583 - val_accuracy: 0.5111\n",
      "Epoch 277/750\n",
      "162/162 [==============================] - 0s 558us/step - loss: 1.0602 - accuracy: 0.5605 - val_loss: 1.1632 - val_accuracy: 0.4963\n",
      "Epoch 278/750\n",
      "162/162 [==============================] - 0s 646us/step - loss: 1.0781 - accuracy: 0.5568 - val_loss: 1.2743 - val_accuracy: 0.4148\n",
      "Epoch 279/750\n",
      "162/162 [==============================] - 0s 616us/step - loss: 1.0784 - accuracy: 0.5568 - val_loss: 1.0594 - val_accuracy: 0.6037\n",
      "Epoch 280/750\n",
      "162/162 [==============================] - 0s 600us/step - loss: 1.0620 - accuracy: 0.5617 - val_loss: 1.0574 - val_accuracy: 0.5926\n",
      "Epoch 281/750\n",
      "162/162 [==============================] - 0s 569us/step - loss: 1.0704 - accuracy: 0.5716 - val_loss: 1.0570 - val_accuracy: 0.5815\n",
      "Epoch 282/750\n",
      "162/162 [==============================] - 0s 770us/step - loss: 1.0795 - accuracy: 0.5642 - val_loss: 1.0486 - val_accuracy: 0.6000\n",
      "Epoch 283/750\n",
      "162/162 [==============================] - 0s 694us/step - loss: 1.0528 - accuracy: 0.5753 - val_loss: 1.0820 - val_accuracy: 0.5667\n",
      "Epoch 284/750\n",
      "162/162 [==============================] - 0s 610us/step - loss: 1.0656 - accuracy: 0.5728 - val_loss: 1.1016 - val_accuracy: 0.5185\n",
      "Epoch 285/750\n",
      "162/162 [==============================] - 0s 621us/step - loss: 1.0727 - accuracy: 0.5852 - val_loss: 1.0807 - val_accuracy: 0.5963\n",
      "Epoch 286/750\n",
      "162/162 [==============================] - 0s 608us/step - loss: 1.0586 - accuracy: 0.5519 - val_loss: 1.0532 - val_accuracy: 0.6037\n",
      "Epoch 287/750\n",
      "162/162 [==============================] - 0s 603us/step - loss: 1.0700 - accuracy: 0.5704 - val_loss: 1.5107 - val_accuracy: 0.4778\n",
      "Epoch 288/750\n",
      "162/162 [==============================] - 0s 622us/step - loss: 1.1549 - accuracy: 0.5012 - val_loss: 1.1156 - val_accuracy: 0.5296\n",
      "Epoch 289/750\n",
      "162/162 [==============================] - 0s 609us/step - loss: 1.0792 - accuracy: 0.5531 - val_loss: 1.2065 - val_accuracy: 0.5519\n",
      "Epoch 290/750\n",
      "162/162 [==============================] - 0s 614us/step - loss: 1.0959 - accuracy: 0.5321 - val_loss: 1.1631 - val_accuracy: 0.5593\n",
      "Epoch 291/750\n",
      "162/162 [==============================] - 0s 662us/step - loss: 1.0883 - accuracy: 0.5519 - val_loss: 1.1566 - val_accuracy: 0.4815\n",
      "Epoch 292/750\n",
      "162/162 [==============================] - 0s 634us/step - loss: 1.0622 - accuracy: 0.5704 - val_loss: 1.0399 - val_accuracy: 0.6185\n",
      "Epoch 293/750\n",
      "162/162 [==============================] - 0s 566us/step - loss: 1.0644 - accuracy: 0.5704 - val_loss: 1.0465 - val_accuracy: 0.6111\n",
      "Epoch 294/750\n",
      "162/162 [==============================] - 0s 621us/step - loss: 1.0580 - accuracy: 0.5617 - val_loss: 1.1046 - val_accuracy: 0.5704\n",
      "Epoch 295/750\n",
      "162/162 [==============================] - 0s 734us/step - loss: 1.0486 - accuracy: 0.5815 - val_loss: 1.0464 - val_accuracy: 0.5519\n",
      "Epoch 296/750\n",
      "162/162 [==============================] - 0s 681us/step - loss: 1.0659 - accuracy: 0.5494 - val_loss: 1.0946 - val_accuracy: 0.5593\n",
      "Epoch 297/750\n",
      "162/162 [==============================] - 0s 657us/step - loss: 1.0647 - accuracy: 0.5605 - val_loss: 1.0355 - val_accuracy: 0.6259\n",
      "Epoch 298/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.0615 - accuracy: 0.5716 - val_loss: 1.0342 - val_accuracy: 0.5963\n",
      "Epoch 299/750\n",
      "162/162 [==============================] - 0s 595us/step - loss: 1.0597 - accuracy: 0.5630 - val_loss: 1.0607 - val_accuracy: 0.5963\n",
      "Epoch 300/750\n",
      "162/162 [==============================] - 0s 590us/step - loss: 1.0625 - accuracy: 0.5630 - val_loss: 1.1007 - val_accuracy: 0.5667\n",
      "Epoch 301/750\n",
      "162/162 [==============================] - 0s 614us/step - loss: 1.0616 - accuracy: 0.5580 - val_loss: 1.0165 - val_accuracy: 0.6074\n",
      "Epoch 302/750\n",
      "162/162 [==============================] - 0s 607us/step - loss: 1.0508 - accuracy: 0.5704 - val_loss: 1.0761 - val_accuracy: 0.5926\n",
      "Epoch 303/750\n",
      "162/162 [==============================] - 0s 630us/step - loss: 1.0891 - accuracy: 0.5370 - val_loss: 1.1319 - val_accuracy: 0.4407\n",
      "Epoch 304/750\n",
      "162/162 [==============================] - 0s 763us/step - loss: 1.0418 - accuracy: 0.5778 - val_loss: 1.2112 - val_accuracy: 0.4852\n",
      "Epoch 305/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0622 - accuracy: 0.5617 - val_loss: 1.0532 - val_accuracy: 0.5704\n",
      "Epoch 306/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.1040 - accuracy: 0.5407 - val_loss: 1.0682 - val_accuracy: 0.6074\n",
      "Epoch 307/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0566 - accuracy: 0.5704 - val_loss: 1.2376 - val_accuracy: 0.4519\n",
      "Epoch 308/750\n",
      "162/162 [==============================] - 0s 660us/step - loss: 1.0670 - accuracy: 0.5605 - val_loss: 1.0580 - val_accuracy: 0.6074\n",
      "Epoch 309/750\n",
      "162/162 [==============================] - 0s 533us/step - loss: 1.0642 - accuracy: 0.5519 - val_loss: 1.0398 - val_accuracy: 0.5963\n",
      "Epoch 310/750\n",
      "162/162 [==============================] - 0s 573us/step - loss: 1.0563 - accuracy: 0.5802 - val_loss: 1.0323 - val_accuracy: 0.6074\n",
      "Epoch 311/750\n",
      "162/162 [==============================] - 0s 603us/step - loss: 1.0671 - accuracy: 0.5630 - val_loss: 1.0790 - val_accuracy: 0.5852\n",
      "Epoch 312/750\n",
      "162/162 [==============================] - 0s 686us/step - loss: 1.0451 - accuracy: 0.5889 - val_loss: 1.0241 - val_accuracy: 0.6037\n",
      "Epoch 313/750\n",
      "162/162 [==============================] - 0s 629us/step - loss: 1.0685 - accuracy: 0.5543 - val_loss: 1.0849 - val_accuracy: 0.5741\n",
      "Epoch 314/750\n",
      "162/162 [==============================] - 0s 579us/step - loss: 1.0389 - accuracy: 0.5667 - val_loss: 1.0784 - val_accuracy: 0.6037\n",
      "Epoch 315/750\n",
      "162/162 [==============================] - 0s 637us/step - loss: 1.0588 - accuracy: 0.5728 - val_loss: 1.0173 - val_accuracy: 0.5926\n",
      "Epoch 316/750\n",
      "162/162 [==============================] - 0s 618us/step - loss: 1.0492 - accuracy: 0.5605 - val_loss: 1.1101 - val_accuracy: 0.5185\n",
      "Epoch 317/750\n",
      "162/162 [==============================] - 0s 616us/step - loss: 1.1049 - accuracy: 0.5370 - val_loss: 1.2691 - val_accuracy: 0.4148\n",
      "Epoch 318/750\n",
      "162/162 [==============================] - 0s 636us/step - loss: 1.0696 - accuracy: 0.5605 - val_loss: 1.0757 - val_accuracy: 0.5630\n",
      "Epoch 319/750\n",
      "162/162 [==============================] - 0s 662us/step - loss: 1.0642 - accuracy: 0.5642 - val_loss: 1.0445 - val_accuracy: 0.5519\n",
      "Epoch 320/750\n",
      "162/162 [==============================] - 0s 645us/step - loss: 1.1140 - accuracy: 0.5358 - val_loss: 1.1144 - val_accuracy: 0.5185\n",
      "Epoch 321/750\n",
      "162/162 [==============================] - 0s 639us/step - loss: 1.0598 - accuracy: 0.5630 - val_loss: 1.0601 - val_accuracy: 0.5963\n",
      "Epoch 322/750\n",
      "162/162 [==============================] - 0s 589us/step - loss: 1.0762 - accuracy: 0.5580 - val_loss: 1.0345 - val_accuracy: 0.5852\n",
      "Epoch 323/750\n",
      "162/162 [==============================] - 0s 614us/step - loss: 1.0809 - accuracy: 0.5543 - val_loss: 1.0609 - val_accuracy: 0.5704\n",
      "Epoch 324/750\n",
      "162/162 [==============================] - 0s 623us/step - loss: 1.0770 - accuracy: 0.5593 - val_loss: 1.0453 - val_accuracy: 0.6037\n",
      "Epoch 325/750\n",
      "162/162 [==============================] - 0s 624us/step - loss: 1.0562 - accuracy: 0.5716 - val_loss: 1.0450 - val_accuracy: 0.5778\n",
      "Epoch 326/750\n",
      "162/162 [==============================] - 0s 637us/step - loss: 1.0591 - accuracy: 0.5728 - val_loss: 1.0273 - val_accuracy: 0.6111\n",
      "Epoch 327/750\n",
      "162/162 [==============================] - 0s 752us/step - loss: 1.0501 - accuracy: 0.5617 - val_loss: 1.0231 - val_accuracy: 0.5963\n",
      "Epoch 328/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0949 - accuracy: 0.5457 - val_loss: 1.0357 - val_accuracy: 0.5852\n",
      "Epoch 329/750\n",
      "162/162 [==============================] - 0s 670us/step - loss: 1.0614 - accuracy: 0.5691 - val_loss: 1.0812 - val_accuracy: 0.5741\n",
      "Epoch 330/750\n",
      "162/162 [==============================] - 0s 589us/step - loss: 1.0418 - accuracy: 0.5642 - val_loss: 1.0258 - val_accuracy: 0.6259\n",
      "Epoch 331/750\n",
      "162/162 [==============================] - 0s 622us/step - loss: 1.0648 - accuracy: 0.5790 - val_loss: 1.0580 - val_accuracy: 0.6185\n",
      "Epoch 332/750\n",
      "162/162 [==============================] - 0s 665us/step - loss: 1.0445 - accuracy: 0.5630 - val_loss: 1.0457 - val_accuracy: 0.6222\n",
      "Epoch 333/750\n",
      "162/162 [==============================] - 0s 597us/step - loss: 1.0489 - accuracy: 0.5728 - val_loss: 1.1032 - val_accuracy: 0.5481\n",
      "Epoch 334/750\n",
      "162/162 [==============================] - 0s 615us/step - loss: 1.0544 - accuracy: 0.5593 - val_loss: 1.0588 - val_accuracy: 0.6074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335/750\n",
      "162/162 [==============================] - 0s 618us/step - loss: 1.0376 - accuracy: 0.5667 - val_loss: 1.0348 - val_accuracy: 0.6148\n",
      "Epoch 336/750\n",
      "162/162 [==============================] - 0s 663us/step - loss: 1.0466 - accuracy: 0.5617 - val_loss: 1.0377 - val_accuracy: 0.5704\n",
      "Epoch 337/750\n",
      "162/162 [==============================] - 0s 578us/step - loss: 1.0811 - accuracy: 0.5753 - val_loss: 1.1291 - val_accuracy: 0.5667\n",
      "Epoch 338/750\n",
      "162/162 [==============================] - 0s 613us/step - loss: 1.0525 - accuracy: 0.5716 - val_loss: 1.0313 - val_accuracy: 0.6111\n",
      "Epoch 339/750\n",
      "162/162 [==============================] - 0s 633us/step - loss: 1.0825 - accuracy: 0.5630 - val_loss: 1.0720 - val_accuracy: 0.5444\n",
      "Epoch 340/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0736 - accuracy: 0.5568 - val_loss: 1.1000 - val_accuracy: 0.6111\n",
      "Epoch 341/750\n",
      "162/162 [==============================] - ETA: 0s - loss: 1.1172 - accuracy: 0.55 - 0s 585us/step - loss: 1.0662 - accuracy: 0.5716 - val_loss: 1.0338 - val_accuracy: 0.5667\n",
      "Epoch 342/750\n",
      "162/162 [==============================] - 0s 578us/step - loss: 1.0335 - accuracy: 0.5691 - val_loss: 1.0429 - val_accuracy: 0.6074\n",
      "Epoch 343/750\n",
      "162/162 [==============================] - 0s 615us/step - loss: 1.0544 - accuracy: 0.5753 - val_loss: 1.2049 - val_accuracy: 0.5222\n",
      "Epoch 344/750\n",
      "162/162 [==============================] - ETA: 0s - loss: 1.0692 - accuracy: 0.59 - 0s 580us/step - loss: 1.0627 - accuracy: 0.5765 - val_loss: 1.1765 - val_accuracy: 0.5593\n",
      "Epoch 345/750\n",
      "162/162 [==============================] - 0s 576us/step - loss: 1.0581 - accuracy: 0.5617 - val_loss: 1.0400 - val_accuracy: 0.6148\n",
      "Epoch 346/750\n",
      "162/162 [==============================] - 0s 678us/step - loss: 1.0523 - accuracy: 0.5901 - val_loss: 1.0671 - val_accuracy: 0.5481\n",
      "Epoch 347/750\n",
      "162/162 [==============================] - 0s 525us/step - loss: 1.0774 - accuracy: 0.5519 - val_loss: 1.0411 - val_accuracy: 0.6000\n",
      "Epoch 348/750\n",
      "162/162 [==============================] - 0s 571us/step - loss: 1.0659 - accuracy: 0.5679 - val_loss: 1.0270 - val_accuracy: 0.6037\n",
      "Epoch 349/750\n",
      "162/162 [==============================] - 0s 609us/step - loss: 1.0656 - accuracy: 0.5420 - val_loss: 1.0292 - val_accuracy: 0.6000\n",
      "Epoch 350/750\n",
      "162/162 [==============================] - 0s 620us/step - loss: 1.0333 - accuracy: 0.5889 - val_loss: 1.0548 - val_accuracy: 0.5963\n",
      "Epoch 351/750\n",
      "162/162 [==============================] - 0s 622us/step - loss: 1.0758 - accuracy: 0.5407 - val_loss: 1.0312 - val_accuracy: 0.5704\n",
      "Epoch 352/750\n",
      "162/162 [==============================] - 0s 627us/step - loss: 1.0729 - accuracy: 0.5580 - val_loss: 1.1263 - val_accuracy: 0.5111\n",
      "Epoch 353/750\n",
      "162/162 [==============================] - 0s 628us/step - loss: 1.0500 - accuracy: 0.5864 - val_loss: 1.0403 - val_accuracy: 0.5889\n",
      "Epoch 354/750\n",
      "162/162 [==============================] - 0s 662us/step - loss: 1.1004 - accuracy: 0.5321 - val_loss: 1.2045 - val_accuracy: 0.4889\n",
      "Epoch 355/750\n",
      "162/162 [==============================] - 0s 583us/step - loss: 1.0617 - accuracy: 0.5568 - val_loss: 1.2171 - val_accuracy: 0.5556\n",
      "Epoch 356/750\n",
      "162/162 [==============================] - 0s 628us/step - loss: 1.0802 - accuracy: 0.5679 - val_loss: 1.0355 - val_accuracy: 0.5963\n",
      "Epoch 357/750\n",
      "162/162 [==============================] - 0s 568us/step - loss: 1.0524 - accuracy: 0.5556 - val_loss: 1.0309 - val_accuracy: 0.5778\n",
      "Epoch 358/750\n",
      "162/162 [==============================] - 0s 574us/step - loss: 1.0322 - accuracy: 0.5679 - val_loss: 1.1105 - val_accuracy: 0.5407\n",
      "Epoch 359/750\n",
      "162/162 [==============================] - 0s 650us/step - loss: 1.0369 - accuracy: 0.5716 - val_loss: 1.0372 - val_accuracy: 0.5667\n",
      "Epoch 360/750\n",
      "162/162 [==============================] - 0s 707us/step - loss: 1.0293 - accuracy: 0.5753 - val_loss: 1.0603 - val_accuracy: 0.5444\n",
      "Epoch 361/750\n",
      "162/162 [==============================] - 0s 739us/step - loss: 1.0430 - accuracy: 0.5741 - val_loss: 1.0408 - val_accuracy: 0.5667\n",
      "Epoch 362/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.0403 - accuracy: 0.5642 - val_loss: 1.0574 - val_accuracy: 0.6259\n",
      "Epoch 363/750\n",
      "162/162 [==============================] - 0s 772us/step - loss: 1.0302 - accuracy: 0.5864 - val_loss: 1.0709 - val_accuracy: 0.5778\n",
      "Epoch 364/750\n",
      "162/162 [==============================] - 0s 738us/step - loss: 1.0455 - accuracy: 0.5741 - val_loss: 1.0367 - val_accuracy: 0.5926\n",
      "Epoch 365/750\n",
      "162/162 [==============================] - 0s 673us/step - loss: 1.0620 - accuracy: 0.5679 - val_loss: 1.0342 - val_accuracy: 0.6185\n",
      "Epoch 366/750\n",
      "162/162 [==============================] - 0s 770us/step - loss: 1.0897 - accuracy: 0.5691 - val_loss: 1.2427 - val_accuracy: 0.4259\n",
      "Epoch 367/750\n",
      "162/162 [==============================] - 0s 646us/step - loss: 1.0625 - accuracy: 0.5617 - val_loss: 1.0835 - val_accuracy: 0.5407\n",
      "Epoch 368/750\n",
      "162/162 [==============================] - 0s 657us/step - loss: 1.0609 - accuracy: 0.5605 - val_loss: 1.1897 - val_accuracy: 0.4333\n",
      "Epoch 369/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0801 - accuracy: 0.5358 - val_loss: 1.0501 - val_accuracy: 0.5778\n",
      "Epoch 370/750\n",
      "162/162 [==============================] - 0s 710us/step - loss: 1.0427 - accuracy: 0.5642 - val_loss: 1.0209 - val_accuracy: 0.6259\n",
      "Epoch 371/750\n",
      "162/162 [==============================] - 0s 638us/step - loss: 1.0274 - accuracy: 0.5790 - val_loss: 1.0620 - val_accuracy: 0.6148\n",
      "Epoch 372/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.0619 - accuracy: 0.5704 - val_loss: 1.0387 - val_accuracy: 0.6222\n",
      "Epoch 373/750\n",
      "162/162 [==============================] - 0s 743us/step - loss: 1.0747 - accuracy: 0.5580 - val_loss: 1.1256 - val_accuracy: 0.5630\n",
      "Epoch 374/750\n",
      "162/162 [==============================] - 0s 650us/step - loss: 1.0472 - accuracy: 0.5704 - val_loss: 1.0763 - val_accuracy: 0.6000\n",
      "Epoch 375/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0525 - accuracy: 0.5827 - val_loss: 1.0528 - val_accuracy: 0.5963\n",
      "Epoch 376/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0343 - accuracy: 0.5667 - val_loss: 1.0612 - val_accuracy: 0.5963\n",
      "Epoch 377/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0429 - accuracy: 0.5667 - val_loss: 1.0515 - val_accuracy: 0.5704\n",
      "Epoch 378/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0548 - accuracy: 0.5654 - val_loss: 1.0785 - val_accuracy: 0.5704\n",
      "Epoch 379/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0439 - accuracy: 0.5704 - val_loss: 1.1474 - val_accuracy: 0.5407\n",
      "Epoch 380/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0758 - accuracy: 0.5556 - val_loss: 1.0316 - val_accuracy: 0.5778\n",
      "Epoch 381/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0456 - accuracy: 0.5716 - val_loss: 1.1831 - val_accuracy: 0.5222\n",
      "Epoch 382/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0486 - accuracy: 0.5753 - val_loss: 1.0778 - val_accuracy: 0.5667\n",
      "Epoch 383/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0836 - accuracy: 0.5580 - val_loss: 1.0257 - val_accuracy: 0.5963\n",
      "Epoch 384/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0329 - accuracy: 0.5704 - val_loss: 1.1988 - val_accuracy: 0.5000\n",
      "Epoch 385/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0528 - accuracy: 0.5667 - val_loss: 1.0568 - val_accuracy: 0.5741\n",
      "Epoch 386/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0789 - accuracy: 0.5395 - val_loss: 1.1199 - val_accuracy: 0.5852\n",
      "Epoch 387/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0449 - accuracy: 0.5617 - val_loss: 1.0319 - val_accuracy: 0.6000\n",
      "Epoch 388/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.0373 - accuracy: 0.5741 - val_loss: 1.1039 - val_accuracy: 0.5556\n",
      "Epoch 389/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0574 - accuracy: 0.5654 - val_loss: 1.0707 - val_accuracy: 0.5889\n",
      "Epoch 390/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0348 - accuracy: 0.5827 - val_loss: 1.0285 - val_accuracy: 0.6185\n",
      "Epoch 391/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0257 - accuracy: 0.5679 - val_loss: 1.1326 - val_accuracy: 0.5852\n",
      "Epoch 392/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0560 - accuracy: 0.5654 - val_loss: 1.0366 - val_accuracy: 0.6222\n",
      "Epoch 393/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0252 - accuracy: 0.5864 - val_loss: 1.0787 - val_accuracy: 0.6111\n",
      "Epoch 394/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0143 - accuracy: 0.5864 - val_loss: 1.0422 - val_accuracy: 0.5556\n",
      "Epoch 395/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0445 - accuracy: 0.5667 - val_loss: 1.0787 - val_accuracy: 0.5481\n",
      "Epoch 396/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0725 - accuracy: 0.5481 - val_loss: 1.0653 - val_accuracy: 0.5741\n",
      "Epoch 397/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0620 - accuracy: 0.5753 - val_loss: 1.0285 - val_accuracy: 0.5778\n",
      "Epoch 398/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0303 - accuracy: 0.5815 - val_loss: 0.9965 - val_accuracy: 0.6037\n",
      "Epoch 399/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0458 - accuracy: 0.5580 - val_loss: 1.0979 - val_accuracy: 0.5704\n",
      "Epoch 400/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0821 - accuracy: 0.5457 - val_loss: 1.0626 - val_accuracy: 0.6148\n",
      "Epoch 401/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0430 - accuracy: 0.5617 - val_loss: 1.0338 - val_accuracy: 0.5630\n",
      "Epoch 402/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0443 - accuracy: 0.5765 - val_loss: 1.1941 - val_accuracy: 0.5741\n",
      "Epoch 403/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0531 - accuracy: 0.5519 - val_loss: 1.0273 - val_accuracy: 0.5778\n",
      "Epoch 404/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0350 - accuracy: 0.5630 - val_loss: 1.0486 - val_accuracy: 0.6148\n",
      "Epoch 405/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0483 - accuracy: 0.5815 - val_loss: 1.0676 - val_accuracy: 0.5815\n",
      "Epoch 406/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0543 - accuracy: 0.5864 - val_loss: 1.0956 - val_accuracy: 0.5630\n",
      "Epoch 407/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0532 - accuracy: 0.5765 - val_loss: 1.1330 - val_accuracy: 0.5444\n",
      "Epoch 408/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0605 - accuracy: 0.5716 - val_loss: 1.2336 - val_accuracy: 0.4926\n",
      "Epoch 409/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0340 - accuracy: 0.5864 - val_loss: 1.0403 - val_accuracy: 0.6296\n",
      "Epoch 410/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0696 - accuracy: 0.5630 - val_loss: 1.0561 - val_accuracy: 0.6074\n",
      "Epoch 411/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0522 - accuracy: 0.5679 - val_loss: 1.1641 - val_accuracy: 0.5444\n",
      "Epoch 412/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0672 - accuracy: 0.5654 - val_loss: 1.0001 - val_accuracy: 0.6148\n",
      "Epoch 413/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0221 - accuracy: 0.5741 - val_loss: 1.0552 - val_accuracy: 0.5852\n",
      "Epoch 414/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.0468 - accuracy: 0.5765 - val_loss: 1.0246 - val_accuracy: 0.6111\n",
      "Epoch 415/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0377 - accuracy: 0.5630 - val_loss: 1.0153 - val_accuracy: 0.6185\n",
      "Epoch 416/750\n",
      "162/162 [==============================] - 0s 614us/step - loss: 1.0385 - accuracy: 0.5753 - val_loss: 1.0493 - val_accuracy: 0.5926\n",
      "Epoch 417/750\n",
      "162/162 [==============================] - 0s 562us/step - loss: 1.0481 - accuracy: 0.5617 - val_loss: 1.0324 - val_accuracy: 0.6074\n",
      "Epoch 418/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0342 - accuracy: 0.5728 - val_loss: 1.0267 - val_accuracy: 0.6222\n",
      "Epoch 419/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0321 - accuracy: 0.5827 - val_loss: 1.1330 - val_accuracy: 0.5444\n",
      "Epoch 420/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0472 - accuracy: 0.5642 - val_loss: 1.0223 - val_accuracy: 0.5667\n",
      "Epoch 421/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0506 - accuracy: 0.5654 - val_loss: 1.0549 - val_accuracy: 0.6000\n",
      "Epoch 422/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0552 - accuracy: 0.5617 - val_loss: 1.0800 - val_accuracy: 0.5815\n",
      "Epoch 423/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0260 - accuracy: 0.5889 - val_loss: 1.1030 - val_accuracy: 0.5519\n",
      "Epoch 424/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0763 - accuracy: 0.5778 - val_loss: 1.0336 - val_accuracy: 0.5963\n",
      "Epoch 425/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.0565 - accuracy: 0.5704 - val_loss: 1.0215 - val_accuracy: 0.5741\n",
      "Epoch 426/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0194 - accuracy: 0.5790 - val_loss: 1.1092 - val_accuracy: 0.5481\n",
      "Epoch 427/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0524 - accuracy: 0.5630 - val_loss: 1.0684 - val_accuracy: 0.5630\n",
      "Epoch 428/750\n",
      "162/162 [==============================] - 0s 641us/step - loss: 1.0850 - accuracy: 0.5481 - val_loss: 1.1279 - val_accuracy: 0.5630\n",
      "Epoch 429/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0414 - accuracy: 0.5790 - val_loss: 1.0418 - val_accuracy: 0.6296\n",
      "Epoch 430/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0506 - accuracy: 0.5704 - val_loss: 1.1994 - val_accuracy: 0.4889\n",
      "Epoch 431/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0370 - accuracy: 0.5815 - val_loss: 1.2689 - val_accuracy: 0.4926\n",
      "Epoch 432/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0617 - accuracy: 0.5457 - val_loss: 1.0218 - val_accuracy: 0.5630\n",
      "Epoch 433/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0576 - accuracy: 0.5704 - val_loss: 1.0582 - val_accuracy: 0.5963\n",
      "Epoch 434/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0296 - accuracy: 0.5691 - val_loss: 1.1000 - val_accuracy: 0.5481\n",
      "Epoch 435/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0773 - accuracy: 0.5506 - val_loss: 1.0574 - val_accuracy: 0.6185\n",
      "Epoch 436/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0584 - accuracy: 0.5457 - val_loss: 1.1373 - val_accuracy: 0.5556\n",
      "Epoch 437/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0541 - accuracy: 0.5605 - val_loss: 1.0478 - val_accuracy: 0.5963\n",
      "Epoch 438/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0200 - accuracy: 0.5864 - val_loss: 1.0557 - val_accuracy: 0.5481\n",
      "Epoch 439/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0526 - accuracy: 0.5580 - val_loss: 1.0456 - val_accuracy: 0.5815\n",
      "Epoch 440/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0429 - accuracy: 0.5716 - val_loss: 1.1225 - val_accuracy: 0.5704\n",
      "Epoch 441/750\n",
      "162/162 [==============================] - 0s 593us/step - loss: 1.0583 - accuracy: 0.5691 - val_loss: 1.0576 - val_accuracy: 0.5778\n",
      "Epoch 442/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0336 - accuracy: 0.5654 - val_loss: 1.0427 - val_accuracy: 0.5741\n",
      "Epoch 443/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0337 - accuracy: 0.5617 - val_loss: 1.0395 - val_accuracy: 0.6074\n",
      "Epoch 444/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0146 - accuracy: 0.5778 - val_loss: 1.0338 - val_accuracy: 0.5852\n",
      "Epoch 445/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0541 - accuracy: 0.5568 - val_loss: 1.0470 - val_accuracy: 0.5778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 446/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0392 - accuracy: 0.5716 - val_loss: 1.2398 - val_accuracy: 0.5037\n",
      "Epoch 447/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0636 - accuracy: 0.5383 - val_loss: 1.0833 - val_accuracy: 0.5852\n",
      "Epoch 448/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0268 - accuracy: 0.5877 - val_loss: 1.0519 - val_accuracy: 0.5778\n",
      "Epoch 449/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0470 - accuracy: 0.5642 - val_loss: 1.0863 - val_accuracy: 0.5852\n",
      "Epoch 450/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0191 - accuracy: 0.5765 - val_loss: 1.0351 - val_accuracy: 0.6296\n",
      "Epoch 451/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0588 - accuracy: 0.5642 - val_loss: 1.0771 - val_accuracy: 0.5963\n",
      "Epoch 452/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0335 - accuracy: 0.5840 - val_loss: 1.0589 - val_accuracy: 0.5852\n",
      "Epoch 453/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0348 - accuracy: 0.5753 - val_loss: 1.0547 - val_accuracy: 0.5704\n",
      "Epoch 454/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0214 - accuracy: 0.5691 - val_loss: 1.0327 - val_accuracy: 0.5852\n",
      "Epoch 455/750\n",
      "162/162 [==============================] - 0s 593us/step - loss: 1.0310 - accuracy: 0.5741 - val_loss: 1.0465 - val_accuracy: 0.5889\n",
      "Epoch 456/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0207 - accuracy: 0.5753 - val_loss: 1.0566 - val_accuracy: 0.5815\n",
      "Epoch 457/750\n",
      "162/162 [==============================] - 0s 595us/step - loss: 1.0186 - accuracy: 0.5827 - val_loss: 1.0357 - val_accuracy: 0.6111\n",
      "Epoch 458/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0281 - accuracy: 0.5778 - val_loss: 1.0112 - val_accuracy: 0.6000\n",
      "Epoch 459/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.0816 - accuracy: 0.5741 - val_loss: 1.1643 - val_accuracy: 0.4593\n",
      "Epoch 460/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0323 - accuracy: 0.5877 - val_loss: 1.0629 - val_accuracy: 0.5963\n",
      "Epoch 461/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0376 - accuracy: 0.5654 - val_loss: 1.0860 - val_accuracy: 0.5630\n",
      "Epoch 462/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0397 - accuracy: 0.5716 - val_loss: 1.0203 - val_accuracy: 0.6407\n",
      "Epoch 463/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0635 - accuracy: 0.5617 - val_loss: 1.0575 - val_accuracy: 0.5852\n",
      "Epoch 464/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0436 - accuracy: 0.5741 - val_loss: 1.1255 - val_accuracy: 0.5667\n",
      "Epoch 465/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0208 - accuracy: 0.5630 - val_loss: 1.0114 - val_accuracy: 0.5852\n",
      "Epoch 466/750\n",
      "162/162 [==============================] - 0s 590us/step - loss: 1.0354 - accuracy: 0.5654 - val_loss: 1.0810 - val_accuracy: 0.5481\n",
      "Epoch 467/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0390 - accuracy: 0.5901 - val_loss: 1.1377 - val_accuracy: 0.5407\n",
      "Epoch 468/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0497 - accuracy: 0.5852 - val_loss: 1.0882 - val_accuracy: 0.5259\n",
      "Epoch 469/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0386 - accuracy: 0.5531 - val_loss: 1.0974 - val_accuracy: 0.5741\n",
      "Epoch 470/750\n",
      "162/162 [==============================] - 0s 590us/step - loss: 1.0289 - accuracy: 0.5790 - val_loss: 1.0360 - val_accuracy: 0.5778\n",
      "Epoch 471/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0421 - accuracy: 0.5642 - val_loss: 1.1077 - val_accuracy: 0.5667\n",
      "Epoch 472/750\n",
      "162/162 [==============================] - 0s 597us/step - loss: 1.0320 - accuracy: 0.5864 - val_loss: 1.0425 - val_accuracy: 0.5926\n",
      "Epoch 473/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0143 - accuracy: 0.5926 - val_loss: 1.0223 - val_accuracy: 0.6074\n",
      "Epoch 474/750\n",
      "162/162 [==============================] - 0s 593us/step - loss: 1.0268 - accuracy: 0.5716 - val_loss: 1.0106 - val_accuracy: 0.6148\n",
      "Epoch 475/750\n",
      "162/162 [==============================] - 0s 593us/step - loss: 1.0551 - accuracy: 0.5630 - val_loss: 0.9935 - val_accuracy: 0.6185\n",
      "Epoch 476/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.0266 - accuracy: 0.5728 - val_loss: 1.0697 - val_accuracy: 0.5778\n",
      "Epoch 477/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0642 - accuracy: 0.5741 - val_loss: 1.3570 - val_accuracy: 0.4074\n",
      "Epoch 478/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0427 - accuracy: 0.5605 - val_loss: 0.9993 - val_accuracy: 0.6111\n",
      "Epoch 479/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0315 - accuracy: 0.5679 - val_loss: 1.0113 - val_accuracy: 0.6111\n",
      "Epoch 480/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0206 - accuracy: 0.5778 - val_loss: 1.0270 - val_accuracy: 0.5889\n",
      "Epoch 481/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0767 - accuracy: 0.5667 - val_loss: 1.0081 - val_accuracy: 0.6074\n",
      "Epoch 482/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0254 - accuracy: 0.5827 - val_loss: 1.0831 - val_accuracy: 0.5741\n",
      "Epoch 483/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0290 - accuracy: 0.5877 - val_loss: 1.0498 - val_accuracy: 0.6074\n",
      "Epoch 484/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0630 - accuracy: 0.5679 - val_loss: 1.0828 - val_accuracy: 0.5778\n",
      "Epoch 485/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0581 - accuracy: 0.5568 - val_loss: 1.0565 - val_accuracy: 0.5481\n",
      "Epoch 486/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0212 - accuracy: 0.5667 - val_loss: 1.0668 - val_accuracy: 0.5704\n",
      "Epoch 487/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0706 - accuracy: 0.5630 - val_loss: 1.0220 - val_accuracy: 0.5963\n",
      "Epoch 488/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0212 - accuracy: 0.5716 - val_loss: 1.0901 - val_accuracy: 0.5778\n",
      "Epoch 489/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0731 - accuracy: 0.5568 - val_loss: 1.1511 - val_accuracy: 0.5630\n",
      "Epoch 490/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0469 - accuracy: 0.5580 - val_loss: 1.0602 - val_accuracy: 0.5926\n",
      "Epoch 491/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0452 - accuracy: 0.5617 - val_loss: 1.0959 - val_accuracy: 0.5667\n",
      "Epoch 492/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0398 - accuracy: 0.5741 - val_loss: 1.0138 - val_accuracy: 0.5852\n",
      "Epoch 493/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0377 - accuracy: 0.5753 - val_loss: 1.1529 - val_accuracy: 0.5519\n",
      "Epoch 494/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0234 - accuracy: 0.5840 - val_loss: 1.0373 - val_accuracy: 0.5519\n",
      "Epoch 495/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0492 - accuracy: 0.5778 - val_loss: 1.0186 - val_accuracy: 0.6111\n",
      "Epoch 496/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0439 - accuracy: 0.5556 - val_loss: 1.1548 - val_accuracy: 0.5630\n",
      "Epoch 497/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0341 - accuracy: 0.5716 - val_loss: 1.0324 - val_accuracy: 0.6000\n",
      "Epoch 498/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0305 - accuracy: 0.5716 - val_loss: 1.0017 - val_accuracy: 0.6111\n",
      "Epoch 499/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0288 - accuracy: 0.5741 - val_loss: 1.0616 - val_accuracy: 0.5630\n",
      "Epoch 500/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0281 - accuracy: 0.5654 - val_loss: 1.0093 - val_accuracy: 0.6074\n",
      "Epoch 501/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0392 - accuracy: 0.5790 - val_loss: 1.0016 - val_accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 502/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0322 - accuracy: 0.5840 - val_loss: 1.0352 - val_accuracy: 0.5407\n",
      "Epoch 503/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0113 - accuracy: 0.5790 - val_loss: 1.0078 - val_accuracy: 0.6370\n",
      "Epoch 504/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0164 - accuracy: 0.5691 - val_loss: 1.0370 - val_accuracy: 0.6000\n",
      "Epoch 505/750\n",
      "162/162 [==============================] - 0s 661us/step - loss: 1.0347 - accuracy: 0.5568 - val_loss: 1.2359 - val_accuracy: 0.5370\n",
      "Epoch 506/750\n",
      "162/162 [==============================] - 0s 764us/step - loss: 1.0426 - accuracy: 0.5667 - val_loss: 1.0243 - val_accuracy: 0.6185\n",
      "Epoch 507/750\n",
      "162/162 [==============================] - 0s 638us/step - loss: 1.0300 - accuracy: 0.5679 - val_loss: 1.0804 - val_accuracy: 0.5296\n",
      "Epoch 508/750\n",
      "162/162 [==============================] - 0s 740us/step - loss: 1.0222 - accuracy: 0.5765 - val_loss: 1.0696 - val_accuracy: 0.5000\n",
      "Epoch 509/750\n",
      "162/162 [==============================] - 0s 666us/step - loss: 1.0151 - accuracy: 0.5679 - val_loss: 1.0228 - val_accuracy: 0.5778\n",
      "Epoch 510/750\n",
      "162/162 [==============================] - 0s 699us/step - loss: 1.0311 - accuracy: 0.5691 - val_loss: 1.0510 - val_accuracy: 0.5741\n",
      "Epoch 511/750\n",
      "162/162 [==============================] - 0s 676us/step - loss: 1.0256 - accuracy: 0.5827 - val_loss: 1.0330 - val_accuracy: 0.6259\n",
      "Epoch 512/750\n",
      "162/162 [==============================] - 0s 640us/step - loss: 1.0285 - accuracy: 0.5864 - val_loss: 1.0668 - val_accuracy: 0.5593\n",
      "Epoch 513/750\n",
      "162/162 [==============================] - 0s 666us/step - loss: 1.0261 - accuracy: 0.5741 - val_loss: 1.0201 - val_accuracy: 0.6074\n",
      "Epoch 514/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0248 - accuracy: 0.5840 - val_loss: 0.9981 - val_accuracy: 0.6148\n",
      "Epoch 515/750\n",
      "162/162 [==============================] - 0s 596us/step - loss: 1.0605 - accuracy: 0.5654 - val_loss: 1.5497 - val_accuracy: 0.5370\n",
      "Epoch 516/750\n",
      "162/162 [==============================] - 0s 614us/step - loss: 1.0715 - accuracy: 0.5593 - val_loss: 1.0089 - val_accuracy: 0.6000\n",
      "Epoch 517/750\n",
      "162/162 [==============================] - 0s 576us/step - loss: 1.0507 - accuracy: 0.5815 - val_loss: 1.0578 - val_accuracy: 0.5741\n",
      "Epoch 518/750\n",
      "162/162 [==============================] - 0s 620us/step - loss: 1.0441 - accuracy: 0.5667 - val_loss: 1.0299 - val_accuracy: 0.5778\n",
      "Epoch 519/750\n",
      "162/162 [==============================] - 0s 629us/step - loss: 1.0115 - accuracy: 0.5704 - val_loss: 1.0199 - val_accuracy: 0.6185\n",
      "Epoch 520/750\n",
      "162/162 [==============================] - 0s 622us/step - loss: 1.0406 - accuracy: 0.5753 - val_loss: 1.0493 - val_accuracy: 0.5963\n",
      "Epoch 521/750\n",
      "162/162 [==============================] - 0s 618us/step - loss: 1.0193 - accuracy: 0.5827 - val_loss: 1.0408 - val_accuracy: 0.6185\n",
      "Epoch 522/750\n",
      "162/162 [==============================] - 0s 612us/step - loss: 1.0521 - accuracy: 0.5679 - val_loss: 1.0406 - val_accuracy: 0.6074\n",
      "Epoch 523/750\n",
      "162/162 [==============================] - 0s 645us/step - loss: 1.0255 - accuracy: 0.5827 - val_loss: 1.0164 - val_accuracy: 0.6259\n",
      "Epoch 524/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0187 - accuracy: 0.5877 - val_loss: 1.0926 - val_accuracy: 0.5667\n",
      "Epoch 525/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.0488 - accuracy: 0.5543 - val_loss: 0.9987 - val_accuracy: 0.6185\n",
      "Epoch 526/750\n",
      "162/162 [==============================] - 0s 762us/step - loss: 1.0067 - accuracy: 0.5852 - val_loss: 1.0114 - val_accuracy: 0.6111\n",
      "Epoch 527/750\n",
      "162/162 [==============================] - 0s 680us/step - loss: 1.0369 - accuracy: 0.5827 - val_loss: 1.0201 - val_accuracy: 0.5963\n",
      "Epoch 528/750\n",
      "162/162 [==============================] - 0s 614us/step - loss: 1.0481 - accuracy: 0.5617 - val_loss: 1.0667 - val_accuracy: 0.6074\n",
      "Epoch 529/750\n",
      "162/162 [==============================] - 0s 771us/step - loss: 1.0131 - accuracy: 0.5741 - val_loss: 1.2220 - val_accuracy: 0.4963\n",
      "Epoch 530/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0393 - accuracy: 0.5642 - val_loss: 1.0079 - val_accuracy: 0.6111\n",
      "Epoch 531/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0391 - accuracy: 0.5691 - val_loss: 0.9843 - val_accuracy: 0.6074\n",
      "Epoch 532/750\n",
      "162/162 [==============================] - 0s 770us/step - loss: 1.0203 - accuracy: 0.5827 - val_loss: 1.0089 - val_accuracy: 0.5889\n",
      "Epoch 533/750\n",
      "162/162 [==============================] - 0s 732us/step - loss: 1.0172 - accuracy: 0.5802 - val_loss: 1.1211 - val_accuracy: 0.5741\n",
      "Epoch 534/750\n",
      "162/162 [==============================] - 0s 687us/step - loss: 1.0370 - accuracy: 0.5642 - val_loss: 1.1492 - val_accuracy: 0.4704\n",
      "Epoch 535/750\n",
      "162/162 [==============================] - 0s 639us/step - loss: 1.0128 - accuracy: 0.5840 - val_loss: 1.0000 - val_accuracy: 0.6148\n",
      "Epoch 536/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.0067 - accuracy: 0.5827 - val_loss: 1.0244 - val_accuracy: 0.6037\n",
      "Epoch 537/750\n",
      "162/162 [==============================] - 0s 674us/step - loss: 1.0275 - accuracy: 0.5704 - val_loss: 1.1800 - val_accuracy: 0.5074\n",
      "Epoch 538/750\n",
      "162/162 [==============================] - 0s 681us/step - loss: 1.0147 - accuracy: 0.5802 - val_loss: 1.0443 - val_accuracy: 0.5926\n",
      "Epoch 539/750\n",
      "162/162 [==============================] - 0s 662us/step - loss: 1.0443 - accuracy: 0.5864 - val_loss: 1.0754 - val_accuracy: 0.5481\n",
      "Epoch 540/750\n",
      "162/162 [==============================] - 0s 675us/step - loss: 1.0288 - accuracy: 0.5728 - val_loss: 1.1724 - val_accuracy: 0.4889\n",
      "Epoch 541/750\n",
      "162/162 [==============================] - 0s 641us/step - loss: 1.0128 - accuracy: 0.5988 - val_loss: 1.0932 - val_accuracy: 0.5296\n",
      "Epoch 542/750\n",
      "162/162 [==============================] - 0s 727us/step - loss: 1.0313 - accuracy: 0.5605 - val_loss: 1.0210 - val_accuracy: 0.5963\n",
      "Epoch 543/750\n",
      "162/162 [==============================] - 0s 666us/step - loss: 1.0573 - accuracy: 0.5519 - val_loss: 1.0363 - val_accuracy: 0.5630\n",
      "Epoch 544/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.0330 - accuracy: 0.5704 - val_loss: 1.0150 - val_accuracy: 0.6037\n",
      "Epoch 545/750\n",
      "162/162 [==============================] - 0s 644us/step - loss: 1.0279 - accuracy: 0.5654 - val_loss: 1.0017 - val_accuracy: 0.6148\n",
      "Epoch 546/750\n",
      "162/162 [==============================] - 0s 743us/step - loss: 1.0197 - accuracy: 0.5654 - val_loss: 1.0678 - val_accuracy: 0.5370\n",
      "Epoch 547/750\n",
      "162/162 [==============================] - 0s 673us/step - loss: 1.0058 - accuracy: 0.5889 - val_loss: 1.0976 - val_accuracy: 0.5741\n",
      "Epoch 548/750\n",
      "162/162 [==============================] - 0s 640us/step - loss: 1.0233 - accuracy: 0.5815 - val_loss: 1.0228 - val_accuracy: 0.6185\n",
      "Epoch 549/750\n",
      "162/162 [==============================] - 0s 640us/step - loss: 1.0578 - accuracy: 0.5605 - val_loss: 1.0711 - val_accuracy: 0.5778\n",
      "Epoch 550/750\n",
      "162/162 [==============================] - 0s 647us/step - loss: 1.0461 - accuracy: 0.5519 - val_loss: 1.0199 - val_accuracy: 0.5741\n",
      "Epoch 551/750\n",
      "162/162 [==============================] - 0s 672us/step - loss: 1.0119 - accuracy: 0.5889 - val_loss: 1.0054 - val_accuracy: 0.5889\n",
      "Epoch 552/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0125 - accuracy: 0.5827 - val_loss: 1.0351 - val_accuracy: 0.5704\n",
      "Epoch 553/750\n",
      "162/162 [==============================] - 0s 646us/step - loss: 1.0149 - accuracy: 0.5877 - val_loss: 1.0682 - val_accuracy: 0.5741\n",
      "Epoch 554/750\n",
      "162/162 [==============================] - 0s 639us/step - loss: 1.0446 - accuracy: 0.5765 - val_loss: 1.0495 - val_accuracy: 0.6259\n",
      "Epoch 555/750\n",
      "162/162 [==============================] - 0s 680us/step - loss: 1.0366 - accuracy: 0.5679 - val_loss: 1.0457 - val_accuracy: 0.6185\n",
      "Epoch 556/750\n",
      "162/162 [==============================] - 0s 667us/step - loss: 1.0258 - accuracy: 0.5877 - val_loss: 1.2013 - val_accuracy: 0.5444\n",
      "Epoch 557/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0414 - accuracy: 0.5679 - val_loss: 0.9988 - val_accuracy: 0.5926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 558/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0481 - accuracy: 0.5654 - val_loss: 1.0720 - val_accuracy: 0.5667\n",
      "Epoch 559/750\n",
      "162/162 [==============================] - 0s 695us/step - loss: 1.0439 - accuracy: 0.5667 - val_loss: 1.0486 - val_accuracy: 0.5926\n",
      "Epoch 560/750\n",
      "162/162 [==============================] - 0s 689us/step - loss: 1.0263 - accuracy: 0.5815 - val_loss: 1.0185 - val_accuracy: 0.5741\n",
      "Epoch 561/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0294 - accuracy: 0.5889 - val_loss: 1.0185 - val_accuracy: 0.6148\n",
      "Epoch 562/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0054 - accuracy: 0.5901 - val_loss: 0.9953 - val_accuracy: 0.5741\n",
      "Epoch 563/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0421 - accuracy: 0.5617 - val_loss: 1.1219 - val_accuracy: 0.5593\n",
      "Epoch 564/750\n",
      "162/162 [==============================] - 0s 609us/step - loss: 1.0279 - accuracy: 0.5778 - val_loss: 1.0298 - val_accuracy: 0.5778\n",
      "Epoch 565/750\n",
      "162/162 [==============================] - 0s 666us/step - loss: 1.0313 - accuracy: 0.5642 - val_loss: 1.0871 - val_accuracy: 0.6111\n",
      "Epoch 566/750\n",
      "162/162 [==============================] - 0s 665us/step - loss: 1.0239 - accuracy: 0.5926 - val_loss: 1.0151 - val_accuracy: 0.6000\n",
      "Epoch 567/750\n",
      "162/162 [==============================] - 0s 703us/step - loss: 1.0170 - accuracy: 0.5901 - val_loss: 1.0984 - val_accuracy: 0.5963\n",
      "Epoch 568/750\n",
      "162/162 [==============================] - 0s 722us/step - loss: 1.1060 - accuracy: 0.5284 - val_loss: 1.0371 - val_accuracy: 0.5778\n",
      "Epoch 569/750\n",
      "162/162 [==============================] - 0s 672us/step - loss: 1.0524 - accuracy: 0.5790 - val_loss: 1.0185 - val_accuracy: 0.5704\n",
      "Epoch 570/750\n",
      "162/162 [==============================] - 0s 655us/step - loss: 1.0300 - accuracy: 0.5827 - val_loss: 1.0097 - val_accuracy: 0.6148\n",
      "Epoch 571/750\n",
      "162/162 [==============================] - 0s 653us/step - loss: 1.1043 - accuracy: 0.5481 - val_loss: 1.1200 - val_accuracy: 0.6000\n",
      "Epoch 572/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0568 - accuracy: 0.5778 - val_loss: 1.0460 - val_accuracy: 0.5630\n",
      "Epoch 573/750\n",
      "162/162 [==============================] - 0s 716us/step - loss: 1.0461 - accuracy: 0.5778 - val_loss: 1.0208 - val_accuracy: 0.6111\n",
      "Epoch 574/750\n",
      "162/162 [==============================] - 0s 665us/step - loss: 1.0500 - accuracy: 0.5593 - val_loss: 1.0277 - val_accuracy: 0.5926\n",
      "Epoch 575/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0356 - accuracy: 0.5741 - val_loss: 1.0286 - val_accuracy: 0.6000\n",
      "Epoch 576/750\n",
      "162/162 [==============================] - 0s 652us/step - loss: 1.0192 - accuracy: 0.5827 - val_loss: 1.0833 - val_accuracy: 0.5481\n",
      "Epoch 577/750\n",
      "162/162 [==============================] - 0s 593us/step - loss: 1.0575 - accuracy: 0.5728 - val_loss: 1.0372 - val_accuracy: 0.5481\n",
      "Epoch 578/750\n",
      "162/162 [==============================] - 0s 621us/step - loss: 1.0360 - accuracy: 0.5741 - val_loss: 1.0252 - val_accuracy: 0.5889\n",
      "Epoch 579/750\n",
      "162/162 [==============================] - 0s 628us/step - loss: 1.0237 - accuracy: 0.5679 - val_loss: 1.1245 - val_accuracy: 0.5593\n",
      "Epoch 580/750\n",
      "162/162 [==============================] - 0s 685us/step - loss: 1.0295 - accuracy: 0.5815 - val_loss: 1.0245 - val_accuracy: 0.5963\n",
      "Epoch 581/750\n",
      "162/162 [==============================] - 0s 575us/step - loss: 1.0011 - accuracy: 0.5765 - val_loss: 1.1404 - val_accuracy: 0.5444\n",
      "Epoch 582/750\n",
      "162/162 [==============================] - 0s 790us/step - loss: 1.0805 - accuracy: 0.5568 - val_loss: 1.0764 - val_accuracy: 0.5741\n",
      "Epoch 583/750\n",
      "162/162 [==============================] - 0s 716us/step - loss: 1.0425 - accuracy: 0.5889 - val_loss: 1.0324 - val_accuracy: 0.6111\n",
      "Epoch 584/750\n",
      "162/162 [==============================] - 0s 771us/step - loss: 1.0118 - accuracy: 0.5815 - val_loss: 1.0184 - val_accuracy: 0.6074\n",
      "Epoch 585/750\n",
      "162/162 [==============================] - 0s 678us/step - loss: 1.0243 - accuracy: 0.5852 - val_loss: 1.0231 - val_accuracy: 0.6185\n",
      "Epoch 586/750\n",
      "162/162 [==============================] - 0s 698us/step - loss: 1.0030 - accuracy: 0.5864 - val_loss: 0.9994 - val_accuracy: 0.6037\n",
      "Epoch 587/750\n",
      "162/162 [==============================] - 0s 638us/step - loss: 1.0393 - accuracy: 0.5667 - val_loss: 1.0949 - val_accuracy: 0.5222\n",
      "Epoch 588/750\n",
      "162/162 [==============================] - 0s 688us/step - loss: 1.0372 - accuracy: 0.5753 - val_loss: 0.9985 - val_accuracy: 0.6111\n",
      "Epoch 589/750\n",
      "162/162 [==============================] - 0s 597us/step - loss: 1.0014 - accuracy: 0.6000 - val_loss: 1.0257 - val_accuracy: 0.5815\n",
      "Epoch 590/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0448 - accuracy: 0.5630 - val_loss: 0.9971 - val_accuracy: 0.5963\n",
      "Epoch 591/750\n",
      "162/162 [==============================] - 0s 873us/step - loss: 1.0322 - accuracy: 0.5778 - val_loss: 1.2188 - val_accuracy: 0.5259\n",
      "Epoch 592/750\n",
      "162/162 [==============================] - 0s 738us/step - loss: 1.0090 - accuracy: 0.5938 - val_loss: 1.0589 - val_accuracy: 0.5000\n",
      "Epoch 593/750\n",
      "162/162 [==============================] - 0s 740us/step - loss: 1.0397 - accuracy: 0.5914 - val_loss: 1.0168 - val_accuracy: 0.6185\n",
      "Epoch 594/750\n",
      "162/162 [==============================] - 0s 841us/step - loss: 1.0462 - accuracy: 0.5630 - val_loss: 1.0164 - val_accuracy: 0.6074\n",
      "Epoch 595/750\n",
      "162/162 [==============================] - 0s 653us/step - loss: 1.0172 - accuracy: 0.5889 - val_loss: 1.0147 - val_accuracy: 0.5889\n",
      "Epoch 596/750\n",
      "162/162 [==============================] - 0s 655us/step - loss: 1.0715 - accuracy: 0.5519 - val_loss: 1.0820 - val_accuracy: 0.5704\n",
      "Epoch 597/750\n",
      "162/162 [==============================] - 0s 787us/step - loss: 1.0268 - accuracy: 0.5716 - val_loss: 1.0616 - val_accuracy: 0.5815\n",
      "Epoch 598/750\n",
      "162/162 [==============================] - 0s 740us/step - loss: 1.0559 - accuracy: 0.5642 - val_loss: 1.2137 - val_accuracy: 0.4852\n",
      "Epoch 599/750\n",
      "162/162 [==============================] - 0s 740us/step - loss: 1.0239 - accuracy: 0.5901 - val_loss: 1.0353 - val_accuracy: 0.6111\n",
      "Epoch 600/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0335 - accuracy: 0.5704 - val_loss: 1.0518 - val_accuracy: 0.5741\n",
      "Epoch 601/750\n",
      "162/162 [==============================] - 0s 740us/step - loss: 1.0263 - accuracy: 0.5790 - val_loss: 1.0343 - val_accuracy: 0.6074\n",
      "Epoch 602/750\n",
      "162/162 [==============================] - 0s 695us/step - loss: 1.0022 - accuracy: 0.5728 - val_loss: 1.0293 - val_accuracy: 0.6185\n",
      "Epoch 603/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0318 - accuracy: 0.5765 - val_loss: 1.2183 - val_accuracy: 0.5296\n",
      "Epoch 604/750\n",
      "162/162 [==============================] - 0s 694us/step - loss: 1.0439 - accuracy: 0.5728 - val_loss: 1.1390 - val_accuracy: 0.5148\n",
      "Epoch 605/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0283 - accuracy: 0.5617 - val_loss: 0.9972 - val_accuracy: 0.6111\n",
      "Epoch 606/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0443 - accuracy: 0.5679 - val_loss: 1.0235 - val_accuracy: 0.6037\n",
      "Epoch 607/750\n",
      "162/162 [==============================] - 0s 644us/step - loss: 1.0231 - accuracy: 0.5778 - val_loss: 1.0085 - val_accuracy: 0.5889\n",
      "Epoch 608/750\n",
      "162/162 [==============================] - 0s 820us/step - loss: 1.0259 - accuracy: 0.5778 - val_loss: 1.0652 - val_accuracy: 0.6111\n",
      "Epoch 609/750\n",
      "162/162 [==============================] - 0s 612us/step - loss: 1.0353 - accuracy: 0.5741 - val_loss: 1.0193 - val_accuracy: 0.6074\n",
      "Epoch 610/750\n",
      "162/162 [==============================] - 0s 714us/step - loss: 1.0212 - accuracy: 0.5852 - val_loss: 1.0191 - val_accuracy: 0.5889\n",
      "Epoch 611/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0268 - accuracy: 0.5506 - val_loss: 1.0165 - val_accuracy: 0.6000\n",
      "Epoch 612/750\n",
      "162/162 [==============================] - 0s 651us/step - loss: 1.0350 - accuracy: 0.5679 - val_loss: 1.1239 - val_accuracy: 0.5481\n",
      "Epoch 613/750\n",
      "162/162 [==============================] - 0s 529us/step - loss: 1.0194 - accuracy: 0.5728 - val_loss: 1.0291 - val_accuracy: 0.5815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 614/750\n",
      "162/162 [==============================] - 0s 658us/step - loss: 1.0470 - accuracy: 0.5654 - val_loss: 1.1731 - val_accuracy: 0.6000\n",
      "Epoch 615/750\n",
      "162/162 [==============================] - 0s 622us/step - loss: 1.0533 - accuracy: 0.5778 - val_loss: 1.1006 - val_accuracy: 0.5222\n",
      "Epoch 616/750\n",
      "162/162 [==============================] - 0s 630us/step - loss: 1.0424 - accuracy: 0.5617 - val_loss: 1.0184 - val_accuracy: 0.5963\n",
      "Epoch 617/750\n",
      "162/162 [==============================] - 0s 628us/step - loss: 1.0129 - accuracy: 0.5790 - val_loss: 1.0561 - val_accuracy: 0.5926\n",
      "Epoch 618/750\n",
      "162/162 [==============================] - 0s 645us/step - loss: 1.0388 - accuracy: 0.5617 - val_loss: 1.0474 - val_accuracy: 0.6074\n",
      "Epoch 619/750\n",
      "162/162 [==============================] - 0s 672us/step - loss: 1.0286 - accuracy: 0.5827 - val_loss: 0.9923 - val_accuracy: 0.6222\n",
      "Epoch 620/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0143 - accuracy: 0.5815 - val_loss: 1.0204 - val_accuracy: 0.5778\n",
      "Epoch 621/750\n",
      "162/162 [==============================] - 0s 662us/step - loss: 1.0481 - accuracy: 0.5667 - val_loss: 1.0371 - val_accuracy: 0.5963\n",
      "Epoch 622/750\n",
      "162/162 [==============================] - 0s 715us/step - loss: 1.0263 - accuracy: 0.5654 - val_loss: 0.9923 - val_accuracy: 0.5889\n",
      "Epoch 623/750\n",
      "162/162 [==============================] - 0s 849us/step - loss: 1.0353 - accuracy: 0.5790 - val_loss: 1.0341 - val_accuracy: 0.5889\n",
      "Epoch 624/750\n",
      "162/162 [==============================] - 0s 690us/step - loss: 1.0185 - accuracy: 0.5753 - val_loss: 1.0877 - val_accuracy: 0.5519\n",
      "Epoch 625/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0238 - accuracy: 0.5593 - val_loss: 1.0146 - val_accuracy: 0.5852\n",
      "Epoch 626/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.0344 - accuracy: 0.5802 - val_loss: 1.1663 - val_accuracy: 0.5407\n",
      "Epoch 627/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0408 - accuracy: 0.5802 - val_loss: 1.0266 - val_accuracy: 0.6074\n",
      "Epoch 628/750\n",
      "162/162 [==============================] - 0s 588us/step - loss: 1.0703 - accuracy: 0.5593 - val_loss: 1.0353 - val_accuracy: 0.6037\n",
      "Epoch 629/750\n",
      "162/162 [==============================] - 0s 743us/step - loss: 1.0233 - accuracy: 0.5753 - val_loss: 1.0196 - val_accuracy: 0.6259\n",
      "Epoch 630/750\n",
      "162/162 [==============================] - 0s 790us/step - loss: 1.0065 - accuracy: 0.5864 - val_loss: 0.9916 - val_accuracy: 0.6000\n",
      "Epoch 631/750\n",
      "162/162 [==============================] - 0s 639us/step - loss: 1.0138 - accuracy: 0.5877 - val_loss: 1.0625 - val_accuracy: 0.5926\n",
      "Epoch 632/750\n",
      "162/162 [==============================] - 0s 938us/step - loss: 1.0569 - accuracy: 0.5642 - val_loss: 1.0601 - val_accuracy: 0.5815\n",
      "Epoch 633/750\n",
      "162/162 [==============================] - 0s 740us/step - loss: 1.0300 - accuracy: 0.5802 - val_loss: 1.0169 - val_accuracy: 0.6222\n",
      "Epoch 634/750\n",
      "162/162 [==============================] - 0s 839us/step - loss: 1.0001 - accuracy: 0.5840 - val_loss: 1.0670 - val_accuracy: 0.6185\n",
      "Epoch 635/750\n",
      "162/162 [==============================] - 0s 740us/step - loss: 1.0069 - accuracy: 0.5852 - val_loss: 1.0337 - val_accuracy: 0.6148\n",
      "Epoch 636/750\n",
      "162/162 [==============================] - 0s 740us/step - loss: 1.0337 - accuracy: 0.5790 - val_loss: 1.0414 - val_accuracy: 0.6296\n",
      "Epoch 637/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0060 - accuracy: 0.5852 - val_loss: 1.0096 - val_accuracy: 0.5815\n",
      "Epoch 638/750\n",
      "162/162 [==============================] - 0s 740us/step - loss: 1.0148 - accuracy: 0.5988 - val_loss: 1.0510 - val_accuracy: 0.5889\n",
      "Epoch 639/750\n",
      "162/162 [==============================] - 0s 889us/step - loss: 1.0400 - accuracy: 0.5444 - val_loss: 1.1165 - val_accuracy: 0.5333\n",
      "Epoch 640/750\n",
      "162/162 [==============================] - 0s 839us/step - loss: 1.0154 - accuracy: 0.5926 - val_loss: 1.0408 - val_accuracy: 0.5667\n",
      "Epoch 641/750\n",
      "162/162 [==============================] - 0s 740us/step - loss: 1.0209 - accuracy: 0.5877 - val_loss: 1.0429 - val_accuracy: 0.6037\n",
      "Epoch 642/750\n",
      "162/162 [==============================] - 0s 630us/step - loss: 1.0045 - accuracy: 0.5926 - val_loss: 1.1252 - val_accuracy: 0.5593\n",
      "Epoch 643/750\n",
      "162/162 [==============================] - 0s 716us/step - loss: 1.0616 - accuracy: 0.5667 - val_loss: 1.0098 - val_accuracy: 0.5926\n",
      "Epoch 644/750\n",
      "162/162 [==============================] - 0s 839us/step - loss: 1.0416 - accuracy: 0.5667 - val_loss: 1.0143 - val_accuracy: 0.6185\n",
      "Epoch 645/750\n",
      "162/162 [==============================] - 0s 743us/step - loss: 1.0018 - accuracy: 0.5926 - val_loss: 1.0604 - val_accuracy: 0.5963\n",
      "Epoch 646/750\n",
      "162/162 [==============================] - 0s 677us/step - loss: 1.0157 - accuracy: 0.5963 - val_loss: 1.0138 - val_accuracy: 0.6148\n",
      "Epoch 647/750\n",
      "162/162 [==============================] - 0s 640us/step - loss: 1.0167 - accuracy: 0.5741 - val_loss: 0.9891 - val_accuracy: 0.5889\n",
      "Epoch 648/750\n",
      "162/162 [==============================] - 0s 740us/step - loss: 1.0215 - accuracy: 0.5889 - val_loss: 1.2087 - val_accuracy: 0.5185\n",
      "Epoch 649/750\n",
      "162/162 [==============================] - 0s 648us/step - loss: 1.0625 - accuracy: 0.5728 - val_loss: 1.0705 - val_accuracy: 0.5630\n",
      "Epoch 650/750\n",
      "162/162 [==============================] - 0s 657us/step - loss: 1.0292 - accuracy: 0.5802 - val_loss: 1.0176 - val_accuracy: 0.6037\n",
      "Epoch 651/750\n",
      "162/162 [==============================] - 0s 765us/step - loss: 1.0510 - accuracy: 0.5827 - val_loss: 1.0648 - val_accuracy: 0.6074\n",
      "Epoch 652/750\n",
      "162/162 [==============================] - 0s 668us/step - loss: 1.0322 - accuracy: 0.5765 - val_loss: 1.0194 - val_accuracy: 0.6111\n",
      "Epoch 653/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0085 - accuracy: 0.5802 - val_loss: 0.9982 - val_accuracy: 0.5926\n",
      "Epoch 654/750\n",
      "162/162 [==============================] - 0s 701us/step - loss: 1.0159 - accuracy: 0.5963 - val_loss: 1.0637 - val_accuracy: 0.5481\n",
      "Epoch 655/750\n",
      "162/162 [==============================] - 0s 761us/step - loss: 1.0020 - accuracy: 0.5988 - val_loss: 1.0629 - val_accuracy: 0.6000\n",
      "Epoch 656/750\n",
      "162/162 [==============================] - 0s 647us/step - loss: 1.0501 - accuracy: 0.5741 - val_loss: 1.0266 - val_accuracy: 0.5741\n",
      "Epoch 657/750\n",
      "162/162 [==============================] - 0s 807us/step - loss: 1.0174 - accuracy: 0.5938 - val_loss: 1.0154 - val_accuracy: 0.6111\n",
      "Epoch 658/750\n",
      "162/162 [==============================] - 0s 703us/step - loss: 1.0778 - accuracy: 0.5580 - val_loss: 1.0850 - val_accuracy: 0.5630\n",
      "Epoch 659/750\n",
      "162/162 [==============================] - 0s 650us/step - loss: 1.0311 - accuracy: 0.5840 - val_loss: 1.0018 - val_accuracy: 0.5963\n",
      "Epoch 660/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0356 - accuracy: 0.5716 - val_loss: 1.0193 - val_accuracy: 0.6000\n",
      "Epoch 661/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0243 - accuracy: 0.5728 - val_loss: 0.9898 - val_accuracy: 0.6000\n",
      "Epoch 662/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0222 - accuracy: 0.5815 - val_loss: 1.0241 - val_accuracy: 0.5704\n",
      "Epoch 663/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0053 - accuracy: 0.5778 - val_loss: 1.0209 - val_accuracy: 0.6074\n",
      "Epoch 664/750\n",
      "162/162 [==============================] - 0s 987us/step - loss: 1.0355 - accuracy: 0.5852 - val_loss: 1.1354 - val_accuracy: 0.5407\n",
      "Epoch 665/750\n",
      "162/162 [==============================] - 0s 694us/step - loss: 1.0078 - accuracy: 0.5889 - val_loss: 1.1170 - val_accuracy: 0.5519\n",
      "Epoch 666/750\n",
      "162/162 [==============================] - 0s 688us/step - loss: 0.9983 - accuracy: 0.5901 - val_loss: 1.1260 - val_accuracy: 0.5741\n",
      "Epoch 667/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0106 - accuracy: 0.5827 - val_loss: 1.0168 - val_accuracy: 0.6185\n",
      "Epoch 668/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0405 - accuracy: 0.5802 - val_loss: 1.1588 - val_accuracy: 0.4889\n",
      "Epoch 669/750\n",
      "162/162 [==============================] - 0s 592us/step - loss: 1.0351 - accuracy: 0.5568 - val_loss: 0.9895 - val_accuracy: 0.6259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 670/750\n",
      "162/162 [==============================] - 0s 631us/step - loss: 1.0489 - accuracy: 0.5889 - val_loss: 1.0077 - val_accuracy: 0.6333\n",
      "Epoch 671/750\n",
      "162/162 [==============================] - 0s 581us/step - loss: 0.9981 - accuracy: 0.6074 - val_loss: 1.0217 - val_accuracy: 0.6111\n",
      "Epoch 672/750\n",
      "162/162 [==============================] - 0s 627us/step - loss: 1.0218 - accuracy: 0.5753 - val_loss: 1.1739 - val_accuracy: 0.5111\n",
      "Epoch 673/750\n",
      "162/162 [==============================] - 0s 620us/step - loss: 1.0057 - accuracy: 0.5926 - val_loss: 1.0308 - val_accuracy: 0.5630\n",
      "Epoch 674/750\n",
      "162/162 [==============================] - 0s 632us/step - loss: 1.0111 - accuracy: 0.5815 - val_loss: 1.0486 - val_accuracy: 0.5815\n",
      "Epoch 675/750\n",
      "162/162 [==============================] - 0s 630us/step - loss: 1.0292 - accuracy: 0.5802 - val_loss: 1.0212 - val_accuracy: 0.6296\n",
      "Epoch 676/750\n",
      "162/162 [==============================] - 0s 614us/step - loss: 1.0099 - accuracy: 0.5938 - val_loss: 1.0221 - val_accuracy: 0.5667\n",
      "Epoch 677/750\n",
      "162/162 [==============================] - 0s 577us/step - loss: 1.0149 - accuracy: 0.5877 - val_loss: 1.3074 - val_accuracy: 0.5000\n",
      "Epoch 678/750\n",
      "162/162 [==============================] - 0s 566us/step - loss: 1.0418 - accuracy: 0.5914 - val_loss: 0.9941 - val_accuracy: 0.6000\n",
      "Epoch 679/750\n",
      "162/162 [==============================] - 0s 677us/step - loss: 1.0604 - accuracy: 0.5679 - val_loss: 1.0325 - val_accuracy: 0.6185\n",
      "Epoch 680/750\n",
      "162/162 [==============================] - 0s 621us/step - loss: 1.0030 - accuracy: 0.5889 - val_loss: 1.0247 - val_accuracy: 0.6074\n",
      "Epoch 681/750\n",
      "162/162 [==============================] - 0s 607us/step - loss: 1.0024 - accuracy: 0.5889 - val_loss: 1.0144 - val_accuracy: 0.6148\n",
      "Epoch 682/750\n",
      "162/162 [==============================] - 0s 649us/step - loss: 1.0003 - accuracy: 0.5728 - val_loss: 1.0832 - val_accuracy: 0.5407\n",
      "Epoch 683/750\n",
      "162/162 [==============================] - 0s 586us/step - loss: 1.0549 - accuracy: 0.5667 - val_loss: 1.1930 - val_accuracy: 0.4259\n",
      "Epoch 684/750\n",
      "162/162 [==============================] - 0s 611us/step - loss: 1.0019 - accuracy: 0.5716 - val_loss: 0.9990 - val_accuracy: 0.6037\n",
      "Epoch 685/750\n",
      "162/162 [==============================] - 0s 627us/step - loss: 1.0253 - accuracy: 0.5716 - val_loss: 1.0425 - val_accuracy: 0.5815\n",
      "Epoch 686/750\n",
      "162/162 [==============================] - 0s 672us/step - loss: 1.0043 - accuracy: 0.5901 - val_loss: 1.0520 - val_accuracy: 0.5704\n",
      "Epoch 687/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0231 - accuracy: 0.5877 - val_loss: 1.1035 - val_accuracy: 0.5481\n",
      "Epoch 688/750\n",
      "162/162 [==============================] - 0s 667us/step - loss: 1.0092 - accuracy: 0.5778 - val_loss: 1.1996 - val_accuracy: 0.4963\n",
      "Epoch 689/750\n",
      "162/162 [==============================] - 0s 673us/step - loss: 1.0065 - accuracy: 0.5963 - val_loss: 1.0078 - val_accuracy: 0.5889\n",
      "Epoch 690/750\n",
      "162/162 [==============================] - 0s 606us/step - loss: 1.0619 - accuracy: 0.5716 - val_loss: 1.0498 - val_accuracy: 0.6000\n",
      "Epoch 691/750\n",
      "162/162 [==============================] - 0s 685us/step - loss: 1.0024 - accuracy: 0.5802 - val_loss: 1.2472 - val_accuracy: 0.4630\n",
      "Epoch 692/750\n",
      "162/162 [==============================] - 0s 591us/step - loss: 1.0800 - accuracy: 0.5580 - val_loss: 1.1808 - val_accuracy: 0.5148\n",
      "Epoch 693/750\n",
      "162/162 [==============================] - 0s 634us/step - loss: 1.0288 - accuracy: 0.5704 - val_loss: 1.0434 - val_accuracy: 0.5852\n",
      "Epoch 694/750\n",
      "162/162 [==============================] - 0s 693us/step - loss: 1.0438 - accuracy: 0.5568 - val_loss: 1.0409 - val_accuracy: 0.6185\n",
      "Epoch 695/750\n",
      "162/162 [==============================] - 0s 634us/step - loss: 1.0192 - accuracy: 0.5914 - val_loss: 1.0415 - val_accuracy: 0.5704\n",
      "Epoch 696/750\n",
      "162/162 [==============================] - 0s 689us/step - loss: 1.0469 - accuracy: 0.5716 - val_loss: 1.0382 - val_accuracy: 0.5778\n",
      "Epoch 697/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0011 - accuracy: 0.5802 - val_loss: 1.1301 - val_accuracy: 0.5852\n",
      "Epoch 698/750\n",
      "162/162 [==============================] - 0s 552us/step - loss: 1.0169 - accuracy: 0.5778 - val_loss: 1.1310 - val_accuracy: 0.5000\n",
      "Epoch 699/750\n",
      "162/162 [==============================] - 0s 621us/step - loss: 0.9994 - accuracy: 0.5815 - val_loss: 0.9857 - val_accuracy: 0.6185\n",
      "Epoch 700/750\n",
      "162/162 [==============================] - 0s 687us/step - loss: 1.0180 - accuracy: 0.5741 - val_loss: 1.0439 - val_accuracy: 0.6111\n",
      "Epoch 701/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0639 - accuracy: 0.5765 - val_loss: 1.0151 - val_accuracy: 0.6222\n",
      "Epoch 702/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0307 - accuracy: 0.5963 - val_loss: 1.0820 - val_accuracy: 0.5407\n",
      "Epoch 703/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0273 - accuracy: 0.5827 - val_loss: 1.0281 - val_accuracy: 0.5481\n",
      "Epoch 704/750\n",
      "162/162 [==============================] - 0s 718us/step - loss: 1.0056 - accuracy: 0.5753 - val_loss: 1.0032 - val_accuracy: 0.6148\n",
      "Epoch 705/750\n",
      "162/162 [==============================] - 0s 688us/step - loss: 1.0162 - accuracy: 0.5840 - val_loss: 1.1247 - val_accuracy: 0.5074\n",
      "Epoch 706/750\n",
      "162/162 [==============================] - 0s 643us/step - loss: 1.0324 - accuracy: 0.5827 - val_loss: 0.9991 - val_accuracy: 0.5926\n",
      "Epoch 707/750\n",
      "162/162 [==============================] - 0s 681us/step - loss: 1.0046 - accuracy: 0.5827 - val_loss: 0.9906 - val_accuracy: 0.5815\n",
      "Epoch 708/750\n",
      "162/162 [==============================] - 0s 601us/step - loss: 1.0047 - accuracy: 0.5827 - val_loss: 0.9921 - val_accuracy: 0.5963\n",
      "Epoch 709/750\n",
      "162/162 [==============================] - 0s 627us/step - loss: 1.0054 - accuracy: 0.5827 - val_loss: 1.0103 - val_accuracy: 0.5889\n",
      "Epoch 710/750\n",
      "162/162 [==============================] - 0s 644us/step - loss: 1.0201 - accuracy: 0.5901 - val_loss: 1.0287 - val_accuracy: 0.5963\n",
      "Epoch 711/750\n",
      "162/162 [==============================] - 0s 724us/step - loss: 1.0024 - accuracy: 0.5802 - val_loss: 1.0013 - val_accuracy: 0.6037\n",
      "Epoch 712/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0236 - accuracy: 0.5963 - val_loss: 1.0108 - val_accuracy: 0.5704\n",
      "Epoch 713/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0415 - accuracy: 0.5802 - val_loss: 1.0690 - val_accuracy: 0.6000\n",
      "Epoch 714/750\n",
      "162/162 [==============================] - 0s 691us/step - loss: 1.0276 - accuracy: 0.5901 - val_loss: 0.9961 - val_accuracy: 0.6333\n",
      "Epoch 715/750\n",
      "162/162 [==============================] - 0s 719us/step - loss: 0.9997 - accuracy: 0.6086 - val_loss: 0.9946 - val_accuracy: 0.6074\n",
      "Epoch 716/750\n",
      "162/162 [==============================] - 0s 653us/step - loss: 1.0171 - accuracy: 0.5815 - val_loss: 1.0416 - val_accuracy: 0.6111\n",
      "Epoch 717/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 0.9984 - accuracy: 0.5926 - val_loss: 1.0086 - val_accuracy: 0.6481\n",
      "Epoch 718/750\n",
      "162/162 [==============================] - 0s 722us/step - loss: 1.0208 - accuracy: 0.5889 - val_loss: 0.9813 - val_accuracy: 0.5963\n",
      "Epoch 719/750\n",
      "162/162 [==============================] - 0s 695us/step - loss: 1.0183 - accuracy: 0.5691 - val_loss: 1.0160 - val_accuracy: 0.5926\n",
      "Epoch 720/750\n",
      "162/162 [==============================] - 0s 690us/step - loss: 0.9952 - accuracy: 0.6000 - val_loss: 1.0073 - val_accuracy: 0.5852\n",
      "Epoch 721/750\n",
      "162/162 [==============================] - 0s 587us/step - loss: 1.0308 - accuracy: 0.5840 - val_loss: 1.0129 - val_accuracy: 0.6407\n",
      "Epoch 722/750\n",
      "162/162 [==============================] - 0s 781us/step - loss: 1.0496 - accuracy: 0.5827 - val_loss: 1.1186 - val_accuracy: 0.5593\n",
      "Epoch 723/750\n",
      "162/162 [==============================] - 0s 676us/step - loss: 1.0342 - accuracy: 0.5877 - val_loss: 0.9902 - val_accuracy: 0.6222\n",
      "Epoch 724/750\n",
      "162/162 [==============================] - 0s 703us/step - loss: 1.0163 - accuracy: 0.5840 - val_loss: 1.0080 - val_accuracy: 0.6185\n",
      "Epoch 725/750\n",
      "162/162 [==============================] - 0s 771us/step - loss: 1.0233 - accuracy: 0.5852 - val_loss: 0.9939 - val_accuracy: 0.6148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 726/750\n",
      "162/162 [==============================] - 0s 640us/step - loss: 1.0112 - accuracy: 0.5938 - val_loss: 0.9795 - val_accuracy: 0.6074\n",
      "Epoch 727/750\n",
      "162/162 [==============================] - 0s 773us/step - loss: 0.9791 - accuracy: 0.5951 - val_loss: 1.0009 - val_accuracy: 0.5852\n",
      "Epoch 728/750\n",
      "162/162 [==============================] - 0s 742us/step - loss: 0.9980 - accuracy: 0.5827 - val_loss: 1.1036 - val_accuracy: 0.6037\n",
      "Epoch 729/750\n",
      "162/162 [==============================] - 0s 661us/step - loss: 1.0504 - accuracy: 0.5580 - val_loss: 1.1296 - val_accuracy: 0.5481\n",
      "Epoch 730/750\n",
      "162/162 [==============================] - 0s 722us/step - loss: 1.0329 - accuracy: 0.5778 - val_loss: 1.0640 - val_accuracy: 0.5593\n",
      "Epoch 731/750\n",
      "162/162 [==============================] - 0s 692us/step - loss: 1.0372 - accuracy: 0.5827 - val_loss: 0.9820 - val_accuracy: 0.5889\n",
      "Epoch 732/750\n",
      "162/162 [==============================] - 0s 672us/step - loss: 0.9799 - accuracy: 0.6086 - val_loss: 1.0155 - val_accuracy: 0.5926\n",
      "Epoch 733/750\n",
      "162/162 [==============================] - 0s 613us/step - loss: 1.0324 - accuracy: 0.5593 - val_loss: 1.1018 - val_accuracy: 0.5519\n",
      "Epoch 734/750\n",
      "162/162 [==============================] - 0s 647us/step - loss: 1.0424 - accuracy: 0.5568 - val_loss: 1.0185 - val_accuracy: 0.6074\n",
      "Epoch 735/750\n",
      "162/162 [==============================] - 0s 634us/step - loss: 0.9934 - accuracy: 0.6074 - val_loss: 1.0088 - val_accuracy: 0.5778\n",
      "Epoch 736/750\n",
      "162/162 [==============================] - 0s 588us/step - loss: 1.0323 - accuracy: 0.5605 - val_loss: 1.0330 - val_accuracy: 0.5889\n",
      "Epoch 737/750\n",
      "162/162 [==============================] - 0s 667us/step - loss: 1.0016 - accuracy: 0.5938 - val_loss: 1.1361 - val_accuracy: 0.5519\n",
      "Epoch 738/750\n",
      "162/162 [==============================] - 0s 658us/step - loss: 1.0321 - accuracy: 0.5852 - val_loss: 1.0591 - val_accuracy: 0.5926\n",
      "Epoch 739/750\n",
      "162/162 [==============================] - 0s 618us/step - loss: 0.9926 - accuracy: 0.5975 - val_loss: 1.0963 - val_accuracy: 0.5481\n",
      "Epoch 740/750\n",
      "162/162 [==============================] - 0s 560us/step - loss: 1.0091 - accuracy: 0.5815 - val_loss: 1.1537 - val_accuracy: 0.5185\n",
      "Epoch 741/750\n",
      "162/162 [==============================] - 0s 614us/step - loss: 1.0428 - accuracy: 0.5630 - val_loss: 1.1360 - val_accuracy: 0.5407\n",
      "Epoch 742/750\n",
      "162/162 [==============================] - 0s 588us/step - loss: 1.0317 - accuracy: 0.5617 - val_loss: 1.0258 - val_accuracy: 0.5926\n",
      "Epoch 743/750\n",
      "162/162 [==============================] - 0s 619us/step - loss: 1.0017 - accuracy: 0.5938 - val_loss: 1.0323 - val_accuracy: 0.5926\n",
      "Epoch 744/750\n",
      "162/162 [==============================] - 0s 621us/step - loss: 1.0086 - accuracy: 0.5765 - val_loss: 1.0477 - val_accuracy: 0.5852\n",
      "Epoch 745/750\n",
      "162/162 [==============================] - 0s 636us/step - loss: 1.0024 - accuracy: 0.5901 - val_loss: 1.0015 - val_accuracy: 0.5815\n",
      "Epoch 746/750\n",
      "162/162 [==============================] - 0s 693us/step - loss: 1.0314 - accuracy: 0.5654 - val_loss: 1.1378 - val_accuracy: 0.4667\n",
      "Epoch 747/750\n",
      "162/162 [==============================] - 0s 641us/step - loss: 1.0388 - accuracy: 0.5753 - val_loss: 1.0425 - val_accuracy: 0.5889\n",
      "Epoch 748/750\n",
      "162/162 [==============================] - 0s 735us/step - loss: 0.9890 - accuracy: 0.5951 - val_loss: 1.0854 - val_accuracy: 0.5741\n",
      "Epoch 749/750\n",
      "162/162 [==============================] - 0s 660us/step - loss: 1.0388 - accuracy: 0.5593 - val_loss: 1.0130 - val_accuracy: 0.6000\n",
      "Epoch 750/750\n",
      "162/162 [==============================] - 0s 642us/step - loss: 1.0285 - accuracy: 0.5704 - val_loss: 0.9879 - val_accuracy: 0.6222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.33      0.43        30\n",
      "           1       0.62      0.97      0.76        40\n",
      "           2       0.33      0.35      0.34        40\n",
      "           3       0.73      0.47      0.58        40\n",
      "           4       0.63      0.65      0.64        40\n",
      "           5       0.57      0.53      0.55        40\n",
      "           6       0.89      1.00      0.94        40\n",
      "\n",
      "    accuracy                           0.63       270\n",
      "   macro avg       0.63      0.62      0.61       270\n",
      "weighted avg       0.63      0.63      0.61       270\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 750\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.25)\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Models in C code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models in Pipeline\n",
    "modelsInPipeline = []\n",
    "modelsInPipeline.append('LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    prepath = 'exportedModels/'\n",
    "    path = prepath + name + '.h'\n",
    "    if name in modelsInPipeline:\n",
    "        model = model[1]\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(port(model, optimize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
