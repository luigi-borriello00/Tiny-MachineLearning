{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Comparison for TinyML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "import pickle\n",
    "from pandas import read_csv\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,  classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold,StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dense, Input, concatenate, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from micromlgen import port\n",
    "import tinymlgen as tiny\n",
    "\n",
    "import warnings\n",
    "import seaborn as sbs\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['../data/X-intensity.pkl', '../data/X-all.pkl', '../data/X-10-25.pkl', '../data/X-1-2.pkl', '../data/X-25_50-50_25.pkl', '../data/X-100-150-200.pkl']\n",
    "labels = ['../data/y-intensity.pkl', '../data/y-all.pkl', '../data/y-10-25.pkl', '../data/y-1-2.pkl', '../data/y-25_50-50_25.pkl', '../data/y-100-150-200.pkl']\n",
    "choosenIndex = 5\n",
    "tasks = ['intensity', 'all','10-25','1-2', '25-50', '100-200']\n",
    "with open(data[choosenIndex], 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open(labels[choosenIndex], 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if choosenIndex == 1:\n",
    "    X = X[165:-13]\n",
    "    y = y[165:-13]\n",
    "if choosenIndex == 2:\n",
    "    X = X[146:-13]\n",
    "    y = y[146:-13]\n",
    "if choosenIndex == 3:\n",
    "    X = X[101:-13]\n",
    "    y = y[101:-13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4666, 60)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)\n",
    "uniques = np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04  0.12  0.96  0.01  0.03  1.01 -0.    0.1   0.99 -0.02  0.01  0.97\n",
      "  -0.02  0.08  0.97  0.05  0.13  0.98 -0.01  0.08  0.98  0.01  0.11  0.96\n",
      "  -0.04  0.06  0.99 -0.03  0.11  0.95 -0.01  0.12  0.98  0.    0.08  0.96\n",
      "  -0.03  0.06  0.95  0.04  0.1   0.99  0.03  0.08  0.99  0.03  0.1   0.96\n",
      "   0.01  0.09  1.   -0.01  0.08  1.01 -0.03  0.04  1.03 -0.04  0.07  0.97]\n",
      " [ 0.06  0.13  0.98 -0.01  0.09  0.97 -0.07  0.09  0.95 -0.01  0.07  0.95\n",
      "   0.03  0.13  0.97 -0.04  0.06  0.98  0.05  0.12  0.99  0.01  0.07  0.98\n",
      "  -0.03  0.12  0.98 -0.03  0.06  0.96  0.01  0.07  0.99  0.03  0.09  1.01\n",
      "  -0.02  0.07  0.96 -0.03  0.04  0.99 -0.01  0.08  1.02 -0.05  0.02  0.97\n",
      "   0.02  0.11  0.95 -0.02  0.11  0.95 -0.    0.1   1.02 -0.03  0.12  0.94]\n",
      " [ 0.    0.01  1.02  0.02  0.01  0.96  0.1   0.04  0.99 -0.02 -0.03  0.94\n",
      "   0.02 -0.    0.98  0.02  0.01  1.    0.04 -0.02  0.93  0.05  0.    0.94\n",
      "  -0.03 -0.01  0.98 -0.01  0.01  1.02  0.08  0.    0.91  0.02  0.03  1.\n",
      "   0.02  0.01  0.99  0.02 -0.    0.98 -0.04  0.    1.03 -0.02  0.01  1.02\n",
      "   0.01 -0.    0.96  0.09  0.03  0.99 -0.02 -0.05  0.89  0.02 -0.    0.98]\n",
      " [-0.06 -0.02  0.98 -0.06  0.02  0.98 -0.04 -0.03  0.98 -0.06  0.03  0.99\n",
      "  -0.05 -0.    0.98 -0.05 -0.02  0.98 -0.05 -0.02  0.98 -0.04  0.01  0.99\n",
      "  -0.02 -0.02  0.98 -0.06  0.    0.98 -0.06 -0.    0.98 -0.03 -0.01  0.97\n",
      "  -0.07  0.02  0.98 -0.05  0.03  0.99 -0.09 -0.01  0.97 -0.05 -0.    0.99\n",
      "  -0.09  0.03  0.99 -0.06  0.02  0.99 -0.06  0.01  0.98 -0.03 -0.01  0.98]\n",
      " [ 0.03  0.11  0.99  0.    0.11  0.99  0.03  0.14  0.98  0.03  0.1   1.\n",
      "   0.03  0.1   1.    0.03  0.1   0.96  0.03  0.14  0.99 -0.    0.09  0.94\n",
      "   0.05  0.11  1.03 -0.04  0.06  0.99 -0.05  0.09  0.94 -0.    0.11  1.01\n",
      "  -0.05  0.13  0.97  0.02  0.1   0.96  0.06  0.12  1.   -0.01  0.1   1.\n",
      "   0.01  0.07  1.04  0.01  0.1   0.99 -0.05  0.05  0.98 -0.02  0.07  0.97]\n",
      " [-0.04  0.01  0.99 -0.06  0.03  0.99 -0.05  0.    0.98 -0.06 -0.    0.99\n",
      "  -0.05 -0.02  0.98 -0.07  0.01  0.98 -0.03  0.    0.98 -0.03  0.03  0.99\n",
      "  -0.05 -0.    0.99 -0.03 -0.04  0.97 -0.05  0.02  0.99 -0.05 -0.01  0.98\n",
      "  -0.06  0.03  0.99 -0.08  0.    0.98 -0.05 -0.    0.99 -0.02 -0.03  0.98\n",
      "  -0.04 -0.01  0.98 -0.04  0.01  0.99 -0.06 -0.03  0.98 -0.05 -0.    0.98]\n",
      " [-0.06  0.06  0.95  0.    0.11  1.01 -0.07  0.08  0.98 -0.04  0.07  0.98\n",
      "   0.05  0.15  0.99 -0.01  0.11  1.01  0.04  0.16  1.01  0.01  0.11  0.96\n",
      "  -0.03  0.03  0.99 -0.    0.11  0.99  0.02  0.14  0.99 -0.    0.09  0.96\n",
      "  -0.01  0.11  0.96  0.01  0.12  0.99  0.01  0.09  0.96 -0.01  0.09  0.95\n",
      "   0.04  0.1   0.99  0.03  0.08  1.03  0.03  0.1   0.97 -0.01  0.07  1.  ]\n",
      " [ 0.02  0.01  1.    0.04 -0.02  0.93  0.05  0.    0.94 -0.03 -0.01  0.98\n",
      "  -0.01  0.01  1.02  0.08  0.    0.91  0.02  0.03  1.    0.02  0.01  0.99\n",
      "   0.02 -0.    0.98 -0.04  0.    1.03 -0.02  0.01  1.02  0.01 -0.    0.96\n",
      "   0.09  0.03  0.99 -0.02 -0.05  0.89  0.02 -0.    0.98  0.02  0.01  0.98\n",
      "   0.04  0.02  0.99 -0.05 -0.01  0.96 -0.03  0.    1.    0.06  0.02  0.99]\n",
      " [ 0.03  0.16  0.97 -0.01  0.11  1.01  0.01  0.17  0.99 -0.04  0.07  0.98\n",
      "  -0.    0.03  1.   -0.01  0.09  0.98  0.02  0.15  0.96 -0.04  0.06  0.99\n",
      "  -0.05  0.08  0.95 -0.01  0.12  0.99 -0.01  0.06  1.02 -0.03  0.06  0.96\n",
      "   0.04  0.11  0.97  0.04  0.16  0.98  0.02  0.1   0.96  0.03  0.13  0.99\n",
      "   0.    0.1   0.95  0.05  0.12  1.04  0.03  0.12  0.97 -0.06  0.06  0.95]\n",
      " [ 0.02 -0.01  0.99  0.02 -0.01  0.99 -0.   -0.07  0.98  0.   -0.01  0.96\n",
      "   0.01 -0.    1.    0.06  0.01  1.03  0.08 -0.01  0.98  0.03  0.01  0.97\n",
      "   0.02  0.    0.99  0.02 -0.    0.98  0.05 -0.01  0.98  0.02  0.    1.01\n",
      "  -0.02 -0.02  0.94  0.   -0.01  0.96  0.07 -0.01  0.97  0.02 -0.01  0.98\n",
      "   0.02 -0.01  0.98  0.02 -0.02  0.98  0.09  0.01  1.01 -0.03 -0.02  0.96]\n",
      " [ 0.03  0.05  1.01 -0.01  0.1   1.   -0.    0.05  1.02  0.    0.09  0.99\n",
      "  -0.07  0.06  0.97 -0.03  0.08  0.98  0.02  0.14  0.99 -0.01  0.08  0.96\n",
      "  -0.05  0.07  0.96 -0.03  0.11  0.97  0.01  0.1   0.99 -0.04  0.04  0.98\n",
      "  -0.02  0.1   0.95 -0.06  0.06  0.92 -0.04  0.08  0.99 -0.01  0.1   0.98\n",
      "  -0.02  0.07  0.94 -0.    0.13  0.93 -0.05  0.06  0.99  0.02  0.07  1.01]\n",
      " [-0.04 -0.01  0.98 -0.02 -0.01  0.98 -0.06  0.03  0.99 -0.05  0.    0.98\n",
      "  -0.04 -0.02  0.98 -0.04  0.01  0.98 -0.06  0.01  0.98 -0.04 -0.02  0.98\n",
      "  -0.05 -0.01  0.98 -0.05  0.01  0.98 -0.08  0.01  0.99 -0.04 -0.    0.98\n",
      "  -0.05 -0.02  0.98 -0.02 -0.01  0.99 -0.05  0.    0.98 -0.06 -0.04  0.97\n",
      "  -0.06 -0.01  0.98 -0.05 -0.01  0.98 -0.06  0.03  0.99 -0.06  0.04  0.99]\n",
      " [-0.01 -0.01  0.99  0.05  0.01  1.03 -0.04 -0.01  0.98  0.02 -0.03  0.99\n",
      "   0.02 -0.    0.98  0.02 -0.    0.98  0.05 -0.01  0.98 -0.02 -0.02  0.94\n",
      "   0.07  0.01  0.99 -0.02 -0.01  0.97  0.07 -0.    0.97  0.03  0.01  0.97\n",
      "   0.02 -0.03  0.99  0.02  0.01  0.99  0.1   0.01  1.    0.07  0.01  0.98\n",
      "  -0.01 -0.02  0.94  0.08  0.    0.99  0.04  0.05  0.97  0.02 -0.02  0.99]\n",
      " [-0.03  0.11  0.95 -0.01  0.12  0.98  0.    0.08  0.96 -0.03  0.06  0.95\n",
      "   0.04  0.1   0.99  0.03  0.08  0.99  0.03  0.1   0.96  0.01  0.09  1.\n",
      "  -0.01  0.08  1.01 -0.03  0.04  1.03 -0.04  0.07  0.97 -0.01  0.14  0.95\n",
      "  -0.    0.1   1.03 -0.03  0.12  0.94  0.02  0.1   0.98 -0.04  0.01  0.99\n",
      "  -0.01  0.07  0.93 -0.01  0.11  0.91  0.01  0.09  0.98  0.04  0.12  0.98]\n",
      " [-0.    0.07  0.97 -0.    0.07  1.    0.01  0.1   0.95  0.03  0.12  0.97\n",
      "  -0.02  0.11  0.99 -0.03  0.05  1.    0.02  0.11  0.96 -0.05  0.07  0.97\n",
      "  -0.03  0.07  0.97  0.03  0.1   0.97 -0.    0.07  1.   -0.03  0.07  1.01\n",
      "   0.    0.06  1.03 -0.03  0.07  0.99 -0.03  0.04  1.    0.    0.1   0.99\n",
      "   0.03  0.09  1.04 -0.04  0.1   0.98 -0.07  0.03  0.96 -0.01  0.08  0.94]\n",
      " [-0.05 -0.    0.98 -0.06 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.99]\n",
      " [-0.03 -0.01  0.97  0.08 -0.    0.9   0.04  0.02  1.01  0.02  0.01  0.98\n",
      "   0.02  0.01  0.99  0.09  0.01  0.94  0.08  0.01  0.96  0.02  0.01  1.01\n",
      "   0.1   0.03  0.99  0.07  0.02  0.95  0.03  0.02  0.98  0.02  0.01  0.99\n",
      "   0.02  0.02  1.    0.09  0.03  1.    0.08  0.02  0.98  0.02  0.    0.96\n",
      "   0.06  0.03  0.98  0.03 -0.01  0.96  0.02  0.01  0.98  0.02  0.01  0.98]\n",
      " [-0.07  0.04  1.01  0.03  0.11  0.95  0.03  0.17  0.97 -0.    0.11  1.02\n",
      "   0.03  0.17  1.02 -0.01  0.09  0.99  0.02  0.03  1.01 -0.01  0.1   0.99\n",
      "  -0.    0.06  1.03 -0.03  0.08  0.98 -0.03  0.05  0.99 -0.02  0.1   0.98\n",
      "  -0.04  0.14  0.99 -0.01  0.08  0.96 -0.05  0.09  0.95 -0.03  0.09  0.97\n",
      "  -0.03  0.05  0.98  0.02  0.11  0.99 -0.03  0.1   0.95 -0.02  0.13  0.97]\n",
      " [-0.04 -0.02  0.96  0.06  0.01  1.03  0.05 -0.    0.97 -0.01 -0.03  0.98\n",
      "   0.02 -0.01  0.98  0.02 -0.01  0.99  0.02 -0.01  0.98  0.03 -0.01  0.98\n",
      "   0.07  0.01  0.99  0.07  0.01  1.02  0.01 -0.    0.99  0.02 -0.04  0.98\n",
      "   0.02 -0.01  0.98  0.02 -0.01  0.97  0.02 -0.02  1.    0.07  0.01  1.\n",
      "   0.   -0.    1.    0.06  0.    0.97 -0.03 -0.01  0.97  0.07  0.02  0.97]\n",
      " [-0.   -0.02  0.94  0.03  0.01  1.03  0.07  0.01  1.01  0.06  0.01  0.97\n",
      "   0.02 -0.01  0.98  0.02 -0.01  0.97 -0.01 -0.04  1.01  0.09  0.01  1.01\n",
      "  -0.02 -0.01  0.97  0.02  0.    1.02  0.01 -0.03  0.98  0.02  0.02  0.99\n",
      "   0.02 -0.01  0.98  0.02 -0.    0.97  0.06  0.02  1.    0.09  0.01  1.01\n",
      "   0.07  0.01  1.01 -0.04 -0.02  0.94 -0.03  0.    0.99  0.02 -0.02  0.98]\n",
      " [-0.04 -0.01  0.98 -0.06 -0.01  0.98 -0.02 -0.02  0.98 -0.07  0.02  0.99\n",
      "  -0.06  0.03  0.98 -0.03 -0.01  0.98 -0.04  0.02  0.99 -0.06 -0.02  0.98\n",
      "  -0.01 -0.01  0.98 -0.06  0.    0.98 -0.06  0.01  0.99 -0.07 -0.02  0.98\n",
      "  -0.05  0.02  0.99 -0.04  0.01  0.99 -0.06  0.03  0.99 -0.05  0.    0.98\n",
      "  -0.03  0.03  0.99 -0.05  0.03  0.99 -0.06  0.01  0.99 -0.04 -0.02  0.98]\n",
      " [-0.08  0.03  0.97 -0.05  0.06  0.99  0.05  0.16  0.98 -0.    0.09  1.\n",
      "   0.02  0.17  1.    0.04  0.11  0.96  0.03  0.05  1.01 -0.01  0.1   1.\n",
      "  -0.    0.05  1.02  0.    0.09  0.99 -0.07  0.06  0.97 -0.03  0.08  0.98\n",
      "   0.02  0.14  0.99 -0.01  0.08  0.96 -0.05  0.07  0.96 -0.03  0.11  0.97\n",
      "   0.01  0.1   0.99 -0.04  0.04  0.98 -0.02  0.1   0.95 -0.06  0.06  0.92]\n",
      " [ 0.02  0.11  1.    0.01  0.09  1.01  0.02  0.08  1.04 -0.04  0.06  0.98\n",
      "  -0.04  0.11  0.94 -0.    0.11  1.02 -0.08  0.04  0.99 -0.03  0.07  0.98\n",
      "   0.05  0.15  0.99 -0.    0.09  1.01  0.04  0.13  1.03 -0.03  0.07  0.99\n",
      "  -0.07  0.05  0.96  0.    0.08  0.96  0.03  0.14  0.96  0.03  0.12  0.96\n",
      "   0.02  0.06  1.01 -0.02  0.09  0.98  0.01  0.13  0.99 -0.01  0.09  0.96]\n",
      " [ 0.02 -0.    0.99  0.02  0.    0.99 -0.01  0.03  1.04  0.02  0.02  1.02\n",
      "   0.05  0.    0.95 -0.02 -0.    0.97 -0.02  0.02  1.06  0.01 -0.    0.96\n",
      "   0.02  0.01  0.98  0.02 -0.    0.98  0.09  0.01  0.91  0.01  0.02  1.02\n",
      "  -0.03  0.    1.   -0.04 -0.    1.01 -0.03 -0.01  0.99  0.02  0.02  0.98\n",
      "   0.03  0.01  0.98  0.03 -0.    0.91  0.09  0.01  0.95  0.02  0.01  1.02]\n",
      " [-0.04 -0.    0.99  0.04  0.02  1.01 -0.02 -0.02  0.96 -0.   -0.02  0.95\n",
      "   0.02  0.    0.98  0.03  0.01  0.98  0.1   0.02  0.96  0.02  0.02  1.02\n",
      "   0.08  0.02  0.97  0.01 -0.01  0.95  0.06  0.05  1.05  0.03  0.01  1.\n",
      "   0.02  0.01  0.98  0.03  0.02  0.98  0.06 -0.    0.95 -0.03  0.    1.01\n",
      "   0.02  0.    0.95 -0.05 -0.01  0.98  0.02  0.05  1.07  0.02  0.01  0.98]\n",
      " [ 0.    0.12  1.   -0.02  0.07  1.01 -0.03  0.03  0.96 -0.04  0.07  0.98\n",
      "   0.02  0.13  0.96  0.03  0.1   0.96 -0.04  0.05  1.03  0.02  0.11  0.95\n",
      "   0.05  0.15  0.98 -0.01  0.08  0.99  0.04  0.13  0.93 -0.01  0.08  0.99\n",
      "  -0.05  0.02  0.98 -0.01  0.07  0.93 -0.04  0.05  0.91  0.04  0.11  0.96\n",
      "   0.    0.13  0.96  0.    0.08  0.97  0.04  0.14  0.97  0.03  0.12  0.97]\n",
      " [-0.02  0.    0.98 -0.05  0.    0.99 -0.05  0.    0.98 -0.03 -0.02  0.98\n",
      "  -0.04  0.01  0.99 -0.07  0.    0.98 -0.03  0.02  0.99 -0.06  0.    0.98\n",
      "  -0.05 -0.02  0.98 -0.08  0.01  0.98 -0.04 -0.01  0.98 -0.04 -0.02  0.98\n",
      "  -0.08 -0.02  0.97 -0.06  0.    0.99 -0.05  0.05  1.   -0.04  0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.08 -0.01  0.98 -0.05  0.02  0.99 -0.05  0.    0.98]\n",
      " [ 0.05  0.    0.95 -0.02 -0.    0.97 -0.02  0.02  1.06  0.01 -0.    0.96\n",
      "   0.02  0.01  0.98  0.02 -0.    0.98  0.09  0.01  0.91  0.01  0.02  1.02\n",
      "  -0.03  0.    1.   -0.04 -0.    1.01 -0.03 -0.01  0.99  0.02  0.02  0.98\n",
      "   0.03  0.01  0.98  0.03 -0.    0.91  0.09  0.01  0.95  0.02  0.01  1.02\n",
      "   0.08  0.01  0.96 -0.   -0.02  0.95  0.01 -0.05  0.89  0.02  0.    0.98]\n",
      " [ 0.02 -0.    0.99  0.03 -0.03  0.96  0.09  0.01  1.01 -0.   -0.01  1.\n",
      "   0.04  0.01  1.03  0.07  0.02  0.99  0.01 -0.05  1.    0.02 -0.    0.99\n",
      "   0.02  0.    0.99  0.01 -0.03  0.97 -0.03 -0.02  0.95 -0.01 -0.02  0.94\n",
      "   0.08  0.01  1.02  0.07 -0.01  0.97  0.03  0.01  0.98  0.02  0.    0.99\n",
      "   0.01 -0.03  1.    0.09  0.01  1.01  0.08  0.01  1.02 -0.01 -0.02  0.94]\n",
      " [-0.07  0.    0.98 -0.06 -0.02  0.98 -0.03  0.03  0.99 -0.05  0.    0.98\n",
      "  -0.07 -0.    0.99 -0.04  0.    0.99 -0.03 -0.    0.98 -0.07 -0.02  0.98\n",
      "  -0.03  0.03  1.   -0.05 -0.    0.98 -0.03 -0.04  0.97 -0.06 -0.02  0.98\n",
      "  -0.05  0.02  0.99 -0.06 -0.03  0.98 -0.06  0.03  1.   -0.05 -0.    0.98\n",
      "  -0.08  0.02  0.99 -0.07  0.01  0.98 -0.04 -0.01  0.98 -0.09  0.02  0.98]\n",
      " [-0.03 -0.02  0.96 -0.04 -0.03  0.98  0.02 -0.03  0.99  0.02 -0.01  0.99\n",
      "   0.02  0.    0.98  0.09  0.02  0.99  0.07  0.01  1.03 -0.02 -0.01  0.97\n",
      "  -0.01 -0.02  0.96  0.07  0.01  0.97  0.03  0.01  0.97  0.02 -0.    0.97\n",
      "   0.01 -0.01  0.99 -0.01 -0.    0.98 -0.01 -0.01  0.99  0.06  0.01  1.02\n",
      "   0.09  0.02  1.    0.04  0.03  0.97  0.02 -0.    0.98  0.02 -0.    0.97]\n",
      " [ 0.    0.11  1.02 -0.08  0.03  0.97  0.    0.1   0.96  0.04  0.16  0.98\n",
      "  -0.01  0.09  1.01  0.04  0.16  1.01  0.03  0.11  0.97  0.02  0.05  1.01\n",
      "  -0.01  0.1   1.    0.03  0.1   1.04  0.03  0.11  0.98 -0.07  0.09  0.95\n",
      "   0.    0.09  0.97 -0.03  0.04  0.98  0.01  0.1   0.99  0.05  0.12  0.98\n",
      "   0.03  0.1   0.99  0.02  0.12  0.98  0.03  0.13  0.97  0.02  0.08  1.  ]\n",
      " [-0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05  0.    0.98 -0.05  0.    0.99]\n",
      " [ 0.05  0.14  1.01 -0.01  0.08  0.96  0.02  0.08  1.01  0.01  0.11  0.96\n",
      "   0.06  0.15  1.01 -0.04  0.07  0.99 -0.02  0.03  1.    0.01  0.11  0.98\n",
      "  -0.01  0.04  1.    0.02  0.11  0.98 -0.04  0.13  0.94 -0.01  0.07  0.96\n",
      "  -0.06  0.04  0.98  0.02  0.1   0.98 -0.01  0.15  0.95  0.03  0.09  0.98\n",
      "   0.01  0.08  1.   -0.    0.12  1.    0.04  0.13  0.98  0.01  0.09  0.98]\n",
      " [-0.03 -0.02  0.98 -0.02 -0.01  0.98 -0.05 -0.    0.98 -0.11  0.    0.98\n",
      "  -0.05 -0.02  0.98 -0.04  0.01  0.99 -0.09  0.03  0.99 -0.06  0.01  0.99\n",
      "  -0.05  0.    0.98 -0.03 -0.01  0.98 -0.07  0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.02  0.02  0.99 -0.05  0.    0.98 -0.07  0.    0.99 -0.04 -0.    0.98\n",
      "  -0.04 -0.01  0.98 -0.06  0.03  0.99 -0.01  0.02  1.   -0.05 -0.    0.98]\n",
      " [-0.05  0.    0.99 -0.06 -0.01  0.98 -0.07  0.01  0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.04  0.01  0.98 -0.07  0.    0.98 -0.03 -0.01  0.99\n",
      "  -0.06 -0.    0.98 -0.05 -0.    0.99 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.99 -0.06 -0.01  0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [ 0.02  0.11  0.96  0.01  0.05  1.01 -0.01  0.1   1.   -0.03  0.06  0.98\n",
      "   0.    0.09  0.99  0.01  0.07  1.    0.    0.07  0.98 -0.    0.09  1.\n",
      "  -0.01  0.08  0.95  0.03  0.09  1.    0.05  0.13  1.01 -0.01  0.07  0.96\n",
      "   0.03  0.13  0.98  0.02  0.11  0.99  0.    0.06  1.03 -0.03  0.07  0.99\n",
      "   0.01  0.15  0.96 -0.02  0.07  0.96  0.05  0.07  0.98  0.02  0.1   0.98]\n",
      " [ 0.02  0.    0.98 -0.02 -0.01  0.99  0.1   0.02  0.97 -0.03 -0.    1.\n",
      "   0.06  0.02  1.   -0.02  0.02  1.05  0.05  0.04  1.02  0.02  0.    0.99\n",
      "   0.02  0.01  0.98  0.01 -0.02  0.93  0.02 -0.01  0.94  0.06  0.01  0.95\n",
      "   0.07  0.03  1.01 -0.01 -0.04  0.9   0.02  0.02  0.98  0.02  0.01  0.98\n",
      "   0.03  0.02  1.   -0.06 -0.01  0.99 -0.04 -0.    0.99  0.08  0.02  0.98]\n",
      " [-0.05  0.03  0.98 -0.07 -0.02  0.98 -0.04  0.02  0.99 -0.04 -0.01  0.98\n",
      "  -0.01  0.02  0.99 -0.05  0.    0.98 -0.04 -0.03  0.97 -0.04 -0.01  0.98\n",
      "  -0.04 -0.01  0.98 -0.07  0.02  0.99 -0.05  0.04  1.   -0.06  0.    0.99\n",
      "  -0.02 -0.03  0.98 -0.07 -0.02  0.98 -0.05  0.02  0.99 -0.06 -0.03  0.98\n",
      "  -0.06  0.02  1.   -0.06  0.    0.98 -0.08  0.02  0.99 -0.06  0.03  0.99]\n",
      " [-0.03  0.14  0.94 -0.01  0.07  0.97  0.03  0.16  1.01 -0.03  0.07  1.\n",
      "   0.04  0.07  1.01 -0.01  0.09  0.99 -0.02  0.05  1.03 -0.02  0.07  1.01\n",
      "  -0.01  0.06  1.   -0.01  0.09  0.98  0.01  0.13  0.99 -0.03  0.07  0.96\n",
      "   0.01  0.08  1.01  0.01  0.05  0.98 -0.04  0.06  0.98 -0.    0.08  1.01\n",
      "  -0.01  0.08  1.01 -0.02  0.05  1.03 -0.04  0.07  0.98 -0.05  0.04  0.96]\n",
      " [-0.03  0.04  1.   -0.06 -0.02  0.98 -0.07 -0.01  0.98 -0.07  0.03  0.99\n",
      "  -0.09 -0.01  0.97 -0.06  0.    0.98 -0.04  0.03  0.99 -0.07 -0.01  0.98\n",
      "  -0.04  0.01  0.99 -0.01  0.01  0.99 -0.06  0.    0.99 -0.05  0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07  0.01  0.98 -0.04 -0.02  0.98 -0.04 -0.03  0.98\n",
      "  -0.05  0.    0.98 -0.09  0.01  0.99 -0.04  0.    0.99 -0.07 -0.01  0.98]\n",
      " [ 0.    0.1   0.96 -0.03  0.12  0.95  0.    0.08  0.97  0.01  0.08  1.01\n",
      "  -0.01  0.08  1.    0.02  0.07  1.    0.02  0.07  0.99 -0.01  0.1   1.\n",
      "  -0.    0.1   0.95 -0.    0.07  1.01 -0.02  0.03  0.97  0.01  0.08  0.99\n",
      "  -0.04  0.03  0.98 -0.03  0.07  1.   -0.08  0.03  0.98  0.03  0.11  0.95\n",
      "   0.02  0.15  0.96 -0.01  0.08  0.97 -0.04  0.08  0.91  0.02  0.09  0.99]\n",
      " [-0.04  0.01  0.99 -0.07  0.01  0.98 -0.08  0.03  0.99 -0.05 -0.    0.99\n",
      "  -0.04 -0.03  0.97 -0.07 -0.02  0.98 -0.07  0.01  0.99 -0.06  0.03  0.99\n",
      "  -0.09  0.01  0.98 -0.06  0.    0.98 -0.02  0.01  1.   -0.07 -0.01  0.98\n",
      "  -0.05 -0.01  0.98 -0.02 -0.02  0.98 -0.05 -0.02  0.98 -0.06  0.    0.98\n",
      "  -0.09 -0.01  0.98 -0.05  0.02  0.99 -0.05 -0.02  0.98 -0.06 -0.03  0.98]\n",
      " [-0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.06 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.05 -0.01  0.98  0.04  0.06  1.09  0.02  0.    0.98  0.02  0.01  0.98\n",
      "  -0.02 -0.05  0.92  0.    0.01  1.02  0.01 -0.    0.96 -0.04 -0.    1.\n",
      "  -0.05 -0.01  1.01  0.02 -0.01  0.97  0.02  0.    0.99  0.02  0.    0.98\n",
      "  -0.05 -0.    1.03 -0.04 -0.    0.99 -0.03 -0.    1.    0.07  0.01  0.95\n",
      "  -0.03 -0.01  1.01  0.02 -0.01  0.98  0.02  0.01  0.98  0.03  0.02  0.99]\n",
      " [-0.07 -0.02  0.98 -0.07  0.01  0.99 -0.03  0.01  0.99 -0.07  0.01  0.99\n",
      "  -0.05 -0.    0.98 -0.05 -0.04  0.98 -0.07 -0.01  0.98 -0.05 -0.02  0.98\n",
      "  -0.09 -0.02  0.98 -0.05 -0.    0.98 -0.06 -0.    0.99 -0.05 -0.02  0.98\n",
      "  -0.04  0.01  0.99 -0.07  0.01  0.98 -0.08  0.03  0.99 -0.05 -0.    0.99\n",
      "  -0.04 -0.03  0.97 -0.07 -0.02  0.98 -0.07  0.01  0.99 -0.06  0.03  0.99]\n",
      " [ 0.1   0.03  0.98  0.05  0.05  1.04  0.02  0.01  0.98  0.02  0.01  0.97\n",
      "  -0.03 -0.05  0.9   0.09  0.02  0.98  0.07  0.01  0.95 -0.04 -0.    0.99\n",
      "   0.03 -0.03  0.9   0.02  0.01  0.97  0.02  0.01  0.98  0.02  0.01  0.98\n",
      "  -0.01  0.02  1.01  0.08  0.01  0.95  0.02  0.01  1.01  0.1   0.02  0.97\n",
      "   0.04 -0.03  0.87  0.02  0.01  0.98  0.02  0.01  0.98  0.02  0.03  1.02]\n",
      " [ 0.02  0.15  0.99 -0.04  0.08  1.   -0.    0.1   0.95  0.02  0.11  0.96\n",
      "   0.05  0.14  1.02 -0.04  0.06  1.   -0.02  0.04  1.    0.01  0.11  0.99\n",
      "   0.01  0.06  1.02  0.03  0.12  0.96 -0.05  0.11  0.95 -0.01  0.08  0.96\n",
      "  -0.08  0.07  0.98  0.01  0.09  0.98  0.04  0.15  0.98 -0.01  0.09  0.99\n",
      "   0.02  0.09  1.01 -0.02  0.09  0.97  0.01  0.07  1.   -0.01  0.08  0.98]\n",
      " [ 0.02  0.01  0.99  0.02  0.    0.99  0.04 -0.03  0.86 -0.01 -0.01  0.95\n",
      "   0.03  0.    0.95  0.07  0.03  1.01  0.06 -0.02  0.88  0.02 -0.    0.97\n",
      "   0.02  0.01  0.98  0.02 -0.01  0.98  0.01 -0.01  0.95 -0.01 -0.01  0.96\n",
      "   0.05  0.02  1.   -0.06 -0.01  0.99  0.07  0.04  1.01  0.02  0.    0.99\n",
      "   0.02  0.01  0.98  0.01  0.03  1.03 -0.02 -0.01  0.96  0.08  0.01  0.95]\n",
      " [-0.04 -0.    1.    0.04  0.01  1.01 -0.05 -0.    1.02  0.06  0.02  1.\n",
      "   0.02  0.01  0.99  0.02  0.    0.99  0.04 -0.03  0.86 -0.01 -0.01  0.95\n",
      "   0.03  0.    0.95  0.07  0.03  1.01  0.06 -0.02  0.88  0.02 -0.    0.97\n",
      "   0.02  0.01  0.98  0.02 -0.01  0.98  0.01 -0.01  0.95 -0.01 -0.01  0.96\n",
      "   0.05  0.02  1.   -0.06 -0.01  0.99  0.07  0.04  1.01  0.02  0.    0.99]\n",
      " [-0.01 -0.01  0.99 -0.01 -0.03  0.98  0.01 -0.02  1.    0.02 -0.    0.98\n",
      "   0.02 -0.    0.98 -0.02  0.04  0.99 -0.03 -0.02  0.94  0.03 -0.01  0.95\n",
      "   0.08  0.01  1.02 -0.01 -0.05  0.99  0.02  0.01  0.98  0.02 -0.01  0.97\n",
      "   0.01 -0.03  1.    0.07 -0.01  0.99 -0.03 -0.02  0.94  0.04  0.01  1.03\n",
      "   0.05  0.02  0.99 -0.    0.01  1.    0.02  0.01  0.98  0.02 -0.    0.98]\n",
      " [ 0.09  0.02  1.    0.03 -0.02  0.98  0.02  0.01  0.98  0.02 -0.    0.99\n",
      "  -0.04 -0.03  0.99  0.01 -0.    1.    0.02  0.    1.02  0.04 -0.    0.97\n",
      "   0.04 -0.03  0.98  0.03  0.01  0.98  0.02 -0.    0.99  0.02 -0.    0.97\n",
      "  -0.03 -0.02  0.96 -0.03 -0.02  0.96  0.07  0.    0.98 -0.05 -0.02  0.97\n",
      "   0.06  0.04  0.98  0.03 -0.02  0.99  0.02 -0.    0.99 -0.   -0.05  0.99]\n",
      " [ 0.02 -0.    0.99  0.02 -0.    0.99  0.1   0.    0.98  0.08  0.01  1.02\n",
      "   0.05  0.01  1.03 -0.04 -0.02  0.95  0.05  0.04  0.98  0.02 -0.01  0.99\n",
      "   0.02 -0.    0.99  0.02  0.    0.97  0.08  0.    0.99  0.04 -0.    0.96\n",
      "   0.02  0.    1.02 -0.03 -0.02  0.97  0.05  0.05  0.98  0.02  0.    0.98\n",
      "   0.02 -0.    0.98 -0.01 -0.02  1.   -0.03 -0.01  0.97  0.07  0.01  1.03]\n",
      " [ 0.05  0.07  1.01  0.01  0.07  0.97 -0.03  0.03  1.01  0.03  0.06  1.\n",
      "   0.05  0.11  0.99  0.02  0.08  0.98 -0.02  0.1   1.   -0.02  0.08  0.95\n",
      "   0.03  0.1   1.    0.05  0.14  1.01 -0.01  0.08  0.96  0.02  0.08  1.01\n",
      "   0.01  0.11  0.96  0.06  0.15  1.01 -0.04  0.07  0.99 -0.02  0.03  1.\n",
      "   0.01  0.11  0.98 -0.01  0.04  1.    0.02  0.11  0.98 -0.04  0.13  0.94]\n",
      " [-0.04 -0.    0.98 -0.02 -0.    0.99 -0.08 -0.02  0.97 -0.06  0.    0.98\n",
      "  -0.03 -0.02  0.98 -0.06  0.02  0.99 -0.07 -0.01  0.98 -0.02 -0.02  0.98\n",
      "  -0.06 -0.    0.99 -0.06  0.01  0.99 -0.08  0.    0.99 -0.05  0.02  0.99\n",
      "  -0.05 -0.02  0.98 -0.05  0.04  1.   -0.05  0.    0.99 -0.02 -0.03  0.97\n",
      "  -0.04  0.02  0.99 -0.05 -0.01  0.98 -0.07  0.03  0.99 -0.07 -0.03  0.97]\n",
      " [ 0.06  0.01  0.95 -0.02 -0.    0.97 -0.04 -0.02  0.98  0.03  0.01  0.99\n",
      "   0.02  0.    0.97  0.02  0.    0.98  0.04  0.02  1.01 -0.04 -0.    1.\n",
      "   0.02 -0.01  0.98 -0.04 -0.01  0.97  0.04 -0.    0.96  0.07  0.01  1.01\n",
      "   0.09  0.02  1.01  0.06  0.04  0.97  0.02 -0.01  0.98  0.02 -0.    0.98\n",
      "   0.01 -0.03  0.99 -0.03 -0.02  0.95  0.08  0.01  1.   -0.01 -0.02  0.94]\n",
      " [-0.03  0.02  0.99 -0.04  0.05  0.98  0.04  0.1   1.    0.01  0.06  0.98\n",
      "  -0.03  0.06  0.97  0.03  0.13  0.98  0.01  0.1   1.01 -0.03  0.02  0.99\n",
      "  -0.01  0.08  0.99 -0.03  0.11  0.95 -0.02  0.08  1.   -0.05  0.07  0.91\n",
      "  -0.04  0.06  0.98  0.04  0.09  1.01 -0.01  0.09  0.98 -0.02 -0.    0.95\n",
      "  -0.05  0.06  0.99 -0.03  0.13  0.95 -0.    0.07  0.96 -0.    0.07  1.  ]\n",
      " [-0.05 -0.03  0.98 -0.06 -0.01  0.99 -0.09  0.02  0.99 -0.06 -0.01  0.98\n",
      "  -0.07 -0.01  0.98 -0.03 -0.01  0.98 -0.03 -0.02  0.98 -0.05 -0.01  0.98\n",
      "  -0.1  -0.02  0.97 -0.08 -0.01  0.98 -0.05  0.01  0.99 -0.09  0.    0.99\n",
      "  -0.03  0.01  0.98 -0.05 -0.    0.98 -0.04  0.02  0.99 -0.04  0.02  0.99\n",
      "  -0.06 -0.02  0.98 -0.02  0.02  0.99 -0.05  0.    0.98 -0.07 -0.    0.99]\n",
      " [ 0.02  0.01  0.98 -0.02 -0.05  0.92  0.    0.01  1.02  0.01 -0.    0.96\n",
      "  -0.04 -0.    1.   -0.05 -0.01  1.01  0.02 -0.01  0.97  0.02  0.    0.99\n",
      "   0.02  0.    0.98 -0.05 -0.    1.03 -0.04 -0.    0.99 -0.03 -0.    1.\n",
      "   0.07  0.01  0.95 -0.03 -0.01  1.01  0.02 -0.01  0.98  0.02  0.01  0.98\n",
      "   0.03  0.02  0.99 -0.03 -0.01  0.96 -0.   -0.01  0.96  0.05  0.01  1.  ]\n",
      " [-0.09 -0.01  0.98 -0.05  0.    0.99 -0.06  0.    0.98 -0.08  0.01  0.98\n",
      "  -0.05 -0.02  0.98 -0.04 -0.    0.98 -0.04  0.03  0.99 -0.05  0.01  0.98\n",
      "  -0.07  0.    0.99 -0.04 -0.01  0.98 -0.04 -0.    0.98 -0.06 -0.02  0.98\n",
      "  -0.04  0.04  1.   -0.05  0.    0.98 -0.01 -0.02  1.   -0.04  0.02  0.99\n",
      "  -0.06 -0.01  0.98 -0.05  0.02  0.99 -0.05 -0.03  0.97 -0.05  0.    0.98]\n",
      " [-0.07  0.08  0.95 -0.01  0.1   0.98 -0.02  0.08  1.    0.03  0.12  0.96\n",
      "  -0.01  0.11  0.95  0.02  0.13  0.99  0.01  0.12  0.99  0.03  0.14  0.98\n",
      "   0.03  0.1   1.    0.01  0.05  1.   -0.04  0.06  0.99 -0.05  0.03  0.97\n",
      "  -0.03  0.07  1.   -0.07  0.07  0.95  0.    0.08  0.99  0.05  0.09  1.01\n",
      "  -0.    0.09  0.99  0.05  0.13  0.95 -0.05  0.06  0.99 -0.05  0.02  0.98]\n",
      " [-0.04  0.06  0.99  0.02  0.12  0.97 -0.    0.08  1.01 -0.03  0.02  0.98\n",
      "   0.02  0.1   0.97 -0.03  0.08  0.95 -0.03  0.07  0.98 -0.05  0.07  0.92\n",
      "  -0.    0.08  0.99  0.03  0.11  1.01 -0.01  0.07  0.93 -0.    0.03  0.99\n",
      "   0.01  0.1   0.98  0.03  0.16  0.97 -0.01  0.1   1.01 -0.07  0.11  0.99\n",
      "  -0.01  0.08  0.99  0.06  0.12  1.    0.    0.07  0.97 -0.01  0.05  1.03]\n",
      " [ 0.05 -0.01  0.98 -0.02 -0.02  0.94  0.07  0.01  0.99 -0.02 -0.01  0.97\n",
      "   0.07 -0.    0.97  0.03  0.01  0.97  0.02 -0.03  0.99  0.02  0.01  0.99\n",
      "   0.1   0.01  1.    0.07  0.01  0.98 -0.01 -0.02  0.94  0.08  0.    0.99\n",
      "   0.04  0.05  0.97  0.02 -0.02  0.99  0.02 -0.    0.99 -0.01 -0.07  0.98\n",
      "   0.06  0.    0.98 -0.03 -0.02  0.94  0.   -0.    1.    0.07 -0.01  0.98]\n",
      " [-0.03 -0.03  1.    0.02  0.01  0.97  0.02  0.    0.97  0.02 -0.05  0.99\n",
      "   0.08  0.    0.99  0.01  0.    1.01  0.07  0.01  0.99 -0.05 -0.01  0.98\n",
      "   0.02  0.03  0.99  0.02  0.    0.98  0.02 -0.    0.99  0.02 -0.06  1.\n",
      "  -0.01 -0.02  0.95  0.01 -0.01  0.94  0.04  0.01  1.01  0.08  0.03  0.98\n",
      "   0.02 -0.    0.97  0.02 -0.    0.99  0.03  0.01  0.97  0.1   0.01  1.  ]\n",
      " [ 0.02 -0.01  0.99  0.02 -0.01  0.98  0.04  0.04  0.98  0.1   0.01  1.\n",
      "   0.04  0.01  1.03  0.04 -0.01  0.95  0.03 -0.02  0.98  0.05  0.    0.97\n",
      "   0.02  0.01  0.98  0.02 -0.    0.99 -0.02 -0.05  0.99  0.   -0.01  0.99\n",
      "   0.02  0.    1.02  0.06  0.    0.98  0.09  0.02  0.98  0.03  0.02  0.97\n",
      "   0.02 -0.01  0.97  0.02 -0.01  1.    0.09  0.    0.98  0.05  0.01  1.02]\n",
      " [-0.03  0.05  0.99 -0.04  0.05  0.99 -0.01  0.08  0.99 -0.04  0.04  0.98\n",
      "  -0.01  0.09  1.02 -0.07  0.04  0.93 -0.05  0.06  0.99  0.05  0.14  1.\n",
      "  -0.01  0.07  0.97  0.05  0.11  1.    0.01  0.1   0.98 -0.05  0.01  0.98\n",
      "  -0.    0.09  0.97 -0.05  0.01  0.93 -0.05  0.06  0.99  0.06  0.1   1.\n",
      "  -0.01  0.09  1.    0.02  0.14  0.98 -0.01  0.08  0.96 -0.    0.12  0.96]\n",
      " [-0.01  0.06  0.97 -0.04  0.07  0.99  0.02  0.12  0.96 -0.06  0.08  1.01\n",
      "  -0.03  0.03  0.97  0.03  0.1   0.96 -0.05  0.05  0.96 -0.03  0.07  0.98\n",
      "  -0.06  0.06  0.92  0.03  0.11  0.97  0.04  0.14  0.99 -0.01  0.07  0.94\n",
      "   0.04  0.05  0.99 -0.02  0.08  0.97 -0.05  0.12  0.94 -0.01  0.08  0.96\n",
      "   0.02  0.14  0.94 -0.02  0.07  1.    0.05  0.14  0.98  0.01  0.08  0.97]\n",
      " [-0.06 -0.02  0.98 -0.04  0.    0.98 -0.08 -0.02  0.98 -0.05 -0.01  0.98\n",
      "  -0.06 -0.    0.99 -0.03  0.    0.98 -0.07  0.01  0.99 -0.06 -0.02  0.98\n",
      "  -0.1   0.01  0.98 -0.06  0.    0.99 -0.03  0.03  1.   -0.04 -0.01  0.98\n",
      "  -0.05 -0.02  0.98 -0.08  0.01  0.98 -0.1  -0.    0.98 -0.05  0.    0.98\n",
      "  -0.02  0.02  1.   -0.07 -0.02  0.98 -0.04  0.01  0.99 -0.01 -0.02  0.98]\n",
      " [ 0.01  0.11  0.96 -0.03  0.03  0.99 -0.    0.11  0.99  0.02  0.14  0.99\n",
      "  -0.    0.09  0.96 -0.01  0.11  0.96  0.01  0.12  0.99  0.01  0.09  0.96\n",
      "  -0.01  0.09  0.95  0.04  0.1   0.99  0.03  0.08  1.03  0.03  0.1   0.97\n",
      "  -0.01  0.07  1.    0.    0.11  1.02 -0.08  0.03  0.97  0.    0.1   0.96\n",
      "   0.04  0.16  0.98 -0.01  0.09  1.01  0.04  0.16  1.01  0.03  0.11  0.97]\n",
      " [ 0.01  0.07  0.98  0.02  0.12  0.99  0.01  0.11  0.95  0.02  0.08  1.\n",
      "   0.02  0.08  1.03  0.01  0.08  0.98 -0.03  0.05  1.    0.    0.1   1.02\n",
      "  -0.07  0.03  0.99 -0.04  0.06  1.    0.03  0.16  0.97 -0.01  0.1   1.\n",
      "   0.01  0.18  1.    0.04  0.11  0.97  0.02  0.05  1.01 -0.01  0.1   1.\n",
      "  -0.04  0.04  0.99  0.02  0.11  0.98 -0.04  0.05  0.98 -0.01  0.07  0.97]\n",
      " [-0.07 -0.    0.98 -0.05 -0.02  0.97 -0.06  0.02  0.98 -0.09 -0.01  0.97\n",
      "  -0.05 -0.    0.98 -0.05  0.01  0.98 -0.08  0.01  0.98 -0.04  0.01  0.98\n",
      "  -0.04 -0.01  0.98 -0.03  0.02  0.99 -0.06  0.    0.98 -0.06  0.02  0.99\n",
      "  -0.04 -0.02  0.98 -0.05  0.02  0.99 -0.06  0.03  0.99 -0.01  0.01  0.99\n",
      "  -0.05  0.01  0.98 -0.02 -0.03  0.97 -0.07 -0.01  0.98 -0.04 -0.    0.98]\n",
      " [-0.01 -0.07  0.98  0.06  0.    0.98 -0.03 -0.02  0.94  0.   -0.    1.\n",
      "   0.07 -0.01  0.98  0.02 -0.01  0.98  0.02 -0.01  0.98  0.03  0.01  0.96\n",
      "   0.01 -0.02  0.98  0.01 -0.    1.   -0.01 -0.02  0.94 -0.05 -0.02  0.96\n",
      "  -0.03 -0.03  1.    0.02  0.01  0.97  0.02  0.    0.97  0.02 -0.05  0.99\n",
      "   0.08  0.    0.99  0.01  0.    1.01  0.07  0.01  0.99 -0.05 -0.01  0.98]\n",
      " [ 0.09  0.01  0.91  0.01  0.02  1.02 -0.03  0.    1.   -0.04 -0.    1.01\n",
      "  -0.03 -0.01  0.99  0.02  0.02  0.98  0.03  0.01  0.98  0.03 -0.    0.91\n",
      "   0.09  0.01  0.95  0.02  0.01  1.02  0.08  0.01  0.96 -0.   -0.02  0.95\n",
      "   0.01 -0.05  0.89  0.02  0.    0.98  0.02  0.01  0.98  0.04 -0.02  0.91\n",
      "   0.02  0.02  1.02  0.06  0.02  1.    0.08  0.01  0.96  0.08  0.01  0.91]\n",
      " [-0.06  0.02  0.99 -0.06  0.02  0.99 -0.05 -0.01  0.98 -0.05  0.03  0.99\n",
      "  -0.06  0.    0.99 -0.06 -0.01  0.98 -0.06 -0.02  0.97 -0.04  0.01  0.98\n",
      "  -0.04  0.01  0.99 -0.09  0.01  0.98 -0.05  0.    0.98 -0.04  0.04  1.\n",
      "  -0.04 -0.01  0.99 -0.04 -0.    0.98 -0.08 -0.    0.98 -0.07 -0.03  0.97\n",
      "  -0.06  0.    0.98 -0.09  0.03  0.98 -0.06  0.02  0.99 -0.07 -0.01  0.98]\n",
      " [-0.06  0.    0.98 -0.02  0.01  1.   -0.07 -0.01  0.98 -0.05 -0.01  0.98\n",
      "  -0.02 -0.02  0.98 -0.05 -0.02  0.98 -0.06  0.    0.98 -0.09 -0.01  0.98\n",
      "  -0.05  0.02  0.99 -0.05 -0.02  0.98 -0.06 -0.03  0.98 -0.06 -0.    0.99\n",
      "  -0.07  0.03  1.   -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.04 -0.02  0.98\n",
      "  -0.1  -0.01  0.97 -0.06 -0.    0.99 -0.02  0.04  1.   -0.04  0.02  0.99]\n",
      " [-0.    0.07  0.95 -0.04  0.    0.95 -0.    0.08  0.99 -0.    0.14  0.96\n",
      "  -0.    0.07  0.96  0.02  0.09  0.99  0.02  0.11  0.96 -0.04  0.05  0.98\n",
      "  -0.02  0.1   0.98  0.01  0.15  0.95  0.02  0.12  0.96 -0.02  0.1   0.96\n",
      "  -0.01  0.07  0.97 -0.    0.07  0.96 -0.    0.09  1.   -0.03  0.09  0.95\n",
      "  -0.04  0.1   0.95  0.02  0.12  0.99 -0.01  0.1   0.95 -0.02  0.08  0.94]\n",
      " [-0.04 -0.    0.98 -0.04 -0.01  0.98 -0.06  0.03  0.99 -0.01  0.02  1.\n",
      "  -0.05 -0.    0.98 -0.04 -0.04  0.97 -0.06 -0.02  0.98 -0.05  0.02  0.99\n",
      "  -0.08 -0.02  0.98 -0.04  0.03  0.99 -0.06  0.    0.98 -0.08  0.02  0.99\n",
      "  -0.07  0.01  0.98 -0.04 -0.01  0.98 -0.06  0.03  0.99 -0.06  0.    0.99\n",
      "  -0.06 -0.01  0.98 -0.08 -0.    0.98 -0.04  0.    0.98 -0.06 -0.02  0.98]\n",
      " [-0.05 -0.    0.98 -0.03  0.01  0.98 -0.05  0.02  0.99 -0.06 -0.02  0.98\n",
      "  -0.01  0.    0.99 -0.06  0.    0.98 -0.07  0.01  0.99 -0.08  0.    0.99\n",
      "  -0.04  0.01  0.99 -0.06 -0.02  0.98 -0.02 -0.02  0.98 -0.06 -0.    0.98\n",
      "  -0.1  -0.02  0.98 -0.04 -0.01  0.98 -0.07 -0.    0.98 -0.03  0.    0.99\n",
      "  -0.06  0.03  1.   -0.06 -0.    0.98 -0.05 -0.04  0.97 -0.06 -0.02  0.98]\n",
      " [-0.05 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05  0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.06  0.01  0.99 -0.07 -0.02  0.98 -0.06 -0.04  0.97 -0.05 -0.    0.98\n",
      "  -0.04  0.02  0.99 -0.07  0.01  0.99 -0.06 -0.01  0.98 -0.01 -0.02  0.98\n",
      "  -0.05 -0.01  0.98 -0.06 -0.01  0.98 -0.09  0.    0.99 -0.03  0.    0.98\n",
      "  -0.06  0.02  0.98 -0.05 -0.03  0.98 -0.06 -0.01  0.99 -0.09  0.02  0.99\n",
      "  -0.06 -0.01  0.98 -0.07 -0.01  0.98 -0.03 -0.01  0.98 -0.03 -0.02  0.98]\n",
      " [-0.04  0.06  1.    0.02  0.08  1.    0.02  0.07  0.98 -0.    0.11  1.\n",
      "   0.02  0.11  1.   -0.04  0.06  0.99 -0.07  0.03  0.94 -0.02  0.09  0.97\n",
      "   0.    0.12  0.95 -0.01  0.07  0.96  0.04  0.13  0.93  0.04  0.11  0.96\n",
      "  -0.01  0.02  1.01 -0.    0.1   0.98  0.04  0.07  1.   -0.02  0.07  0.99\n",
      "  -0.05  0.04  0.98 -0.01  0.1   0.99 -0.04  0.02  0.97 -0.04  0.06  1.  ]\n",
      " [-0.02  0.08  0.96 -0.05  0.05  0.95 -0.    0.11  1.02 -0.02  0.14  0.96\n",
      "  -0.04  0.07  0.98 -0.03  0.01  0.99 -0.    0.08  0.95  0.04  0.04  0.97\n",
      "   0.02  0.11  0.96 -0.07  0.1   0.95 -0.01  0.1   0.98  0.01  0.14  0.92\n",
      "  -0.03  0.06  1.   -0.    0.12  0.96  0.01  0.1   0.98  0.01  0.1   0.96\n",
      "  -0.03  0.06  1.    0.05  0.12  0.99  0.02  0.14  0.99 -0.03  0.09  1.  ]\n",
      " [-0.06  0.02  0.98 -0.06 -0.01  0.97 -0.05  0.03  0.98 -0.05  0.01  0.97\n",
      "  -0.05 -0.01  0.97 -0.06  0.03  0.98 -0.05  0.02  0.98 -0.06 -0.02  0.97\n",
      "  -0.09 -0.02  0.97 -0.05  0.    0.98 -0.1   0.02  0.97 -0.07  0.01  0.98\n",
      "  -0.04  0.    0.98 -0.09  0.01  0.98 -0.03  0.    0.98 -0.05  0.    0.98\n",
      "  -0.07 -0.    0.98 -0.05 -0.02  0.97 -0.06  0.02  0.98 -0.09 -0.01  0.97]\n",
      " [ 0.02  0.    1.01  0.01 -0.04  0.98  0.02 -0.01  0.99  0.02 -0.01  0.98\n",
      "   0.02 -0.02  0.99  0.05  0.02  0.99  0.01 -0.01  0.95  0.07  0.01  1.01\n",
      "   0.09  0.01  1.    0.03 -0.05  0.97  0.02 -0.02  0.99  0.02 -0.    0.99\n",
      "  -0.    0.01  0.99  0.09  0.01  1.01 -0.02 -0.02  0.94  0.01  0.    1.02\n",
      "  -0.04 -0.01  0.98  0.   -0.02  0.98  0.02 -0.    0.99  0.02 -0.    0.98]\n",
      " [-0.01  0.09  1.02 -0.07  0.04  0.93 -0.05  0.06  0.99  0.05  0.14  1.\n",
      "  -0.01  0.07  0.97  0.05  0.11  1.    0.01  0.1   0.98 -0.05  0.01  0.98\n",
      "  -0.    0.09  0.97 -0.05  0.01  0.93 -0.05  0.06  0.99  0.06  0.1   1.\n",
      "  -0.01  0.09  1.    0.02  0.14  0.98 -0.01  0.08  0.96 -0.    0.12  0.96\n",
      "  -0.01  0.12  0.98 -0.01  0.06  0.96 -0.04  0.05  0.96  0.02  0.12  0.96]\n",
      " [-0.07 -0.    0.98 -0.03  0.    0.99 -0.06  0.03  1.   -0.06 -0.    0.98\n",
      "  -0.05 -0.04  0.97 -0.06 -0.02  0.98 -0.04  0.    0.98 -0.08 -0.02  0.98\n",
      "  -0.05 -0.01  0.98 -0.06 -0.    0.99 -0.03  0.    0.98 -0.07  0.01  0.99\n",
      "  -0.06 -0.02  0.98 -0.1   0.01  0.98 -0.06  0.    0.99 -0.03  0.03  1.\n",
      "  -0.04 -0.01  0.98 -0.05 -0.02  0.98 -0.08  0.01  0.98 -0.1  -0.    0.98]\n",
      " [-0.03  0.06  0.97 -0.    0.07  1.01  0.    0.09  1.02 -0.01  0.03  0.99\n",
      "  -0.02  0.08  0.96 -0.05  0.05  0.95 -0.    0.11  1.02 -0.02  0.14  0.96\n",
      "  -0.04  0.07  0.98 -0.03  0.01  0.99 -0.    0.08  0.95  0.04  0.04  0.97\n",
      "   0.02  0.11  0.96 -0.07  0.1   0.95 -0.01  0.1   0.98  0.01  0.14  0.92\n",
      "  -0.03  0.06  1.   -0.    0.12  0.96  0.01  0.1   0.98  0.01  0.1   0.96]\n",
      " [ 0.02  0.01  0.99  0.01 -0.06  0.86  0.09  0.02  0.96 -0.03 -0.01  0.97\n",
      "   0.02  0.01  1.02 -0.01 -0.03  0.94  0.02  0.    0.96  0.02  0.01  0.98\n",
      "   0.02  0.01  0.99 -0.05  0.    1.02  0.09  0.02  0.98 -0.01  0.01  1.01\n",
      "   0.07  0.03  1.01  0.07  0.03  0.97  0.02  0.    0.99  0.02  0.    0.99\n",
      "   0.02  0.03  1.01 -0.04 -0.01  0.97  0.08  0.02  0.98  0.   -0.    0.96]\n",
      " [ 0.03  0.09  0.99 -0.01  0.08  0.96  0.03  0.13  0.98  0.01  0.09  1.01\n",
      "   0.04  0.1   1.04 -0.01  0.07  0.99 -0.03  0.04  0.99 -0.    0.11  1.02\n",
      "  -0.07  0.03  0.99 -0.05  0.06  0.99  0.05  0.15  0.99 -0.01  0.08  0.98\n",
      "   0.05  0.09  1.01 -0.03  0.07  0.99 -0.07  0.06  0.96 -0.    0.09  0.98\n",
      "  -0.04  0.05  0.91 -0.03  0.07  0.98  0.    0.13  0.96  0.    0.12  0.98]\n",
      " [ 0.03 -0.02  0.98  0.02  0.01  0.98  0.02 -0.    0.99 -0.04 -0.03  0.99\n",
      "   0.01 -0.    1.    0.02  0.    1.02  0.04 -0.    0.97  0.04 -0.03  0.98\n",
      "   0.03  0.01  0.98  0.02 -0.    0.99  0.02 -0.    0.97 -0.03 -0.02  0.96\n",
      "  -0.03 -0.02  0.96  0.07  0.    0.98 -0.05 -0.02  0.97  0.06  0.04  0.98\n",
      "   0.03 -0.02  0.99  0.02 -0.    0.99 -0.   -0.05  0.99 -0.04 -0.02  0.96]\n",
      " [-0.01  0.1   0.98 -0.02  0.08  1.    0.03  0.12  0.96 -0.01  0.11  0.95\n",
      "   0.02  0.13  0.99  0.01  0.12  0.99  0.03  0.14  0.98  0.03  0.1   1.\n",
      "   0.01  0.05  1.   -0.04  0.06  0.99 -0.05  0.03  0.97 -0.03  0.07  1.\n",
      "  -0.07  0.07  0.95  0.    0.08  0.99  0.05  0.09  1.01 -0.    0.09  0.99\n",
      "   0.05  0.13  0.95 -0.05  0.06  0.99 -0.05  0.02  0.98 -0.    0.08  0.95]\n",
      " [-0.06  0.01  0.99 -0.05 -0.    0.98 -0.04  0.01  0.98 -0.07  0.    0.98\n",
      "  -0.06 -0.02  0.98 -0.03  0.03  0.99 -0.05  0.    0.98 -0.07 -0.    0.99\n",
      "  -0.04  0.    0.99 -0.03 -0.    0.98 -0.07 -0.02  0.98 -0.03  0.03  1.\n",
      "  -0.05 -0.    0.98 -0.03 -0.04  0.97 -0.06 -0.02  0.98 -0.05  0.02  0.99\n",
      "  -0.06 -0.03  0.98 -0.06  0.03  1.   -0.05 -0.    0.98 -0.08  0.02  0.99]\n",
      " [-0.04 -0.    1.   -0.05 -0.01  1.01  0.02 -0.01  0.97  0.02  0.    0.99\n",
      "   0.02  0.    0.98 -0.05 -0.    1.03 -0.04 -0.    0.99 -0.03 -0.    1.\n",
      "   0.07  0.01  0.95 -0.03 -0.01  1.01  0.02 -0.01  0.98  0.02  0.01  0.98\n",
      "   0.03  0.02  0.99 -0.03 -0.01  0.96 -0.   -0.01  0.96  0.05  0.01  1.\n",
      "   0.02  0.03  1.03 -0.01 -0.02  0.96  0.02  0.    0.98  0.02  0.01  0.98]\n",
      " [ 0.02 -0.    0.97  0.03 -0.06  0.99  0.05  0.01  1.01 -0.01 -0.02  0.94\n",
      "  -0.01 -0.01  0.99 -0.01 -0.03  0.98  0.01 -0.02  1.    0.02 -0.    0.98\n",
      "   0.02 -0.    0.98 -0.02  0.04  0.99 -0.03 -0.02  0.94  0.03 -0.01  0.95\n",
      "   0.08  0.01  1.02 -0.01 -0.05  0.99  0.02  0.01  0.98  0.02 -0.01  0.97\n",
      "   0.01 -0.03  1.    0.07 -0.01  0.99 -0.03 -0.02  0.94  0.04  0.01  1.03]\n",
      " [ 0.02  0.1   0.98 -0.01  0.15  0.95  0.03  0.09  0.98  0.01  0.08  1.\n",
      "  -0.    0.12  1.    0.04  0.13  0.98  0.01  0.09  0.98  0.02  0.11  0.96\n",
      "  -0.03  0.06  1.    0.05  0.12  0.98  0.02  0.15  0.99 -0.04  0.08  1.\n",
      "  -0.    0.1   0.95  0.02  0.11  0.96  0.05  0.14  1.02 -0.04  0.06  1.\n",
      "  -0.02  0.04  1.    0.01  0.11  0.99  0.01  0.06  1.02  0.03  0.12  0.96]\n",
      " [-0.03  0.    1.   -0.04 -0.01  0.98 -0.06 -0.01  0.98 -0.07  0.02  0.99\n",
      "  -0.09  0.02  0.98 -0.05  0.    0.98 -0.01 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.04 -0.    0.98 -0.01 -0.01  0.98 -0.05 -0.02  0.97 -0.05 -0.    0.98\n",
      "  -0.06 -0.02  0.98 -0.07  0.    0.98 -0.04  0.02  0.99 -0.02 -0.01  0.99\n",
      "  -0.06 -0.    0.99 -0.07 -0.02  0.98 -0.05  0.02  0.99 -0.07  0.    0.98]\n",
      " [ 0.04  0.    0.95 -0.03 -0.02  0.98  0.02 -0.02  0.97  0.03  0.01  0.98\n",
      "   0.02  0.01  0.98  0.07 -0.    0.91  0.08  0.02  1.    0.01  0.01  1.01\n",
      "  -0.03  0.01  1.01 -0.02 -0.04  0.92  0.02  0.01  0.98  0.02  0.    0.99\n",
      "   0.04  0.03  0.98  0.07  0.    0.94  0.07  0.02  0.99  0.01  0.    0.96\n",
      "  -0.03  0.02  1.03  0.02  0.05  1.04  0.02 -0.    0.99  0.02  0.    0.98]\n",
      " [-0.04 -0.01  0.98 -0.05  0.02  0.99 -0.09  0.02  0.99 -0.06  0.02  0.99\n",
      "  -0.05  0.    0.98 -0.02  0.    0.98 -0.06 -0.01  0.98 -0.04 -0.    0.98\n",
      "  -0.07  0.03  0.99 -0.05  0.01  0.98 -0.06 -0.01  0.98 -0.06  0.03  0.99\n",
      "  -0.07  0.01  0.99 -0.05 -0.02  0.98 -0.1  -0.01  0.97 -0.05  0.01  0.98\n",
      "  -0.07  0.05  1.   -0.07  0.01  0.98 -0.04  0.    0.98 -0.09  0.02  0.99]\n",
      " [ 0.05  0.    0.95 -0.02  0.01  0.95 -0.01  0.04  1.09  0.02 -0.    0.99\n",
      "   0.02  0.01  0.98  0.03  0.    0.97  0.1   0.02  0.97 -0.01 -0.01  0.96\n",
      "   0.03  0.01  1.01 -0.01 -0.02  0.94  0.06  0.06  1.06  0.02 -0.    0.99\n",
      "   0.02  0.    0.99 -0.01  0.03  1.04  0.02  0.02  1.02  0.05  0.    0.95\n",
      "  -0.02 -0.    0.97 -0.02  0.02  1.06  0.01 -0.    0.96  0.02  0.01  0.98]\n",
      " [-0.04 -0.01  0.98  0.02  0.04  0.98  0.02 -0.01  0.99  0.02 -0.    0.99\n",
      "   0.1   0.    0.97 -0.03  0.01  1.02  0.05  0.01  1.03  0.07  0.    0.99\n",
      "   0.01  0.03  0.99  0.03  0.01  0.97  0.02 -0.    0.99  0.03  0.01  0.98\n",
      "   0.04 -0.02  0.98  0.   -0.01  0.99 -0.01 -0.02  0.94 -0.05 -0.02  0.96\n",
      "   0.01  0.04  0.99  0.02  0.01  0.97  0.02 -0.    0.97  0.01 -0.06  0.98]\n",
      " [ 0.02  0.1   0.98  0.01  0.03  1.01 -0.01  0.11  1.   -0.03  0.02  0.99\n",
      "   0.    0.1   0.96 -0.03  0.12  0.95  0.    0.08  0.97  0.01  0.08  1.01\n",
      "  -0.01  0.08  1.    0.02  0.07  1.    0.02  0.07  0.99 -0.01  0.1   1.\n",
      "  -0.    0.1   0.95 -0.    0.07  1.01 -0.02  0.03  0.97  0.01  0.08  0.99\n",
      "  -0.04  0.03  0.98 -0.03  0.07  1.   -0.08  0.03  0.98  0.03  0.11  0.95]\n",
      " [-0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.99\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.01  0.98 -0.06 -0.01  0.98\n",
      "  -0.05 -0.01  0.98 -0.06  0.01  0.98 -0.05 -0.    0.98 -0.05  0.    0.98]\n",
      " [ 0.04  0.04  1.03  0.01 -0.02  0.95  0.02  0.    0.99  0.02  0.    0.98\n",
      "  -0.01  0.05  1.11  0.06  0.02  1.01  0.07  0.02  0.99 -0.02 -0.01  0.96\n",
      "  -0.04 -0.    1.02  0.02  0.    0.98  0.02  0.01  0.98  0.02  0.01  0.98\n",
      "   0.09  0.02  0.99  0.08  0.01  0.95 -0.    0.01  1.01  0.04 -0.01  0.93\n",
      "   0.01  0.05  1.08  0.02 -0.    0.98  0.02  0.01  0.99  0.02 -0.03  0.92]\n",
      " [-0.09 -0.01  0.98 -0.03 -0.02  0.98 -0.05 -0.    0.98 -0.07  0.02  0.98\n",
      "  -0.05 -0.02  0.98 -0.04  0.    0.99 -0.04  0.03  0.99 -0.05  0.    0.98\n",
      "  -0.06 -0.    0.99 -0.05 -0.02  0.98 -0.07  0.    0.98 -0.03  0.01  0.99\n",
      "  -0.09  0.02  0.98 -0.05  0.    0.99 -0.03  0.02  0.99 -0.06 -0.02  0.98\n",
      "  -0.06  0.01  0.99 -0.07 -0.02  0.98 -0.06 -0.04  0.97 -0.05 -0.    0.98]\n",
      " [ 0.01 -0.01  0.96  0.01 -0.02  0.94 -0.01 -0.02  0.94  0.09  0.02  0.99\n",
      "   0.02  0.01  0.99  0.02  0.    0.99  0.02 -0.    0.97  0.07  0.02  1.\n",
      "   0.01 -0.    1.    0.02 -0.01  0.94  0.08  0.01  1.    0.06  0.04  0.98\n",
      "   0.02  0.01  0.97  0.02 -0.    0.99  0.03  0.03  0.99  0.05  0.01  1.\n",
      "   0.02  0.    1.02  0.07  0.01  1.    0.09  0.02  1.    0.04  0.05  0.97]\n",
      " [-0.01  0.07  0.93 -0.    0.04  1.01  0.01  0.1   0.96  0.01  0.16  0.96\n",
      "  -0.    0.07  0.97  0.03  0.14  1.01 -0.05  0.06  0.99  0.03  0.07  1.01\n",
      "  -0.01  0.09  0.98 -0.    0.09  0.96 -0.03  0.06  1.   -0.02  0.05  0.99\n",
      "   0.    0.06  0.98 -0.01  0.1   1.    0.01  0.11  0.95 -0.04  0.06  0.99\n",
      "  -0.04  0.05  0.96 -0.03  0.07  0.99 -0.04  0.04  0.97 -0.04  0.07  0.96]\n",
      " [ 0.02  0.02  0.99  0.02 -0.01  0.98  0.02 -0.    0.97  0.06  0.02  1.\n",
      "   0.09  0.01  1.01  0.07  0.01  1.01 -0.04 -0.02  0.94 -0.03  0.    0.99\n",
      "   0.02 -0.02  0.98  0.02 -0.01  0.97  0.02 -0.02  1.    0.09  0.01  1.\n",
      "   0.08  0.01  1.02 -0.02 -0.02  0.95  0.04 -0.01  0.98  0.05  0.05  0.98\n",
      "   0.03 -0.01  0.98  0.02 -0.    0.97  0.03 -0.05  0.98  0.05  0.01  1.01]\n",
      " [-0.05  0.01  0.98 -0.01  0.07  0.93 -0.    0.04  1.01  0.01  0.1   0.96\n",
      "   0.01  0.16  0.96 -0.    0.07  0.97  0.03  0.14  1.01 -0.05  0.06  0.99\n",
      "   0.03  0.07  1.01 -0.01  0.09  0.98 -0.    0.09  0.96 -0.03  0.06  1.\n",
      "  -0.02  0.05  0.99  0.    0.06  0.98 -0.01  0.1   1.    0.01  0.11  0.95\n",
      "  -0.04  0.06  0.99 -0.04  0.05  0.96 -0.03  0.07  0.99 -0.04  0.04  0.97]\n",
      " [ 0.02  0.01  0.99  0.02  0.    0.99  0.02 -0.    0.97  0.07  0.02  1.\n",
      "   0.01 -0.    1.    0.02 -0.01  0.94  0.08  0.01  1.    0.06  0.04  0.98\n",
      "   0.02  0.01  0.97  0.02 -0.    0.99  0.03  0.03  0.99  0.05  0.01  1.\n",
      "   0.02  0.    1.02  0.07  0.01  1.    0.09  0.02  1.    0.04  0.05  0.97\n",
      "   0.02 -0.01  0.99  0.02 -0.    0.97 -0.   -0.07  0.98  0.08  0.01  1.02]\n",
      " [-0.06 -0.    0.98 -0.03 -0.01  0.97 -0.07  0.02  0.98 -0.05  0.03  0.99\n",
      "  -0.09 -0.01  0.97 -0.05 -0.    0.99 -0.09  0.03  0.99 -0.06  0.02  0.99\n",
      "  -0.06  0.01  0.98 -0.03 -0.01  0.98 -0.02  0.01  0.99 -0.06  0.    0.98\n",
      "  -0.04 -0.04  0.98 -0.07  0.    0.98 -0.04  0.01  0.98 -0.03 -0.03  0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.02  0.98 -0.05 -0.01  0.98]\n",
      " [ 0.07  0.01  0.95 -0.04 -0.    0.99  0.03 -0.03  0.9   0.02  0.01  0.97\n",
      "   0.02  0.01  0.98  0.02  0.01  0.98 -0.01  0.02  1.01  0.08  0.01  0.95\n",
      "   0.02  0.01  1.01  0.1   0.02  0.97  0.04 -0.03  0.87  0.02  0.01  0.98\n",
      "   0.02  0.01  0.98  0.02  0.03  1.02  0.1   0.02  0.96 -0.03 -0.01  0.98\n",
      "   0.01  0.01  1.01  0.04  0.04  1.03  0.02 -0.03  0.94  0.02  0.01  0.98]\n",
      " [ 0.02 -0.01  0.97  0.01 -0.03  1.    0.07 -0.01  0.99 -0.03 -0.02  0.94\n",
      "   0.04  0.01  1.03  0.05  0.02  0.99 -0.    0.01  1.    0.02  0.01  0.98\n",
      "   0.02 -0.    0.98  0.06 -0.01  0.97  0.02 -0.01  0.96  0.02  0.    1.02\n",
      "   0.07  0.01  1.02  0.09  0.03  0.99  0.03  0.02  0.98  0.02 -0.    0.98\n",
      "   0.02 -0.01  0.99 -0.02 -0.02  0.98  0.07  0.01  1.    0.06  0.01  1.02]\n",
      " [-0.08 -0.03  0.98 -0.06 -0.    0.98 -0.06  0.02  0.98 -0.06 -0.01  0.97\n",
      "  -0.06 -0.01  0.98 -0.07  0.01  0.98 -0.03  0.03  0.99 -0.05  0.    0.98\n",
      "  -0.01 -0.01  1.   -0.05  0.02  0.98 -0.04 -0.01  0.98 -0.02 -0.01  0.98\n",
      "  -0.06  0.03  0.99 -0.05  0.    0.98 -0.04 -0.02  0.98 -0.04  0.01  0.98\n",
      "  -0.06  0.01  0.98 -0.04 -0.02  0.98 -0.05 -0.01  0.98 -0.05  0.01  0.98]\n",
      " [-0.05 -0.    0.98 -0.04  0.02  0.99 -0.07  0.01  0.99 -0.06 -0.01  0.98\n",
      "  -0.01 -0.02  0.98 -0.05 -0.01  0.98 -0.06 -0.01  0.98 -0.09  0.    0.99\n",
      "  -0.03  0.    0.98 -0.06  0.02  0.98 -0.05 -0.03  0.98 -0.06 -0.01  0.99\n",
      "  -0.09  0.02  0.99 -0.06 -0.01  0.98 -0.07 -0.01  0.98 -0.03 -0.01  0.98\n",
      "  -0.03 -0.02  0.98 -0.05 -0.01  0.98 -0.1  -0.02  0.97 -0.08 -0.01  0.98]\n",
      " [-0.    0.07  0.98  0.03  0.12  1.03  0.02  0.11  0.98  0.03  0.08  1.01\n",
      "  -0.01  0.08  0.98  0.02  0.12  0.96 -0.04  0.06  1.    0.02  0.08  1.\n",
      "   0.02  0.07  0.98 -0.    0.11  1.    0.02  0.11  1.   -0.04  0.06  0.99\n",
      "  -0.07  0.03  0.94 -0.02  0.09  0.97  0.    0.12  0.95 -0.01  0.07  0.96\n",
      "   0.04  0.13  0.93  0.04  0.11  0.96 -0.01  0.02  1.01 -0.    0.1   0.98]\n",
      " [-0.04 -0.01  0.98 -0.03  0.03  0.99 -0.05  0.01  0.98 -0.06 -0.01  0.98\n",
      "  -0.06  0.03  0.98 -0.06  0.01  0.98 -0.04  0.02  0.99 -0.08 -0.02  0.97\n",
      "  -0.05  0.    0.98 -0.08  0.04  0.97 -0.06 -0.02  0.98 -0.06  0.02  0.98\n",
      "  -0.04 -0.03  0.98 -0.06  0.03  0.99 -0.05 -0.    0.98 -0.05 -0.02  0.98\n",
      "  -0.05 -0.02  0.98 -0.04  0.01  0.99 -0.02 -0.02  0.98 -0.06  0.    0.98]\n",
      " [ 0.03  0.09  1.01 -0.02  0.07  0.96 -0.03  0.04  0.99 -0.01  0.08  1.02\n",
      "  -0.05  0.02  0.97  0.02  0.11  0.95 -0.02  0.11  0.95 -0.    0.1   1.02\n",
      "  -0.03  0.12  0.94  0.01  0.1   0.98 -0.07  0.03  0.96 -0.01  0.08  0.95\n",
      "   0.04  0.14  0.99 -0.05  0.06  0.99 -0.04  0.05  0.99 -0.01  0.11  0.99\n",
      "  -0.02  0.11  0.93 -0.01  0.09  0.96 -0.04  0.09  0.95 -0.04  0.09  0.97]\n",
      " [ 0.01  0.05  1.   -0.04  0.06  0.99 -0.05  0.03  0.97 -0.03  0.07  1.\n",
      "  -0.07  0.07  0.95  0.    0.08  0.99  0.05  0.09  1.01 -0.    0.09  0.99\n",
      "   0.05  0.13  0.95 -0.05  0.06  0.99 -0.05  0.02  0.98 -0.    0.08  0.95\n",
      "  -0.03  0.08  0.93  0.02  0.1   0.98 -0.02  0.11  0.95  0.01  0.09  0.98\n",
      "  -0.03  0.02  0.99 -0.04  0.05  0.98  0.04  0.1   1.    0.01  0.06  0.98]\n",
      " [ 0.04  0.01  1.03  0.04 -0.01  0.95  0.03 -0.02  0.98  0.05  0.    0.97\n",
      "   0.02  0.01  0.98  0.02 -0.    0.99 -0.02 -0.05  0.99  0.   -0.01  0.99\n",
      "   0.02  0.    1.02  0.06  0.    0.98  0.09  0.02  0.98  0.03  0.02  0.97\n",
      "   0.02 -0.01  0.97  0.02 -0.01  1.    0.09  0.    0.98  0.05  0.01  1.02\n",
      "  -0.01 -0.02  0.94  0.06  0.01  1.    0.04 -0.04  0.97  0.02 -0.01  0.98]\n",
      " [-0.02 -0.03  0.94  0.02 -0.    0.98  0.02  0.01  1.    0.04 -0.02  0.93\n",
      "   0.05  0.    0.94 -0.03 -0.01  0.98 -0.01  0.01  1.02  0.08  0.    0.91\n",
      "   0.02  0.03  1.    0.02  0.01  0.99  0.02 -0.    0.98 -0.04  0.    1.03\n",
      "  -0.02  0.01  1.02  0.01 -0.    0.96  0.09  0.03  0.99 -0.02 -0.05  0.89\n",
      "   0.02 -0.    0.98  0.02  0.01  0.98  0.04  0.02  0.99 -0.05 -0.01  0.96]\n",
      " [ 0.02 -0.01  0.98  0.06 -0.01  0.99  0.08  0.01  1.02  0.03  0.    1.02\n",
      "  -0.04 -0.02  0.95  0.08 -0.02  0.97  0.02  0.01  0.97  0.02 -0.    0.97\n",
      "   0.   -0.04  0.99 -0.05 -0.02  0.96  0.05 -0.    0.97 -0.02 -0.01  0.97\n",
      "   0.02  0.01  0.99  0.01  0.03  0.99  0.02 -0.    0.98  0.02 -0.    0.97\n",
      "  -0.01 -0.06  0.98  0.02  0.    1.    0.03  0.01  1.02  0.08  0.01  1.  ]\n",
      " [-0.03  0.06  1.   -0.    0.12  0.96  0.01  0.1   0.98  0.01  0.1   0.96\n",
      "  -0.03  0.06  1.    0.05  0.12  0.99  0.02  0.14  0.99 -0.03  0.09  1.\n",
      "   0.    0.1   0.95  0.02  0.11  0.98  0.04  0.1   1.02  0.03  0.09  0.95\n",
      "  -0.05  0.03  0.97 -0.    0.09  1.02 -0.07  0.05  1.02  0.03  0.11  0.97\n",
      "  -0.01  0.13  0.95 -0.    0.11  1.02 -0.06  0.12  0.97 -0.    0.09  0.99]\n",
      " [ 0.04  0.1   1.01 -0.01  0.07  1.   -0.01  0.05  1.    0.01  0.11  1.\n",
      "   0.    0.07  1.04  0.02  0.11  0.96 -0.05  0.07  0.95 -0.01  0.1   1.\n",
      "  -0.07  0.12  0.99 -0.01  0.08  0.99  0.05  0.08  1.01 -0.    0.11  1.01\n",
      "   0.02  0.11  1.04 -0.04  0.07  0.99  0.02  0.07  1.   -0.01  0.08  0.98\n",
      "  -0.04  0.05  0.99 -0.04  0.06  0.98 -0.03  0.05  0.99  0.01  0.06  0.98]\n",
      " [ 0.03  0.01  0.98  0.02  0.01  0.99  0.01 -0.04  0.92  0.08  0.03  1.\n",
      "  -0.03  0.    1.01  0.02  0.01  1.01  0.04 -0.02  0.91  0.04  0.04  1.02\n",
      "   0.02  0.01  0.98  0.02  0.01  0.98  0.08  0.03  0.99  0.09  0.02  0.98\n",
      "   0.07  0.02  0.99 -0.05 -0.01  0.99  0.07  0.05  1.02  0.02  0.02  0.98\n",
      "   0.02  0.01  0.99  0.03  0.03  0.99 -0.06 -0.01  0.99 -0.02  0.01  1.02]\n",
      " [ 0.02 -0.01  0.97  0.06 -0.02  0.97  0.02 -0.    0.98  0.02 -0.01  0.98\n",
      "   0.03  0.04  0.99 -0.03 -0.01  0.97  0.   -0.02  0.94 -0.   -0.    1.01\n",
      "   0.09  0.02  1.    0.03 -0.02  0.98  0.02  0.01  0.98  0.02 -0.    0.99\n",
      "  -0.04 -0.03  0.99  0.01 -0.    1.    0.02  0.    1.02  0.04 -0.    0.97\n",
      "   0.04 -0.03  0.98  0.03  0.01  0.98  0.02 -0.    0.99  0.02 -0.    0.97]\n",
      " [-0.02 -0.03  0.97 -0.07 -0.01  0.98 -0.04 -0.    0.98 -0.03  0.01  0.98\n",
      "  -0.05 -0.01  0.97 -0.05 -0.    0.98 -0.04  0.01  0.98 -0.07 -0.01  0.98\n",
      "  -0.04 -0.01  0.98 -0.03  0.03  0.99 -0.05  0.01  0.98 -0.06 -0.01  0.98\n",
      "  -0.06  0.03  0.98 -0.06  0.01  0.98 -0.04  0.02  0.99 -0.08 -0.02  0.97\n",
      "  -0.05  0.    0.98 -0.08  0.04  0.97 -0.06 -0.02  0.98 -0.06  0.02  0.98]\n",
      " [-0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.06 -0.    0.99 -0.06 -0.03  0.98 -0.05  0.02  0.99 -0.05 -0.02  0.98\n",
      "  -0.05  0.01  0.99 -0.06 -0.01  0.98 -0.05  0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.99 -0.05 -0.02  0.97 -0.05  0.01  0.99 -0.05 -0.    0.98\n",
      "  -0.05  0.02  0.98 -0.06 -0.01  0.98 -0.05  0.01  0.98 -0.05  0.    0.98]\n",
      " [ 0.05  0.    0.97 -0.04 -0.01  0.98  0.02 -0.    0.99  0.02 -0.    0.98\n",
      "   0.02  0.    0.99  0.08  0.02  0.99 -0.03 -0.02  0.94  0.06  0.    0.97\n",
      "   0.05 -0.    0.98  0.05 -0.03  0.97  0.03 -0.02  0.99  0.02 -0.    0.99\n",
      "   0.03  0.03  0.97  0.07  0.01  1.01 -0.03 -0.02  0.95  0.04  0.01  1.03\n",
      "   0.05 -0.02  0.98  0.02 -0.06  0.98  0.02  0.01  0.98  0.02 -0.    0.98]\n",
      " [ 0.06  0.01  1.03 -0.04 -0.01  0.98  0.02  0.04  0.98  0.02 -0.01  0.99\n",
      "   0.02 -0.    0.99  0.1   0.    0.97 -0.03  0.01  1.02  0.05  0.01  1.03\n",
      "   0.07  0.    0.99  0.01  0.03  0.99  0.03  0.01  0.97  0.02 -0.    0.99\n",
      "   0.03  0.01  0.98  0.04 -0.02  0.98  0.   -0.01  0.99 -0.01 -0.02  0.94\n",
      "  -0.05 -0.02  0.96  0.01  0.04  0.99  0.02  0.01  0.97  0.02 -0.    0.97]\n",
      " [-0.04  0.    0.99 -0.07 -0.01  0.98 -0.03 -0.02  0.98 -0.02 -0.01  0.98\n",
      "  -0.05 -0.    0.98 -0.11  0.    0.98 -0.05 -0.02  0.98 -0.04  0.01  0.99\n",
      "  -0.09  0.03  0.99 -0.06  0.01  0.99 -0.05  0.    0.98 -0.03 -0.01  0.98\n",
      "  -0.07  0.01  0.98 -0.07 -0.02  0.98 -0.02  0.02  0.99 -0.05  0.    0.98\n",
      "  -0.07  0.    0.99 -0.04 -0.    0.98 -0.04 -0.01  0.98 -0.06  0.03  0.99]\n",
      " [ 0.02  0.01  0.97  0.02 -0.    0.98  0.02 -0.02  1.    0.01 -0.01  0.97\n",
      "   0.02 -0.01  0.95  0.02  0.    1.02 -0.05 -0.02  0.96  0.06 -0.    0.97\n",
      "   0.02  0.01  0.98  0.02 -0.    0.98  0.    0.01  1.   -0.04 -0.02  0.94\n",
      "  -0.01 -0.01  0.99  0.05  0.01  1.03 -0.04 -0.01  0.98  0.02 -0.03  0.99\n",
      "   0.02 -0.    0.98  0.02 -0.    0.98  0.05 -0.01  0.98 -0.02 -0.02  0.94]\n",
      " [-0.04 -0.01  0.98  0.   -0.02  0.98  0.02 -0.    0.99  0.02 -0.    0.98\n",
      "  -0.04 -0.02  0.97 -0.02 -0.02  0.94  0.02 -0.01  0.94  0.07  0.01  1.02\n",
      "   0.06 -0.03  0.97  0.02  0.01  0.97  0.02 -0.    0.98  0.02 -0.02  1.\n",
      "   0.01 -0.01  0.97  0.02 -0.01  0.95  0.02  0.    1.02 -0.05 -0.02  0.96\n",
      "   0.06 -0.    0.97  0.02  0.01  0.98  0.02 -0.    0.98  0.    0.01  1.  ]\n",
      " [ 0.05 -0.    0.98  0.01 -0.05  0.98  0.02 -0.01  0.98  0.02 -0.    0.98\n",
      "   0.04  0.03  0.97  0.03 -0.    0.99  0.02 -0.01  0.94 -0.02 -0.01  0.97\n",
      "  -0.02 -0.03  0.97  0.02  0.02  0.99  0.02 -0.01  0.99  0.02 -0.    0.99\n",
      "  -0.04 -0.05  0.99  0.05  0.    0.95 -0.02 -0.02  0.94  0.02  0.    1.01\n",
      "   0.01 -0.04  0.98  0.02 -0.01  0.99  0.02 -0.01  0.98  0.02 -0.02  0.99]\n",
      " [-0.03  0.01  1.01  0.02 -0.    0.94  0.07  0.    0.95  0.07  0.04  1.02\n",
      "   0.03  0.01  0.99  0.02  0.01  0.98  0.    0.01  1.   -0.06 -0.01  0.98\n",
      "   0.05 -0.    0.96 -0.03  0.    1.    0.06  0.03  1.02  0.04  0.01  0.99\n",
      "   0.02  0.01  0.98  0.02  0.    0.99 -0.04 -0.03  0.94  0.02  0.02  1.02\n",
      "   0.03  0.02  1.01  0.01 -0.01  0.95 -0.03 -0.02  0.96  0.02  0.01  0.98]\n",
      " [ 0.02  0.08  0.98 -0.02  0.1   1.   -0.02  0.08  0.95  0.03  0.1   1.\n",
      "   0.05  0.14  1.01 -0.01  0.08  0.96  0.02  0.08  1.01  0.01  0.11  0.96\n",
      "   0.06  0.15  1.01 -0.04  0.07  0.99 -0.02  0.03  1.    0.01  0.11  0.98\n",
      "  -0.01  0.04  1.    0.02  0.11  0.98 -0.04  0.13  0.94 -0.01  0.07  0.96\n",
      "  -0.06  0.04  0.98  0.02  0.1   0.98 -0.01  0.15  0.95  0.03  0.09  0.98]\n",
      " [-0.05 -0.    0.98 -0.11  0.02  0.98 -0.05 -0.02  0.98 -0.04  0.01  0.99\n",
      "  -0.08  0.02  0.99 -0.06  0.01  0.99 -0.05 -0.    0.98 -0.04  0.01  0.98\n",
      "  -0.07  0.    0.98 -0.06 -0.02  0.98 -0.03  0.03  0.99 -0.05  0.    0.98\n",
      "  -0.07 -0.    0.99 -0.04  0.    0.99 -0.03 -0.    0.98 -0.07 -0.02  0.98\n",
      "  -0.03  0.03  1.   -0.05 -0.    0.98 -0.03 -0.04  0.97 -0.06 -0.02  0.98]\n",
      " [ 0.02  0.11  0.98  0.04  0.1   1.02  0.03  0.09  0.95 -0.05  0.03  0.97\n",
      "  -0.    0.09  1.02 -0.07  0.05  1.02  0.03  0.11  0.97 -0.01  0.13  0.95\n",
      "  -0.    0.11  1.02 -0.06  0.12  0.97 -0.    0.09  0.99  0.05  0.07  1.01\n",
      "  -0.    0.11  1.01 -0.02  0.05  1.03  0.01  0.1   0.96 -0.05  0.03  0.98\n",
      "  -0.02  0.09  0.98 -0.    0.13  0.95  0.01  0.1   0.95 -0.06  0.06  0.96]\n",
      " [-0.07 -0.02  0.98 -0.04  0.01  0.99 -0.06  0.01  0.99 -0.02  0.02  1.\n",
      "  -0.05  0.    0.98 -0.03 -0.04  0.97 -0.06 -0.02  0.98 -0.07 -0.01  0.98\n",
      "  -0.06  0.03  0.99 -0.07  0.03  0.99 -0.05  0.    0.98 -0.   -0.01  0.99\n",
      "  -0.08 -0.02  0.98 -0.04  0.02  0.99 -0.02 -0.02  0.98 -0.05 -0.02  0.98\n",
      "  -0.05 -0.01  0.98 -0.09 -0.    0.98 -0.05  0.02  0.99 -0.04 -0.02  0.98]\n",
      " [-0.01 -0.02  0.97  0.03  0.    1.02  0.05 -0.    0.96 -0.01 -0.02  0.96\n",
      "  -0.01  0.01  1.    0.02  0.01  0.98  0.02 -0.    0.99  0.06 -0.01  0.97\n",
      "  -0.04 -0.02  0.95  0.07  0.01  1.02  0.05 -0.    0.96  0.06  0.03  0.99\n",
      "   0.02 -0.    0.99  0.02 -0.01  0.98  0.02 -0.01  0.98  0.06 -0.01  0.99\n",
      "   0.08  0.01  1.02  0.03  0.    1.02 -0.04 -0.02  0.95  0.08 -0.02  0.97]\n",
      " [ 0.05  0.16  0.99 -0.02  0.08  0.98 -0.04  0.02  0.99 -0.01  0.07  0.94\n",
      "  -0.05  0.04  1.    0.04  0.11  0.97  0.03  0.16  0.97 -0.01  0.09  0.99\n",
      "   0.02  0.1   1.01 -0.03  0.08  0.98  0.03  0.08  1.   -0.    0.08  0.98\n",
      "  -0.03  0.04  1.03 -0.03  0.07  1.01  0.03  0.08  1.   -0.01  0.06  0.97\n",
      "  -0.04  0.07  0.99  0.02  0.12  0.96 -0.06  0.08  1.01 -0.03  0.03  0.97]\n",
      " [-0.02  0.02  0.99 -0.05  0.    0.98 -0.07  0.    0.99 -0.04 -0.    0.98\n",
      "  -0.04 -0.01  0.98 -0.06  0.03  0.99 -0.01  0.02  1.   -0.05 -0.    0.98\n",
      "  -0.04 -0.04  0.97 -0.06 -0.02  0.98 -0.05  0.02  0.99 -0.08 -0.02  0.98\n",
      "  -0.04  0.03  0.99 -0.06  0.    0.98 -0.08  0.02  0.99 -0.07  0.01  0.98\n",
      "  -0.04 -0.01  0.98 -0.06  0.03  0.99 -0.06  0.    0.99 -0.06 -0.01  0.98]\n",
      " [ 0.02  0.03  1.01 -0.04 -0.01  0.97  0.08  0.02  0.98  0.   -0.    0.96\n",
      "   0.1   0.02  0.96  0.03 -0.02  0.94  0.02  0.    0.98  0.02  0.01  0.98\n",
      "  -0.03  0.03  1.08 -0.02  0.01  1.01 -0.02  0.    1.01  0.08  0.02  0.95\n",
      "  -0.01  0.03  1.08  0.03  0.01  1.    0.02  0.    0.99  0.03  0.02  0.98\n",
      "   0.1   0.01  0.95 -0.01  0.01  1.02  0.05  0.01  0.95 -0.05 -0.01  0.98]\n",
      " [-0.05  0.02  0.99 -0.1   0.    0.98 -0.05  0.    0.98 -0.04  0.04  1.\n",
      "  -0.04  0.01  0.99 -0.04  0.01  0.99 -0.08 -0.01  0.98 -0.08 -0.03  0.97\n",
      "  -0.05  0.    0.98 -0.09  0.03  0.98 -0.07  0.01  0.98 -0.07 -0.01  0.98\n",
      "  -0.06  0.03  0.99 -0.06  0.01  0.99 -0.05  0.01  0.98 -0.03 -0.02  0.98\n",
      "  -0.04 -0.    0.98 -0.06  0.02  0.99 -0.01  0.01  0.99 -0.05  0.    0.98]\n",
      " [ 0.02 -0.01  0.99  0.02 -0.05  0.97  0.   -0.    0.98 -0.   -0.02  0.93\n",
      "  -0.02 -0.01  0.97  0.06 -0.02  0.98  0.03  0.03  0.97  0.02  0.    0.98\n",
      "   0.02 -0.    0.98  0.04 -0.03  0.97 -0.01 -0.02  0.95 -0.   -0.02  0.93\n",
      "   0.08  0.01  1.02  0.04 -0.03  0.97  0.02  0.01  0.97  0.02 -0.    0.97\n",
      "   0.03  0.02  0.98  0.09  0.01  1.    0.08  0.01  1.02 -0.01 -0.02  0.94]\n",
      " [ 0.01 -0.05  0.99  0.02  0.    0.98  0.02 -0.    0.99  0.01  0.05  0.99\n",
      "   0.02 -0.01  0.96 -0.02 -0.02  0.94  0.04  0.01  1.02 -0.04 -0.01  0.99\n",
      "   0.02  0.01  0.98  0.02  0.    0.99  0.02 -0.01  0.98  0.04 -0.01  0.98\n",
      "  -0.01 -0.01  0.98  0.01 -0.02  0.94 -0.03 -0.02  0.96 -0.02 -0.04  0.99\n",
      "   0.02  0.    0.98  0.02 -0.    0.98  0.04  0.04  0.98 -0.04 -0.02  0.96]\n",
      " [ 0.   -0.01  0.99  0.02  0.    1.02  0.06  0.    0.98  0.09  0.02  0.98\n",
      "   0.03  0.02  0.97  0.02 -0.01  0.97  0.02 -0.01  1.    0.09  0.    0.98\n",
      "   0.05  0.01  1.02 -0.01 -0.02  0.94  0.06  0.01  1.    0.04 -0.04  0.97\n",
      "   0.02 -0.01  0.98  0.02 -0.    0.97  0.02  0.03  1.   -0.05 -0.02  0.95\n",
      "   0.03 -0.01  0.95 -0.02 -0.02  0.95  0.03  0.02  0.99  0.02 -0.04  0.99]\n",
      " [-0.03  0.06  0.96  0.01  0.07  0.99  0.03  0.09  1.01 -0.02  0.07  0.96\n",
      "  -0.03  0.04  0.99 -0.01  0.08  1.02 -0.05  0.02  0.97  0.02  0.11  0.95\n",
      "  -0.02  0.11  0.95 -0.    0.1   1.02 -0.03  0.12  0.94  0.01  0.1   0.98\n",
      "  -0.07  0.03  0.96 -0.01  0.08  0.95  0.04  0.14  0.99 -0.05  0.06  0.99\n",
      "  -0.04  0.05  0.99 -0.01  0.11  0.99 -0.02  0.11  0.93 -0.01  0.09  0.96]\n",
      " [-0.01 -0.01  0.99 -0.06 -0.    0.99 -0.08 -0.01  0.98 -0.04  0.01  0.99\n",
      "  -0.05  0.01  0.99 -0.07  0.02  0.99 -0.01 -0.01  0.99 -0.05  0.    0.98\n",
      "  -0.09 -0.04  0.96 -0.04 -0.01  0.98 -0.07 -0.01  0.98 -0.09 -0.01  0.98\n",
      "  -0.03 -0.02  0.98 -0.05 -0.    0.98 -0.07  0.02  0.98 -0.05 -0.02  0.98\n",
      "  -0.04  0.    0.99 -0.04  0.03  0.99 -0.05  0.    0.98 -0.06 -0.    0.99]\n",
      " [-0.02 -0.01  0.97 -0.05 -0.01  0.97 -0.05 -0.    0.97 -0.05 -0.02  0.97\n",
      "  -0.06  0.02  0.98 -0.06 -0.01  0.97 -0.05  0.03  0.98 -0.05  0.01  0.97\n",
      "  -0.05 -0.01  0.97 -0.06  0.03  0.98 -0.05  0.02  0.98 -0.06 -0.02  0.97\n",
      "  -0.09 -0.02  0.97 -0.05  0.    0.98 -0.1   0.02  0.97 -0.07  0.01  0.98\n",
      "  -0.04  0.    0.98 -0.09  0.01  0.98 -0.03  0.    0.98 -0.05  0.    0.98]\n",
      " [-0.04  0.02  0.99  0.03  0.12  0.97  0.04  0.08  1.01  0.02  0.08  0.98\n",
      "  -0.01  0.07  0.96 -0.02  0.07  0.95  0.04  0.1   0.99  0.02  0.08  0.99\n",
      "   0.    0.09  0.96  0.03  0.13  0.99  0.01  0.08  1.01  0.    0.05  1.04\n",
      "   0.    0.08  0.99 -0.01  0.06  1.    0.    0.11  0.98  0.06  0.13  1.03\n",
      "  -0.02  0.07  0.99 -0.03  0.1   0.95 -0.01  0.09  1.02 -0.07  0.05  0.93]\n",
      " [ 0.02  0.    0.98  0.02  0.01  0.98  0.02  0.01  0.98  0.09  0.02  0.99\n",
      "   0.08  0.01  0.95 -0.    0.01  1.01  0.04 -0.01  0.93  0.01  0.05  1.08\n",
      "   0.02 -0.    0.98  0.02  0.01  0.99  0.02 -0.03  0.92  0.04 -0.    0.94\n",
      "  -0.04 -0.    0.99  0.03  0.02  1.02  0.08 -0.    0.91  0.02  0.03  1.\n",
      "   0.02  0.    0.99  0.02  0.01  0.98  0.03  0.04  1.08  0.08  0.01  0.95]\n",
      " [-0.04 -0.02  0.98 -0.04  0.01  0.99 -0.03  0.01  0.99 -0.06 -0.04  0.97\n",
      "  -0.05 -0.    0.99 -0.11  0.    0.98 -0.06 -0.02  0.99 -0.06  0.    0.98\n",
      "  -0.08  0.01  0.99 -0.05  0.01  0.98 -0.05 -0.    0.98 -0.04  0.02  0.99\n",
      "  -0.07 -0.01  0.98 -0.05 -0.02  0.98 -0.01 -0.01  0.98 -0.06 -0.    0.99\n",
      "  -0.05  0.01  0.98 -0.03 -0.01  0.98 -0.04  0.    0.99 -0.08 -0.01  0.98]\n",
      " [ 0.02  0.    0.98  0.02 -0.    0.98  0.01 -0.02  1.   -0.05 -0.02  0.96\n",
      "   0.05  0.    0.97 -0.02 -0.02  0.96  0.09  0.01  0.99  0.03 -0.03  0.98\n",
      "   0.02 -0.01  0.99  0.02 -0.01  0.98 -0.04 -0.01  1.   -0.04 -0.02  0.95\n",
      "  -0.01 -0.02  0.94  0.08  0.02  1.02  0.06 -0.02  0.97  0.02 -0.02  0.99\n",
      "   0.02  0.    0.99  0.02 -0.01  0.97 -0.05 -0.03  0.97  0.08  0.01  1.  ]\n",
      " [ 0.02 -0.    0.97  0.   -0.04  0.99 -0.05 -0.02  0.96  0.05 -0.    0.97\n",
      "  -0.02 -0.01  0.97  0.02  0.01  0.99  0.01  0.03  0.99  0.02 -0.    0.98\n",
      "   0.02 -0.    0.97 -0.01 -0.06  0.98  0.02  0.    1.    0.03  0.01  1.02\n",
      "   0.08  0.01  1.    0.04  0.03  0.99  0.02  0.    0.97  0.02 -0.    0.99\n",
      "   0.02 -0.01  0.98 -0.03 -0.    0.97  0.05  0.01  1.02 -0.01 -0.02  0.94]\n",
      " [-0.01 -0.02  0.94 -0.01 -0.02  0.97  0.05 -0.01  0.97  0.02  0.01  0.98\n",
      "   0.02 -0.    0.98 -0.02 -0.02  0.99  0.08  0.01  1.    0.04 -0.01  0.95\n",
      "  -0.01 -0.02  0.94  0.05 -0.02  0.98  0.03  0.02  0.97  0.02 -0.01  0.97\n",
      "   0.03 -0.    0.99 -0.03 -0.01  0.97 -0.02 -0.01  0.98  0.03 -0.01  0.95\n",
      "   0.08  0.02  1.02 -0.02 -0.02  1.    0.02 -0.02  0.99  0.02 -0.    0.99]\n",
      " [-0.02  0.03  1.07  0.01 -0.03  0.95  0.02  0.01  0.99  0.02  0.01  0.98\n",
      "   0.07  0.02  0.98  0.02 -0.    0.95  0.07  0.02  0.99  0.1   0.02  0.97\n",
      "   0.07  0.02  0.95  0.03  0.02  0.98  0.02  0.    0.99  0.02  0.02  0.99\n",
      "   0.1   0.03  0.99  0.08  0.02  0.98  0.04  0.    0.95  0.08  0.03  1.\n",
      "   0.05  0.01  0.99  0.03  0.02  0.98  0.02  0.01  0.98  0.07 -0.04  0.86]\n",
      " [ 0.05  0.01  1.03 -0.04 -0.01  0.98  0.02 -0.03  0.99  0.02 -0.    0.98\n",
      "   0.02 -0.    0.98  0.05 -0.01  0.98 -0.02 -0.02  0.94  0.07  0.01  0.99\n",
      "  -0.02 -0.01  0.97  0.07 -0.    0.97  0.03  0.01  0.97  0.02 -0.03  0.99\n",
      "   0.02  0.01  0.99  0.1   0.01  1.    0.07  0.01  0.98 -0.01 -0.02  0.94\n",
      "   0.08  0.    0.99  0.04  0.05  0.97  0.02 -0.02  0.99  0.02 -0.    0.99]\n",
      " [ 0.02  0.09  1.   -0.04  0.06  0.99 -0.05  0.08  0.93  0.03  0.11  0.97\n",
      "   0.04  0.15  0.98 -0.    0.09  0.93  0.04  0.13  0.93 -0.04  0.06  1.\n",
      "  -0.05  0.01  0.98 -0.01  0.07  0.93 -0.    0.04  1.01  0.01  0.1   0.96\n",
      "   0.01  0.16  0.96 -0.    0.07  0.97  0.03  0.14  1.01 -0.05  0.06  0.99\n",
      "   0.03  0.07  1.01 -0.01  0.09  0.98 -0.    0.09  0.96 -0.03  0.06  1.  ]\n",
      " [ 0.02 -0.03  0.98  0.09  0.01  1.   -0.03 -0.02  0.97  0.06  0.01  1.03\n",
      "  -0.04 -0.01  0.98  0.02  0.04  0.98  0.02 -0.01  0.99  0.02 -0.    0.99\n",
      "   0.1   0.    0.97 -0.03  0.01  1.02  0.05  0.01  1.03  0.07  0.    0.99\n",
      "   0.01  0.03  0.99  0.03  0.01  0.97  0.02 -0.    0.99  0.03  0.01  0.98\n",
      "   0.04 -0.02  0.98  0.   -0.01  0.99 -0.01 -0.02  0.94 -0.05 -0.02  0.96]\n",
      " [ 0.04  0.15  1.02 -0.    0.09  0.99 -0.05  0.04  0.98 -0.    0.1   0.99\n",
      "  -0.04  0.06  0.92 -0.03  0.07  0.97  0.01  0.13  0.96 -0.    0.12  0.99\n",
      "  -0.02  0.05  0.96 -0.    0.09  1.    0.04  0.12  0.97  0.03  0.11  0.99\n",
      "   0.    0.11  0.99  0.03  0.14  0.98  0.03  0.1   1.    0.03  0.1   1.\n",
      "   0.03  0.1   0.96  0.03  0.14  0.99 -0.    0.09  0.94  0.05  0.11  1.03]\n",
      " [-0.04  0.11  0.94 -0.    0.11  1.02 -0.08  0.04  0.99 -0.03  0.07  0.98\n",
      "   0.05  0.15  0.99 -0.    0.09  1.01  0.04  0.13  1.03 -0.03  0.07  0.99\n",
      "  -0.07  0.05  0.96  0.    0.08  0.96  0.03  0.14  0.96  0.03  0.12  0.96\n",
      "   0.02  0.06  1.01 -0.02  0.09  0.98  0.01  0.13  0.99 -0.01  0.09  0.96\n",
      "  -0.06  0.05  0.99 -0.04  0.06  0.96 -0.    0.07  0.96 -0.04  0.04  0.97]\n",
      " [-0.05  0.05  0.98 -0.02  0.03  0.98  0.02  0.09  0.95 -0.    0.09  1.01\n",
      "  -0.02  0.08  1.01 -0.06  0.02  0.94 -0.04  0.08  1.    0.01  0.15  0.96\n",
      "  -0.02  0.07  0.98 -0.04  0.11  0.95 -0.04  0.06  1.    0.05  0.14  0.99\n",
      "  -0.    0.11  1.01  0.06  0.13  0.98 -0.01  0.09  0.97 -0.07  0.09  0.95\n",
      "  -0.01  0.07  0.95  0.03  0.13  0.97 -0.04  0.06  0.98  0.05  0.12  0.99]\n",
      " [-0.1  -0.03  0.97 -0.04 -0.01  0.98 -0.06 -0.01  0.98 -0.02 -0.02  0.98\n",
      "  -0.07  0.02  0.99 -0.06  0.03  0.98 -0.03 -0.01  0.98 -0.04  0.02  0.99\n",
      "  -0.06 -0.02  0.98 -0.01 -0.01  0.98 -0.06  0.    0.98 -0.06  0.01  0.99\n",
      "  -0.07 -0.02  0.98 -0.05  0.02  0.99 -0.04  0.01  0.99 -0.06  0.03  0.99\n",
      "  -0.05  0.    0.98 -0.03  0.03  0.99 -0.05  0.03  0.99 -0.06  0.01  0.99]\n",
      " [-0.04  0.13  0.94 -0.01  0.07  0.96 -0.06  0.04  0.98  0.02  0.1   0.98\n",
      "  -0.01  0.15  0.95  0.03  0.09  0.98  0.01  0.08  1.   -0.    0.12  1.\n",
      "   0.04  0.13  0.98  0.01  0.09  0.98  0.02  0.11  0.96 -0.03  0.06  1.\n",
      "   0.05  0.12  0.98  0.02  0.15  0.99 -0.04  0.08  1.   -0.    0.1   0.95\n",
      "   0.02  0.11  0.96  0.05  0.14  1.02 -0.04  0.06  1.   -0.02  0.04  1.  ]\n",
      " [ 0.03  0.02  1.    0.02  0.    0.98  0.02  0.01  0.99  0.07  0.02  0.99\n",
      "   0.06  0.02  1.01  0.05  0.02  1.   -0.05 -0.01  0.99  0.06  0.05  1.03\n",
      "   0.02  0.01  0.98  0.02  0.    0.99  0.02 -0.    0.97  0.09  0.01  0.95\n",
      "   0.05  0.    0.95  0.01  0.01  1.01 -0.06 -0.01  0.99  0.05  0.01  0.95\n",
      "   0.02  0.01  0.98  0.02  0.    0.98  0.02  0.04  1.04 -0.03  0.    1.01]\n",
      " [-0.01  0.08  0.96 -0.06  0.06  1.   -0.05  0.05  0.94  0.03  0.11  0.97\n",
      "  -0.01  0.1   0.95 -0.01  0.09  0.94 -0.02  0.12  0.93 -0.    0.1   0.97\n",
      "   0.    0.13  0.95 -0.01  0.07  0.98  0.07  0.12  0.95 -0.02  0.09  0.97\n",
      "  -0.06  0.08  0.95 -0.01  0.09  0.97  0.03  0.07  1.02 -0.04  0.07  0.98\n",
      "  -0.05  0.12  0.94  0.    0.07  0.96  0.03  0.15  0.97 -0.05  0.06  1.  ]\n",
      " [ 0.03  0.08  1.01 -0.01  0.08  0.98  0.02  0.12  0.96 -0.04  0.06  1.\n",
      "   0.02  0.08  1.    0.02  0.07  0.98 -0.    0.11  1.    0.02  0.11  1.\n",
      "  -0.04  0.06  0.99 -0.07  0.03  0.94 -0.02  0.09  0.97  0.    0.12  0.95\n",
      "  -0.01  0.07  0.96  0.04  0.13  0.93  0.04  0.11  0.96 -0.01  0.02  1.01\n",
      "  -0.    0.1   0.98  0.04  0.07  1.   -0.02  0.07  0.99 -0.05  0.04  0.98]\n",
      " [-0.04  0.07  0.99 -0.    0.1   0.95  0.02  0.08  0.97  0.04  0.11  1.02\n",
      "   0.01  0.09  0.95 -0.01  0.04  1.    0.01  0.11  0.99 -0.    0.06  1.03\n",
      "   0.03  0.12  0.97 -0.05  0.09  0.95 -0.    0.11  1.02 -0.07  0.1   0.99\n",
      "  -0.04  0.07  0.99  0.06  0.12  1.   -0.01  0.1   1.    0.02  0.18  1.01\n",
      "  -0.02  0.08  0.97  0.06  0.09  1.01 -0.01  0.09  0.99 -0.04  0.02  0.99]\n",
      " [-0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.06  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.06  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [ 0.04  0.14  1.03  0.03  0.12  0.97  0.02  0.06  1.01 -0.02  0.09  0.99\n",
      "   0.01  0.13  0.99 -0.01  0.08  0.96 -0.06  0.06  1.   -0.05  0.05  0.94\n",
      "   0.03  0.11  0.97 -0.01  0.1   0.95 -0.01  0.09  0.94 -0.02  0.12  0.93\n",
      "  -0.    0.1   0.97  0.    0.13  0.95 -0.01  0.07  0.98  0.07  0.12  0.95\n",
      "  -0.02  0.09  0.97 -0.06  0.08  0.95 -0.01  0.09  0.97  0.03  0.07  1.02]\n",
      " [ 0.01 -0.03  0.95  0.02  0.01  0.99  0.02  0.01  0.98  0.07  0.02  0.98\n",
      "   0.02 -0.    0.95  0.07  0.02  0.99  0.1   0.02  0.97  0.07  0.02  0.95\n",
      "   0.03  0.02  0.98  0.02  0.    0.99  0.02  0.02  0.99  0.1   0.03  0.99\n",
      "   0.08  0.02  0.98  0.04  0.    0.95  0.08  0.03  1.    0.05  0.01  0.99\n",
      "   0.03  0.02  0.98  0.02  0.01  0.98  0.07 -0.04  0.86 -0.04 -0.    1.01]\n",
      " [ 0.    0.09  1.02 -0.08  0.05  1.01  0.02  0.11  0.97  0.01  0.15  0.95\n",
      "  -0.    0.11  1.02 -0.08  0.1   0.98  0.03  0.11  0.97  0.05  0.15  0.98\n",
      "  -0.01  0.1   1.    0.02  0.09  1.02 -0.03  0.08  0.98  0.05  0.07  1.01\n",
      "   0.01  0.07  0.97 -0.03  0.03  1.01  0.03  0.06  1.    0.05  0.11  0.99\n",
      "   0.02  0.08  0.98 -0.02  0.1   1.   -0.02  0.08  0.95  0.03  0.1   1.  ]\n",
      " [ 0.02 -0.    0.97  0.01 -0.06  0.98  0.04  0.    1.01  0.07  0.01  1.01\n",
      "  -0.01 -0.02  0.94  0.07 -0.02  0.98  0.02 -0.01  0.99  0.02 -0.01  0.98\n",
      "   0.03  0.01  0.99  0.03 -0.02  0.98 -0.04 -0.02  0.95  0.07  0.01  0.99\n",
      "  -0.02 -0.02  0.96 -0.02  0.01  1.    0.02  0.    0.98  0.02 -0.    0.98\n",
      "   0.01 -0.02  1.   -0.05 -0.02  0.96  0.05  0.    0.97 -0.02 -0.02  0.96]\n",
      " [ 0.01  0.    0.96 -0.03  0.02  1.03  0.02  0.05  1.04  0.02 -0.    0.99\n",
      "   0.02  0.    0.98  0.02  0.05  1.06  0.   -0.01  0.94 -0.03 -0.    0.98\n",
      "  -0.02  0.01  1.01  0.    0.04  1.08  0.02  0.01  0.98  0.02  0.01  0.98\n",
      "   0.02  0.01  0.99 -0.05  0.01  1.06 -0.03  0.01  1.01  0.02 -0.    0.94\n",
      "   0.07  0.    0.95  0.07  0.04  1.02  0.03  0.01  0.99  0.02  0.01  0.98]\n",
      " [-0.09  0.02  0.98 -0.05  0.    0.98 -0.04 -0.01  0.98 -0.05 -0.02  0.97\n",
      "  -0.04  0.    0.98 -0.03  0.    0.99 -0.1   0.01  0.98 -0.06 -0.    0.98\n",
      "  -0.03  0.04  1.   -0.06 -0.02  0.98 -0.07 -0.01  0.98 -0.07  0.03  0.99\n",
      "  -0.09 -0.01  0.97 -0.06  0.    0.98 -0.04  0.03  0.99 -0.07 -0.01  0.98\n",
      "  -0.04  0.01  0.99 -0.01  0.01  0.99 -0.06  0.    0.99 -0.05  0.01  0.98]\n",
      " [-0.08  0.02  0.99 -0.05  0.01  0.98 -0.05 -0.    0.98 -0.03  0.01  0.98\n",
      "  -0.07 -0.    0.98 -0.06 -0.02  0.98 -0.01 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06  0.01  0.99 -0.04 -0.01  0.98 -0.04  0.    0.98 -0.07 -0.02  0.98\n",
      "  -0.01  0.    0.99 -0.05 -0.    0.98 -0.09 -0.02  0.97 -0.04 -0.01  0.98\n",
      "  -0.07 -0.    0.98 -0.04 -0.03  0.98 -0.05  0.04  1.   -0.05  0.    0.98]\n",
      " [-0.07 -0.02  0.98 -0.05  0.02  0.99 -0.06 -0.03  0.98 -0.06  0.02  1.\n",
      "  -0.06  0.    0.98 -0.08  0.02  0.99 -0.06  0.03  0.99 -0.01  0.02  1.\n",
      "  -0.05 -0.    0.98 -0.06 -0.    0.99 -0.06  0.    0.98 -0.05  0.    0.98\n",
      "  -0.06  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98]\n",
      " [ 0.04  0.02  1.01  0.02  0.01  0.98  0.02  0.01  0.99  0.09  0.01  0.94\n",
      "   0.08  0.01  0.96  0.02  0.01  1.01  0.1   0.03  0.99  0.07  0.02  0.95\n",
      "   0.03  0.02  0.98  0.02  0.01  0.99  0.02  0.02  1.    0.09  0.03  1.\n",
      "   0.08  0.02  0.98  0.02  0.    0.96  0.06  0.03  0.98  0.03 -0.01  0.96\n",
      "   0.02  0.01  0.98  0.02  0.01  0.98 -0.05  0.    1.03  0.07  0.01  0.95]\n",
      " [-0.05  0.05  1.   -0.04  0.01  0.99 -0.07 -0.01  0.98 -0.08 -0.01  0.98\n",
      "  -0.05  0.02  0.99 -0.05  0.    0.98 -0.04  0.02  0.99 -0.04  0.    0.98\n",
      "  -0.05  0.02  0.99 -0.09  0.01  0.98 -0.05  0.    0.98 -0.05 -0.01  0.98\n",
      "  -0.06 -0.01  0.98 -0.05 -0.02  0.98 -0.03  0.    0.99 -0.05 -0.03  0.97\n",
      "  -0.05  0.    0.98 -0.1  -0.03  0.97 -0.04 -0.01  0.98 -0.06 -0.01  0.98]\n",
      " [-0.09  0.02  0.99 -0.05 -0.03  0.97 -0.05 -0.    0.98 -0.03  0.01  0.98\n",
      "  -0.05  0.02  0.99 -0.06 -0.02  0.98 -0.01  0.    0.99 -0.06  0.    0.98\n",
      "  -0.07  0.01  0.99 -0.08  0.    0.99 -0.04  0.01  0.99 -0.06 -0.02  0.98\n",
      "  -0.02 -0.02  0.98 -0.06 -0.    0.98 -0.1  -0.02  0.98 -0.04 -0.01  0.98\n",
      "  -0.07 -0.    0.98 -0.03  0.    0.99 -0.06  0.03  1.   -0.06 -0.    0.98]\n",
      " [-0.03 -0.01  0.97  0.06  0.06  1.07  0.02  0.01  0.99  0.02  0.    0.99\n",
      "   0.01 -0.01  0.97  0.08  0.03  1.01  0.09  0.02  0.97 -0.01 -0.    0.97\n",
      "   0.04 -0.01  0.93  0.04 -0.01  0.96  0.03  0.01  0.98  0.02  0.01  0.98\n",
      "   0.02  0.07  1.11 -0.05 -0.01  0.98  0.02  0.01  1.01  0.09  0.02  0.97\n",
      "  -0.04  0.01  1.05  0.02  0.02  0.98  0.02  0.01  0.99  0.02  0.01  1.  ]\n",
      " [ 0.01  0.14  0.96 -0.    0.07  0.97 -0.    0.07  1.    0.01  0.1   0.95\n",
      "   0.03  0.12  0.97 -0.02  0.11  0.99 -0.03  0.05  1.    0.02  0.11  0.96\n",
      "  -0.05  0.07  0.97 -0.03  0.07  0.97  0.03  0.1   0.97 -0.    0.07  1.\n",
      "  -0.03  0.07  1.01  0.    0.06  1.03 -0.03  0.07  0.99 -0.03  0.04  1.\n",
      "   0.    0.1   0.99  0.03  0.09  1.04 -0.04  0.1   0.98 -0.07  0.03  0.96]\n",
      " [-0.06  0.06  1.   -0.05  0.05  0.94  0.03  0.11  0.97 -0.01  0.1   0.95\n",
      "  -0.01  0.09  0.94 -0.02  0.12  0.93 -0.    0.1   0.97  0.    0.13  0.95\n",
      "  -0.01  0.07  0.98  0.07  0.12  0.95 -0.02  0.09  0.97 -0.06  0.08  0.95\n",
      "  -0.01  0.09  0.97  0.03  0.07  1.02 -0.04  0.07  0.98 -0.05  0.12  0.94\n",
      "   0.    0.07  0.96  0.03  0.15  0.97 -0.05  0.06  1.    0.05  0.09  1.  ]\n",
      " [ 0.02 -0.    0.99  0.03  0.03  0.99  0.05  0.01  1.    0.02  0.    1.02\n",
      "   0.07  0.01  1.    0.09  0.02  1.    0.04  0.05  0.97  0.02 -0.01  0.99\n",
      "   0.02 -0.    0.97 -0.   -0.07  0.98  0.08  0.01  1.02  0.07  0.01  0.99\n",
      "   0.   -0.02  0.94  0.09  0.02  0.98  0.02 -0.01  0.98  0.02 -0.01  0.98\n",
      "   0.02 -0.01  0.98 -0.02 -0.03  0.98  0.02  0.    1.01 -0.01 -0.02  0.94]\n",
      " [ 0.02  0.01  0.98  0.07 -0.04  0.86 -0.04 -0.    1.01 -0.01  0.01  1.02\n",
      "   0.09  0.02  0.98  0.    0.04  1.09  0.03  0.01  0.99  0.02  0.01  0.99\n",
      "   0.03  0.03  0.99  0.1   0.02  0.97  0.03 -0.    0.95  0.05  0.02  1.\n",
      "   0.1   0.02  0.96  0.04 -0.04  0.87  0.02  0.01  0.98  0.02  0.01  0.98\n",
      "  -0.    0.04  1.04  0.05  0.03  1.02  0.06  0.01  0.95 -0.02 -0.    0.97]\n",
      " [-0.    0.11  1.02 -0.07  0.03  0.99 -0.05  0.06  0.99  0.05  0.15  0.99\n",
      "  -0.01  0.08  0.98  0.05  0.09  1.01 -0.03  0.07  0.99 -0.07  0.06  0.96\n",
      "  -0.    0.09  0.98 -0.04  0.05  0.91 -0.03  0.07  0.98  0.    0.13  0.96\n",
      "   0.    0.12  0.98 -0.02  0.05  0.97 -0.02  0.07  1.    0.01  0.06  1.\n",
      "  -0.04  0.05  0.95 -0.04  0.07  0.99 -0.02  0.09  0.95 -0.    0.1   0.94]\n",
      " [-0.05  0.02  0.99 -0.04  0.02  0.99 -0.09  0.02  0.98 -0.06  0.    0.99\n",
      "  -0.02  0.03  1.   -0.04 -0.01  0.99 -0.05 -0.02  0.98 -0.08  0.01  0.98\n",
      "  -0.1  -0.01  0.97 -0.05  0.    0.98 -0.04  0.04  1.   -0.05 -0.02  0.98\n",
      "  -0.06  0.01  0.99 -0.02  0.02  0.99 -0.05 -0.01  0.97 -0.05  0.    0.98\n",
      "  -0.06 -0.03  0.98 -0.06 -0.01  0.98 -0.03  0.    0.99 -0.04 -0.03  0.98]\n",
      " [-0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98]\n",
      " [ 0.09  0.01  1.    0.03 -0.05  0.97  0.02 -0.02  0.99  0.02 -0.    0.99\n",
      "  -0.    0.01  0.99  0.09  0.01  1.01 -0.02 -0.02  0.94  0.01  0.    1.02\n",
      "  -0.04 -0.01  0.98  0.   -0.02  0.98  0.02 -0.    0.99  0.02 -0.    0.98\n",
      "  -0.04 -0.02  0.97 -0.02 -0.02  0.94  0.02 -0.01  0.94  0.07  0.01  1.02\n",
      "   0.06 -0.03  0.97  0.02  0.01  0.97  0.02 -0.    0.98  0.02 -0.02  1.  ]\n",
      " [-0.04  0.06  0.99  0.05  0.16  0.98 -0.01  0.1   1.    0.01  0.17  1.\n",
      "   0.04  0.11  0.96  0.04  0.06  1.01 -0.01  0.1   1.   -0.02  0.04  1.02\n",
      "   0.02  0.11  0.96  0.05  0.08  1.01  0.02  0.08  0.98 -0.02  0.05  0.96\n",
      "  -0.03  0.06  0.96  0.04  0.11  0.99  0.02  0.08  0.99 -0.    0.08  0.96\n",
      "   0.03  0.13  0.99  0.    0.08  1.01  0.02  0.08  1.04 -0.02  0.07  0.99]\n",
      " [ 0.04  0.04  0.98  0.03  0.01  0.97  0.02 -0.01  0.98  0.02 -0.01  1.\n",
      "   0.04  0.02  0.99 -0.   -0.02  0.94  0.03  0.01  1.03  0.07  0.01  1.01\n",
      "   0.06  0.01  0.97  0.02 -0.01  0.98  0.02 -0.01  0.97 -0.01 -0.04  1.01\n",
      "   0.09  0.01  1.01 -0.02 -0.01  0.97  0.02  0.    1.02  0.01 -0.03  0.98\n",
      "   0.02  0.02  0.99  0.02 -0.01  0.98  0.02 -0.    0.97  0.06  0.02  1.  ]\n",
      " [ 0.02  0.12  0.98  0.    0.11  0.95  0.02  0.11  0.96  0.02  0.14  0.98\n",
      "   0.02  0.11  0.96  0.03  0.14  0.97 -0.01  0.07  0.93  0.05  0.14  0.95\n",
      "  -0.01  0.07  1.    0.01  0.04  1.01 -0.    0.07  0.94  0.07  0.12  0.94\n",
      "  -0.02  0.07  0.99 -0.04  0.02  0.99 -0.    0.08  0.99  0.03  0.09  1.03\n",
      "  -0.04  0.06  0.99  0.03  0.06  1.01 -0.01  0.09  0.98 -0.04  0.04  1.02]\n",
      " [ 0.02 -0.02  0.99  0.05  0.02  0.99  0.01 -0.01  0.95  0.07  0.01  1.01\n",
      "   0.09  0.01  1.    0.03 -0.05  0.97  0.02 -0.02  0.99  0.02 -0.    0.99\n",
      "  -0.    0.01  0.99  0.09  0.01  1.01 -0.02 -0.02  0.94  0.01  0.    1.02\n",
      "  -0.04 -0.01  0.98  0.   -0.02  0.98  0.02 -0.    0.99  0.02 -0.    0.98\n",
      "  -0.04 -0.02  0.97 -0.02 -0.02  0.94  0.02 -0.01  0.94  0.07  0.01  1.02]\n",
      " [ 0.07  0.    0.95  0.07  0.04  1.02  0.03  0.01  0.99  0.02  0.01  0.98\n",
      "   0.    0.01  1.   -0.06 -0.01  0.98  0.05 -0.    0.96 -0.03  0.    1.\n",
      "   0.06  0.03  1.02  0.04  0.01  0.99  0.02  0.01  0.98  0.02  0.    0.99\n",
      "  -0.04 -0.03  0.94  0.02  0.02  1.02  0.03  0.02  1.01  0.01 -0.01  0.95\n",
      "  -0.03 -0.02  0.96  0.02  0.01  0.98  0.03  0.01  0.98  0.02  0.    0.99]\n",
      " [-0.06  0.03  0.99 -0.07  0.01  0.99 -0.05 -0.02  0.98 -0.1  -0.01  0.97\n",
      "  -0.05  0.01  0.98 -0.07  0.05  1.   -0.07  0.01  0.98 -0.04  0.    0.98\n",
      "  -0.09  0.02  0.99 -0.04 -0.02  0.98 -0.05  0.    0.98 -0.06  0.02  0.99\n",
      "  -0.06  0.02  0.99 -0.05 -0.01  0.98 -0.05  0.03  0.99 -0.06  0.    0.99\n",
      "  -0.06 -0.01  0.98 -0.06 -0.02  0.97 -0.04  0.01  0.98 -0.04  0.01  0.99]\n",
      " [ 0.07 -0.01  0.98  0.02 -0.01  0.98  0.02 -0.01  0.98  0.03  0.01  0.96\n",
      "   0.01 -0.02  0.98  0.01 -0.    1.   -0.01 -0.02  0.94 -0.05 -0.02  0.96\n",
      "  -0.03 -0.03  1.    0.02  0.01  0.97  0.02  0.    0.97  0.02 -0.05  0.99\n",
      "   0.08  0.    0.99  0.01  0.    1.01  0.07  0.01  0.99 -0.05 -0.01  0.98\n",
      "   0.02  0.03  0.99  0.02  0.    0.98  0.02 -0.    0.99  0.02 -0.06  1.  ]\n",
      " [ 0.02 -0.    0.98 -0.01 -0.02  0.98  0.06  0.01  0.99  0.07  0.01  1.02\n",
      "  -0.   -0.    0.98  0.03 -0.04  0.97  0.02 -0.01  0.98  0.02 -0.01  0.97\n",
      "   0.02 -0.    1.    0.07 -0.    0.98  0.08  0.01  1.02 -0.01 -0.02  0.94\n",
      "  -0.01 -0.02  0.97  0.05 -0.01  0.97  0.02  0.01  0.98  0.02 -0.    0.98\n",
      "  -0.02 -0.02  0.99  0.08  0.01  1.    0.04 -0.01  0.95 -0.01 -0.02  0.94]\n",
      " [ 0.01 -0.04  0.91  0.05  0.02  1.01 -0.01 -0.    0.96 -0.02  0.    1.01\n",
      "  -0.04 -0.02  0.99  0.03  0.04  1.03  0.02  0.    0.99  0.02  0.    0.99\n",
      "  -0.05 -0.02  0.98 -0.02  0.    1.01 -0.02  0.    1.01  0.03 -0.    0.94\n",
      "  -0.03 -0.01  1.    0.02  0.02  0.98  0.02  0.01  0.98  0.03  0.01  1.\n",
      "   0.1   0.02  0.94 -0.    0.01  1.02  0.05  0.    0.95  0.08  0.03  1.  ]\n",
      " [ 0.03 -0.    1.   -0.04 -0.02  0.96 -0.02 -0.02  0.94  0.06  0.01  1.02\n",
      "  -0.04 -0.01  0.97  0.04 -0.04  0.97  0.02 -0.02  0.99  0.02 -0.    0.98\n",
      "   0.04  0.05  0.98  0.03  0.    1.    0.08  0.01  1.01  0.03 -0.01  0.95\n",
      "  -0.03  0.    0.98  0.02 -0.03  0.98  0.02  0.    0.99  0.02 -0.    0.98\n",
      "  -0.05 -0.01  0.98  0.08  0.01  1.02  0.01  0.    1.01  0.05 -0.    0.97]\n",
      " [-0.04  0.05  0.99 -0.    0.07  0.98  0.02  0.13  0.97  0.    0.1   0.95\n",
      "   0.03  0.1   0.99  0.02  0.11  0.97 -0.03  0.08  1.01 -0.04  0.05  0.96\n",
      "  -0.02  0.07  0.98 -0.06  0.03  0.94 -0.05  0.07  0.99 -0.03  0.12  0.95\n",
      "  -0.    0.1   1.03 -0.06  0.03  1.01 -0.04  0.06  1.    0.02  0.16  0.96\n",
      "  -0.01  0.09  0.99 -0.06  0.11  0.97 -0.02  0.08  0.97  0.01  0.14  0.96]\n",
      " [-0.02  0.03  0.99 -0.06  0.    0.99 -0.06 -0.01  0.98 -0.08 -0.    0.98\n",
      "  -0.05 -0.01  0.98 -0.05 -0.02  0.98 -0.06  0.04  0.99 -0.06  0.    0.99\n",
      "  -0.01  0.02  0.99 -0.05 -0.02  0.98 -0.06 -0.01  0.98 -0.08  0.01  0.99\n",
      "  -0.07 -0.03  0.97 -0.05  0.    0.98 -0.09  0.03  0.98 -0.04 -0.01  0.98\n",
      "  -0.07  0.    0.98 -0.08  0.03  0.99 -0.06  0.    0.99 -0.06  0.    0.99]\n",
      " [-0.06  0.09  0.96 -0.    0.09  0.97  0.05  0.13  0.98 -0.01  0.08  0.98\n",
      "   0.04  0.14  1.03  0.03  0.12  0.97  0.02  0.06  1.01 -0.02  0.09  0.99\n",
      "   0.01  0.13  0.99 -0.01  0.08  0.96 -0.06  0.06  1.   -0.05  0.05  0.94\n",
      "   0.03  0.11  0.97 -0.01  0.1   0.95 -0.01  0.09  0.94 -0.02  0.12  0.93\n",
      "  -0.    0.1   0.97  0.    0.13  0.95 -0.01  0.07  0.98  0.07  0.12  0.95]\n",
      " [-0.05 -0.01  0.99  0.08  0.01  0.95  0.08  0.02  0.98 -0.03 -0.01  0.97\n",
      "  -0.03 -0.    1.02  0.02  0.    0.98  0.02  0.01  0.98  0.03  0.01  0.98\n",
      "   0.09  0.02  0.98  0.07  0.01  0.95  0.    0.01  1.01  0.04 -0.01  0.93\n",
      "   0.04  0.07  1.08  0.02 -0.    0.98  0.02  0.    0.99 -0.04 -0.04  0.94\n",
      "  -0.05 -0.    1.01  0.07  0.02  0.99  0.04  0.    0.95 -0.03 -0.02  0.98]\n",
      " [-0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.06 -0.    0.99\n",
      "  -0.06 -0.03  0.98 -0.05  0.02  0.99 -0.05 -0.02  0.98 -0.05  0.01  0.99\n",
      "  -0.06 -0.01  0.98 -0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.99\n",
      "  -0.05 -0.02  0.97 -0.05  0.01  0.99 -0.05 -0.    0.98 -0.05  0.02  0.98]\n",
      " [-0.05 -0.03  0.98 -0.06 -0.    0.99 -0.07  0.02  1.   -0.06 -0.02  0.98\n",
      "  -0.06 -0.02  0.98 -0.03 -0.01  0.98 -0.07 -0.03  0.97 -0.05  0.    0.98\n",
      "  -0.11  0.    0.98 -0.06 -0.02  0.98 -0.07  0.01  0.98 -0.09 -0.    0.98\n",
      "  -0.03 -0.02  0.98 -0.05 -0.    0.98 -0.08  0.02  0.98 -0.07 -0.01  0.98\n",
      "  -0.04  0.01  0.99 -0.06  0.03  0.99 -0.05  0.    0.98 -0.06 -0.    0.99]\n",
      " [ 0.04  0.13  0.93 -0.04  0.06  1.   -0.05  0.01  0.98 -0.01  0.07  0.93\n",
      "  -0.    0.04  1.01  0.01  0.1   0.96  0.01  0.16  0.96 -0.    0.07  0.97\n",
      "   0.03  0.14  1.01 -0.05  0.06  0.99  0.03  0.07  1.01 -0.01  0.09  0.98\n",
      "  -0.    0.09  0.96 -0.03  0.06  1.   -0.02  0.05  0.99  0.    0.06  0.98\n",
      "  -0.01  0.1   1.    0.01  0.11  0.95 -0.04  0.06  0.99 -0.04  0.05  0.96]\n",
      " [-0.04  0.05  0.99 -0.01  0.1   0.94  0.02  0.15  0.98 -0.04  0.08  0.99\n",
      "   0.03  0.11  1.01  0.02  0.11  0.97  0.04  0.1   1.    0.    0.1   0.96\n",
      "  -0.06  0.05  0.95 -0.01  0.1   0.99 -0.08  0.09  0.99 -0.04  0.06  1.\n",
      "   0.06  0.13  0.99 -0.01  0.11  1.01  0.02  0.09  1.04 -0.03  0.08  0.98\n",
      "   0.03  0.07  1.01 -0.01  0.08  0.98 -0.04  0.07  0.98 -0.    0.09  0.96]\n",
      " [-0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.99\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.11  0.02  0.98 -0.05 -0.02  0.98 -0.04  0.01  0.99 -0.08  0.02  0.99\n",
      "  -0.06  0.01  0.99 -0.05 -0.    0.98 -0.04  0.01  0.98 -0.07  0.    0.98\n",
      "  -0.06 -0.02  0.98 -0.03  0.03  0.99 -0.05  0.    0.98 -0.07 -0.    0.99\n",
      "  -0.04  0.    0.99 -0.03 -0.    0.98 -0.07 -0.02  0.98 -0.03  0.03  1.\n",
      "  -0.05 -0.    0.98 -0.03 -0.04  0.97 -0.06 -0.02  0.98 -0.05  0.02  0.99]\n",
      " [-0.05 -0.    0.98 -0.04  0.02  0.99 -0.04  0.02  0.99 -0.06 -0.02  0.98\n",
      "  -0.02  0.02  0.99 -0.05  0.    0.98 -0.07 -0.    0.99 -0.08  0.01  0.99\n",
      "  -0.04 -0.    0.98 -0.07 -0.02  0.98 -0.01  0.01  0.99 -0.05  0.    0.99\n",
      "  -0.05 -0.05  0.96 -0.07 -0.02  0.98 -0.07  0.01  0.99 -0.03  0.01  0.99\n",
      "  -0.07  0.01  0.99 -0.05 -0.    0.98 -0.05 -0.04  0.98 -0.07 -0.01  0.98]\n",
      " [ 0.02 -0.    0.99  0.02 -0.01  0.98  0.02 -0.01  0.98  0.06 -0.01  0.99\n",
      "   0.08  0.01  1.02  0.03  0.    1.02 -0.04 -0.02  0.95  0.08 -0.02  0.97\n",
      "   0.02  0.01  0.97  0.02 -0.    0.97  0.   -0.04  0.99 -0.05 -0.02  0.96\n",
      "   0.05 -0.    0.97 -0.02 -0.01  0.97  0.02  0.01  0.99  0.01  0.03  0.99\n",
      "   0.02 -0.    0.98  0.02 -0.    0.97 -0.01 -0.06  0.98  0.02  0.    1.  ]\n",
      " [-0.07  0.    0.98 -0.09  0.01  0.98 -0.05 -0.    0.99 -0.03  0.    1.\n",
      "  -0.04 -0.01  0.98 -0.06 -0.01  0.98 -0.07  0.02  0.99 -0.09  0.02  0.98\n",
      "  -0.05  0.    0.98 -0.01 -0.01  0.99 -0.07 -0.01  0.98 -0.04 -0.    0.98\n",
      "  -0.01 -0.01  0.98 -0.05 -0.02  0.97 -0.05 -0.    0.98 -0.06 -0.02  0.98\n",
      "  -0.07  0.    0.98 -0.04  0.02  0.99 -0.02 -0.01  0.99 -0.06 -0.    0.99]\n",
      " [-0.05 -0.02  0.98 -0.04  0.01  0.99 -0.07  0.01  0.98 -0.08  0.03  0.99\n",
      "  -0.05 -0.    0.99 -0.04 -0.03  0.97 -0.07 -0.02  0.98 -0.07  0.01  0.99\n",
      "  -0.06  0.03  0.99 -0.09  0.01  0.98 -0.06  0.    0.98 -0.02  0.01  1.\n",
      "  -0.07 -0.01  0.98 -0.05 -0.01  0.98 -0.02 -0.02  0.98 -0.05 -0.02  0.98\n",
      "  -0.06  0.    0.98 -0.09 -0.01  0.98 -0.05  0.02  0.99 -0.05 -0.02  0.98]\n",
      " [ 0.02 -0.    0.99 -0.06 -0.    0.98  0.02 -0.    1.    0.05  0.01  1.03\n",
      "  -0.01 -0.02  0.95 -0.03 -0.02  1.    0.02 -0.    0.98  0.02 -0.    0.98\n",
      "   0.03  0.03  0.97 -0.04 -0.02  0.96 -0.03 -0.02  0.95  0.07  0.01  1.01\n",
      "  -0.02 -0.    0.98  0.03 -0.05  1.    0.02  0.01  0.98  0.02 -0.    0.98\n",
      "   0.06  0.06  0.99 -0.02 -0.01  0.98  0.06  0.01  1.03  0.06  0.    0.98]\n",
      " [-0.04  0.01  0.99 -0.06 -0.02  0.98 -0.02 -0.02  0.98 -0.06 -0.    0.98\n",
      "  -0.1  -0.02  0.98 -0.04 -0.01  0.98 -0.07 -0.    0.98 -0.03  0.    0.99\n",
      "  -0.06  0.03  1.   -0.06 -0.    0.98 -0.05 -0.04  0.97 -0.06 -0.02  0.98\n",
      "  -0.04  0.    0.98 -0.08 -0.02  0.98 -0.05 -0.01  0.98 -0.06 -0.    0.99\n",
      "  -0.03  0.    0.98 -0.07  0.01  0.99 -0.06 -0.02  0.98 -0.1   0.01  0.98]\n",
      " [ 0.01 -0.06  0.99  0.02  0.01  0.97  0.02 -0.    0.98 -0.05 -0.01  1.\n",
      "   0.07  0.01  0.99 -0.   -0.02  0.94 -0.02 -0.01  0.98  0.05 -0.02  0.97\n",
      "   0.02 -0.02  0.99  0.02 -0.01  0.98  0.03  0.    0.96  0.06 -0.01  0.99\n",
      "  -0.03 -0.02  0.95  0.03 -0.01  0.95 -0.05 -0.02  0.96 -0.01  0.01  1.\n",
      "   0.02  0.    0.98  0.02 -0.    0.98 -0.01 -0.03  1.    0.09  0.01  1.  ]\n",
      " [ 0.02  0.01  1.02 -0.01 -0.03  0.94  0.02  0.    0.96  0.02  0.01  0.98\n",
      "   0.02  0.01  0.99 -0.05  0.    1.02  0.09  0.02  0.98 -0.01  0.01  1.01\n",
      "   0.07  0.03  1.01  0.07  0.03  0.97  0.02  0.    0.99  0.02  0.    0.99\n",
      "   0.02  0.03  1.01 -0.04 -0.01  0.97  0.08  0.02  0.98  0.   -0.    0.96\n",
      "   0.1   0.02  0.96  0.03 -0.02  0.94  0.02  0.    0.98  0.02  0.01  0.98]\n",
      " [ 0.02  0.01  0.98  0.03  0.02  0.99 -0.03 -0.01  0.96 -0.   -0.01  0.96\n",
      "   0.05  0.01  1.    0.02  0.03  1.03 -0.01 -0.02  0.96  0.02  0.    0.98\n",
      "   0.02  0.01  0.98  0.04 -0.02  0.92  0.07  0.02  1.01  0.08  0.02  0.97\n",
      "  -0.03 -0.01  0.97  0.08 -0.    0.9   0.04  0.02  1.01  0.02  0.01  0.98\n",
      "   0.02  0.01  0.99  0.09  0.01  0.94  0.08  0.01  0.96  0.02  0.01  1.01]\n",
      " [ 0.02  0.1   0.98 -0.04  0.01  0.99 -0.01  0.07  0.93 -0.01  0.11  0.91\n",
      "   0.01  0.09  0.98  0.04  0.12  0.98  0.    0.07  0.97 -0.02  0.04  0.97\n",
      "   0.    0.09  1.    0.05  0.1   1.    0.    0.1   0.97 -0.02  0.04  0.97\n",
      "  -0.02  0.07  1.   -0.03  0.05  0.99 -0.04  0.08  0.95 -0.04  0.08  0.99\n",
      "   0.01  0.12  0.96  0.03  0.11  0.98  0.06  0.13  1.03 -0.02  0.07  1.  ]\n",
      " [ 0.02  0.01  0.98  0.02  0.01  0.97 -0.03 -0.05  0.9   0.09  0.02  0.98\n",
      "   0.07  0.01  0.95 -0.04 -0.    0.99  0.03 -0.03  0.9   0.02  0.01  0.97\n",
      "   0.02  0.01  0.98  0.02  0.01  0.98 -0.01  0.02  1.01  0.08  0.01  0.95\n",
      "   0.02  0.01  1.01  0.1   0.02  0.97  0.04 -0.03  0.87  0.02  0.01  0.98\n",
      "   0.02  0.01  0.98  0.02  0.03  1.02  0.1   0.02  0.96 -0.03 -0.01  0.98]\n",
      " [-0.06  0.01  0.98 -0.03 -0.01  0.98 -0.02  0.01  0.99 -0.06  0.    0.98\n",
      "  -0.04 -0.04  0.98 -0.07  0.    0.98 -0.04  0.01  0.98 -0.03 -0.03  0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.02  0.98 -0.05 -0.01  0.98\n",
      "  -0.05  0.02  0.99 -0.01 -0.    0.99 -0.06  0.    0.98 -0.06 -0.02  0.99\n",
      "  -0.06  0.03  0.99 -0.07 -0.01  0.98 -0.06 -0.02  0.98 -0.09 -0.01  0.97]\n",
      " [ 0.02 -0.    0.97 -0.02  0.02  1.03  0.05  0.    0.95  0.07  0.02  0.98\n",
      "   0.1   0.02  0.96  0.02 -0.04  0.88  0.02  0.    0.98  0.02  0.01  0.99\n",
      "   0.06  0.06  1.03 -0.06 -0.01  0.98  0.08  0.01  0.96 -0.02 -0.    0.98\n",
      "   0.07 -0.01  0.92  0.03  0.03  1.01  0.02  0.01  0.98  0.02  0.    0.99\n",
      "   0.02 -0.04  0.86 -0.04 -0.01  0.99 -0.04 -0.    0.99  0.08  0.01  0.95]\n",
      " [ 0.09  0.02  1.01  0.06  0.04  0.97  0.02 -0.01  0.98  0.02 -0.    0.98\n",
      "   0.01 -0.03  0.99 -0.03 -0.02  0.95  0.08  0.01  1.   -0.01 -0.02  0.94\n",
      "   0.08  0.02  0.99  0.04  0.01  0.97  0.02 -0.    0.99  0.02 -0.01  0.98\n",
      "  -0.01  0.03  0.99  0.05  0.01  1.01  0.04  0.01  1.03  0.07  0.01  0.94\n",
      "  -0.04 -0.01  0.99  0.02  0.01  0.98  0.02 -0.01  0.97  0.03  0.    0.99]\n",
      " [ 0.02 -0.    0.97  0.02  0.01  0.98  0.02 -0.01  0.98  0.01 -0.01  0.95\n",
      "  -0.01 -0.01  0.96  0.05  0.02  1.   -0.06 -0.01  0.99  0.07  0.04  1.01\n",
      "   0.02  0.    0.99  0.02  0.01  0.98  0.01  0.03  1.03 -0.02 -0.01  0.96\n",
      "   0.08  0.01  0.95 -0.   -0.    0.96  0.04  0.04  1.03  0.01 -0.02  0.95\n",
      "   0.02  0.    0.99  0.02  0.    0.98 -0.01  0.05  1.11  0.06  0.02  1.01]\n",
      " [ 0.02  0.01  0.97  0.02  0.01  0.98  0.02  0.01  0.98 -0.01  0.02  1.01\n",
      "   0.08  0.01  0.95  0.02  0.01  1.01  0.1   0.02  0.97  0.04 -0.03  0.87\n",
      "   0.02  0.01  0.98  0.02  0.01  0.98  0.02  0.03  1.02  0.1   0.02  0.96\n",
      "  -0.03 -0.01  0.98  0.01  0.01  1.01  0.04  0.04  1.03  0.02 -0.03  0.94\n",
      "   0.02  0.01  0.98  0.02  0.01  0.99 -0.01  0.05  1.11 -0.01 -0.01  0.96]\n",
      " [-0.02  0.08  0.94 -0.01  0.13  0.95  0.01  0.09  0.99  0.04  0.13  1.\n",
      "  -0.01  0.07  0.94  0.06  0.08  0.98  0.03  0.12  0.96 -0.06  0.09  0.94\n",
      "  -0.01  0.07  0.95 -0.02  0.1   0.91  0.02  0.1   0.98 -0.    0.15  0.96\n",
      "  -0.    0.08  0.97 -0.04  0.06  0.92  0.03  0.11  0.96 -0.07  0.07  0.96\n",
      "  -0.02  0.1   0.99 -0.01  0.12  1.    0.    0.1   0.95 -0.05  0.09  0.95]\n",
      " [-0.04  0.06  0.92  0.03  0.11  0.96 -0.07  0.07  0.96 -0.02  0.1   0.99\n",
      "  -0.01  0.12  1.    0.    0.1   0.95 -0.05  0.09  0.95 -0.04  0.08  0.96\n",
      "  -0.    0.07  0.96 -0.02  0.08  0.95  0.03  0.11  0.97  0.04  0.1   1.01\n",
      "   0.01  0.09  0.99 -0.02  0.05  1.    0.    0.11  1.01 -0.06  0.03  1.01\n",
      "   0.01  0.1   0.95  0.05  0.12  1.   -0.01  0.07  0.94  0.07  0.09  0.99]\n",
      " [-0.07 -0.03  0.98 -0.06  0.01  0.99 -0.05  0.    0.98 -0.07  0.03  0.99\n",
      "  -0.04  0.01  0.99 -0.06  0.02  0.99 -0.1  -0.    0.98 -0.05  0.    0.99\n",
      "  -0.04  0.02  0.99 -0.07 -0.    0.98 -0.06 -0.02  0.98 -0.03 -0.01  0.98\n",
      "  -0.08 -0.03  0.97 -0.05  0.    0.98 -0.09  0.04  1.   -0.07 -0.01  0.98\n",
      "  -0.04  0.01  0.99 -0.09  0.02  0.99 -0.05 -0.03  0.97 -0.05 -0.    0.98]\n",
      " [ 0.05 -0.01  0.98  0.02  0.    1.01 -0.02 -0.02  0.94  0.   -0.01  0.96\n",
      "   0.07 -0.01  0.97  0.02 -0.01  0.98  0.02 -0.01  0.98  0.02 -0.02  0.98\n",
      "   0.09  0.01  1.01 -0.03 -0.02  0.96  0.05  0.01  1.03 -0.03 -0.03  0.97\n",
      "   0.   -0.03  1.    0.02 -0.01  0.98  0.03 -0.    0.98  0.09  0.01  0.97\n",
      "   0.06  0.    0.98  0.06  0.    0.97 -0.02 -0.02  0.94  0.04  0.04  0.98]\n",
      " [-0.03 -0.02  0.96 -0.03 -0.02  0.96  0.07  0.    0.98 -0.05 -0.02  0.97\n",
      "   0.06  0.04  0.98  0.03 -0.02  0.99  0.02 -0.    0.99 -0.   -0.05  0.99\n",
      "  -0.04 -0.02  0.96  0.06  0.01  1.03  0.06  0.    0.98 -0.02 -0.03  0.98\n",
      "   0.03  0.04  0.97  0.02 -0.    0.98  0.02 -0.01  0.98 -0.03  0.01  0.99\n",
      "   0.07  0.01  1.02 -0.02 -0.01  0.97  0.07  0.01  0.99 -0.03 -0.03  1.  ]\n",
      " [ 0.03  0.06  1.    0.05  0.11  0.99  0.02  0.08  0.98 -0.02  0.1   1.\n",
      "  -0.02  0.08  0.95  0.03  0.1   1.    0.05  0.14  1.01 -0.01  0.08  0.96\n",
      "   0.02  0.08  1.01  0.01  0.11  0.96  0.06  0.15  1.01 -0.04  0.07  0.99\n",
      "  -0.02  0.03  1.    0.01  0.11  0.98 -0.01  0.04  1.    0.02  0.11  0.98\n",
      "  -0.04  0.13  0.94 -0.01  0.07  0.96 -0.06  0.04  0.98  0.02  0.1   0.98]\n",
      " [-0.05 -0.02  0.98 -0.05  0.01  0.99 -0.01  0.01  0.99 -0.05 -0.02  0.97\n",
      "  -0.05  0.    0.98 -0.05 -0.03  0.98 -0.07 -0.    0.98 -0.04  0.02  0.99\n",
      "  -0.01 -0.01  0.99 -0.06 -0.    0.99 -0.08 -0.01  0.98 -0.04  0.01  0.99\n",
      "  -0.05  0.01  0.99 -0.07  0.02  0.99 -0.01 -0.01  0.99 -0.05  0.    0.98\n",
      "  -0.09 -0.04  0.96 -0.04 -0.01  0.98 -0.07 -0.01  0.98 -0.09 -0.01  0.98]\n",
      " [ 0.03  0.02  0.98  0.08  0.    0.94 -0.04 -0.    1.    0.03  0.    0.95\n",
      "  -0.05 -0.01  0.99  0.    0.04  1.09  0.02  0.    0.98  0.02  0.    0.98\n",
      "  -0.02 -0.01  0.99  0.1   0.02  0.97 -0.03 -0.    1.    0.06  0.02  1.\n",
      "  -0.02  0.02  1.05  0.05  0.04  1.02  0.02  0.    0.99  0.02  0.01  0.98\n",
      "   0.01 -0.02  0.93  0.02 -0.01  0.94  0.06  0.01  0.95  0.07  0.03  1.01]\n",
      " [-0.07 -0.01  0.98 -0.08 -0.01  0.98 -0.05  0.02  0.99 -0.05  0.    0.98\n",
      "  -0.04  0.02  0.99 -0.04  0.    0.98 -0.05  0.02  0.99 -0.09  0.01  0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.01  0.98 -0.06 -0.01  0.98 -0.05 -0.02  0.98\n",
      "  -0.03  0.    0.99 -0.05 -0.03  0.97 -0.05  0.    0.98 -0.1  -0.03  0.97\n",
      "  -0.04 -0.01  0.98 -0.06 -0.01  0.98 -0.02 -0.02  0.98 -0.07  0.02  0.99]\n",
      " [ 0.02  0.01  0.98 -0.01  0.02  1.01  0.08  0.01  0.95  0.02  0.01  1.01\n",
      "   0.1   0.02  0.97  0.04 -0.03  0.87  0.02  0.01  0.98  0.02  0.01  0.98\n",
      "   0.02  0.03  1.02  0.1   0.02  0.96 -0.03 -0.01  0.98  0.01  0.01  1.01\n",
      "   0.04  0.04  1.03  0.02 -0.03  0.94  0.02  0.01  0.98  0.02  0.01  0.99\n",
      "  -0.01  0.05  1.11 -0.01 -0.01  0.96  0.05  0.    0.95 -0.02  0.01  0.95]\n",
      " [ 0.01 -0.01  0.95  0.07  0.01  1.01  0.09  0.01  1.    0.03 -0.05  0.97\n",
      "   0.02 -0.02  0.99  0.02 -0.    0.99 -0.    0.01  0.99  0.09  0.01  1.01\n",
      "  -0.02 -0.02  0.94  0.01  0.    1.02 -0.04 -0.01  0.98  0.   -0.02  0.98\n",
      "   0.02 -0.    0.99  0.02 -0.    0.98 -0.04 -0.02  0.97 -0.02 -0.02  0.94\n",
      "   0.02 -0.01  0.94  0.07  0.01  1.02  0.06 -0.03  0.97  0.02  0.01  0.97]\n",
      " [ 0.03  0.09  1.    0.01  0.07  0.98  0.02  0.12  0.99  0.01  0.11  0.95\n",
      "   0.02  0.08  1.    0.02  0.08  1.03  0.01  0.08  0.98 -0.03  0.05  1.\n",
      "   0.    0.1   1.02 -0.07  0.03  0.99 -0.04  0.06  1.    0.03  0.16  0.97\n",
      "  -0.01  0.1   1.    0.01  0.18  1.    0.04  0.11  0.97  0.02  0.05  1.01\n",
      "  -0.01  0.1   1.   -0.04  0.04  0.99  0.02  0.11  0.98 -0.04  0.05  0.98]\n",
      " [ 0.02 -0.    0.99  0.02  0.01  0.97  0.04  0.02  1.    0.    0.01  1.02\n",
      "   0.02 -0.    0.95 -0.02  0.    1.01 -0.01 -0.02  0.96  0.01  0.01  0.99\n",
      "   0.02  0.    0.98  0.02  0.    0.99  0.1   0.02  0.96 -0.04 -0.    1.\n",
      "   0.03  0.01  1.01  0.07  0.01  0.95  0.05  0.05  1.06  0.03  0.    0.99\n",
      "   0.02  0.01  0.98  0.02 -0.    0.97 -0.02  0.02  1.03  0.05  0.    0.95]\n",
      " [ 0.06 -0.02  0.97  0.02 -0.02  0.99  0.02  0.    0.99  0.02 -0.01  0.97\n",
      "  -0.05 -0.03  0.97  0.08  0.01  1.   -0.02 -0.01  0.97 -0.03 -0.01  0.98\n",
      "   0.    0.03  0.99  0.02  0.01  0.98  0.02 -0.    0.99  0.01 -0.06  0.97\n",
      "   0.07  0.01  1.01 -0.03 -0.02  0.96  0.01 -0.    1.01  0.05 -0.02  0.98\n",
      "   0.02 -0.01  1.    0.02 -0.01  0.99  0.02 -0.    0.99  0.1   0.01  0.98]\n",
      " [ 0.02 -0.    0.98  0.    0.01  1.   -0.04 -0.02  0.94 -0.01 -0.01  0.99\n",
      "   0.05  0.01  1.03 -0.04 -0.01  0.98  0.02 -0.03  0.99  0.02 -0.    0.98\n",
      "   0.02 -0.    0.98  0.05 -0.01  0.98 -0.02 -0.02  0.94  0.07  0.01  0.99\n",
      "  -0.02 -0.01  0.97  0.07 -0.    0.97  0.03  0.01  0.97  0.02 -0.03  0.99\n",
      "   0.02  0.01  0.99  0.1   0.01  1.    0.07  0.01  0.98 -0.01 -0.02  0.94]\n",
      " [ 0.02  0.01  0.99  0.01 -0.04  0.92  0.08  0.03  1.   -0.03  0.    1.01\n",
      "   0.02  0.01  1.01  0.04 -0.02  0.91  0.04  0.04  1.02  0.02  0.01  0.98\n",
      "   0.02  0.01  0.98  0.08  0.03  0.99  0.09  0.02  0.98  0.07  0.02  0.99\n",
      "  -0.05 -0.01  0.99  0.07  0.05  1.02  0.02  0.02  0.98  0.02  0.01  0.99\n",
      "   0.03  0.03  0.99 -0.06 -0.01  0.99 -0.02  0.01  1.02  0.03  0.    0.95]\n",
      " [-0.1  -0.02  0.98 -0.04 -0.01  0.98 -0.07 -0.    0.98 -0.03  0.    0.99\n",
      "  -0.06  0.03  1.   -0.06 -0.    0.98 -0.05 -0.04  0.97 -0.06 -0.02  0.98\n",
      "  -0.04  0.    0.98 -0.08 -0.02  0.98 -0.05 -0.01  0.98 -0.06 -0.    0.99\n",
      "  -0.03  0.    0.98 -0.07  0.01  0.99 -0.06 -0.02  0.98 -0.1   0.01  0.98\n",
      "  -0.06  0.    0.99 -0.03  0.03  1.   -0.04 -0.01  0.98 -0.05 -0.02  0.98]\n",
      " [-0.04  0.01  0.98 -0.04  0.01  0.99 -0.09  0.01  0.98 -0.05  0.    0.98\n",
      "  -0.04  0.04  1.   -0.04 -0.01  0.99 -0.04 -0.    0.98 -0.08 -0.    0.98\n",
      "  -0.07 -0.03  0.97 -0.06  0.    0.98 -0.09  0.03  0.98 -0.06  0.02  0.99\n",
      "  -0.07 -0.01  0.98 -0.08  0.03  0.99 -0.05  0.    0.99 -0.06  0.    0.98\n",
      "  -0.06 -0.02  0.97 -0.04  0.01  0.99 -0.07  0.    0.98 -0.09  0.01  0.98]\n",
      " [ 0.07  0.01  1.02  0.06 -0.03  0.97  0.02  0.01  0.97  0.02 -0.    0.98\n",
      "   0.02 -0.02  1.    0.01 -0.01  0.97  0.02 -0.01  0.95  0.02  0.    1.02\n",
      "  -0.05 -0.02  0.96  0.06 -0.    0.97  0.02  0.01  0.98  0.02 -0.    0.98\n",
      "   0.    0.01  1.   -0.04 -0.02  0.94 -0.01 -0.01  0.99  0.05  0.01  1.03\n",
      "  -0.04 -0.01  0.98  0.02 -0.03  0.99  0.02 -0.    0.98  0.02 -0.    0.98]\n",
      " [ 0.01  0.1   0.95  0.05  0.12  1.   -0.01  0.07  0.94  0.07  0.09  0.99\n",
      "  -0.05  0.06  0.99 -0.03  0.14  0.94 -0.01  0.07  0.97  0.03  0.16  1.01\n",
      "  -0.03  0.07  1.    0.04  0.07  1.01 -0.01  0.09  0.99 -0.02  0.05  1.03\n",
      "  -0.02  0.07  1.01 -0.01  0.06  1.   -0.01  0.09  0.98  0.01  0.13  0.99\n",
      "  -0.03  0.07  0.96  0.01  0.08  1.01  0.01  0.05  0.98 -0.04  0.06  0.98]\n",
      " [-0.02  0.07  0.99 -0.05  0.04  0.98 -0.01  0.1   0.99 -0.04  0.02  0.97\n",
      "  -0.04  0.06  1.   -0.06  0.1   0.95 -0.03  0.11  0.98 -0.03  0.05  0.99\n",
      "  -0.03  0.06  1.   -0.01  0.11  0.95 -0.01  0.13  0.98 -0.04  0.08  1.\n",
      "   0.01  0.11  0.95  0.03  0.09  1.    0.02  0.08  1.04 -0.03  0.07  0.99\n",
      "  -0.03  0.11  0.94 -0.    0.1   1.03 -0.08  0.04  0.95 -0.04  0.06  0.99]\n",
      " [ 0.02  0.16  0.96 -0.    0.1   1.02  0.    0.12  0.93  0.02  0.1   0.98\n",
      "   0.01  0.03  1.01 -0.01  0.11  1.   -0.03  0.02  0.99  0.    0.1   0.96\n",
      "  -0.03  0.12  0.95  0.    0.08  0.97  0.01  0.08  1.01 -0.01  0.08  1.\n",
      "   0.02  0.07  1.    0.02  0.07  0.99 -0.01  0.1   1.   -0.    0.1   0.95\n",
      "  -0.    0.07  1.01 -0.02  0.03  0.97  0.01  0.08  0.99 -0.04  0.03  0.98]\n",
      " [-0.05  0.02  0.99 -0.09  0.01  0.98 -0.05  0.    0.98 -0.05 -0.01  0.98\n",
      "  -0.06 -0.01  0.98 -0.05 -0.02  0.98 -0.03  0.    0.99 -0.05 -0.03  0.97\n",
      "  -0.05  0.    0.98 -0.1  -0.03  0.97 -0.04 -0.01  0.98 -0.06 -0.01  0.98\n",
      "  -0.02 -0.02  0.98 -0.07  0.02  0.99 -0.06  0.03  0.98 -0.03 -0.01  0.98\n",
      "  -0.04  0.02  0.99 -0.06 -0.02  0.98 -0.01 -0.01  0.98 -0.06  0.    0.98]\n",
      " [ 0.02  0.05  1.01 -0.01  0.1   1.   -0.04  0.04  0.99  0.02  0.11  0.98\n",
      "  -0.04  0.05  0.98 -0.01  0.07  0.97  0.02  0.14  0.97 -0.04  0.06  0.99\n",
      "  -0.06  0.05  0.97 -0.02  0.08  0.99  0.01  0.14  1.    0.    0.1   0.95\n",
      "  -0.01  0.06  1.    0.04  0.11  1.01 -0.03  0.06  0.97  0.03  0.13  0.98\n",
      "   0.01  0.1   1.01  0.04  0.1   1.01 -0.01  0.07  0.96 -0.02  0.04  1.  ]\n",
      " [ 0.02 -0.01  0.98  0.03 -0.    0.98  0.09  0.01  0.97  0.06  0.    0.98\n",
      "   0.06  0.    0.97 -0.02 -0.02  0.94  0.04  0.04  0.98  0.03  0.01  0.97\n",
      "   0.02 -0.01  0.98  0.02 -0.01  1.    0.04  0.02  0.99 -0.   -0.02  0.94\n",
      "   0.03  0.01  1.03  0.07  0.01  1.01  0.06  0.01  0.97  0.02 -0.01  0.98\n",
      "   0.02 -0.01  0.97 -0.01 -0.04  1.01  0.09  0.01  1.01 -0.02 -0.01  0.97]\n",
      " [-0.06  0.03  1.   -0.05 -0.    0.98 -0.08  0.02  0.99 -0.07  0.01  0.98\n",
      "  -0.04 -0.01  0.98 -0.09  0.02  0.98 -0.05  0.    0.98 -0.04 -0.01  0.98\n",
      "  -0.05 -0.02  0.97 -0.04  0.    0.98 -0.03  0.    0.99 -0.1   0.01  0.98\n",
      "  -0.06 -0.    0.98 -0.03  0.04  1.   -0.06 -0.02  0.98 -0.07 -0.01  0.98\n",
      "  -0.07  0.03  0.99 -0.09 -0.01  0.97 -0.06  0.    0.98 -0.04  0.03  0.99]\n",
      " [-0.04  0.08  1.   -0.    0.1   0.95  0.02  0.11  0.96  0.05  0.14  1.02\n",
      "  -0.04  0.06  1.   -0.02  0.04  1.    0.01  0.11  0.99  0.01  0.06  1.02\n",
      "   0.03  0.12  0.96 -0.05  0.11  0.95 -0.01  0.08  0.96 -0.08  0.07  0.98\n",
      "   0.01  0.09  0.98  0.04  0.15  0.98 -0.01  0.09  0.99  0.02  0.09  1.01\n",
      "  -0.02  0.09  0.97  0.01  0.07  1.   -0.01  0.08  0.98 -0.01  0.12  1.  ]\n",
      " [-0.05  0.01  0.98 -0.01  0.07  0.93  0.07  0.1   0.98 -0.05  0.06  0.99\n",
      "  -0.07  0.04  0.97 -0.    0.07  0.94 -0.04  0.06  0.92  0.04  0.11  0.96\n",
      "  -0.06  0.12  0.94 -0.01  0.1   0.98  0.03  0.14  0.95  0.03  0.12  0.97\n",
      "  -0.    0.12  0.96  0.    0.1   0.98  0.03  0.11  0.97 -0.04  0.05  0.99\n",
      "  -0.01  0.1   0.94  0.02  0.15  0.98 -0.04  0.08  0.99  0.03  0.11  1.01]\n",
      " [ 0.04 -0.    0.94 -0.04 -0.    0.99  0.03  0.02  1.02  0.08 -0.    0.91\n",
      "   0.02  0.03  1.    0.02  0.    0.99  0.02  0.01  0.98  0.03  0.04  1.08\n",
      "   0.08  0.01  0.95  0.05  0.02  1.    0.02  0.02  1.03 -0.03 -0.    1.02\n",
      "   0.02 -0.    0.98  0.02  0.01  0.98  0.04  0.04  1.01  0.04 -0.01  0.94\n",
      "  -0.01 -0.    0.96 -0.    0.01  1.01 -0.06 -0.01  0.93  0.01 -0.05  0.9 ]\n",
      " [-0.06  0.03  1.01  0.01  0.1   0.95  0.05  0.12  1.   -0.01  0.07  0.94\n",
      "   0.07  0.09  0.99 -0.05  0.06  0.99 -0.03  0.14  0.94 -0.01  0.07  0.97\n",
      "   0.03  0.16  1.01 -0.03  0.07  1.    0.04  0.07  1.01 -0.01  0.09  0.99\n",
      "  -0.02  0.05  1.03 -0.02  0.07  1.01 -0.01  0.06  1.   -0.01  0.09  0.98\n",
      "   0.01  0.13  0.99 -0.03  0.07  0.96  0.01  0.08  1.01  0.01  0.05  0.98]\n",
      " [-0.02  0.    0.98  0.02 -0.02  0.98  0.02 -0.    0.99  0.02 -0.    0.99\n",
      "   0.1   0.    0.98  0.08  0.01  1.02  0.05  0.01  1.03 -0.04 -0.02  0.95\n",
      "   0.05  0.04  0.98  0.02 -0.01  0.99  0.02 -0.    0.99  0.02  0.    0.97\n",
      "   0.08  0.    0.99  0.04 -0.    0.96  0.02  0.    1.02 -0.03 -0.02  0.97\n",
      "   0.05  0.05  0.98  0.02  0.    0.98  0.02 -0.    0.98 -0.01 -0.02  1.  ]\n",
      " [-0.03 -0.    0.98 -0.02  0.01  1.01  0.    0.04  1.08  0.02  0.01  0.98\n",
      "   0.02  0.01  0.98  0.02  0.01  0.99 -0.05  0.01  1.06 -0.03  0.01  1.01\n",
      "   0.02 -0.    0.94  0.07  0.    0.95  0.07  0.04  1.02  0.03  0.01  0.99\n",
      "   0.02  0.01  0.98  0.    0.01  1.   -0.06 -0.01  0.98  0.05 -0.    0.96\n",
      "  -0.03  0.    1.    0.06  0.03  1.02  0.04  0.01  0.99  0.02  0.01  0.98]\n",
      " [-0.05 -0.02  0.96 -0.01  0.01  1.    0.02  0.    0.98  0.02 -0.    0.98\n",
      "  -0.01 -0.03  1.    0.09  0.01  1.   -0.02 -0.01  0.97  0.06  0.01  1.02\n",
      "  -0.02  0.    0.98  0.02 -0.02  0.98  0.02 -0.    0.99  0.02 -0.    0.99\n",
      "   0.1   0.    0.98  0.08  0.01  1.02  0.05  0.01  1.03 -0.04 -0.02  0.95\n",
      "   0.05  0.04  0.98  0.02 -0.01  0.99  0.02 -0.    0.99  0.02  0.    0.97]\n",
      " [-0.01  0.12  0.98 -0.01  0.06  0.96 -0.04  0.05  0.96  0.02  0.12  0.96\n",
      "   0.03  0.13  1.    0.03  0.11  0.97  0.02  0.11  1.    0.01  0.09  1.01\n",
      "   0.02  0.08  1.04 -0.04  0.06  0.98 -0.04  0.11  0.94 -0.    0.11  1.02\n",
      "  -0.08  0.04  0.99 -0.03  0.07  0.98  0.05  0.15  0.99 -0.    0.09  1.01\n",
      "   0.04  0.13  1.03 -0.03  0.07  0.99 -0.07  0.05  0.96  0.    0.08  0.96]\n",
      " [ 0.02 -0.01  0.99  0.01 -0.03  0.99 -0.05 -0.02  0.96  0.05 -0.    0.97\n",
      "  -0.02 -0.02  0.96 -0.03 -0.    0.98 -0.   -0.03  1.    0.02  0.01  0.97\n",
      "   0.02 -0.    0.98  0.06  0.06  0.99  0.07  0.01  0.99  0.06  0.    0.98\n",
      "   0.   -0.02  0.94 -0.04 -0.02  0.99  0.02 -0.02  1.    0.02 -0.01  0.98\n",
      "   0.02 -0.01  0.98  0.1   0.01  1.    0.05  0.01  1.02 -0.02 -0.02  0.94]\n",
      " [-0.05  0.05  0.94  0.03  0.11  0.97 -0.01  0.1   0.95 -0.01  0.09  0.94\n",
      "  -0.02  0.12  0.93 -0.    0.1   0.97  0.    0.13  0.95 -0.01  0.07  0.98\n",
      "   0.07  0.12  0.95 -0.02  0.09  0.97 -0.06  0.08  0.95 -0.01  0.09  0.97\n",
      "   0.03  0.07  1.02 -0.04  0.07  0.98 -0.05  0.12  0.94  0.    0.07  0.96\n",
      "   0.03  0.15  0.97 -0.05  0.06  1.    0.05  0.09  1.   -0.01  0.06  0.97]\n",
      " [-0.02 -0.01  0.96  0.09  0.01  0.99 -0.02 -0.02  1.    0.02 -0.    0.99\n",
      "   0.02 -0.    0.99  0.03 -0.03  0.96  0.09  0.01  1.01 -0.   -0.01  1.\n",
      "   0.04  0.01  1.03  0.07  0.02  0.99  0.01 -0.05  1.    0.02 -0.    0.99\n",
      "   0.02  0.    0.99  0.01 -0.03  0.97 -0.03 -0.02  0.95 -0.01 -0.02  0.94\n",
      "   0.08  0.01  1.02  0.07 -0.01  0.97  0.03  0.01  0.98  0.02  0.    0.99]\n",
      " [ 0.02  0.04  1.01 -0.01  0.11  1.   -0.06  0.04  1.    0.03  0.12  0.97\n",
      "  -0.01  0.05  1.   -0.    0.07  0.98 -0.    0.11  0.96 -0.03  0.06  1.\n",
      "  -0.01  0.06  1.   -0.01  0.06  0.97  0.01  0.13  0.96  0.03  0.13  0.98\n",
      "  -0.03  0.05  0.99 -0.04  0.05  0.99 -0.01  0.08  0.99 -0.04  0.04  0.98\n",
      "  -0.01  0.09  1.02 -0.07  0.04  0.93 -0.05  0.06  0.99  0.05  0.14  1.  ]\n",
      " [-0.04  0.01  0.99 -0.04  0.03  0.99 -0.01 -0.01  0.99 -0.06 -0.    0.98\n",
      "  -0.11 -0.03  0.97 -0.06 -0.02  0.98 -0.06  0.01  0.99 -0.07 -0.02  0.97\n",
      "  -0.03  0.01  0.98 -0.05  0.    0.98 -0.08  0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.05  0.02  0.99 -0.09  0.01  0.98 -0.05  0.    0.99 -0.05 -0.01  0.98\n",
      "  -0.03 -0.01  0.98 -0.07  0.01  0.99 -0.05  0.02  0.99 -0.1   0.    0.98]\n",
      " [-0.06  0.02  1.   -0.06  0.    0.98 -0.08  0.02  0.99 -0.06  0.03  0.99\n",
      "  -0.01  0.02  1.   -0.05 -0.    0.98 -0.06 -0.    0.99 -0.06  0.    0.98\n",
      "  -0.05  0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.05 -0.    0.98 -0.06 -0.    0.99 -0.03  0.01  0.99 -0.07  0.    0.99\n",
      "  -0.05 -0.02  0.98 -0.08 -0.02  0.97 -0.05 -0.    0.99 -0.03  0.01  0.98\n",
      "  -0.04  0.    0.99 -0.05 -0.01  0.98 -0.08 -0.    0.98 -0.08 -0.03  0.97\n",
      "  -0.06  0.    0.99 -0.06  0.04  0.99 -0.05 -0.02  0.98 -0.05  0.01  0.99\n",
      "  -0.01  0.01  0.99 -0.05 -0.02  0.97 -0.05  0.    0.98 -0.05 -0.03  0.98]\n",
      " [ 0.06  0.02  1.    0.06  0.01  0.95 -0.02  0.03  1.07  0.01 -0.03  0.95\n",
      "   0.02  0.01  0.99  0.02  0.01  0.98  0.07  0.02  0.98  0.02 -0.    0.95\n",
      "   0.07  0.02  0.99  0.1   0.02  0.97  0.07  0.02  0.95  0.03  0.02  0.98\n",
      "   0.02  0.    0.99  0.02  0.02  0.99  0.1   0.03  0.99  0.08  0.02  0.98\n",
      "   0.04  0.    0.95  0.08  0.03  1.    0.05  0.01  0.99  0.03  0.02  0.98]\n",
      " [-0.01  0.1   1.    0.01  0.17  1.    0.04  0.11  0.96  0.04  0.06  1.01\n",
      "  -0.01  0.1   1.   -0.02  0.04  1.02  0.02  0.11  0.96  0.05  0.08  1.01\n",
      "   0.02  0.08  0.98 -0.02  0.05  0.96 -0.03  0.06  0.96  0.04  0.11  0.99\n",
      "   0.02  0.08  0.99 -0.    0.08  0.96  0.03  0.13  0.99  0.    0.08  1.01\n",
      "   0.02  0.08  1.04 -0.02  0.07  0.99 -0.04  0.04  0.95 -0.    0.1   1.03]\n",
      " [ 0.03  0.04  1.03  0.02  0.01  0.98  0.02  0.01  0.98  0.1   0.01  0.93\n",
      "  -0.03 -0.01  0.97 -0.03 -0.01  0.98  0.08  0.02  0.98  0.07  0.03  0.99\n",
      "   0.03  0.02  1.01  0.02  0.01  0.98  0.03  0.01  1.    0.1   0.02  0.97\n",
      "   0.05  0.    0.95  0.02  0.01  1.01 -0.01 -0.02  0.95  0.02  0.06  1.09\n",
      "   0.02  0.01  0.98  0.02  0.01  0.99  0.01 -0.04  0.92 -0.02 -0.01  0.95]\n",
      " [-0.05  0.    0.98 -0.08  0.02  0.99 -0.06  0.02  0.98 -0.07 -0.01  0.98\n",
      "  -0.09 -0.01  0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.04  0.02  0.99\n",
      "  -0.07  0.01  0.98 -0.07  0.    0.98 -0.01  0.    0.99 -0.05  0.    0.98\n",
      "  -0.05 -0.05  0.96 -0.05  0.03  0.98 -0.04  0.01  0.98 -0.04  0.01  0.99\n",
      "  -0.08  0.02  0.99 -0.06  0.    0.98 -0.05  0.03  0.99 -0.06 -0.02  0.98]\n",
      " [-0.02 -0.02  1.    0.02 -0.02  0.99  0.02 -0.    0.99  0.04  0.02  0.97\n",
      "   0.07  0.    0.99  0.07  0.01  1.    0.   -0.02  0.94 -0.01 -0.02  0.97\n",
      "   0.01 -0.05  0.99  0.02  0.    0.98  0.02 -0.    0.99  0.01  0.05  0.99\n",
      "   0.02 -0.01  0.96 -0.02 -0.02  0.94  0.04  0.01  1.02 -0.04 -0.01  0.99\n",
      "   0.02  0.01  0.98  0.02  0.    0.99  0.02 -0.01  0.98  0.04 -0.01  0.98]\n",
      " [-0.02  0.07  0.96  0.05  0.07  0.98  0.02  0.1   0.98 -0.07  0.06  0.95\n",
      "  -0.01  0.07  0.95 -0.05  0.08  0.94 -0.04  0.06  0.99  0.04  0.13  0.98\n",
      "   0.01  0.09  0.98 -0.03  0.02  0.99 -0.04  0.05  0.98 -0.    0.06  1.\n",
      "  -0.01  0.07  0.97 -0.04  0.06  0.99  0.02  0.12  0.97 -0.    0.08  1.01\n",
      "  -0.03  0.02  0.98  0.02  0.1   0.97 -0.03  0.08  0.95 -0.03  0.07  0.98]\n",
      " [-0.01 -0.02  0.96  0.07  0.01  0.97  0.03  0.01  0.97  0.02 -0.    0.97\n",
      "   0.01 -0.01  0.99 -0.01 -0.    0.98 -0.01 -0.01  0.99  0.06  0.01  1.02\n",
      "   0.09  0.02  1.    0.04  0.03  0.97  0.02 -0.    0.98  0.02 -0.    0.97\n",
      "   0.01 -0.06  0.98  0.04  0.    1.01  0.07  0.01  1.01 -0.01 -0.02  0.94\n",
      "   0.07 -0.02  0.98  0.02 -0.01  0.99  0.02 -0.01  0.98  0.03  0.01  0.99]\n",
      " [ 0.02  0.01  0.99  0.02  0.01  0.98  0.07  0.02  0.98  0.02 -0.    0.95\n",
      "   0.07  0.02  0.99  0.1   0.02  0.97  0.07  0.02  0.95  0.03  0.02  0.98\n",
      "   0.02  0.    0.99  0.02  0.02  0.99  0.1   0.03  0.99  0.08  0.02  0.98\n",
      "   0.04  0.    0.95  0.08  0.03  1.    0.05  0.01  0.99  0.03  0.02  0.98\n",
      "   0.02  0.01  0.98  0.07 -0.04  0.86 -0.04 -0.    1.01 -0.01  0.01  1.02]\n",
      " [ 0.02  0.01  0.97  0.02 -0.01  0.99  0.02 -0.05  0.97  0.   -0.    0.98\n",
      "  -0.   -0.02  0.93 -0.02 -0.01  0.97  0.06 -0.02  0.98  0.03  0.03  0.97\n",
      "   0.02  0.    0.98  0.02 -0.    0.98  0.04 -0.03  0.97 -0.01 -0.02  0.95\n",
      "  -0.   -0.02  0.93  0.08  0.01  1.02  0.04 -0.03  0.97  0.02  0.01  0.97\n",
      "   0.02 -0.    0.97  0.03  0.02  0.98  0.09  0.01  1.    0.08  0.01  1.02]\n",
      " [-0.04  0.01  0.99 -0.08  0.02  0.99 -0.06  0.01  0.99 -0.05 -0.    0.98\n",
      "  -0.04  0.01  0.98 -0.07  0.    0.98 -0.06 -0.02  0.98 -0.03  0.03  0.99\n",
      "  -0.05  0.    0.98 -0.07 -0.    0.99 -0.04  0.    0.99 -0.03 -0.    0.98\n",
      "  -0.07 -0.02  0.98 -0.03  0.03  1.   -0.05 -0.    0.98 -0.03 -0.04  0.97\n",
      "  -0.06 -0.02  0.98 -0.05  0.02  0.99 -0.06 -0.03  0.98 -0.06  0.03  1.  ]\n",
      " [ 0.02  0.01  0.98  0.02  0.07  1.11 -0.05 -0.01  0.98  0.02  0.01  1.01\n",
      "   0.09  0.02  0.97 -0.04  0.01  1.05  0.02  0.02  0.98  0.02  0.01  0.99\n",
      "   0.02  0.01  1.    0.06  0.03  0.99  0.03 -0.    0.95  0.08  0.02  0.98\n",
      "   0.07  0.    0.94  0.   -0.05  0.87  0.02  0.02  0.98  0.02  0.01  0.98\n",
      "   0.01 -0.04  0.91  0.05  0.02  1.01 -0.01 -0.    0.96 -0.02  0.    1.01]\n",
      " [-0.    0.01  1.01 -0.06 -0.01  0.93  0.01 -0.05  0.9   0.02  0.01  0.98\n",
      "   0.02  0.01  0.98 -0.02  0.04  1.09  0.01 -0.01  0.94 -0.03  0.    1.01\n",
      "   0.08  0.02  0.99 -0.02  0.03  1.07  0.03  0.02  0.99  0.02  0.01  0.99\n",
      "   0.03  0.02  0.98  0.08  0.    0.94 -0.04 -0.    1.    0.03  0.    0.95\n",
      "  -0.05 -0.01  0.99  0.    0.04  1.09  0.02  0.    0.98  0.02  0.    0.98]\n",
      " [-0.03  0.02  0.99 -0.01  0.08  0.99 -0.03  0.11  0.95 -0.02  0.08  1.\n",
      "  -0.05  0.07  0.91 -0.04  0.06  0.98  0.04  0.09  1.01 -0.01  0.09  0.98\n",
      "  -0.02 -0.    0.95 -0.05  0.06  0.99 -0.03  0.13  0.95 -0.    0.07  0.96\n",
      "  -0.    0.07  1.    0.02  0.11  0.95  0.06  0.1   1.    0.02  0.07  0.99\n",
      "   0.01  0.13  0.99 -0.02  0.08  0.96 -0.02  0.06  1.   -0.03  0.04  0.96]\n",
      " [-0.07 -0.01  0.98 -0.07  0.03  0.99 -0.09 -0.01  0.97 -0.06  0.    0.98\n",
      "  -0.04  0.03  0.99 -0.07 -0.01  0.98 -0.04  0.01  0.99 -0.01  0.01  0.99\n",
      "  -0.06  0.    0.99 -0.05  0.01  0.98 -0.07 -0.02  0.98 -0.07  0.01  0.98\n",
      "  -0.04 -0.02  0.98 -0.04 -0.03  0.98 -0.05  0.    0.98 -0.09  0.01  0.99\n",
      "  -0.04  0.    0.99 -0.07 -0.01  0.98 -0.03 -0.02  0.98 -0.02 -0.01  0.98]\n",
      " [-0.01 -0.02  0.94  0.07 -0.02  0.98  0.02 -0.01  0.99  0.02 -0.01  0.98\n",
      "   0.03  0.01  0.99  0.03 -0.02  0.98 -0.04 -0.02  0.95  0.07  0.01  0.99\n",
      "  -0.02 -0.02  0.96 -0.02  0.01  1.    0.02  0.    0.98  0.02 -0.    0.98\n",
      "   0.01 -0.02  1.   -0.05 -0.02  0.96  0.05  0.    0.97 -0.02 -0.02  0.96\n",
      "   0.09  0.01  0.99  0.03 -0.03  0.98  0.02 -0.01  0.99  0.02 -0.01  0.98]\n",
      " [ 0.01  0.07  0.97  0.02  0.12  0.97 -0.04  0.05  0.98  0.03  0.09  1.\n",
      "   0.03  0.13  0.99 -0.03  0.09  1.    0.02  0.09  1.   -0.04  0.06  0.99\n",
      "  -0.05  0.08  0.93  0.03  0.11  0.97  0.04  0.15  0.98 -0.    0.09  0.93\n",
      "   0.04  0.13  0.93 -0.04  0.06  1.   -0.05  0.01  0.98 -0.01  0.07  0.93\n",
      "  -0.    0.04  1.01  0.01  0.1   0.96  0.01  0.16  0.96 -0.    0.07  0.97]\n",
      " [ 0.02 -0.04  0.88  0.02  0.    0.98  0.02  0.01  0.99  0.06  0.06  1.03\n",
      "  -0.06 -0.01  0.98  0.08  0.01  0.96 -0.02 -0.    0.98  0.07 -0.01  0.92\n",
      "   0.03  0.03  1.01  0.02  0.01  0.98  0.02  0.    0.99  0.02 -0.04  0.86\n",
      "  -0.04 -0.01  0.99 -0.04 -0.    0.99  0.08  0.01  0.95  0.01 -0.04  0.9\n",
      "   0.02  0.03  0.98  0.02  0.01  0.98  0.02 -0.01  0.98  0.07  0.    0.94]\n",
      " [ 0.07  0.01  0.99  0.02 -0.01  0.94 -0.03 -0.03  0.98  0.03  0.04  0.97\n",
      "   0.02 -0.    0.98  0.02 -0.01  0.98  0.06 -0.04  0.98  0.03  0.    1.01\n",
      "  -0.02 -0.01  0.97  0.08  0.02  1.02  0.08  0.03  0.98  0.02 -0.02  0.99\n",
      "   0.02 -0.01  0.98  0.02 -0.01  0.98  0.06  0.01  1.    0.07  0.01  0.99\n",
      "  -0.02 -0.02  0.95  0.1   0.01  1.    0.01  0.02  0.99  0.02  0.    0.98]\n",
      " [ 0.02 -0.02  0.99  0.02 -0.    0.98  0.04  0.05  0.98  0.03  0.    1.\n",
      "   0.08  0.01  1.01  0.03 -0.01  0.95 -0.03  0.    0.98  0.02 -0.03  0.98\n",
      "   0.02  0.    0.99  0.02 -0.    0.98 -0.05 -0.01  0.98  0.08  0.01  1.02\n",
      "   0.01  0.    1.01  0.05 -0.    0.97 -0.02 -0.07  0.88  0.01 -0.31  1.08\n",
      "   0.05  0.79  0.91  0.07  0.01  1.1   0.1   0.11  0.92  0.04  0.09  0.93]\n",
      " [ 0.07  0.    0.94  0.05  0.    0.95 -0.01  0.01  1.01 -0.06 -0.01  1.01\n",
      "   0.02 -0.04  0.88  0.02  0.    0.98  0.02  0.    0.99  0.05 -0.    0.95\n",
      "   0.09  0.02  0.96  0.   -0.    0.96 -0.04 -0.    1.    0.09  0.02  0.95\n",
      "   0.02  0.01  0.98  0.02  0.    0.99  0.02  0.    0.98 -0.01  0.04  1.1\n",
      "   0.09  0.02  0.97  0.02  0.01  1.01 -0.03 -0.01  0.97  0.06  0.06  1.07]\n",
      " [ 0.04  0.03  1.01  0.02  0.01  0.98  0.03  0.01  0.99  0.1   0.01  0.92\n",
      "   0.07  0.01  0.95  0.03  0.01  1.01 -0.05 -0.    1.    0.04 -0.04  0.87\n",
      "   0.02  0.    0.98  0.02  0.01  0.98  0.04  0.05  1.02  0.09  0.03  0.99\n",
      "   0.06  0.02  1.    0.07  0.01  0.95  0.07 -0.    0.93  0.02 -0.03  0.93\n",
      "   0.02  0.01  0.98  0.02  0.    0.99 -0.03  0.04  1.09  0.08  0.01  0.95]\n",
      " [-0.04  0.03  0.96 -0.04  0.07  0.98 -0.04  0.06  0.95 -0.03  0.07  1.\n",
      "  -0.08  0.03  0.95  0.03  0.11  0.96  0.05  0.16  0.98 -0.    0.09  1.01\n",
      "  -0.    0.17  0.99  0.03  0.11  0.96  0.05  0.09  1.01 -0.01  0.11  1.\n",
      "   0.04  0.15  1.02 -0.    0.09  0.99 -0.05  0.04  0.98 -0.    0.1   0.99\n",
      "  -0.04  0.06  0.92 -0.03  0.07  0.97  0.01  0.13  0.96 -0.    0.12  0.99]\n",
      " [-0.03  0.    1.   -0.04 -0.    1.01 -0.03 -0.01  0.99  0.02  0.02  0.98\n",
      "   0.03  0.01  0.98  0.03 -0.    0.91  0.09  0.01  0.95  0.02  0.01  1.02\n",
      "   0.08  0.01  0.96 -0.   -0.02  0.95  0.01 -0.05  0.89  0.02  0.    0.98\n",
      "   0.02  0.01  0.98  0.04 -0.02  0.91  0.02  0.02  1.02  0.06  0.02  1.\n",
      "   0.08  0.01  0.96  0.08  0.01  0.91  0.04  0.03  1.01  0.02  0.01  0.98]\n",
      " [ 0.08  0.02  1.    0.06  0.02  1.   -0.05 -0.01  0.98  0.06  0.05  1.05\n",
      "   0.03  0.01  0.99  0.02  0.01  0.99  0.01  0.    0.98  0.1   0.01  0.96\n",
      "   0.07  0.01  0.95 -0.03  0.01  1.01 -0.01 -0.02  0.95  0.06  0.06  1.07\n",
      "   0.03  0.01  0.98  0.03  0.01  0.97 -0.02  0.04  1.07 -0.05 -0.01  0.98\n",
      "   0.02  0.01  1.01  0.08  0.01  0.95 -0.03  0.02  1.06  0.01 -0.02  0.95]\n",
      " [ 0.06  0.06  0.99  0.07  0.01  0.99  0.06  0.    0.98  0.   -0.02  0.94\n",
      "  -0.04 -0.02  0.99  0.02 -0.02  1.    0.02 -0.01  0.98  0.02 -0.01  0.98\n",
      "   0.1   0.01  1.    0.05  0.01  1.02 -0.02 -0.02  0.94  0.02 -0.01  0.97\n",
      "   0.06 -0.02  0.97  0.02 -0.    0.98  0.02 -0.01  0.98  0.03  0.04  0.99\n",
      "  -0.03 -0.01  0.97  0.   -0.02  0.94 -0.   -0.    1.01  0.09  0.02  1.  ]\n",
      " [-0.07 -0.01  0.98 -0.02 -0.02  0.98 -0.06 -0.    0.99 -0.06  0.01  0.99\n",
      "  -0.08  0.    0.99 -0.05  0.02  0.99 -0.05 -0.02  0.98 -0.05  0.04  1.\n",
      "  -0.05  0.    0.99 -0.02 -0.03  0.97 -0.04  0.02  0.99 -0.05 -0.01  0.98\n",
      "  -0.07  0.03  0.99 -0.07 -0.03  0.97 -0.05  0.    0.98 -0.03  0.    0.98\n",
      "  -0.04  0.02  0.99 -0.07  0.01  0.99 -0.02  0.02  0.99 -0.06  0.    0.99]\n",
      " [ 0.07  0.01  0.99 -0.02 -0.01  0.97  0.07 -0.    0.97  0.03  0.01  0.97\n",
      "   0.02 -0.03  0.99  0.02  0.01  0.99  0.1   0.01  1.    0.07  0.01  0.98\n",
      "  -0.01 -0.02  0.94  0.08  0.    0.99  0.04  0.05  0.97  0.02 -0.02  0.99\n",
      "   0.02 -0.    0.99 -0.01 -0.07  0.98  0.06  0.    0.98 -0.03 -0.02  0.94\n",
      "   0.   -0.    1.    0.07 -0.01  0.98  0.02 -0.01  0.98  0.02 -0.01  0.98]\n",
      " [ 0.02  0.1   0.98 -0.    0.15  0.96 -0.    0.08  0.97 -0.04  0.06  0.92\n",
      "   0.03  0.11  0.96 -0.07  0.07  0.96 -0.02  0.1   0.99 -0.01  0.12  1.\n",
      "   0.    0.1   0.95 -0.05  0.09  0.95 -0.04  0.08  0.96 -0.    0.07  0.96\n",
      "  -0.02  0.08  0.95  0.03  0.11  0.97  0.04  0.1   1.01  0.01  0.09  0.99\n",
      "  -0.02  0.05  1.    0.    0.11  1.01 -0.06  0.03  1.01  0.01  0.1   0.95]\n",
      " [-0.05 -0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06  0.    0.99 -0.06 -0.    0.98 -0.06 -0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.03  0.01  0.99 -0.07  0.    0.99 -0.05 -0.02  0.98 -0.08 -0.02  0.97\n",
      "  -0.05 -0.    0.99 -0.03  0.01  0.98 -0.04  0.    0.99 -0.05 -0.01  0.98\n",
      "  -0.08 -0.    0.98 -0.08 -0.03  0.97 -0.06  0.    0.99 -0.06  0.04  0.99\n",
      "  -0.05 -0.02  0.98 -0.05  0.01  0.99 -0.01  0.01  0.99 -0.05 -0.02  0.97\n",
      "  -0.05  0.    0.98 -0.05 -0.03  0.98 -0.07 -0.    0.98 -0.04  0.02  0.99]\n",
      " [ 0.04  0.11  1.02  0.01  0.09  0.95 -0.01  0.04  1.    0.01  0.11  0.99\n",
      "  -0.    0.06  1.03  0.03  0.12  0.97 -0.05  0.09  0.95 -0.    0.11  1.02\n",
      "  -0.07  0.1   0.99 -0.04  0.07  0.99  0.06  0.12  1.   -0.01  0.1   1.\n",
      "   0.02  0.18  1.01 -0.02  0.08  0.97  0.06  0.09  1.01 -0.01  0.09  0.99\n",
      "  -0.04  0.02  0.99  0.03  0.12  0.97  0.04  0.08  1.01  0.02  0.08  0.98]\n",
      " [-0.04  0.05  0.96 -0.02  0.07  0.98 -0.06  0.03  0.94 -0.05  0.07  0.99\n",
      "  -0.03  0.12  0.95 -0.    0.1   1.03 -0.06  0.03  1.01 -0.04  0.06  1.\n",
      "   0.02  0.16  0.96 -0.01  0.09  0.99 -0.06  0.11  0.97 -0.02  0.08  0.97\n",
      "   0.01  0.14  0.96 -0.    0.07  0.97 -0.    0.07  1.    0.01  0.1   0.95\n",
      "   0.03  0.12  0.97 -0.02  0.11  0.99 -0.03  0.05  1.    0.02  0.11  0.96]\n",
      " [ 0.02  0.01  0.97  0.02 -0.    0.97  0.03  0.02  0.98  0.09  0.01  1.\n",
      "   0.08  0.01  1.02 -0.01 -0.02  0.94  0.02 -0.02  0.97  0.05  0.04  0.97\n",
      "   0.02  0.    0.98  0.02  0.    0.97 -0.05 -0.01  0.99  0.06  0.    0.98\n",
      "  -0.   -0.02  0.93  0.01 -0.    1.01 -0.04 -0.03  0.98  0.02 -0.03  0.99\n",
      "   0.02  0.    0.99  0.02 -0.01  0.98  0.1   0.02  1.   -0.02 -0.02  0.94]\n",
      " [ 0.07  0.01  1.02  0.06  0.    0.94  0.08  0.03  0.98  0.02  0.02  0.98\n",
      "   0.02 -0.    0.99  0.02 -0.01  0.98 -0.01  0.    0.99  0.06  0.01  1.02\n",
      "  -0.02 -0.02  0.96  0.08  0.01  1.   -0.03 -0.01  1.    0.02 -0.02  0.99\n",
      "   0.02 -0.    0.98  0.04  0.02  0.97 -0.02 -0.02  0.96 -0.02 -0.02  0.94\n",
      "   0.01  0.    1.01  0.04 -0.02  0.98  0.01 -0.06  0.99  0.02  0.01  0.97]\n",
      " [ 0.02 -0.    0.97  0.01 -0.06  0.98  0.05  0.01  1.   -0.   -0.02  0.94\n",
      "  -0.   -0.01  1.   -0.02 -0.03  0.98  0.03  0.03  0.97  0.02  0.    0.98\n",
      "   0.02 -0.    0.99  0.04  0.01  0.99  0.01 -0.    1.   -0.02 -0.01  0.98\n",
      "   0.04 -0.    0.98 -0.04 -0.01  0.99  0.02 -0.02  0.99  0.02 -0.01  0.98\n",
      "   0.03 -0.    1.   -0.04 -0.02  0.96 -0.02 -0.02  0.94  0.06  0.01  1.02]\n",
      " [-0.03  0.1   0.95 -0.    0.14  0.98 -0.    0.11  1.   -0.01  0.08  0.95\n",
      "   0.03  0.1   0.99  0.03  0.08  1.01 -0.03  0.06  1.   -0.03  0.04  1.\n",
      "   0.01  0.1   1.02 -0.02  0.06  1.04  0.02  0.11  0.95 -0.01  0.15  0.95\n",
      "  -0.01  0.11  1.01 -0.08  0.1   0.99  0.03  0.11  0.98  0.06  0.1   1.01\n",
      "  -0.    0.11  1.01  0.02  0.11  1.04 -0.05  0.06  0.99  0.05  0.09  1.  ]\n",
      " [-0.01 -0.05  0.98  0.02  0.    0.98  0.02  0.    0.99  0.09  0.03  0.97\n",
      "   0.09  0.03  1.    0.04  0.    0.95 -0.03 -0.    1.    0.09  0.03  0.97\n",
      "   0.02  0.01  0.98  0.02  0.    0.99  0.02  0.    0.98 -0.05 -0.    1.04\n",
      "   0.06  0.01  0.95  0.07  0.02  0.99  0.09  0.03  0.99  0.03  0.06  1.1\n",
      "   0.02 -0.    0.99  0.02  0.01  0.97  0.04  0.02  1.    0.    0.01  1.02]\n",
      " [-0.03  0.03  0.99 -0.05  0.    0.98 -0.01 -0.01  1.   -0.05  0.02  0.98\n",
      "  -0.04 -0.01  0.98 -0.02 -0.01  0.98 -0.06  0.03  0.99 -0.05  0.    0.98\n",
      "  -0.04 -0.02  0.98 -0.04  0.01  0.98 -0.06  0.01  0.98 -0.04 -0.02  0.98\n",
      "  -0.05 -0.01  0.98 -0.05  0.01  0.98 -0.08  0.01  0.99 -0.04 -0.    0.98\n",
      "  -0.05 -0.02  0.98 -0.02 -0.01  0.99 -0.05  0.    0.98 -0.06 -0.04  0.97]\n",
      " [ 0.08  0.02  0.98  0.05  0.03  1.02 -0.01 -0.03  0.95  0.02  0.02  0.98\n",
      "   0.02  0.01  0.98  0.07  0.02  0.98  0.02  0.02  1.02  0.07  0.02  1.\n",
      "   0.08  0.01  0.96  0.08  0.03  0.98  0.03  0.03  1.01  0.02  0.01  0.98\n",
      "   0.03  0.01  0.99  0.07  0.03  0.99  0.09  0.02  0.96  0.02  0.01  1.01\n",
      "  -0.05 -0.    1.    0.03 -0.04  0.86  0.02  0.    0.98  0.02  0.01  0.98]\n",
      " [ 0.08 -0.01  0.98  0.03  0.01  0.97  0.02  0.    0.99  0.02 -0.    0.98\n",
      "   0.05 -0.01  0.98  0.02  0.    1.01 -0.02 -0.02  0.94  0.   -0.01  0.96\n",
      "   0.07 -0.01  0.97  0.02 -0.01  0.98  0.02 -0.01  0.98  0.02 -0.02  0.98\n",
      "   0.09  0.01  1.01 -0.03 -0.02  0.96  0.05  0.01  1.03 -0.03 -0.03  0.97\n",
      "   0.   -0.03  1.    0.02 -0.01  0.98  0.03 -0.    0.98  0.09  0.01  0.97]\n",
      " [-0.02  0.09  0.97  0.01  0.07  1.   -0.01  0.08  0.98 -0.01  0.12  1.\n",
      "  -0.01  0.09  0.95 -0.03  0.05  0.99 -0.    0.04  0.99  0.01  0.08  0.95\n",
      "   0.02  0.12  1.   -0.    0.09  1.02 -0.02  0.03  0.99  0.01  0.1   0.95\n",
      "  -0.05  0.07  0.95 -0.01  0.09  1.02 -0.08  0.09  0.99 -0.03  0.07  0.99\n",
      "   0.06  0.14  0.99 -0.01  0.11  1.01  0.01  0.17  0.99 -0.04  0.07  0.99]\n",
      " [ 0.06  0.01  1.    0.08  0.01  1.01 -0.01 -0.01  0.99  0.09  0.01  1.\n",
      "   0.02 -0.05  0.98  0.02  0.01  0.98  0.02 -0.    0.97  0.03 -0.06  0.99\n",
      "   0.05  0.01  1.01 -0.01 -0.02  0.94 -0.01 -0.01  0.99 -0.01 -0.03  0.98\n",
      "   0.01 -0.02  1.    0.02 -0.    0.98  0.02 -0.    0.98 -0.02  0.04  0.99\n",
      "  -0.03 -0.02  0.94  0.03 -0.01  0.95  0.08  0.01  1.02 -0.01 -0.05  0.99]\n",
      " [ 0.04 -0.04  0.87  0.02  0.01  0.98  0.02  0.01  0.98 -0.    0.04  1.04\n",
      "   0.05  0.03  1.02  0.06  0.01  0.95 -0.02 -0.    0.97 -0.04 -0.02  0.98\n",
      "   0.03  0.01  0.99  0.02  0.    0.97  0.02  0.    0.98  0.04  0.02  1.01\n",
      "  -0.04 -0.    1.    0.02 -0.01  0.98 -0.04 -0.01  0.97  0.04 -0.    0.96\n",
      "   0.07  0.01  1.01  0.09  0.02  1.01  0.06  0.04  0.97  0.02 -0.01  0.98]\n",
      " [ 0.06  0.05  1.05  0.03  0.01  1.    0.02  0.01  0.98  0.03  0.02  0.98\n",
      "   0.06 -0.    0.95 -0.03  0.    1.01  0.02  0.    0.95 -0.05 -0.01  0.98\n",
      "   0.02  0.05  1.07  0.02  0.01  0.98  0.02  0.01  0.98 -0.02 -0.04  0.94\n",
      "   0.1   0.02  0.98 -0.04 -0.    1.    0.04  0.01  1.01 -0.05 -0.    1.02\n",
      "   0.06  0.02  1.    0.02  0.01  0.99  0.02  0.    0.99  0.04 -0.03  0.86]\n",
      " [-0.04  0.01  0.99 -0.08  0.    0.98 -0.08  0.03  0.99 -0.05  0.    0.99\n",
      "  -0.02 -0.    0.98 -0.07  0.02  0.99 -0.06 -0.01  0.98 -0.05 -0.02  0.98\n",
      "  -0.06 -0.03  0.97 -0.05 -0.    0.99 -0.08  0.05  1.   -0.04 -0.01  0.98\n",
      "  -0.05  0.02  0.99 -0.09  0.02  0.99 -0.06  0.02  0.99 -0.05  0.    0.98\n",
      "  -0.02  0.    0.98 -0.06 -0.01  0.98 -0.04 -0.    0.98 -0.07  0.03  0.99]\n",
      " [-0.02 -0.01  0.98  0.04 -0.    0.98 -0.04 -0.01  0.99  0.02 -0.02  0.99\n",
      "   0.02 -0.01  0.98  0.03 -0.    1.   -0.04 -0.02  0.96 -0.02 -0.02  0.94\n",
      "   0.06  0.01  1.02 -0.04 -0.01  0.97  0.04 -0.04  0.97  0.02 -0.02  0.99\n",
      "   0.02 -0.    0.98  0.04  0.05  0.98  0.03  0.    1.    0.08  0.01  1.01\n",
      "   0.03 -0.01  0.95 -0.03  0.    0.98  0.02 -0.03  0.98  0.02  0.    0.99]\n",
      " [-0.   -0.07  0.98  0.08  0.01  1.02  0.07  0.01  0.99  0.   -0.02  0.94\n",
      "   0.09  0.02  0.98  0.02 -0.01  0.98  0.02 -0.01  0.98  0.02 -0.01  0.98\n",
      "  -0.02 -0.03  0.98  0.02  0.    1.01 -0.01 -0.02  0.94 -0.04 -0.02  0.96\n",
      "   0.01  0.04  0.99  0.02 -0.01  0.98  0.02 -0.01  0.99  0.01 -0.04  0.99\n",
      "   0.05 -0.    0.98  0.01 -0.    1.    0.07  0.01  1.01  0.03  0.02  0.99]\n",
      " [ 0.03 -0.01  0.97  0.02 -0.01  0.98  0.02 -0.    0.99 -0.05 -0.02  0.97\n",
      "   0.08  0.01  1.02  0.01  0.    1.01  0.04  0.01  1.    0.03  0.05  0.99\n",
      "   0.02 -0.01  0.99  0.02 -0.    0.98  0.02 -0.03  0.98  0.09  0.01  1.\n",
      "  -0.03 -0.02  0.97  0.06  0.01  1.03 -0.04 -0.01  0.98  0.02  0.04  0.98\n",
      "   0.02 -0.01  0.99  0.02 -0.    0.99  0.1   0.    0.97 -0.03  0.01  1.02]\n",
      " [ 0.01  0.01  1.01 -0.03 -0.02  0.97  0.01  0.01  0.98  0.02  0.01  0.98\n",
      "   0.02  0.    0.99  0.1   0.01  0.93  0.09  0.02  0.98  0.06  0.02  1.\n",
      "  -0.05 -0.01  0.99  0.06  0.06  1.06  0.03  0.    1.    0.02  0.01  0.99\n",
      "   0.02  0.    0.97 -0.02 -0.01  0.99  0.09  0.02  0.97 -0.04 -0.    0.99\n",
      "   0.02 -0.01  0.94  0.06  0.07  1.08  0.02  0.    0.99  0.02  0.01  0.97]\n",
      " [-0.03 -0.    1.02  0.02 -0.    0.98  0.02  0.01  0.98  0.04  0.04  1.01\n",
      "   0.04 -0.01  0.94 -0.01 -0.    0.96 -0.    0.01  1.01 -0.06 -0.01  0.93\n",
      "   0.01 -0.05  0.9   0.02  0.01  0.98  0.02  0.01  0.98 -0.02  0.04  1.09\n",
      "   0.01 -0.01  0.94 -0.03  0.    1.01  0.08  0.02  0.99 -0.02  0.03  1.07\n",
      "   0.03  0.02  0.99  0.02  0.01  0.99  0.03  0.02  0.98  0.08  0.    0.94]\n",
      " [-0.04 -0.    0.99  0.02 -0.01  0.94  0.06  0.07  1.08  0.02  0.    0.99\n",
      "   0.02  0.01  0.97 -0.03 -0.04  0.94  0.06  0.03  1.01 -0.03 -0.    0.98\n",
      "  -0.03  0.    1.01 -0.03 -0.02  0.91  0.04  0.03  1.01  0.02  0.01  0.98\n",
      "   0.02  0.01  0.99  0.04  0.03  1.03  0.    0.01  1.02 -0.02  0.    1.01\n",
      "  -0.   -0.01  0.95  0.06  0.06  1.06  0.03  0.02  0.99  0.03  0.01  0.98]\n",
      " [ 0.04  0.12  1.01 -0.03  0.06  0.97 -0.    0.07  1.01  0.    0.09  1.02\n",
      "  -0.01  0.03  0.99 -0.02  0.08  0.96 -0.05  0.05  0.95 -0.    0.11  1.02\n",
      "  -0.02  0.14  0.96 -0.04  0.07  0.98 -0.03  0.01  0.99 -0.    0.08  0.95\n",
      "   0.04  0.04  0.97  0.02  0.11  0.96 -0.07  0.1   0.95 -0.01  0.1   0.98\n",
      "   0.01  0.14  0.92 -0.03  0.06  1.   -0.    0.12  0.96  0.01  0.1   0.98]\n",
      " [-0.02  0.08  1.    0.03  0.12  0.96 -0.01  0.11  0.95  0.02  0.13  0.99\n",
      "   0.01  0.12  0.99  0.03  0.14  0.98  0.03  0.1   1.    0.01  0.05  1.\n",
      "  -0.04  0.06  0.99 -0.05  0.03  0.97 -0.03  0.07  1.   -0.07  0.07  0.95\n",
      "   0.    0.08  0.99  0.05  0.09  1.01 -0.    0.09  0.99  0.05  0.13  0.95\n",
      "  -0.05  0.06  0.99 -0.05  0.02  0.98 -0.    0.08  0.95 -0.03  0.08  0.93]\n",
      " [ 0.03  0.1   0.99  0.04  0.1   1.01 -0.01  0.07  1.   -0.01  0.05  1.\n",
      "   0.01  0.11  1.    0.    0.07  1.04  0.02  0.11  0.96 -0.05  0.07  0.95\n",
      "  -0.01  0.1   1.   -0.07  0.12  0.99 -0.01  0.08  0.99  0.05  0.08  1.01\n",
      "  -0.    0.11  1.01  0.02  0.11  1.04 -0.04  0.07  0.99  0.02  0.07  1.\n",
      "  -0.01  0.08  0.98 -0.04  0.05  0.99 -0.04  0.06  0.98 -0.03  0.05  0.99]\n",
      " [-0.05  0.    0.98 -0.06 -0.02  0.98 -0.05  0.01  0.98 -0.06 -0.02  0.98\n",
      "  -0.05  0.01  0.99 -0.06 -0.01  0.98 -0.   -0.02  0.99 -0.04  0.01  0.98\n",
      "  -0.05  0.    0.99 -0.06 -0.01  0.98 -0.07  0.01  0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.04  0.01  0.98 -0.07  0.    0.98 -0.03 -0.01  0.99\n",
      "  -0.06 -0.    0.98 -0.05 -0.    0.99 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.    0.09  0.96 -0.03  0.06  1.   -0.02  0.05  0.99  0.    0.06  0.98\n",
      "  -0.01  0.1   1.    0.01  0.11  0.95 -0.04  0.06  0.99 -0.04  0.05  0.96\n",
      "  -0.03  0.07  0.99 -0.04  0.04  0.97 -0.04  0.07  0.96 -0.06  0.06  0.91\n",
      "   0.03  0.11  0.96  0.05  0.11  1.01 -0.01  0.07  0.95  0.02  0.13  0.92\n",
      "   0.04  0.12  0.96  0.01  0.03  1.01 -0.    0.1   0.99 -0.02  0.01  0.97]\n",
      " [ 0.    0.11  0.99 -0.01  0.13  0.99 -0.01  0.07  1.01 -0.04  0.03  0.96\n",
      "  -0.04  0.07  0.98 -0.04  0.06  0.95 -0.03  0.07  1.   -0.08  0.03  0.95\n",
      "   0.03  0.11  0.96  0.05  0.16  0.98 -0.    0.09  1.01 -0.    0.17  0.99\n",
      "   0.03  0.11  0.96  0.05  0.09  1.01 -0.01  0.11  1.    0.04  0.15  1.02\n",
      "  -0.    0.09  0.99 -0.05  0.04  0.98 -0.    0.1   0.99 -0.04  0.06  0.92]\n",
      " [ 0.03  0.01  1.01  0.07  0.01  0.95  0.05  0.05  1.06  0.03  0.    0.99\n",
      "   0.02  0.01  0.98  0.02 -0.    0.97 -0.02  0.02  1.03  0.05  0.    0.95\n",
      "   0.07  0.02  0.98  0.1   0.02  0.96  0.02 -0.04  0.88  0.02  0.    0.98\n",
      "   0.02  0.01  0.99  0.06  0.06  1.03 -0.06 -0.01  0.98  0.08  0.01  0.96\n",
      "  -0.02 -0.    0.98  0.07 -0.01  0.92  0.03  0.03  1.01  0.02  0.01  0.98]\n",
      " [-0.01  0.1   1.01 -0.07  0.11  0.99 -0.01  0.08  0.99  0.06  0.12  1.\n",
      "   0.    0.07  0.97 -0.01  0.05  1.03 -0.01  0.08  1.    0.05  0.11  1.\n",
      "   0.01  0.07  0.97  0.02  0.12  0.97 -0.04  0.05  0.98  0.03  0.09  1.\n",
      "   0.03  0.13  0.99 -0.03  0.09  1.    0.02  0.09  1.   -0.04  0.06  0.99\n",
      "  -0.05  0.08  0.93  0.03  0.11  0.97  0.04  0.15  0.98 -0.    0.09  0.93]\n",
      " [-0.05  0.01  0.99 -0.06 -0.01  0.98 -0.05  0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.99 -0.05 -0.02  0.97 -0.05  0.01  0.99 -0.05 -0.    0.98\n",
      "  -0.05  0.02  0.98 -0.06 -0.01  0.98 -0.05  0.01  0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.06 -0.02  0.98 -0.05  0.01  0.98\n",
      "  -0.06 -0.02  0.98 -0.05  0.01  0.99 -0.06 -0.01  0.98 -0.   -0.02  0.99]\n",
      " [-0.01  0.09  0.96 -0.06  0.05  0.99 -0.04  0.06  0.96 -0.    0.07  0.96\n",
      "  -0.04  0.04  0.97 -0.03  0.09  0.95 -0.01  0.13  0.96  0.03  0.11  0.97\n",
      "   0.03  0.15  0.98  0.    0.09  0.94  0.07  0.13  0.95 -0.04  0.06  0.99\n",
      "  -0.05  0.01  0.98 -0.01  0.07  0.93  0.07  0.1   0.98 -0.05  0.06  0.99\n",
      "  -0.07  0.04  0.97 -0.    0.07  0.94 -0.04  0.06  0.92  0.04  0.11  0.96]\n",
      " [-0.02 -0.    0.95 -0.05  0.06  0.99 -0.03  0.13  0.95 -0.    0.07  0.96\n",
      "  -0.    0.07  1.    0.02  0.11  0.95  0.06  0.1   1.    0.02  0.07  0.99\n",
      "   0.01  0.13  0.99 -0.02  0.08  0.96 -0.02  0.06  1.   -0.03  0.04  0.96\n",
      "  -0.01  0.08  0.99 -0.04  0.04  0.98 -0.04  0.07  0.98 -0.08  0.03  0.94\n",
      "   0.01  0.1   0.96  0.04  0.16  0.98 -0.01  0.09  1.01  0.03  0.13  0.93]\n",
      " [-0.05 -0.    0.98 -0.05  0.    0.98 -0.06 -0.02  0.98 -0.05  0.01  0.98\n",
      "  -0.06 -0.02  0.98 -0.05  0.01  0.99 -0.06 -0.01  0.98 -0.   -0.02  0.99\n",
      "  -0.04  0.01  0.98 -0.05  0.    0.99 -0.06 -0.01  0.98 -0.07  0.01  0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.04  0.01  0.98 -0.07  0.    0.98\n",
      "  -0.03 -0.01  0.99 -0.06 -0.    0.98 -0.05 -0.    0.99 -0.05 -0.    0.98]\n",
      " [ 0.    0.1   0.97 -0.02  0.04  0.97 -0.02  0.07  1.   -0.03  0.05  0.99\n",
      "  -0.04  0.08  0.95 -0.04  0.08  0.99  0.01  0.12  0.96  0.03  0.11  0.98\n",
      "   0.06  0.13  1.03 -0.02  0.07  1.   -0.06  0.06  0.95  0.    0.11  1.01\n",
      "  -0.07  0.08  0.98 -0.04  0.07  0.98  0.05  0.15  0.99 -0.01  0.11  1.01\n",
      "   0.04  0.16  1.01  0.01  0.11  0.96 -0.03  0.03  0.99 -0.    0.11  0.99]\n",
      " [-0.02 -0.02  0.98 -0.05 -0.02  0.98 -0.05 -0.01  0.98 -0.09 -0.    0.98\n",
      "  -0.05  0.02  0.99 -0.04 -0.02  0.98 -0.08 -0.02  0.98 -0.06 -0.    0.98\n",
      "  -0.06  0.02  0.99 -0.04 -0.02  0.98 -0.04  0.01  0.99 -0.03  0.01  0.99\n",
      "  -0.06 -0.04  0.97 -0.05 -0.    0.99 -0.11  0.    0.98 -0.06 -0.02  0.99\n",
      "  -0.06  0.    0.98 -0.08  0.01  0.99 -0.05  0.01  0.98 -0.05 -0.    0.98]\n",
      " [-0.05 -0.    0.98 -0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.99 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.01  0.98 -0.06 -0.01  0.98 -0.05 -0.01  0.98 -0.06  0.01  0.98]\n",
      " [ 0.06  0.02  0.99  0.02  0.03  1.03  0.02  0.04  1.02  0.02  0.    0.99\n",
      "   0.02  0.01  0.98 -0.04 -0.06  0.89  0.05  0.    0.94 -0.04 -0.    0.99\n",
      "   0.04  0.02  1.01 -0.01 -0.03  0.95  0.02  0.    0.97  0.02  0.01  0.98\n",
      "   0.02  0.01  0.99  0.09  0.03  0.96 -0.02 -0.01  0.97  0.08  0.01  0.95\n",
      "   0.03 -0.01  0.94 -0.   -0.05  0.86  0.02  0.01  0.98  0.02  0.01  0.98]\n",
      " [ 0.    0.09  0.99  0.01  0.07  1.    0.    0.07  0.98 -0.    0.09  1.\n",
      "  -0.01  0.08  0.95  0.03  0.09  1.    0.05  0.13  1.01 -0.01  0.07  0.96\n",
      "   0.03  0.13  0.98  0.02  0.11  0.99  0.    0.06  1.03 -0.03  0.07  0.99\n",
      "   0.01  0.15  0.96 -0.02  0.07  0.96  0.05  0.07  0.98  0.02  0.1   0.98\n",
      "  -0.07  0.06  0.95 -0.01  0.07  0.95 -0.05  0.08  0.94 -0.04  0.06  0.99]\n",
      " [-0.05 -0.03  0.97 -0.05 -0.    0.98 -0.03  0.01  0.98 -0.05  0.02  0.99\n",
      "  -0.06 -0.02  0.98 -0.01  0.    0.99 -0.06  0.    0.98 -0.07  0.01  0.99\n",
      "  -0.08  0.    0.99 -0.04  0.01  0.99 -0.06 -0.02  0.98 -0.02 -0.02  0.98\n",
      "  -0.06 -0.    0.98 -0.1  -0.02  0.98 -0.04 -0.01  0.98 -0.07 -0.    0.98\n",
      "  -0.03  0.    0.99 -0.06  0.03  1.   -0.06 -0.    0.98 -0.05 -0.04  0.97]\n",
      " [-0.    0.03  1.   -0.01  0.09  0.98  0.02  0.15  0.96 -0.04  0.06  0.99\n",
      "  -0.05  0.08  0.95 -0.01  0.12  0.99 -0.01  0.06  1.02 -0.03  0.06  0.96\n",
      "   0.04  0.11  0.97  0.04  0.16  0.98  0.02  0.1   0.96  0.03  0.13  0.99\n",
      "   0.    0.1   0.95  0.05  0.12  1.04  0.03  0.12  0.97 -0.06  0.06  0.95\n",
      "  -0.    0.11  1.   -0.06  0.09  0.96 -0.    0.09  0.97  0.05  0.13  0.98]\n",
      " [ 0.02 -0.    0.99  0.02  0.01  0.99 -0.01 -0.03  0.94  0.    0.01  1.02\n",
      "   0.08  0.01  0.96  0.03  0.    0.95 -0.05 -0.    1.02  0.04  0.02  1.01\n",
      "   0.02  0.01  0.98  0.02  0.01  0.98 -0.05 -0.01  0.99  0.08  0.01  0.95\n",
      "   0.08  0.02  0.98 -0.03 -0.01  0.97 -0.03 -0.    1.02  0.02  0.    0.98\n",
      "   0.02  0.01  0.98  0.03  0.01  0.98  0.09  0.02  0.98  0.07  0.01  0.95]\n",
      " [-0.04  0.01  0.99 -0.02 -0.02  0.98 -0.06  0.    0.98 -0.06 -0.    0.98\n",
      "  -0.03 -0.01  0.97 -0.07  0.02  0.98 -0.05  0.03  0.99 -0.09 -0.01  0.97\n",
      "  -0.05 -0.    0.99 -0.09  0.03  0.99 -0.06  0.02  0.99 -0.06  0.01  0.98\n",
      "  -0.03 -0.01  0.98 -0.02  0.01  0.99 -0.06  0.    0.98 -0.04 -0.04  0.98\n",
      "  -0.07  0.    0.98 -0.04  0.01  0.98 -0.03 -0.03  0.98 -0.06 -0.    0.98]\n",
      " [ 0.02 -0.    0.97  0.02  0.01  0.98  0.02 -0.01  0.98  0.01  0.02  1.01\n",
      "   0.05  0.    0.95  0.06  0.02  0.99  0.1   0.02  0.97  0.07  0.    0.93\n",
      "   0.02 -0.    0.98  0.02  0.    0.99  0.04 -0.    0.96  0.09  0.03  0.99\n",
      "  -0.03  0.    1.    0.07  0.02  0.99 -0.02 -0.02  0.96 -0.   -0.03  0.93\n",
      "   0.02  0.01  0.98  0.02  0.01  0.98  0.12  0.02  0.97  0.06  0.01  0.95]\n",
      " [ 0.02 -0.01  0.97  0.02 -0.02  1.    0.09  0.01  1.    0.08  0.01  1.02\n",
      "  -0.02 -0.02  0.95  0.04 -0.01  0.98  0.05  0.05  0.98  0.03 -0.01  0.98\n",
      "   0.02 -0.    0.97  0.03 -0.05  0.98  0.05  0.01  1.01  0.07  0.01  1.02\n",
      "   0.06  0.    0.94  0.08  0.03  0.98  0.02  0.02  0.98  0.02 -0.    0.99\n",
      "   0.02 -0.01  0.98 -0.01  0.    0.99  0.06  0.01  1.02 -0.02 -0.02  0.96]\n",
      " [-0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.06  0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.01  0.15  0.95 -0.01  0.11  1.01 -0.08  0.1   0.99  0.03  0.11  0.98\n",
      "   0.06  0.1   1.01 -0.    0.11  1.01  0.02  0.11  1.04 -0.05  0.06  0.99\n",
      "   0.05  0.09  1.    0.    0.07  0.98  0.    0.08  0.96 -0.02  0.07  1.01\n",
      "   0.04  0.1   1.    0.03  0.11  1.   -0.02  0.09  1.   -0.01  0.09  0.95\n",
      "   0.03  0.1   0.99  0.04  0.1   1.01 -0.01  0.07  1.   -0.01  0.05  1.  ]\n",
      " [ 0.04  0.02  0.97 -0.02 -0.02  0.96 -0.02 -0.02  0.94  0.01  0.    1.01\n",
      "   0.04 -0.02  0.98  0.01 -0.06  0.99  0.02  0.01  0.97  0.02 -0.    0.98\n",
      "  -0.05 -0.01  1.    0.07  0.01  0.99 -0.   -0.02  0.94 -0.02 -0.01  0.98\n",
      "   0.05 -0.02  0.97  0.02 -0.02  0.99  0.02 -0.01  0.98  0.03  0.    0.96\n",
      "   0.06 -0.01  0.99 -0.03 -0.02  0.95  0.03 -0.01  0.95 -0.05 -0.02  0.96]\n",
      " [-0.05  0.06  0.99 -0.03  0.13  0.95 -0.    0.07  0.96 -0.    0.07  1.\n",
      "   0.02  0.11  0.95  0.06  0.1   1.    0.02  0.07  0.99  0.01  0.13  0.99\n",
      "  -0.02  0.08  0.96 -0.02  0.06  1.   -0.03  0.04  0.96 -0.01  0.08  0.99\n",
      "  -0.04  0.04  0.98 -0.04  0.07  0.98 -0.08  0.03  0.94  0.01  0.1   0.96\n",
      "   0.04  0.16  0.98 -0.01  0.09  1.01  0.03  0.13  0.93 -0.01  0.09  0.99]\n",
      " [ 0.    0.07  0.98  0.    0.08  0.96 -0.02  0.07  1.01  0.04  0.1   1.\n",
      "   0.03  0.11  1.   -0.02  0.09  1.   -0.01  0.09  0.95  0.03  0.1   0.99\n",
      "   0.04  0.1   1.01 -0.01  0.07  1.   -0.01  0.05  1.    0.01  0.11  1.\n",
      "   0.    0.07  1.04  0.02  0.11  0.96 -0.05  0.07  0.95 -0.01  0.1   1.\n",
      "  -0.07  0.12  0.99 -0.01  0.08  0.99  0.05  0.08  1.01 -0.    0.11  1.01]\n",
      " [ 0.06  0.01  1.    0.07  0.01  0.99 -0.02 -0.02  0.95  0.1   0.01  1.\n",
      "   0.01  0.02  0.99  0.02  0.    0.98  0.02 -0.    0.99  0.03 -0.05  0.98\n",
      "   0.02 -0.01  0.96 -0.02 -0.01  1.    0.04  0.01  1.02  0.06  0.02  0.99\n",
      "   0.02 -0.01  0.99  0.02 -0.01  0.98  0.02 -0.    0.98 -0.01 -0.02  0.98\n",
      "   0.06  0.01  0.99  0.07  0.01  1.02 -0.   -0.    0.98  0.03 -0.04  0.97]\n",
      " [ 0.03  0.02  1.    0.02  0.01  0.98  0.03  0.02  0.99  0.   -0.01  0.97\n",
      "   0.09  0.02  0.97 -0.04  0.    1.    0.07  0.    0.94  0.01  0.04  1.07\n",
      "   0.03  0.01  0.98  0.02  0.01  0.99  0.01 -0.04  0.92  0.08  0.03  1.\n",
      "  -0.03  0.    1.01  0.02  0.01  1.01  0.04 -0.02  0.91  0.04  0.04  1.02\n",
      "   0.02  0.01  0.98  0.02  0.01  0.98  0.08  0.03  0.99  0.09  0.02  0.98]\n",
      " [ 0.07 -0.    0.97  0.03  0.01  0.97  0.02 -0.03  0.99  0.02  0.01  0.99\n",
      "   0.1   0.01  1.    0.07  0.01  0.98 -0.01 -0.02  0.94  0.08  0.    0.99\n",
      "   0.04  0.05  0.97  0.02 -0.02  0.99  0.02 -0.    0.99 -0.01 -0.07  0.98\n",
      "   0.06  0.    0.98 -0.03 -0.02  0.94  0.   -0.    1.    0.07 -0.01  0.98\n",
      "   0.02 -0.01  0.98  0.02 -0.01  0.98  0.03  0.01  0.96  0.01 -0.02  0.98]\n",
      " [ 0.    0.11  1.01 -0.07  0.08  0.98 -0.04  0.07  0.98  0.05  0.15  0.99\n",
      "  -0.01  0.11  1.01  0.04  0.16  1.01  0.01  0.11  0.96 -0.03  0.03  0.99\n",
      "  -0.    0.11  0.99  0.02  0.14  0.99 -0.    0.09  0.96 -0.01  0.11  0.96\n",
      "   0.01  0.12  0.99  0.01  0.09  0.96 -0.01  0.09  0.95  0.04  0.1   0.99\n",
      "   0.03  0.08  1.03  0.03  0.1   0.97 -0.01  0.07  1.    0.    0.11  1.02]\n",
      " [-0.04 -0.03  0.97  0.08  0.01  1.   -0.02 -0.01  0.96  0.09  0.01  0.99\n",
      "  -0.02 -0.02  1.    0.02 -0.    0.99  0.02 -0.    0.99  0.03 -0.03  0.96\n",
      "   0.09  0.01  1.01 -0.   -0.01  1.    0.04  0.01  1.03  0.07  0.02  0.99\n",
      "   0.01 -0.05  1.    0.02 -0.    0.99  0.02  0.    0.99  0.01 -0.03  0.97\n",
      "  -0.03 -0.02  0.95 -0.01 -0.02  0.94  0.08  0.01  1.02  0.07 -0.01  0.97]\n",
      " [-0.07 -0.01  0.98 -0.06  0.03  0.99 -0.06  0.01  0.99 -0.05  0.01  0.98\n",
      "  -0.03 -0.02  0.98 -0.04 -0.    0.98 -0.06  0.02  0.99 -0.01  0.01  0.99\n",
      "  -0.05  0.    0.98 -0.08 -0.01  0.98 -0.07 -0.    0.98 -0.06  0.01  0.99\n",
      "  -0.07  0.02  0.99 -0.03  0.03  1.   -0.05  0.    0.98 -0.08 -0.05  0.96\n",
      "  -0.07  0.01  0.98 -0.05 -0.01  0.98 -0.07 -0.03  0.98 -0.06  0.01  0.99]\n",
      " [-0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.06 -0.    0.99 -0.06 -0.03  0.98 -0.05  0.02  0.99\n",
      "  -0.05 -0.02  0.98 -0.05  0.01  0.99 -0.06 -0.01  0.98 -0.05  0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.99 -0.05 -0.02  0.97 -0.05  0.01  0.99\n",
      "  -0.05 -0.    0.98 -0.05  0.02  0.98 -0.06 -0.01  0.98 -0.05  0.01  0.98]\n",
      " [-0.03 -0.02  0.96 -0.02 -0.04  0.99  0.02  0.    0.98  0.02 -0.    0.98\n",
      "   0.04  0.04  0.98 -0.04 -0.02  0.96  0.07  0.01  1.    0.01 -0.01  0.94\n",
      "   0.05  0.02  0.99  0.04  0.03  0.97  0.02  0.    0.99  0.02 -0.    0.99\n",
      "   0.09 -0.02  0.97  0.07  0.01  1.02  0.03  0.01  1.02  0.08  0.01  1.01\n",
      "  -0.01  0.02  0.99  0.02  0.01  0.97  0.02 -0.01  0.99  0.03  0.01  0.97]\n",
      " [-0.    0.1   1.02 -0.    0.17  0.99  0.03  0.11  0.96  0.05  0.08  1.01\n",
      "  -0.01  0.11  1.   -0.03  0.03  0.99 -0.03  0.06  1.   -0.07  0.08  0.95\n",
      "  -0.01  0.1   0.98 -0.02  0.08  1.    0.03  0.12  0.96 -0.01  0.11  0.95\n",
      "   0.02  0.13  0.99  0.01  0.12  0.99  0.03  0.14  0.98  0.03  0.1   1.\n",
      "   0.01  0.05  1.   -0.04  0.06  0.99 -0.05  0.03  0.97 -0.03  0.07  1.  ]\n",
      " [ 0.01  0.06  0.98 -0.03  0.07  0.99  0.02  0.12  0.96 -0.01  0.07  1.01\n",
      "  -0.03  0.03  0.97  0.03  0.1   0.96 -0.04  0.07  0.95 -0.03  0.07  0.96\n",
      "  -0.05  0.08  0.91  0.02  0.1   0.98  0.04  0.15  0.98 -0.01  0.08  0.97\n",
      "   0.07  0.11  0.95 -0.05  0.06  0.99 -0.07  0.07  0.95 -0.01  0.07  0.95\n",
      "   0.02  0.14  0.93 -0.03  0.07  0.99  0.06  0.11  0.99  0.    0.07  0.97]\n",
      " [-0.08  0.01  0.98 -0.07 -0.02  0.98 -0.05  0.02  0.99 -0.09  0.01  0.98\n",
      "  -0.05  0.    0.99 -0.05 -0.01  0.98 -0.03 -0.01  0.98 -0.07  0.01  0.99\n",
      "  -0.05  0.02  0.99 -0.1   0.    0.98 -0.05  0.    0.98 -0.04  0.04  1.\n",
      "  -0.04  0.01  0.99 -0.04  0.01  0.99 -0.08 -0.01  0.98 -0.08 -0.03  0.97\n",
      "  -0.05  0.    0.98 -0.09  0.03  0.98 -0.07  0.01  0.98 -0.07 -0.01  0.98]\n",
      " [-0.06  0.03  1.01 -0.04  0.06  1.    0.02  0.16  0.96 -0.01  0.09  0.99\n",
      "  -0.06  0.11  0.97 -0.02  0.08  0.97  0.01  0.14  0.96 -0.    0.07  0.97\n",
      "  -0.    0.07  1.    0.01  0.1   0.95  0.03  0.12  0.97 -0.02  0.11  0.99\n",
      "  -0.03  0.05  1.    0.02  0.11  0.96 -0.05  0.07  0.97 -0.03  0.07  0.97\n",
      "   0.03  0.1   0.97 -0.    0.07  1.   -0.03  0.07  1.01  0.    0.06  1.03]\n",
      " [ 0.07  0.01  0.99  0.06  0.    0.98  0.   -0.02  0.94 -0.04 -0.02  0.99\n",
      "   0.02 -0.02  1.    0.02 -0.01  0.98  0.02 -0.01  0.98  0.1   0.01  1.\n",
      "   0.05  0.01  1.02 -0.02 -0.02  0.94  0.02 -0.01  0.97  0.06 -0.02  0.97\n",
      "   0.02 -0.    0.98  0.02 -0.01  0.98  0.03  0.04  0.99 -0.03 -0.01  0.97\n",
      "   0.   -0.02  0.94 -0.   -0.    1.01  0.09  0.02  1.    0.03 -0.02  0.98]\n",
      " [-0.02  0.03  1.   -0.04 -0.01  0.99 -0.05 -0.02  0.98 -0.08  0.01  0.98\n",
      "  -0.1  -0.01  0.97 -0.05  0.    0.98 -0.04  0.04  1.   -0.05 -0.02  0.98\n",
      "  -0.06  0.01  0.99 -0.02  0.02  0.99 -0.05 -0.01  0.97 -0.05  0.    0.98\n",
      "  -0.06 -0.03  0.98 -0.06 -0.01  0.98 -0.03  0.    0.99 -0.04 -0.03  0.98\n",
      "  -0.05  0.    0.98 -0.08  0.02  0.99 -0.05 -0.02  0.98 -0.04  0.    0.98]\n",
      " [-0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.06  0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [ 0.03  0.11  0.98 -0.01  0.03  1.   -0.    0.09  0.93  0.04  0.15  0.96\n",
      "  -0.01  0.09  0.97  0.04  0.08  1.01 -0.    0.1   0.99  0.05  0.1   1.\n",
      "   0.01  0.09  0.98 -0.03  0.03  0.99 -0.01  0.11  0.99 -0.03  0.07  0.97\n",
      "   0.01  0.1   0.99 -0.03  0.05  0.99  0.01  0.07  0.97 -0.03  0.05  0.97\n",
      "   0.    0.09  0.99  0.03  0.09  1.    0.01  0.07  0.98  0.02  0.12  0.99]\n",
      " [-0.06  0.03  1.   -0.06 -0.    0.98 -0.05 -0.04  0.97 -0.06 -0.02  0.98\n",
      "  -0.04  0.    0.98 -0.08 -0.02  0.98 -0.05 -0.01  0.98 -0.06 -0.    0.99\n",
      "  -0.03  0.    0.98 -0.07  0.01  0.99 -0.06 -0.02  0.98 -0.1   0.01  0.98\n",
      "  -0.06  0.    0.99 -0.03  0.03  1.   -0.04 -0.01  0.98 -0.05 -0.02  0.98\n",
      "  -0.08  0.01  0.98 -0.1  -0.    0.98 -0.05  0.    0.98 -0.02  0.02  1.  ]\n",
      " [ 0.02 -0.    0.98  0.01 -0.02  1.   -0.05 -0.02  0.96  0.05  0.    0.97\n",
      "  -0.02 -0.02  0.96  0.09  0.01  0.99  0.03 -0.03  0.98  0.02 -0.01  0.99\n",
      "   0.02 -0.01  0.98 -0.04 -0.01  1.   -0.04 -0.02  0.95 -0.01 -0.02  0.94\n",
      "   0.08  0.02  1.02  0.06 -0.02  0.97  0.02 -0.02  0.99  0.02  0.    0.99\n",
      "   0.02 -0.01  0.97 -0.05 -0.03  0.97  0.08  0.01  1.   -0.02 -0.01  0.97]\n",
      " [ 0.01  0.11  1.   -0.06  0.05  1.   -0.03  0.07  1.   -0.    0.15  0.95\n",
      "  -0.01  0.09  0.99  0.04  0.16  1.02  0.02  0.11  0.96  0.01  0.05  1.01\n",
      "  -0.01  0.1   1.   -0.03  0.06  0.98  0.    0.09  0.99  0.01  0.07  1.\n",
      "   0.    0.07  0.98 -0.    0.09  1.   -0.01  0.08  0.95  0.03  0.09  1.\n",
      "   0.05  0.13  1.01 -0.01  0.07  0.96  0.03  0.13  0.98  0.02  0.11  0.99]\n",
      " [-0.05  0.    0.99 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.01  0.98\n",
      "  -0.06 -0.01  0.98 -0.05 -0.01  0.98 -0.06  0.01  0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.04  0.05  1.03  0.02  0.11  0.95  0.05  0.15  0.98 -0.01  0.08  0.99\n",
      "   0.04  0.13  0.93 -0.01  0.08  0.99 -0.05  0.02  0.98 -0.01  0.07  0.93\n",
      "  -0.04  0.05  0.91  0.04  0.11  0.96  0.    0.13  0.96  0.    0.08  0.97\n",
      "   0.04  0.14  0.97  0.03  0.12  0.97  0.05  0.12  0.99  0.03  0.11  1.\n",
      "  -0.01  0.1   1.    0.02  0.11  0.95 -0.05  0.06  0.98 -0.03  0.11  0.94]\n",
      " [ 0.01 -0.    1.    0.06  0.01  1.03  0.08 -0.01  0.98  0.03  0.01  0.97\n",
      "   0.02  0.    0.99  0.02 -0.    0.98  0.05 -0.01  0.98  0.02  0.    1.01\n",
      "  -0.02 -0.02  0.94  0.   -0.01  0.96  0.07 -0.01  0.97  0.02 -0.01  0.98\n",
      "   0.02 -0.01  0.98  0.02 -0.02  0.98  0.09  0.01  1.01 -0.03 -0.02  0.96\n",
      "   0.05  0.01  1.03 -0.03 -0.03  0.97  0.   -0.03  1.    0.02 -0.01  0.98]\n",
      " [ 0.06  0.15  0.98 -0.01  0.08  1.   -0.04  0.01  0.99 -0.    0.09  0.95\n",
      "   0.02  0.08  1.03 -0.05  0.06  0.99 -0.04  0.12  0.94 -0.01  0.1   0.99\n",
      "  -0.01  0.14  0.95  0.03  0.11  0.97  0.06  0.11  1.   -0.    0.07  0.98\n",
      "   0.03  0.12  1.03  0.02  0.11  0.98  0.03  0.08  1.01 -0.01  0.08  0.98\n",
      "   0.02  0.12  0.96 -0.04  0.06  1.    0.02  0.08  1.    0.02  0.07  0.98]\n",
      " [-0.08 -0.02  0.97 -0.06  0.    0.99 -0.05  0.05  1.   -0.04  0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.08 -0.01  0.98 -0.05  0.02  0.99 -0.05  0.    0.98\n",
      "  -0.04  0.02  0.99 -0.04  0.    0.98 -0.05  0.02  0.99 -0.09  0.01  0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.01  0.98 -0.06 -0.01  0.98 -0.05 -0.02  0.98\n",
      "  -0.03  0.    0.99 -0.05 -0.03  0.97 -0.05  0.    0.98 -0.1  -0.03  0.97]\n",
      " [-0.07  0.03  0.99 -0.05  0.    0.99 -0.02  0.01  0.98 -0.06  0.02  0.99\n",
      "  -0.07 -0.    0.98 -0.05 -0.02  0.98 -0.09 -0.02  0.97 -0.05 -0.    0.98\n",
      "  -0.09  0.03  0.99 -0.05 -0.02  0.98 -0.04  0.01  0.99 -0.09  0.    0.98\n",
      "  -0.06  0.01  0.99 -0.05  0.    0.98 -0.08  0.02  0.99 -0.05 -0.01  0.98\n",
      "  -0.04  0.01  0.99 -0.08 -0.03  0.98 -0.06 -0.    0.98 -0.06  0.02  0.98]\n",
      " [-0.06  0.02  0.99 -0.05  0.    0.98 -0.08  0.02  0.99 -0.06  0.02  0.98\n",
      "  -0.07 -0.01  0.98 -0.09 -0.01  0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.04  0.02  0.99 -0.07  0.01  0.98 -0.07  0.    0.98 -0.01  0.    0.99\n",
      "  -0.05  0.    0.98 -0.05 -0.05  0.96 -0.05  0.03  0.98 -0.04  0.01  0.98\n",
      "  -0.04  0.01  0.99 -0.08  0.02  0.99 -0.06  0.    0.98 -0.05  0.03  0.99]\n",
      " [ 0.02  0.01  0.98  0.02 -0.01  0.98  0.01 -0.01  0.95 -0.01 -0.01  0.96\n",
      "   0.05  0.02  1.   -0.06 -0.01  0.99  0.07  0.04  1.01  0.02  0.    0.99\n",
      "   0.02  0.01  0.98  0.01  0.03  1.03 -0.02 -0.01  0.96  0.08  0.01  0.95\n",
      "  -0.   -0.    0.96  0.04  0.04  1.03  0.01 -0.02  0.95  0.02  0.    0.99\n",
      "   0.02  0.    0.98 -0.01  0.05  1.11  0.06  0.02  1.01  0.07  0.02  0.99]\n",
      " [-0.04  0.    0.99 -0.05 -0.01  0.98 -0.08 -0.    0.98 -0.08 -0.03  0.97\n",
      "  -0.06  0.    0.99 -0.06  0.04  0.99 -0.05 -0.02  0.98 -0.05  0.01  0.99\n",
      "  -0.01  0.01  0.99 -0.05 -0.02  0.97 -0.05  0.    0.98 -0.05 -0.03  0.98\n",
      "  -0.07 -0.    0.98 -0.04  0.02  0.99 -0.01 -0.01  0.99 -0.06 -0.    0.99\n",
      "  -0.08 -0.01  0.98 -0.04  0.01  0.99 -0.05  0.01  0.99 -0.07  0.02  0.99]\n",
      " [-0.05  0.    0.98 -0.09 -0.04  0.96 -0.04 -0.01  0.98 -0.07 -0.01  0.98\n",
      "  -0.09 -0.01  0.98 -0.03 -0.02  0.98 -0.05 -0.    0.98 -0.07  0.02  0.98\n",
      "  -0.05 -0.02  0.98 -0.04  0.    0.99 -0.04  0.03  0.99 -0.05  0.    0.98\n",
      "  -0.06 -0.    0.99 -0.05 -0.02  0.98 -0.07  0.    0.98 -0.03  0.01  0.99\n",
      "  -0.09  0.02  0.98 -0.05  0.    0.99 -0.03  0.02  0.99 -0.06 -0.02  0.98]\n",
      " [ 0.05  0.11  1.    0.01  0.07  0.97  0.02  0.12  0.97 -0.04  0.05  0.98\n",
      "   0.03  0.09  1.    0.03  0.13  0.99 -0.03  0.09  1.    0.02  0.09  1.\n",
      "  -0.04  0.06  0.99 -0.05  0.08  0.93  0.03  0.11  0.97  0.04  0.15  0.98\n",
      "  -0.    0.09  0.93  0.04  0.13  0.93 -0.04  0.06  1.   -0.05  0.01  0.98\n",
      "  -0.01  0.07  0.93 -0.    0.04  1.01  0.01  0.1   0.96  0.01  0.16  0.96]\n",
      " [ 0.04 -0.01  0.98  0.05  0.05  0.98  0.03 -0.01  0.98  0.02 -0.    0.97\n",
      "   0.03 -0.05  0.98  0.05  0.01  1.01  0.07  0.01  1.02  0.06  0.    0.94\n",
      "   0.08  0.03  0.98  0.02  0.02  0.98  0.02 -0.    0.99  0.02 -0.01  0.98\n",
      "  -0.01  0.    0.99  0.06  0.01  1.02 -0.02 -0.02  0.96  0.08  0.01  1.\n",
      "  -0.03 -0.01  1.    0.02 -0.02  0.99  0.02 -0.    0.98  0.04  0.02  0.97]\n",
      " [-0.03  0.05  1.    0.02  0.11  0.96 -0.05  0.07  0.97 -0.03  0.07  0.97\n",
      "   0.03  0.1   0.97 -0.    0.07  1.   -0.03  0.07  1.01  0.    0.06  1.03\n",
      "  -0.03  0.07  0.99 -0.03  0.04  1.    0.    0.1   0.99  0.03  0.09  1.04\n",
      "  -0.04  0.1   0.98 -0.07  0.03  0.96 -0.01  0.08  0.94 -0.    0.01  0.96\n",
      "  -0.04  0.06  0.99 -0.07  0.04  0.96 -0.    0.09  0.97 -0.02  0.01  0.97]\n",
      " [ 0.05  0.11  0.99  0.02  0.08  0.98 -0.02  0.1   1.   -0.02  0.08  0.95\n",
      "   0.03  0.1   1.    0.05  0.14  1.01 -0.01  0.08  0.96  0.02  0.08  1.01\n",
      "   0.01  0.11  0.96  0.06  0.15  1.01 -0.04  0.07  0.99 -0.02  0.03  1.\n",
      "   0.01  0.11  0.98 -0.01  0.04  1.    0.02  0.11  0.98 -0.04  0.13  0.94\n",
      "  -0.01  0.07  0.96 -0.06  0.04  0.98  0.02  0.1   0.98 -0.01  0.15  0.95]\n",
      " [-0.07 -0.    0.99 -0.04  0.    0.99 -0.03 -0.    0.98 -0.07 -0.02  0.98\n",
      "  -0.03  0.03  1.   -0.05 -0.    0.98 -0.03 -0.04  0.97 -0.06 -0.02  0.98\n",
      "  -0.05  0.02  0.99 -0.06 -0.03  0.98 -0.06  0.03  1.   -0.05 -0.    0.98\n",
      "  -0.08  0.02  0.99 -0.07  0.01  0.98 -0.04 -0.01  0.98 -0.09  0.02  0.98\n",
      "  -0.05  0.    0.98 -0.04 -0.01  0.98 -0.05 -0.02  0.97 -0.04  0.    0.98]\n",
      " [-0.04  0.07  0.99  0.02  0.12  0.96 -0.06  0.08  1.01 -0.03  0.03  0.97\n",
      "   0.03  0.1   0.96 -0.05  0.05  0.96 -0.03  0.07  0.98 -0.06  0.06  0.92\n",
      "   0.03  0.11  0.97  0.04  0.14  0.99 -0.01  0.07  0.94  0.04  0.05  0.99\n",
      "  -0.02  0.08  0.97 -0.05  0.12  0.94 -0.01  0.08  0.96  0.02  0.14  0.94\n",
      "  -0.02  0.07  1.    0.05  0.14  0.98  0.01  0.08  0.97  0.03  0.1   1.  ]\n",
      " [ 0.06  0.    0.98 -0.02 -0.03  0.98  0.03  0.04  0.97  0.02 -0.    0.98\n",
      "   0.02 -0.01  0.98 -0.03  0.01  0.99  0.07  0.01  1.02 -0.02 -0.01  0.97\n",
      "   0.07  0.01  0.99 -0.03 -0.03  1.    0.02 -0.01  0.98  0.02 -0.01  0.97\n",
      "   0.02 -0.01  0.98  0.1   0.01  1.    0.02 -0.01  0.94 -0.   -0.    1.\n",
      "   0.06 -0.01  0.98  0.03  0.05  0.98  0.02 -0.01  0.99  0.02 -0.01  0.99]\n",
      " [-0.04 -0.04  0.94 -0.05 -0.    1.01  0.07  0.02  0.99  0.04  0.    0.95\n",
      "  -0.03 -0.02  0.98  0.02 -0.02  0.97  0.03  0.01  0.98  0.02  0.01  0.98\n",
      "   0.07 -0.    0.91  0.08  0.02  1.    0.01  0.01  1.01 -0.03  0.01  1.01\n",
      "  -0.02 -0.04  0.92  0.02  0.01  0.98  0.02  0.    0.99  0.04  0.03  0.98\n",
      "   0.07  0.    0.94  0.07  0.02  0.99  0.01  0.    0.96 -0.03  0.02  1.03]\n",
      " [-0.06  0.12  0.98 -0.01  0.08  0.99  0.05  0.14  0.98 -0.01  0.09  0.99\n",
      "   0.02  0.09  1.   -0.02  0.08  0.97  0.06  0.1   0.98  0.01  0.08  0.97\n",
      "  -0.03  0.03  1.   -0.02  0.07  1.    0.02  0.13  0.96  0.01  0.11  0.98\n",
      "  -0.01  0.1   1.   -0.03  0.06  0.96  0.02  0.12  0.95 -0.01  0.13  0.94\n",
      "  -0.01  0.09  0.97  0.05  0.14  0.99 -0.01  0.07  0.94 -0.01  0.12  0.92]\n",
      " [-0.05 -0.    0.99 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.99\n",
      "  -0.06 -0.01  0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.06 -0.    0.98 -0.06 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.06  0.    0.99 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98]\n",
      " [ 0.01  0.09  0.98  0.02  0.11  0.96 -0.03  0.06  1.    0.05  0.12  0.98\n",
      "   0.02  0.15  0.99 -0.04  0.08  1.   -0.    0.1   0.95  0.02  0.11  0.96\n",
      "   0.05  0.14  1.02 -0.04  0.06  1.   -0.02  0.04  1.    0.01  0.11  0.99\n",
      "   0.01  0.06  1.02  0.03  0.12  0.96 -0.05  0.11  0.95 -0.01  0.08  0.96\n",
      "  -0.08  0.07  0.98  0.01  0.09  0.98  0.04  0.15  0.98 -0.01  0.09  0.99]\n",
      " [-0.06 -0.03  0.98 -0.06 -0.01  0.98 -0.03  0.    0.99 -0.04 -0.03  0.98\n",
      "  -0.05  0.    0.98 -0.08  0.02  0.99 -0.05 -0.02  0.98 -0.04  0.    0.98\n",
      "  -0.03  0.01  0.99 -0.04 -0.    0.99 -0.05 -0.    0.98 -0.11  0.02  0.98\n",
      "  -0.05 -0.02  0.98 -0.04  0.01  0.99 -0.08  0.02  0.99 -0.06  0.01  0.99\n",
      "  -0.05 -0.    0.98 -0.04  0.01  0.98 -0.07  0.    0.98 -0.06 -0.02  0.98]\n",
      " [-0.08  0.07  0.98  0.01  0.09  0.98  0.04  0.15  0.98 -0.01  0.09  0.99\n",
      "   0.02  0.09  1.01 -0.02  0.09  0.97  0.01  0.07  1.   -0.01  0.08  0.98\n",
      "  -0.01  0.12  1.   -0.01  0.09  0.95 -0.03  0.05  0.99 -0.    0.04  0.99\n",
      "   0.01  0.08  0.95  0.02  0.12  1.   -0.    0.09  1.02 -0.02  0.03  0.99\n",
      "   0.01  0.1   0.95 -0.05  0.07  0.95 -0.01  0.09  1.02 -0.08  0.09  0.99]\n",
      " [ 0.06  0.02  1.01  0.07  0.02  0.99 -0.02 -0.01  0.96 -0.04 -0.    1.02\n",
      "   0.02  0.    0.98  0.02  0.01  0.98  0.02  0.01  0.98  0.09  0.02  0.99\n",
      "   0.08  0.01  0.95 -0.    0.01  1.01  0.04 -0.01  0.93  0.01  0.05  1.08\n",
      "   0.02 -0.    0.98  0.02  0.01  0.99  0.02 -0.03  0.92  0.04 -0.    0.94\n",
      "  -0.04 -0.    0.99  0.03  0.02  1.02  0.08 -0.    0.91  0.02  0.03  1.  ]\n",
      " [ 0.04  0.13  1.   -0.01  0.07  0.94  0.06  0.08  0.98  0.03  0.12  0.96\n",
      "  -0.06  0.09  0.94 -0.01  0.07  0.95 -0.02  0.1   0.91  0.02  0.1   0.98\n",
      "  -0.    0.15  0.96 -0.    0.08  0.97 -0.04  0.06  0.92  0.03  0.11  0.96\n",
      "  -0.07  0.07  0.96 -0.02  0.1   0.99 -0.01  0.12  1.    0.    0.1   0.95\n",
      "  -0.05  0.09  0.95 -0.04  0.08  0.96 -0.    0.07  0.96 -0.02  0.08  0.95]\n",
      " [ 0.06  0.14  0.99 -0.01  0.11  1.01  0.01  0.17  0.99 -0.04  0.07  0.99\n",
      "   0.04  0.06  1.01 -0.01  0.11  1.   -0.01  0.06  1.03 -0.    0.1   0.97\n",
      "   0.06  0.09  1.   -0.    0.07  0.97 -0.04  0.04  0.99 -0.04  0.05  0.98\n",
      "   0.02  0.08  1.    0.    0.06  0.97 -0.04  0.07  0.99 -0.    0.1   0.95\n",
      "   0.02  0.08  0.97  0.04  0.11  1.02  0.01  0.09  0.95 -0.01  0.04  1.  ]\n",
      " [ 0.02  0.01  0.98  0.02  0.    0.99  0.02 -0.04  0.86 -0.04 -0.01  0.99\n",
      "  -0.04 -0.    0.99  0.08  0.01  0.95  0.01 -0.04  0.9   0.02  0.03  0.98\n",
      "   0.02  0.01  0.98  0.02 -0.01  0.98  0.07  0.    0.94  0.05  0.    0.95\n",
      "  -0.01  0.01  1.01 -0.06 -0.01  1.01  0.02 -0.04  0.88  0.02  0.    0.98\n",
      "   0.02  0.    0.99  0.05 -0.    0.95  0.09  0.02  0.96  0.   -0.    0.96]\n",
      " [-0.03 -0.    1.02  0.02  0.    0.98  0.02  0.01  0.98  0.03  0.01  0.98\n",
      "   0.09  0.02  0.98  0.07  0.01  0.95  0.    0.01  1.01  0.04 -0.01  0.93\n",
      "   0.04  0.07  1.08  0.02 -0.    0.98  0.02  0.    0.99 -0.04 -0.04  0.94\n",
      "  -0.05 -0.    1.01  0.07  0.02  0.99  0.04  0.    0.95 -0.03 -0.02  0.98\n",
      "   0.02 -0.02  0.97  0.03  0.01  0.98  0.02  0.01  0.98  0.07 -0.    0.91]\n",
      " [-0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05  0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98]\n",
      " [ 0.02 -0.01  0.98  0.02 -0.01  0.97  0.02 -0.01  0.98  0.1   0.01  1.\n",
      "   0.02 -0.01  0.94 -0.   -0.    1.    0.06 -0.01  0.98  0.03  0.05  0.98\n",
      "   0.02 -0.01  0.99  0.02 -0.01  0.99 -0.   -0.07  0.98  0.   -0.01  0.96\n",
      "   0.01 -0.    1.    0.06  0.01  1.03  0.08 -0.01  0.98  0.03  0.01  0.97\n",
      "   0.02  0.    0.99  0.02 -0.    0.98  0.05 -0.01  0.98  0.02  0.    1.01]\n",
      " [-0.04  0.01  0.99 -0.06  0.02  0.99 -0.1  -0.    0.98 -0.05  0.    0.99\n",
      "  -0.04  0.02  0.99 -0.07 -0.    0.98 -0.06 -0.02  0.98 -0.03 -0.01  0.98\n",
      "  -0.08 -0.03  0.97 -0.05  0.    0.98 -0.09  0.04  1.   -0.07 -0.01  0.98\n",
      "  -0.04  0.01  0.99 -0.09  0.02  0.99 -0.05 -0.03  0.97 -0.05 -0.    0.98\n",
      "  -0.03  0.01  0.98 -0.05  0.02  0.99 -0.06 -0.02  0.98 -0.01  0.    0.99]\n",
      " [-0.01  0.11  0.96  0.01  0.12  0.99  0.01  0.09  0.96 -0.01  0.09  0.95\n",
      "   0.04  0.1   0.99  0.03  0.08  1.03  0.03  0.1   0.97 -0.01  0.07  1.\n",
      "   0.    0.11  1.02 -0.08  0.03  0.97  0.    0.1   0.96  0.04  0.16  0.98\n",
      "  -0.01  0.09  1.01  0.04  0.16  1.01  0.03  0.11  0.97  0.02  0.05  1.01\n",
      "  -0.01  0.1   1.    0.03  0.1   1.04  0.03  0.11  0.98 -0.07  0.09  0.95]\n",
      " [-0.03 -0.02  0.94  0.06  0.    0.97  0.05 -0.    0.98  0.05 -0.03  0.97\n",
      "   0.03 -0.02  0.99  0.02 -0.    0.99  0.03  0.03  0.97  0.07  0.01  1.01\n",
      "  -0.03 -0.02  0.95  0.04  0.01  1.03  0.05 -0.02  0.98  0.02 -0.06  0.98\n",
      "   0.02  0.01  0.98  0.02 -0.    0.98  0.08  0.03  0.98  0.01 -0.01  0.96\n",
      "   0.01 -0.02  0.94 -0.01 -0.02  0.94  0.09  0.02  0.99  0.02  0.01  0.99]\n",
      " [ 0.02 -0.    0.98  0.04  0.03  0.97  0.03 -0.    0.99  0.02 -0.01  0.94\n",
      "  -0.02 -0.01  0.97 -0.02 -0.03  0.97  0.02  0.02  0.99  0.02 -0.01  0.99\n",
      "   0.02 -0.    0.99 -0.04 -0.05  0.99  0.05  0.    0.95 -0.02 -0.02  0.94\n",
      "   0.02  0.    1.01  0.01 -0.04  0.98  0.02 -0.01  0.99  0.02 -0.01  0.98\n",
      "   0.02 -0.02  0.99  0.05  0.02  0.99  0.01 -0.01  0.95  0.07  0.01  1.01]\n",
      " [ 0.06  0.01  1.03  0.08 -0.01  0.98  0.03  0.01  0.97  0.02  0.    0.99\n",
      "   0.02 -0.    0.98  0.05 -0.01  0.98  0.02  0.    1.01 -0.02 -0.02  0.94\n",
      "   0.   -0.01  0.96  0.07 -0.01  0.97  0.02 -0.01  0.98  0.02 -0.01  0.98\n",
      "   0.02 -0.02  0.98  0.09  0.01  1.01 -0.03 -0.02  0.96  0.05  0.01  1.03\n",
      "  -0.03 -0.03  0.97  0.   -0.03  1.    0.02 -0.01  0.98  0.03 -0.    0.98]\n",
      " [ 0.07  0.03  0.99  0.09  0.02  0.96  0.02  0.01  1.01 -0.05 -0.    1.\n",
      "   0.03 -0.04  0.86  0.02  0.    0.98  0.02  0.01  0.98  0.03 -0.01  0.97\n",
      "  -0.06 -0.01  1.   -0.04 -0.01  1.01  0.08  0.02  0.98  0.1   0.03  0.98\n",
      "   0.05  0.05  1.04  0.02  0.01  0.98  0.02  0.01  0.97 -0.03 -0.05  0.9\n",
      "   0.09  0.02  0.98  0.07  0.01  0.95 -0.04 -0.    0.99  0.03 -0.03  0.9 ]\n",
      " [-0.01  0.07  0.95  0.02  0.13  0.92  0.04  0.12  0.96  0.01  0.03  1.01\n",
      "  -0.    0.1   0.99 -0.02  0.01  0.97 -0.02  0.08  0.97  0.05  0.13  0.98\n",
      "  -0.01  0.08  0.98  0.01  0.11  0.96 -0.04  0.06  0.99 -0.03  0.11  0.95\n",
      "  -0.01  0.12  0.98  0.    0.08  0.96 -0.03  0.06  0.95  0.04  0.1   0.99\n",
      "   0.03  0.08  0.99  0.03  0.1   0.96  0.01  0.09  1.   -0.01  0.08  1.01]\n",
      " [-0.01 -0.01  0.98 -0.05 -0.02  0.97 -0.05 -0.    0.98 -0.06 -0.02  0.98\n",
      "  -0.07  0.    0.98 -0.04  0.02  0.99 -0.02 -0.01  0.99 -0.06 -0.    0.99\n",
      "  -0.07 -0.02  0.98 -0.05  0.02  0.99 -0.07  0.    0.98 -0.07  0.    0.98\n",
      "  -0.04  0.03  1.   -0.05  0.    0.98 -0.01 -0.02  0.98 -0.04  0.02  0.99\n",
      "  -0.04 -0.    0.98 -0.02 -0.    0.99 -0.08 -0.02  0.97 -0.06  0.    0.98]\n",
      " [-0.01  0.07  0.94  0.07  0.09  0.99 -0.05  0.06  0.99 -0.03  0.14  0.94\n",
      "  -0.01  0.07  0.97  0.03  0.16  1.01 -0.03  0.07  1.    0.04  0.07  1.01\n",
      "  -0.01  0.09  0.99 -0.02  0.05  1.03 -0.02  0.07  1.01 -0.01  0.06  1.\n",
      "  -0.01  0.09  0.98  0.01  0.13  0.99 -0.03  0.07  0.96  0.01  0.08  1.01\n",
      "   0.01  0.05  0.98 -0.04  0.06  0.98 -0.    0.08  1.01 -0.01  0.08  1.01]\n",
      " [-0.07  0.03  0.99 -0.05  0.01  0.99 -0.06  0.01  0.98 -0.02  0.    0.99\n",
      "  -0.04 -0.01  0.98 -0.07 -0.01  0.98 -0.07  0.03  0.99 -0.06  0.    0.99\n",
      "  -0.03 -0.01  0.98 -0.07  0.01  0.99 -0.05 -0.01  0.98 -0.04 -0.02  0.98\n",
      "  -0.09 -0.02  0.97 -0.05 -0.    0.99 -0.1   0.03  0.99 -0.03  0.01  0.99\n",
      "  -0.07  0.    0.98 -0.08 -0.01  0.98 -0.06  0.02  1.   -0.05 -0.    0.99]\n",
      " [-0.05  0.    0.99 -0.05 -0.01  0.98 -0.03 -0.01  0.98 -0.07  0.01  0.99\n",
      "  -0.05  0.02  0.99 -0.1   0.    0.98 -0.05  0.    0.98 -0.04  0.04  1.\n",
      "  -0.04  0.01  0.99 -0.04  0.01  0.99 -0.08 -0.01  0.98 -0.08 -0.03  0.97\n",
      "  -0.05  0.    0.98 -0.09  0.03  0.98 -0.07  0.01  0.98 -0.07 -0.01  0.98\n",
      "  -0.06  0.03  0.99 -0.06  0.01  0.99 -0.05  0.01  0.98 -0.03 -0.02  0.98]\n",
      " [-0.07  0.03  0.99 -0.06  0.    0.99 -0.03 -0.01  0.98 -0.07  0.01  0.99\n",
      "  -0.05 -0.01  0.98 -0.04 -0.02  0.98 -0.09 -0.02  0.97 -0.05 -0.    0.99\n",
      "  -0.1   0.03  0.99 -0.03  0.01  0.99 -0.07  0.    0.98 -0.08 -0.01  0.98\n",
      "  -0.06  0.02  1.   -0.05 -0.    0.99 -0.08  0.01  0.98 -0.04  0.    0.98\n",
      "  -0.05  0.02  0.99 -0.06  0.01  0.98 -0.05  0.    0.98 -0.05 -0.01  0.98]\n",
      " [-0.   -0.02  0.93 -0.02 -0.01  0.97  0.06 -0.02  0.98  0.03  0.03  0.97\n",
      "   0.02  0.    0.98  0.02 -0.    0.98  0.04 -0.03  0.97 -0.01 -0.02  0.95\n",
      "  -0.   -0.02  0.93  0.08  0.01  1.02  0.04 -0.03  0.97  0.02  0.01  0.97\n",
      "   0.02 -0.    0.97  0.03  0.02  0.98  0.09  0.01  1.    0.08  0.01  1.02\n",
      "  -0.01 -0.02  0.94  0.02 -0.02  0.97  0.05  0.04  0.97  0.02  0.    0.98]\n",
      " [-0.01  0.08  0.99 -0.04  0.04  0.98 -0.04  0.07  0.98 -0.08  0.03  0.94\n",
      "   0.01  0.1   0.96  0.04  0.16  0.98 -0.01  0.09  1.01  0.03  0.13  0.93\n",
      "  -0.01  0.09  0.99 -0.03  0.01  0.99 -0.01  0.1   0.99 -0.01  0.02  0.99\n",
      "  -0.04  0.07  0.98 -0.05  0.12  0.95  0.    0.08  0.96  0.04  0.13  0.99\n",
      "   0.03  0.11  0.97  0.05  0.09  1.   -0.    0.06  0.97 -0.02  0.1   1.  ]\n",
      " [ 0.03  0.03  0.99  0.1   0.02  0.97  0.03 -0.    0.95  0.05  0.02  1.\n",
      "   0.1   0.02  0.96  0.04 -0.04  0.87  0.02  0.01  0.98  0.02  0.01  0.98\n",
      "  -0.    0.04  1.04  0.05  0.03  1.02  0.06  0.01  0.95 -0.02 -0.    0.97\n",
      "  -0.04 -0.02  0.98  0.03  0.01  0.99  0.02  0.    0.97  0.02  0.    0.98\n",
      "   0.04  0.02  1.01 -0.04 -0.    1.    0.02 -0.01  0.98 -0.04 -0.01  0.97]\n",
      " [ 0.01 -0.03  0.97 -0.03 -0.02  0.95 -0.01 -0.02  0.94  0.08  0.01  1.02\n",
      "   0.07 -0.01  0.97  0.03  0.01  0.98  0.02  0.    0.99  0.01 -0.03  1.\n",
      "   0.09  0.01  1.01  0.08  0.01  1.02 -0.01 -0.02  0.94  0.07 -0.    0.99\n",
      "   0.02  0.05  0.99  0.02 -0.01  0.99  0.02 -0.    0.98  0.02  0.05  0.99\n",
      "   0.03  0.    1.    0.07  0.01  0.99 -0.01 -0.02  0.94 -0.03 -0.    0.98]\n",
      " [ 0.06  0.02  1.   -0.05 -0.01  0.99  0.06  0.06  1.06  0.03  0.    1.\n",
      "   0.02  0.01  0.99  0.02  0.    0.97 -0.02 -0.01  0.99  0.09  0.02  0.97\n",
      "  -0.04 -0.    0.99  0.02 -0.01  0.94  0.06  0.07  1.08  0.02  0.    0.99\n",
      "   0.02  0.01  0.97 -0.03 -0.04  0.94  0.06  0.03  1.01 -0.03 -0.    0.98\n",
      "  -0.03  0.    1.01 -0.03 -0.02  0.91  0.04  0.03  1.01  0.02  0.01  0.98]\n",
      " [-0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [ 0.08  0.01  0.95  0.01 -0.    0.95  0.01  0.01  1.02 -0.02  0.03  1.07\n",
      "   0.03  0.    0.99  0.03  0.01  0.98  0.02  0.01  0.99  0.1   0.02  0.97\n",
      "   0.03  0.    0.95  0.05  0.02  1.   -0.   -0.02  0.94  0.01  0.05  1.09\n",
      "   0.02 -0.    0.99  0.02  0.01  0.99 -0.01 -0.03  0.94  0.    0.01  1.02\n",
      "   0.08  0.01  0.96  0.03  0.    0.95 -0.05 -0.    1.02  0.04  0.02  1.01]\n",
      " [-0.08  0.02  0.99 -0.06  0.02  0.98 -0.07 -0.01  0.98 -0.09 -0.01  0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.04  0.02  0.99 -0.07  0.01  0.98\n",
      "  -0.07  0.    0.98 -0.01  0.    0.99 -0.05  0.    0.98 -0.05 -0.05  0.96\n",
      "  -0.05  0.03  0.98 -0.04  0.01  0.98 -0.04  0.01  0.99 -0.08  0.02  0.99\n",
      "  -0.06  0.    0.98 -0.05  0.03  0.99 -0.06 -0.02  0.98 -0.06  0.02  0.99]\n",
      " [-0.07  0.06  0.96 -0.    0.09  0.98 -0.04  0.05  0.91 -0.03  0.07  0.98\n",
      "   0.    0.13  0.96  0.    0.12  0.98 -0.02  0.05  0.97 -0.02  0.07  1.\n",
      "   0.01  0.06  1.   -0.04  0.05  0.95 -0.04  0.07  0.99 -0.02  0.09  0.95\n",
      "  -0.    0.1   0.94  0.04  0.14  0.94 -0.01  0.08  1.    0.02  0.09  1.01\n",
      "  -0.01  0.08  0.93  0.05  0.09  1.01 -0.05  0.06  0.99  0.05  0.16  0.98]\n",
      " [-0.08 -0.02  0.97 -0.06  0.    0.98 -0.03 -0.02  0.98 -0.06  0.02  0.99\n",
      "  -0.07 -0.01  0.98 -0.02 -0.02  0.98 -0.06 -0.    0.99 -0.06  0.01  0.99\n",
      "  -0.08  0.    0.99 -0.05  0.02  0.99 -0.05 -0.02  0.98 -0.05  0.04  1.\n",
      "  -0.05  0.    0.99 -0.02 -0.03  0.97 -0.04  0.02  0.99 -0.05 -0.01  0.98\n",
      "  -0.07  0.03  0.99 -0.07 -0.03  0.97 -0.05  0.    0.98 -0.03  0.    0.98]\n",
      " [-0.07  0.    0.98 -0.08 -0.02  0.98 -0.02 -0.01  0.98 -0.05 -0.    0.98\n",
      "  -0.09  0.02  0.98 -0.07 -0.02  0.98 -0.05 -0.01  0.98 -0.03  0.03  0.99\n",
      "  -0.06  0.01  0.99 -0.05  0.03  0.98 -0.07 -0.02  0.98 -0.04  0.02  0.99\n",
      "  -0.04 -0.01  0.98 -0.01  0.02  0.99 -0.05  0.    0.98 -0.04 -0.03  0.97\n",
      "  -0.04 -0.01  0.98 -0.04 -0.01  0.98 -0.07  0.02  0.99 -0.05  0.04  1.  ]\n",
      " [ 0.02  0.15  0.98 -0.02  0.08  0.96 -0.05  0.07  0.96 -0.03  0.1   0.98\n",
      "  -0.03  0.05  0.99 -0.03  0.05  0.99 -0.02  0.1   0.95 -0.02  0.09  0.93\n",
      "  -0.04  0.08  1.   -0.01  0.1   0.95 -0.01  0.07  0.93 -0.    0.13  0.93\n",
      "   0.02  0.11  0.96  0.03  0.09  1.01 -0.01  0.08  0.92  0.06  0.07  0.97\n",
      "   0.04  0.12  0.96 -0.06  0.02  0.98 -0.    0.07  0.95 -0.04  0.    0.95]\n",
      " [ 0.02  0.03  0.98  0.02  0.01  0.98  0.02 -0.01  0.98  0.07  0.    0.94\n",
      "   0.05  0.    0.95 -0.01  0.01  1.01 -0.06 -0.01  1.01  0.02 -0.04  0.88\n",
      "   0.02  0.    0.98  0.02  0.    0.99  0.05 -0.    0.95  0.09  0.02  0.96\n",
      "   0.   -0.    0.96 -0.04 -0.    1.    0.09  0.02  0.95  0.02  0.01  0.98\n",
      "   0.02  0.    0.99  0.02  0.    0.98 -0.01  0.04  1.1   0.09  0.02  0.97]\n",
      " [ 0.04  0.13  1.03 -0.03  0.07  0.99 -0.07  0.05  0.96  0.    0.08  0.96\n",
      "   0.03  0.14  0.96  0.03  0.12  0.96  0.02  0.06  1.01 -0.02  0.09  0.98\n",
      "   0.01  0.13  0.99 -0.01  0.09  0.96 -0.06  0.05  0.99 -0.04  0.06  0.96\n",
      "  -0.    0.07  0.96 -0.04  0.04  0.97 -0.03  0.09  0.95 -0.01  0.13  0.96\n",
      "   0.03  0.11  0.97  0.03  0.15  0.98  0.    0.09  0.94  0.07  0.13  0.95]\n",
      " [ 0.02  0.01  0.99  0.06  0.06  1.03 -0.06 -0.01  0.98  0.08  0.01  0.96\n",
      "  -0.02 -0.    0.98  0.07 -0.01  0.92  0.03  0.03  1.01  0.02  0.01  0.98\n",
      "   0.02  0.    0.99  0.02 -0.04  0.86 -0.04 -0.01  0.99 -0.04 -0.    0.99\n",
      "   0.08  0.01  0.95  0.01 -0.04  0.9   0.02  0.03  0.98  0.02  0.01  0.98\n",
      "   0.02 -0.01  0.98  0.07  0.    0.94  0.05  0.    0.95 -0.01  0.01  1.01]\n",
      " [-0.04 -0.01  0.99 -0.05 -0.02  0.98 -0.08  0.01  0.98 -0.1  -0.01  0.97\n",
      "  -0.05  0.    0.98 -0.04  0.04  1.   -0.05 -0.02  0.98 -0.06  0.01  0.99\n",
      "  -0.02  0.02  0.99 -0.05 -0.01  0.97 -0.05  0.    0.98 -0.06 -0.03  0.98\n",
      "  -0.06 -0.01  0.98 -0.03  0.    0.99 -0.04 -0.03  0.98 -0.05  0.    0.98\n",
      "  -0.08  0.02  0.99 -0.05 -0.02  0.98 -0.04  0.    0.98 -0.03  0.01  0.99]\n",
      " [-0.01  0.08  0.96  0.02  0.08  1.01  0.01  0.11  0.96  0.06  0.15  1.01\n",
      "  -0.04  0.07  0.99 -0.02  0.03  1.    0.01  0.11  0.98 -0.01  0.04  1.\n",
      "   0.02  0.11  0.98 -0.04  0.13  0.94 -0.01  0.07  0.96 -0.06  0.04  0.98\n",
      "   0.02  0.1   0.98 -0.01  0.15  0.95  0.03  0.09  0.98  0.01  0.08  1.\n",
      "  -0.    0.12  1.    0.04  0.13  0.98  0.01  0.09  0.98  0.02  0.11  0.96]\n",
      " [ 0.01  0.12  0.99  0.01  0.09  0.96 -0.01  0.09  0.95  0.04  0.1   0.99\n",
      "   0.03  0.08  1.03  0.03  0.1   0.97 -0.01  0.07  1.    0.    0.11  1.02\n",
      "  -0.08  0.03  0.97  0.    0.1   0.96  0.04  0.16  0.98 -0.01  0.09  1.01\n",
      "   0.04  0.16  1.01  0.03  0.11  0.97  0.02  0.05  1.01 -0.01  0.1   1.\n",
      "   0.03  0.1   1.04  0.03  0.11  0.98 -0.07  0.09  0.95  0.    0.09  0.97]\n",
      " [-0.07 -0.03  0.97 -0.05  0.    0.98 -0.11  0.    0.98 -0.06 -0.02  0.98\n",
      "  -0.07  0.01  0.98 -0.09 -0.    0.98 -0.03 -0.02  0.98 -0.05 -0.    0.98\n",
      "  -0.08  0.02  0.98 -0.07 -0.01  0.98 -0.04  0.01  0.99 -0.06  0.03  0.99\n",
      "  -0.05  0.    0.98 -0.06 -0.    0.99 -0.05 -0.02  0.98 -0.07  0.01  0.98\n",
      "  -0.03  0.    0.98 -0.03  0.03  0.99 -0.05 -0.    0.99 -0.03 -0.04  0.97]\n",
      " [-0.03 -0.02  0.97  0.05  0.05  0.98  0.02  0.    0.98  0.02 -0.    0.98\n",
      "  -0.01 -0.02  1.   -0.03 -0.01  0.97  0.07  0.01  1.03  0.05 -0.    0.96\n",
      "  -0.04 -0.01  0.98  0.02 -0.03  0.98  0.02  0.    0.99  0.02 -0.    0.99\n",
      "  -0.03 -0.02  0.97  0.04  0.01  1.02 -0.02 -0.01  0.97  0.09  0.01  1.01\n",
      "   0.06 -0.03  0.97  0.03  0.01  0.98  0.02  0.    0.99  0.02 -0.02  1.  ]\n",
      " [-0.    0.06  1.03  0.03  0.12  0.97 -0.05  0.09  0.95 -0.    0.11  1.02\n",
      "  -0.07  0.1   0.99 -0.04  0.07  0.99  0.06  0.12  1.   -0.01  0.1   1.\n",
      "   0.02  0.18  1.01 -0.02  0.08  0.97  0.06  0.09  1.01 -0.01  0.09  0.99\n",
      "  -0.04  0.02  0.99  0.03  0.12  0.97  0.04  0.08  1.01  0.02  0.08  0.98\n",
      "  -0.01  0.07  0.96 -0.02  0.07  0.95  0.04  0.1   0.99  0.02  0.08  0.99]\n",
      " [-0.04  0.06  0.98  0.04  0.09  1.01 -0.01  0.09  0.98 -0.02 -0.    0.95\n",
      "  -0.05  0.06  0.99 -0.03  0.13  0.95 -0.    0.07  0.96 -0.    0.07  1.\n",
      "   0.02  0.11  0.95  0.06  0.1   1.    0.02  0.07  0.99  0.01  0.13  0.99\n",
      "  -0.02  0.08  0.96 -0.02  0.06  1.   -0.03  0.04  0.96 -0.01  0.08  0.99\n",
      "  -0.04  0.04  0.98 -0.04  0.07  0.98 -0.08  0.03  0.94  0.01  0.1   0.96]\n",
      " [ 0.04 -0.    0.96  0.07  0.01  1.01  0.09  0.02  1.01  0.06  0.04  0.97\n",
      "   0.02 -0.01  0.98  0.02 -0.    0.98  0.01 -0.03  0.99 -0.03 -0.02  0.95\n",
      "   0.08  0.01  1.   -0.01 -0.02  0.94  0.08  0.02  0.99  0.04  0.01  0.97\n",
      "   0.02 -0.    0.99  0.02 -0.01  0.98 -0.01  0.03  0.99  0.05  0.01  1.01\n",
      "   0.04  0.01  1.03  0.07  0.01  0.94 -0.04 -0.01  0.99  0.02  0.01  0.98]\n",
      " [-0.02  0.09  1.   -0.03  0.07  0.96  0.02  0.12  0.96  0.04  0.16  0.97\n",
      "   0.03  0.11  0.98 -0.01  0.03  1.   -0.    0.09  0.93  0.04  0.15  0.96\n",
      "  -0.01  0.09  0.97  0.04  0.08  1.01 -0.    0.1   0.99  0.05  0.1   1.\n",
      "   0.01  0.09  0.98 -0.03  0.03  0.99 -0.01  0.11  0.99 -0.03  0.07  0.97\n",
      "   0.01  0.1   0.99 -0.03  0.05  0.99  0.01  0.07  0.97 -0.03  0.05  0.97]\n",
      " [ 0.09  0.02  0.99  0.08  0.01  0.95 -0.    0.01  1.01  0.04 -0.01  0.93\n",
      "   0.01  0.05  1.08  0.02 -0.    0.98  0.02  0.01  0.99  0.02 -0.03  0.92\n",
      "   0.04 -0.    0.94 -0.04 -0.    0.99  0.03  0.02  1.02  0.08 -0.    0.91\n",
      "   0.02  0.03  1.    0.02  0.    0.99  0.02  0.01  0.98  0.03  0.04  1.08\n",
      "   0.08  0.01  0.95  0.05  0.02  1.    0.02  0.02  1.03 -0.03 -0.    1.02]\n",
      " [ 0.03  0.07  1.01 -0.01  0.09  0.98 -0.    0.09  0.96 -0.03  0.06  1.\n",
      "  -0.02  0.05  0.99  0.    0.06  0.98 -0.01  0.1   1.    0.01  0.11  0.95\n",
      "  -0.04  0.06  0.99 -0.04  0.05  0.96 -0.03  0.07  0.99 -0.04  0.04  0.97\n",
      "  -0.04  0.07  0.96 -0.06  0.06  0.91  0.03  0.11  0.96  0.05  0.11  1.01\n",
      "  -0.01  0.07  0.95  0.02  0.13  0.92  0.04  0.12  0.96  0.01  0.03  1.01]\n",
      " [-0.06 -0.01  0.97 -0.05  0.03  0.98 -0.05  0.01  0.97 -0.05 -0.01  0.97\n",
      "  -0.06  0.03  0.98 -0.05  0.02  0.98 -0.06 -0.02  0.97 -0.09 -0.02  0.97\n",
      "  -0.05  0.    0.98 -0.1   0.02  0.97 -0.07  0.01  0.98 -0.04  0.    0.98\n",
      "  -0.09  0.01  0.98 -0.03  0.    0.98 -0.05  0.    0.98 -0.07 -0.    0.98\n",
      "  -0.05 -0.02  0.97 -0.06  0.02  0.98 -0.09 -0.01  0.97 -0.05 -0.    0.98]\n",
      " [-0.03 -0.01  0.96 -0.06 -0.01  0.96 -0.06  0.03  0.97 -0.09  0.    0.96\n",
      "  -0.05  0.    0.97 -0.09  0.02  0.98 -0.06  0.01  0.97 -0.06 -0.01  0.97\n",
      "  -0.04  0.    0.98 -0.06  0.03  0.98 -0.06  0.    0.98 -0.01 -0.03  0.97\n",
      "  -0.05  0.02  0.98 -0.07  0.    0.97 -0.02 -0.01  0.97 -0.05 -0.01  0.97\n",
      "  -0.05 -0.    0.97 -0.05 -0.02  0.97 -0.06  0.02  0.98 -0.06 -0.01  0.97]\n",
      " [ 0.02  0.06  1.01 -0.02  0.09  0.98  0.01  0.13  0.99 -0.01  0.09  0.96\n",
      "  -0.06  0.05  0.99 -0.04  0.06  0.96 -0.    0.07  0.96 -0.04  0.04  0.97\n",
      "  -0.03  0.09  0.95 -0.01  0.13  0.96  0.03  0.11  0.97  0.03  0.15  0.98\n",
      "   0.    0.09  0.94  0.07  0.13  0.95 -0.04  0.06  0.99 -0.05  0.01  0.98\n",
      "  -0.01  0.07  0.93  0.07  0.1   0.98 -0.05  0.06  0.99 -0.07  0.04  0.97]\n",
      " [ 0.08  0.01  0.91  0.04  0.03  1.01  0.02  0.01  0.98  0.03  0.01  0.99\n",
      "   0.1   0.01  0.92  0.07  0.01  0.95  0.03  0.01  1.01 -0.05 -0.    1.\n",
      "   0.04 -0.04  0.87  0.02  0.    0.98  0.02  0.01  0.98  0.04  0.05  1.02\n",
      "   0.09  0.03  0.99  0.06  0.02  1.    0.07  0.01  0.95  0.07 -0.    0.93\n",
      "   0.02 -0.03  0.93  0.02  0.01  0.98  0.02  0.    0.99 -0.03  0.04  1.09]\n",
      " [-0.04  0.08  0.91  0.02  0.09  0.99  0.04  0.08  1.01 -0.01  0.07  0.96\n",
      "  -0.03  0.01  0.96  0.01  0.1   0.96 -0.04  0.13  0.94 -0.    0.08  0.97\n",
      "  -0.04  0.05  0.91  0.04  0.11  0.96 -0.02  0.12  0.95  0.01  0.08  0.97\n",
      "  -0.03  0.04  0.98  0.01  0.1   0.99  0.05  0.1   1.    0.03  0.11  1.\n",
      "  -0.01  0.11  1.    0.    0.12  1.   -0.02  0.07  1.01 -0.03  0.03  0.96]\n",
      " [-0.05 -0.01  0.98 -0.05 -0.02  0.98 -0.06  0.04  0.99 -0.06  0.    0.99\n",
      "  -0.01  0.02  0.99 -0.05 -0.02  0.98 -0.06 -0.01  0.98 -0.08  0.01  0.99\n",
      "  -0.07 -0.03  0.97 -0.05  0.    0.98 -0.09  0.03  0.98 -0.04 -0.01  0.98\n",
      "  -0.07  0.    0.98 -0.08  0.03  0.99 -0.06  0.    0.99 -0.06  0.    0.99\n",
      "  -0.03  0.01  0.99 -0.04  0.01  0.99 -0.08  0.    0.98 -0.08  0.03  0.99]\n",
      " [ 0.02  0.01  0.98  0.02  0.01  0.98 -0.05 -0.01  0.99  0.08  0.01  0.95\n",
      "   0.08  0.02  0.98 -0.03 -0.01  0.97 -0.03 -0.    1.02  0.02  0.    0.98\n",
      "   0.02  0.01  0.98  0.03  0.01  0.98  0.09  0.02  0.98  0.07  0.01  0.95\n",
      "   0.    0.01  1.01  0.04 -0.01  0.93  0.04  0.07  1.08  0.02 -0.    0.98\n",
      "   0.02  0.    0.99 -0.04 -0.04  0.94 -0.05 -0.    1.01  0.07  0.02  0.99]\n",
      " [-0.04 -0.    1.    0.02 -0.01  0.98 -0.04 -0.01  0.97  0.04 -0.    0.96\n",
      "   0.07  0.01  1.01  0.09  0.02  1.01  0.06  0.04  0.97  0.02 -0.01  0.98\n",
      "   0.02 -0.    0.98  0.01 -0.03  0.99 -0.03 -0.02  0.95  0.08  0.01  1.\n",
      "  -0.01 -0.02  0.94  0.08  0.02  0.99  0.04  0.01  0.97  0.02 -0.    0.99\n",
      "   0.02 -0.01  0.98 -0.01  0.03  0.99  0.05  0.01  1.01  0.04  0.01  1.03]\n",
      " [ 0.1   0.01  1.   -0.02 -0.01  0.97  0.07  0.01  0.99  0.04  0.01  0.99\n",
      "  -0.01 -0.05  0.99  0.03 -0.    0.98  0.02 -0.    0.99  0.06  0.04  0.96\n",
      "   0.03  0.    1.    0.07  0.01  1.02  0.06  0.    0.98 -0.01  0.01  0.98\n",
      "   0.02 -0.04  0.99  0.02 -0.01  0.99  0.02 -0.    0.98 -0.06 -0.01  0.97\n",
      "  -0.04 -0.02  1.02 -0.02 -0.02  0.96  0.09  0.01  1.01  0.01 -0.05  0.98]\n",
      " [-0.03  0.06  1.   -0.01  0.11  0.95 -0.01  0.13  0.98 -0.04  0.08  1.\n",
      "   0.01  0.11  0.95  0.03  0.09  1.    0.02  0.08  1.04 -0.03  0.07  0.99\n",
      "  -0.03  0.11  0.94 -0.    0.1   1.03 -0.08  0.04  0.95 -0.04  0.06  0.99\n",
      "   0.05  0.16  0.99 -0.01  0.1   1.    0.02  0.17  1.    0.04  0.11  0.96\n",
      "   0.02  0.04  1.01 -0.01  0.11  1.   -0.06  0.04  1.    0.03  0.12  0.97]\n",
      " [-0.06  0.05  0.95 -0.01  0.1   0.99 -0.08  0.09  0.99 -0.04  0.06  1.\n",
      "   0.06  0.13  0.99 -0.01  0.11  1.01  0.02  0.09  1.04 -0.03  0.08  0.98\n",
      "   0.03  0.07  1.01 -0.01  0.08  0.98 -0.04  0.07  0.98 -0.    0.09  0.96\n",
      "  -0.03  0.1   0.95 -0.    0.14  0.98 -0.    0.11  1.   -0.01  0.08  0.95\n",
      "   0.03  0.1   0.99  0.03  0.08  1.01 -0.03  0.06  1.   -0.03  0.04  1.  ]\n",
      " [-0.02 -0.03  0.98  0.03  0.03  0.97  0.02  0.    0.98  0.02 -0.    0.99\n",
      "   0.04  0.01  0.99  0.01 -0.    1.   -0.02 -0.01  0.98  0.04 -0.    0.98\n",
      "  -0.04 -0.01  0.99  0.02 -0.02  0.99  0.02 -0.01  0.98  0.03 -0.    1.\n",
      "  -0.04 -0.02  0.96 -0.02 -0.02  0.94  0.06  0.01  1.02 -0.04 -0.01  0.97\n",
      "   0.04 -0.04  0.97  0.02 -0.02  0.99  0.02 -0.    0.98  0.04  0.05  0.98]\n",
      " [-0.    0.07  0.96  0.02  0.09  0.99  0.02  0.11  0.96 -0.04  0.05  0.98\n",
      "  -0.02  0.1   0.98  0.01  0.15  0.95  0.02  0.12  0.96 -0.02  0.1   0.96\n",
      "  -0.01  0.07  0.97 -0.    0.07  0.96 -0.    0.09  1.   -0.03  0.09  0.95\n",
      "  -0.04  0.1   0.95  0.02  0.12  0.99 -0.01  0.1   0.95 -0.02  0.08  0.94\n",
      "  -0.01  0.13  0.95  0.01  0.09  0.99  0.04  0.13  1.   -0.01  0.07  0.94]\n",
      " [-0.03 -0.01  0.98 -0.07  0.01  0.99 -0.05 -0.01  0.98 -0.04 -0.02  0.98\n",
      "  -0.09 -0.02  0.97 -0.05 -0.    0.99 -0.1   0.03  0.99 -0.03  0.01  0.99\n",
      "  -0.07  0.    0.98 -0.08 -0.01  0.98 -0.06  0.02  1.   -0.05 -0.    0.99\n",
      "  -0.08  0.01  0.98 -0.04  0.    0.98 -0.05  0.02  0.99 -0.06  0.01  0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.01  0.98 -0.07 -0.    0.98 -0.06 -0.01  0.98]\n",
      " [ 0.02  0.02  0.98  0.02  0.01  0.98  0.03  0.01  1.    0.1   0.02  0.94\n",
      "  -0.    0.01  1.02  0.05  0.    0.95  0.08  0.03  1.   -0.02 -0.03  0.93\n",
      "   0.02  0.01  0.98  0.02  0.    0.99  0.06  0.02  0.97 -0.04 -0.    1.01\n",
      "   0.02 -0.    0.95 -0.03 -0.    0.99  0.07 -0.01  0.9   0.04  0.04  1.02\n",
      "   0.02  0.01  0.98  0.02  0.01  0.98  0.08  0.02  0.99  0.08  0.02  1.  ]\n",
      " [-0.05  0.09  0.95 -0.03  0.09  0.97 -0.03  0.05  0.98  0.02  0.11  0.99\n",
      "  -0.03  0.1   0.95 -0.02  0.13  0.97  0.01  0.12  0.99  0.02  0.14  0.97\n",
      "   0.02  0.11  0.95  0.03  0.15  0.94  0.02  0.1   0.98  0.03  0.11  1.\n",
      "  -0.01  0.08  0.93  0.05  0.1   1.02 -0.04  0.07  0.99 -0.06  0.09  0.94\n",
      "  -0.    0.07  0.96 -0.03  0.1   0.93  0.04  0.11  0.96  0.06  0.11  0.99]\n",
      " [-0.05  0.05  1.   -0.04  0.    0.98 -0.07  0.01  0.98 -0.08  0.01  0.98\n",
      "  -0.06  0.02  0.99 -0.05  0.    0.98 -0.08  0.02  0.99 -0.06  0.02  0.98\n",
      "  -0.07 -0.01  0.98 -0.09 -0.01  0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.04  0.02  0.99 -0.07  0.01  0.98 -0.07  0.    0.98 -0.01  0.    0.99\n",
      "  -0.05  0.    0.98 -0.05 -0.05  0.96 -0.05  0.03  0.98 -0.04  0.01  0.98]\n",
      " [ 0.   -0.01  0.96  0.01 -0.    1.    0.06  0.01  1.03  0.08 -0.01  0.98\n",
      "   0.03  0.01  0.97  0.02  0.    0.99  0.02 -0.    0.98  0.05 -0.01  0.98\n",
      "   0.02  0.    1.01 -0.02 -0.02  0.94  0.   -0.01  0.96  0.07 -0.01  0.97\n",
      "   0.02 -0.01  0.98  0.02 -0.01  0.98  0.02 -0.02  0.98  0.09  0.01  1.01\n",
      "  -0.03 -0.02  0.96  0.05  0.01  1.03 -0.03 -0.03  0.97  0.   -0.03  1.  ]\n",
      " [-0.    0.04  0.99  0.01  0.08  0.95  0.02  0.12  1.   -0.    0.09  1.02\n",
      "  -0.02  0.03  0.99  0.01  0.1   0.95 -0.05  0.07  0.95 -0.01  0.09  1.02\n",
      "  -0.08  0.09  0.99 -0.03  0.07  0.99  0.06  0.14  0.99 -0.01  0.11  1.01\n",
      "   0.01  0.17  0.99 -0.04  0.07  0.99  0.04  0.06  1.01 -0.01  0.11  1.\n",
      "  -0.01  0.06  1.03 -0.    0.1   0.97  0.06  0.09  1.   -0.    0.07  0.97]\n",
      " [-0.04 -0.02  0.98 -0.1  -0.01  0.97 -0.06 -0.    0.99 -0.02  0.04  1.\n",
      "  -0.04  0.02  0.99 -0.05 -0.01  0.98 -0.02  0.    0.99 -0.08 -0.01  0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.02  0.98 -0.04  0.01  0.99 -0.06  0.02  0.99\n",
      "  -0.08 -0.02  0.98 -0.06 -0.    0.99 -0.05  0.01  0.98 -0.06  0.02  0.99\n",
      "  -0.04 -0.01  0.98 -0.08 -0.01  0.98 -0.01 -0.02  0.98 -0.05 -0.    0.98]\n",
      " [ 0.06  0.02  1.   -0.02  0.02  1.05  0.05  0.04  1.02  0.02  0.    0.99\n",
      "   0.02  0.01  0.98  0.01 -0.02  0.93  0.02 -0.01  0.94  0.06  0.01  0.95\n",
      "   0.07  0.03  1.01 -0.01 -0.04  0.9   0.02  0.02  0.98  0.02  0.01  0.98\n",
      "   0.03  0.02  1.   -0.06 -0.01  0.99 -0.04 -0.    0.99  0.08  0.02  0.98\n",
      "   0.05  0.03  1.02 -0.01 -0.03  0.95  0.02  0.02  0.98  0.02  0.01  0.98]\n",
      " [-0.05 -0.    0.98  0.    0.01  1.   -0.07 -0.02  0.98 -0.04 -0.01  0.98\n",
      "  -0.05  0.03  0.99 -0.06  0.01  0.99 -0.05 -0.    0.98 -0.08  0.    0.99\n",
      "  -0.04 -0.01  0.98 -0.04  0.02  0.99 -0.04 -0.03  0.98 -0.05 -0.    0.98\n",
      "  -0.09  0.03  1.   -0.06  0.02  0.99 -0.04  0.01  0.99 -0.05 -0.02  0.97\n",
      "  -0.06 -0.04  0.97 -0.06 -0.    0.98 -0.09  0.03  0.98 -0.03  0.01  0.99]\n",
      " [ 0.05  0.03  1.02  0.06  0.01  0.95 -0.02 -0.    0.97 -0.04 -0.02  0.98\n",
      "   0.03  0.01  0.99  0.02  0.    0.97  0.02  0.    0.98  0.04  0.02  1.01\n",
      "  -0.04 -0.    1.    0.02 -0.01  0.98 -0.04 -0.01  0.97  0.04 -0.    0.96\n",
      "   0.07  0.01  1.01  0.09  0.02  1.01  0.06  0.04  0.97  0.02 -0.01  0.98\n",
      "   0.02 -0.    0.98  0.01 -0.03  0.99 -0.03 -0.02  0.95  0.08  0.01  1.  ]\n",
      " [ 0.02  0.12  0.97 -0.    0.08  1.01 -0.03  0.02  0.98  0.02  0.1   0.97\n",
      "  -0.03  0.08  0.95 -0.03  0.07  0.98 -0.05  0.07  0.92 -0.    0.08  0.99\n",
      "   0.03  0.11  1.01 -0.01  0.07  0.93 -0.    0.03  0.99  0.01  0.1   0.98\n",
      "   0.03  0.16  0.97 -0.01  0.1   1.01 -0.07  0.11  0.99 -0.01  0.08  0.99\n",
      "   0.06  0.12  1.    0.    0.07  0.97 -0.01  0.05  1.03 -0.01  0.08  1.  ]\n",
      " [-0.05  0.08  0.95 -0.01  0.12  0.99 -0.01  0.06  1.02 -0.03  0.06  0.96\n",
      "   0.04  0.11  0.97  0.04  0.16  0.98  0.02  0.1   0.96  0.03  0.13  0.99\n",
      "   0.    0.1   0.95  0.05  0.12  1.04  0.03  0.12  0.97 -0.06  0.06  0.95\n",
      "  -0.    0.11  1.   -0.06  0.09  0.96 -0.    0.09  0.97  0.05  0.13  0.98\n",
      "  -0.01  0.08  0.98  0.04  0.14  1.03  0.03  0.12  0.97  0.02  0.06  1.01]\n",
      " [ 0.05  0.05  1.06  0.03  0.    0.99  0.02  0.01  0.98  0.02 -0.    0.97\n",
      "  -0.02  0.02  1.03  0.05  0.    0.95  0.07  0.02  0.98  0.1   0.02  0.96\n",
      "   0.02 -0.04  0.88  0.02  0.    0.98  0.02  0.01  0.99  0.06  0.06  1.03\n",
      "  -0.06 -0.01  0.98  0.08  0.01  0.96 -0.02 -0.    0.98  0.07 -0.01  0.92\n",
      "   0.03  0.03  1.01  0.02  0.01  0.98  0.02  0.    0.99  0.02 -0.04  0.86]\n",
      " [ 0.01  0.1   0.96 -0.03  0.06  1.    0.05  0.12  0.99  0.02  0.14  0.99\n",
      "  -0.03  0.09  1.    0.    0.1   0.95  0.02  0.11  0.98  0.04  0.1   1.02\n",
      "   0.03  0.09  0.95 -0.05  0.03  0.97 -0.    0.09  1.02 -0.07  0.05  1.02\n",
      "   0.03  0.11  0.97 -0.01  0.13  0.95 -0.    0.11  1.02 -0.06  0.12  0.97\n",
      "  -0.    0.09  0.99  0.05  0.07  1.01 -0.    0.11  1.01 -0.02  0.05  1.03]\n",
      " [-0.01  0.08  1.    0.02  0.09  1.01 -0.01  0.08  0.93  0.05  0.09  1.01\n",
      "  -0.05  0.06  0.99  0.05  0.16  0.98 -0.01  0.08  1.   -0.05  0.05  0.94\n",
      "   0.04  0.11  0.96  0.01  0.13  0.96  0.01  0.07  0.97 -0.03  0.03  1.01\n",
      "  -0.04  0.06  0.99 -0.01  0.06  1.   -0.03  0.07  0.97 -0.01  0.06  0.96\n",
      "   0.01  0.1   1.   -0.04  0.06  0.98 -0.05  0.08  0.93 -0.03  0.09  1.01]\n",
      " [-0.06 -0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.99 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98]\n",
      " [-0.05  0.    0.98 -0.05 -0.01  0.98 -0.06 -0.01  0.98 -0.05 -0.02  0.98\n",
      "  -0.03  0.    0.99 -0.05 -0.03  0.97 -0.05  0.    0.98 -0.1  -0.03  0.97\n",
      "  -0.04 -0.01  0.98 -0.06 -0.01  0.98 -0.02 -0.02  0.98 -0.07  0.02  0.99\n",
      "  -0.06  0.03  0.98 -0.03 -0.01  0.98 -0.04  0.02  0.99 -0.06 -0.02  0.98\n",
      "  -0.01 -0.01  0.98 -0.06  0.    0.98 -0.06  0.01  0.99 -0.07 -0.02  0.98]\n",
      " [-0.04 -0.    0.99  0.03 -0.03  0.9   0.02  0.01  0.97  0.02  0.01  0.98\n",
      "   0.02  0.01  0.98 -0.01  0.02  1.01  0.08  0.01  0.95  0.02  0.01  1.01\n",
      "   0.1   0.02  0.97  0.04 -0.03  0.87  0.02  0.01  0.98  0.02  0.01  0.98\n",
      "   0.02  0.03  1.02  0.1   0.02  0.96 -0.03 -0.01  0.98  0.01  0.01  1.01\n",
      "   0.04  0.04  1.03  0.02 -0.03  0.94  0.02  0.01  0.98  0.02  0.01  0.99]\n",
      " [ 0.06  0.1   1.01 -0.    0.11  1.01  0.02  0.11  1.04 -0.05  0.06  0.99\n",
      "   0.05  0.09  1.    0.    0.07  0.98  0.    0.08  0.96 -0.02  0.07  1.01\n",
      "   0.04  0.1   1.    0.03  0.11  1.   -0.02  0.09  1.   -0.01  0.09  0.95\n",
      "   0.03  0.1   0.99  0.04  0.1   1.01 -0.01  0.07  1.   -0.01  0.05  1.\n",
      "   0.01  0.11  1.    0.    0.07  1.04  0.02  0.11  0.96 -0.05  0.07  0.95]\n",
      " [ 0.02 -0.01  0.98  0.1   0.02  1.   -0.02 -0.02  0.94  0.07  0.01  1.\n",
      "   0.05 -0.    0.98  0.01 -0.05  0.98  0.02 -0.01  0.98  0.02 -0.    0.98\n",
      "   0.04  0.03  0.97  0.03 -0.    0.99  0.02 -0.01  0.94 -0.02 -0.01  0.97\n",
      "  -0.02 -0.03  0.97  0.02  0.02  0.99  0.02 -0.01  0.99  0.02 -0.    0.99\n",
      "  -0.04 -0.05  0.99  0.05  0.    0.95 -0.02 -0.02  0.94  0.02  0.    1.01]\n",
      " [-0.01  0.03  0.99 -0.02  0.08  0.96 -0.05  0.05  0.95 -0.    0.11  1.02\n",
      "  -0.02  0.14  0.96 -0.04  0.07  0.98 -0.03  0.01  0.99 -0.    0.08  0.95\n",
      "   0.04  0.04  0.97  0.02  0.11  0.96 -0.07  0.1   0.95 -0.01  0.1   0.98\n",
      "   0.01  0.14  0.92 -0.03  0.06  1.   -0.    0.12  0.96  0.01  0.1   0.98\n",
      "   0.01  0.1   0.96 -0.03  0.06  1.    0.05  0.12  0.99  0.02  0.14  0.99]\n",
      " [-0.    0.11  1.01 -0.04  0.13  0.97  0.03  0.11  0.96  0.04  0.07  1.01\n",
      "  -0.01  0.11  1.   -0.04  0.02  0.97  0.03  0.11  0.96 -0.04  0.11  0.95\n",
      "   0.    0.08  0.97 -0.02  0.04  0.97 -0.    0.09  1.    0.04  0.09  1.\n",
      "   0.03  0.09  0.99 -0.01  0.1   1.    0.03  0.13  0.99  0.    0.08  1.01\n",
      "  -0.02  0.03  0.98 -0.04  0.06  0.98 -0.04  0.05  0.96 -0.    0.1   1.03]]\n",
      "[0 0 2 1 0 1 0 2 0 2 0 1 2 0 0 1 2 0 2 2 1 0 0 2 2 0 1 2 2 1 2 0 1 0 1 1 0\n",
      " 2 1 0 1 0 1 1 2 1 2 0 2 2 2 2 2 0 1 2 0 1 2 1 0 0 2 2 2 0 0 1 0 0 1 2 2 1\n",
      " 1 0 1 1 1 1 0 0 1 2 0 1 0 2 0 2 0 1 2 2 0 1 2 1 2 2 0 1 2 1 2 0 2 0 2 1 2\n",
      " 2 1 1 0 1 0 0 2 2 2 0 0 2 2 1 1 2 2 1 2 2 2 2 0 1 0 1 2 0 1 2 1 2 2 2 0 1\n",
      " 1 0 2 1 2 2 2 2 2 0 2 0 0 0 1 0 2 0 0 0 1 0 2 0 2 2 1 1 1 2 1 1 2 0 0 2 2\n",
      " 0 1 1 2 0 2 0 2 2 1 2 2 2 2 0 1 0 2 1 1 0 0 1 1 1 2 1 1 2 1 2 2 2 0 2 1 2\n",
      " 2 2 2 0 0 1 2 2 0 1 2 1 2 2 0 2 2 2 2 1 1 2 0 0 0 1 0 2 1 0 0 2 0 2 2 2 0\n",
      " 2 0 2 0 1 1 1 2 0 2 1 2 0 2 2 2 1 2 2 0 1 2 0 2 2 2 2 2 0 2 2 2 1 2 0 1 1\n",
      " 0 0 2 2 2 0 2 1 2 2 0 2 2 2 1 2 2 2 2 2 2 0 0 0 1 0 0 2 0 1 0 0 1 0 1 1 2\n",
      " 0 1 0 2 1 2 2 1 0 2 0 0 2 2 2 0 2 1 1 2 0 0 1 0 2 1 1 0 1 2 0 1 0 2 0 1 1\n",
      " 1 2 1 1 0 2 0 0 1 0 2 2 0 1 0 1 0 2 0 0 2 2 1 2 1 0 2 2 2 2 0 1 0 1 1 1 2\n",
      " 0 2 2 2 1 2 1 0 1 1 0 2 0 2 1 0 0 1 2 0 0 2 0 2 0 1 1 0 2 0 1 2 2 2 0 0 2\n",
      " 0 1 2 0 1 2 0 1 2 1 2 0 0 2 0 0 1 1 2 0 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Altrimenti il testSet viene troppo lungo e impossibile da caricare\n",
    "if choosenIndex == 1 or choosenIndex == 5:\n",
    "    half = X_test.shape[0] // 2\n",
    "    X_test = X_test[:half, :]\n",
    "    y_test = y_test[:half]\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Spotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 42\n",
    "scoring = 'f1_macro'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "\n",
    "#models.append(('XGB', XGBClassifier(random_state=seed)))\n",
    "models.append(('GNB', GaussianNB(var_smoothing=2e-9)))\n",
    "models.append(('LR',  LogisticRegression(random_state=seed)))\n",
    "models.append(('CART' , DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('SVC' , SVC(gamma=0.5, random_state=seed)))\n",
    "models.append(('RF', RandomForestClassifier(random_state=seed, n_estimators = 50)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB - 1,00 0,00\n",
      "LR - 1,00 0,00\n",
      "CART - 0,99 0,00\n",
      "SVC - 1,00 0,00\n",
      "RF - 1,00 0,00\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    # Dividere dati in n = num_folds\n",
    "    kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = np.array([])\n",
    "    for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "        X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "        X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "        model.fit(X_cross_train, y_cross_train)  \n",
    "        y_pred = model.predict(X_cross_test)\n",
    "        f1s = f1_score(y_cross_test, y_pred, average=\"weighted\")\n",
    "        cv_results = np.append(cv_results, [f1s])\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #msg = \"%s - %f - %f\" % (name, cv_results.mean(), cv_results.std())\n",
    "    msg = \"{} - {:.2f} {:.2f}\".format(name, cv_results.mean(), cv_results.std()).replace('.', ',')\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "fig.suptitle('Algorithms Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione dei modelli sul Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model GNB: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       292\n",
      "           1       1.00      1.00      1.00       282\n",
      "           2       1.00      0.99      1.00       360\n",
      "\n",
      "    accuracy                           1.00       934\n",
      "   macro avg       1.00      1.00      1.00       934\n",
      "weighted avg       1.00      1.00      1.00       934\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model LR: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       292\n",
      "           1       1.00      1.00      1.00       282\n",
      "           2       1.00      1.00      1.00       360\n",
      "\n",
      "    accuracy                           1.00       934\n",
      "   macro avg       1.00      1.00      1.00       934\n",
      "weighted avg       1.00      1.00      1.00       934\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model CART: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       292\n",
      "           1       0.99      0.99      0.99       282\n",
      "           2       0.99      0.99      0.99       360\n",
      "\n",
      "    accuracy                           0.99       934\n",
      "   macro avg       0.99      0.99      0.99       934\n",
      "weighted avg       0.99      0.99      0.99       934\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model SVC: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       292\n",
      "           1       1.00      1.00      1.00       282\n",
      "           2       1.00      1.00      1.00       360\n",
      "\n",
      "    accuracy                           1.00       934\n",
      "   macro avg       1.00      1.00      1.00       934\n",
      "weighted avg       1.00      1.00      1.00       934\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model RF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       292\n",
      "           1       1.00      1.00      1.00       282\n",
      "           2       1.00      1.00      1.00       360\n",
      "\n",
      "    accuracy                           1.00       934\n",
      "   macro avg       1.00      1.00      1.00       934\n",
      "weighted avg       1.00      1.00      1.00       934\n",
      "\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def classification_report_csv(report, model_name):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    index = 0\n",
    "    row = lines[-4].split('    ')\n",
    "    accuracy = row[-2]\n",
    "    for line in lines[2:-5]:\n",
    "        row = {}\n",
    "        row_data = line.split('      ')\n",
    "        row['class'] = uniques[index]\n",
    "        row['precision'] = float(row_data[2]) \n",
    "        row['recall'] = float(row_data[3]) \n",
    "        row['f1_score'] = float(row_data[4])\n",
    "        row['accuracy'] = accuracy\n",
    "        report_data.append(row)\n",
    "        index += 1\n",
    "    dataframe = pd.DataFrame.from_dict(report_data)\n",
    "    dataframe.to_csv(tasks[choosenIndex]+ '/classificationReports'+ '/'+'classification_report' + model_name +  '.csv', index = False)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train,  y_train)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    print(f\"Model {name}: \")\n",
    "    report = classification_report(y_test, pred_test)\n",
    "    print(report)\n",
    "    classification_report_csv(report, name)\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNetwork():\n",
    "    n = 50\n",
    "    model = Sequential(name=\"Sequential-NN\")\n",
    "    model.add(layers.Dense(X.shape[1], activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(layers.Dense(np.unique(y).size * n, activation='relu'))\n",
    "    model.add(layers.Dense(np.unique(y).size, activation='softmax'))\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 150)               9150      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 13,263\n",
      "Trainable params: 13,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "336/336 [==============================] - 0s 603us/step - loss: 1.0397 - accuracy: 0.4279\n",
      "Epoch 2/100\n",
      "336/336 [==============================] - 0s 371us/step - loss: 0.8056 - accuracy: 0.9780\n",
      "Epoch 3/100\n",
      "336/336 [==============================] - 0s 362us/step - loss: 0.4531 - accuracy: 0.9994\n",
      "Epoch 4/100\n",
      "336/336 [==============================] - 0s 394us/step - loss: 0.2011 - accuracy: 0.9997\n",
      "Epoch 5/100\n",
      "336/336 [==============================] - 0s 367us/step - loss: 0.0861 - accuracy: 0.9997\n",
      "Epoch 6/100\n",
      "336/336 [==============================] - 0s 378us/step - loss: 0.0388 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "336/336 [==============================] - 0s 380us/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "336/336 [==============================] - 0s 387us/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "336/336 [==============================] - 0s 373us/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "336/336 [==============================] - 0s 383us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "336/336 [==============================] - 0s 381us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "336/336 [==============================] - 0s 385us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "336/336 [==============================] - 0s 379us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "336/336 [==============================] - 0s 382us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "336/336 [==============================] - 0s 384us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "336/336 [==============================] - 0s 377us/step - loss: 8.8222e-04 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "336/336 [==============================] - 0s 366us/step - loss: 7.0298e-04 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "336/336 [==============================] - 0s 375us/step - loss: 5.6093e-04 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "336/336 [==============================] - 0s 377us/step - loss: 4.5197e-04 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "336/336 [==============================] - 0s 372us/step - loss: 3.6790e-04 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "336/336 [==============================] - 0s 376us/step - loss: 2.9874e-04 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "336/336 [==============================] - 0s 388us/step - loss: 2.4189e-04 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "336/336 [==============================] - 0s 385us/step - loss: 1.9732e-04 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "336/336 [==============================] - 0s 370us/step - loss: 1.5973e-04 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "336/336 [==============================] - 0s 363us/step - loss: 1.3300e-04 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "336/336 [==============================] - 0s 369us/step - loss: 1.0948e-04 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "336/336 [==============================] - 0s 367us/step - loss: 9.0506e-05 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "336/336 [==============================] - 0s 364us/step - loss: 7.5164e-05 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "336/336 [==============================] - 0s 366us/step - loss: 6.2571e-05 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "336/336 [==============================] - 0s 379us/step - loss: 5.1909e-05 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "336/336 [==============================] - 0s 374us/step - loss: 4.3212e-05 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "336/336 [==============================] - 0s 374us/step - loss: 3.6082e-05 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "336/336 [==============================] - 0s 376us/step - loss: 3.0074e-05 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "336/336 [==============================] - 0s 366us/step - loss: 2.5020e-05 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "336/336 [==============================] - 0s 360us/step - loss: 2.1130e-05 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "336/336 [==============================] - 0s 368us/step - loss: 1.7600e-05 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "336/336 [==============================] - 0s 363us/step - loss: 1.4754e-05 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "336/336 [==============================] - 0s 375us/step - loss: 1.2256e-05 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "336/336 [==============================] - 0s 373us/step - loss: 1.0439e-05 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "336/336 [==============================] - 0s 371us/step - loss: 8.7332e-06 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "336/336 [==============================] - 0s 387us/step - loss: 7.2090e-06 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "336/336 [==============================] - 0s 371us/step - loss: 6.1349e-06 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "336/336 [==============================] - 0s 368us/step - loss: 5.0676e-06 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "336/336 [==============================] - 0s 380us/step - loss: 4.2985e-06 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "336/336 [==============================] - 0s 376us/step - loss: 3.5518e-06 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "336/336 [==============================] - 0s 373us/step - loss: 3.0502e-06 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "336/336 [==============================] - 0s 370us/step - loss: 2.4894e-06 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "336/336 [==============================] - 0s 366us/step - loss: 2.1157e-06 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "336/336 [==============================] - 0s 364us/step - loss: 1.7997e-06 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "336/336 [==============================] - 0s 366us/step - loss: 1.5019e-06 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "336/336 [==============================] - 0s 366us/step - loss: 1.2648e-06 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "336/336 [==============================] - 0s 364us/step - loss: 1.0701e-06 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "336/336 [==============================] - 0s 362us/step - loss: 8.8163e-07 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "336/336 [==============================] - 0s 375us/step - loss: 7.4684e-07 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "336/336 [==============================] - 0s 370us/step - loss: 6.1400e-07 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "336/336 [==============================] - 0s 370us/step - loss: 5.3708e-07 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "336/336 [==============================] - 0s 377us/step - loss: 4.5773e-07 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "336/336 [==============================] - 0s 378us/step - loss: 3.7978e-07 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "336/336 [==============================] - 0s 364us/step - loss: 3.2539e-07 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "336/336 [==============================] - 0s 362us/step - loss: 2.7452e-07 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "336/336 [==============================] - 0s 365us/step - loss: 2.3338e-07 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "336/336 [==============================] - 0s 366us/step - loss: 1.9103e-07 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "336/336 [==============================] - 0s 364us/step - loss: 1.6926e-07 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "336/336 [==============================] - 0s 360us/step - loss: 1.4608e-07 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "336/336 [==============================] - 0s 365us/step - loss: 1.1967e-07 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "336/336 [==============================] - 0s 365us/step - loss: 1.0110e-07 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "336/336 [==============================] - 0s 372us/step - loss: 8.6656e-08 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "336/336 [==============================] - 0s 371us/step - loss: 7.5615e-08 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "336/336 [==============================] - 0s 372us/step - loss: 6.6279e-08 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "336/336 [==============================] - 0s 369us/step - loss: 5.5877e-08 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "336/336 [==============================] - 0s 377us/step - loss: 5.0517e-08 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "336/336 [==============================] - 0s 366us/step - loss: 4.2529e-08 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "336/336 [==============================] - 0s 361us/step - loss: 3.7453e-08 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "336/336 [==============================] - 0s 368us/step - loss: 3.3654e-08 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "336/336 [==============================] - 0s 362us/step - loss: 2.9749e-08 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "336/336 [==============================] - 0s 365us/step - loss: 2.6732e-08 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "336/336 [==============================] - 0s 364us/step - loss: 2.4140e-08 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "336/336 [==============================] - 0s 365us/step - loss: 2.0555e-08 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "336/336 [==============================] - 0s 369us/step - loss: 1.8602e-08 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "336/336 [==============================] - 0s 375us/step - loss: 1.6046e-08 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "336/336 [==============================] - 0s 372us/step - loss: 1.5869e-08 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "336/336 [==============================] - 0s 376us/step - loss: 1.4484e-08 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "336/336 [==============================] - 0s 363us/step - loss: 1.3171e-08 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "336/336 [==============================] - 0s 371us/step - loss: 1.2070e-08 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "336/336 [==============================] - 0s 359us/step - loss: 1.0402e-08 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "336/336 [==============================] - 0s 366us/step - loss: 1.0011e-08 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "336/336 [==============================] - 0s 371us/step - loss: 9.4785e-09 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "336/336 [==============================] - 0s 369us/step - loss: 8.9105e-09 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "336/336 [==============================] - 0s 370us/step - loss: 8.0230e-09 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "336/336 [==============================] - 0s 381us/step - loss: 7.5260e-09 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "336/336 [==============================] - 0s 376us/step - loss: 6.9935e-09 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "336/336 [==============================] - 0s 372us/step - loss: 6.7450e-09 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "336/336 [==============================] - 0s 389us/step - loss: 6.2480e-09 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "336/336 [==============================] - 0s 393us/step - loss: 5.9285e-09 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "336/336 [==============================] - 0s 394us/step - loss: 5.1120e-09 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "336/336 [==============================] - 0s 374us/step - loss: 5.5025e-09 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "336/336 [==============================] - 0s 371us/step - loss: 4.7925e-09 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "336/336 [==============================] - 0s 429us/step - loss: 4.9700e-09 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "336/336 [==============================] - 0s 423us/step - loss: 4.5440e-09 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "336/336 [==============================] - 0s 422us/step - loss: 4.2245e-09 - accuracy: 1.0000\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 150)               9150      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 13,263\n",
      "Trainable params: 13,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "336/336 [==============================] - 0s 406us/step - loss: 0.9896 - accuracy: 0.5816\n",
      "Epoch 2/100\n",
      "336/336 [==============================] - 0s 376us/step - loss: 0.6854 - accuracy: 0.9991\n",
      "Epoch 3/100\n",
      "336/336 [==============================] - 0s 378us/step - loss: 0.3038 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "336/336 [==============================] - 0s 377us/step - loss: 0.1075 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "336/336 [==============================] - 0s 426us/step - loss: 0.0460 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "336/336 [==============================] - 0s 391us/step - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "336/336 [==============================] - 0s 388us/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "336/336 [==============================] - 0s 399us/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "336/336 [==============================] - 0s 412us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "336/336 [==============================] - 0s 394us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "336/336 [==============================] - 0s 384us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "336/336 [==============================] - 0s 386us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "336/336 [==============================] - 0s 366us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "336/336 [==============================] - 0s 366us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "336/336 [==============================] - 0s 365us/step - loss: 8.2672e-04 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "336/336 [==============================] - 0s 367us/step - loss: 6.1590e-04 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "336/336 [==============================] - 0s 374us/step - loss: 4.7269e-04 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "336/336 [==============================] - 0s 380us/step - loss: 3.6728e-04 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "336/336 [==============================] - 0s 360us/step - loss: 2.8709e-04 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "336/336 [==============================] - 0s 389us/step - loss: 2.2725e-04 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "336/336 [==============================] - 0s 372us/step - loss: 1.8177e-04 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "336/336 [==============================] - 0s 380us/step - loss: 1.4604e-04 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "336/336 [==============================] - 0s 386us/step - loss: 1.1824e-04 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "336/336 [==============================] - 0s 480us/step - loss: 9.5892e-05 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "336/336 [==============================] - 0s 407us/step - loss: 7.8068e-05 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "336/336 [==============================] - 0s 393us/step - loss: 6.2571e-05 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "336/336 [==============================] - 0s 365us/step - loss: 5.1709e-05 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "336/336 [==============================] - 0s 365us/step - loss: 4.2567e-05 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "336/336 [==============================] - 0s 365us/step - loss: 3.4907e-05 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "336/336 [==============================] - 0s 368us/step - loss: 2.9489e-05 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "336/336 [==============================] - 0s 367us/step - loss: 2.4497e-05 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "336/336 [==============================] - 0s 392us/step - loss: 2.0346e-05 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "336/336 [==============================] - 0s 371us/step - loss: 1.7177e-05 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "336/336 [==============================] - 0s 451us/step - loss: 1.4206e-05 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "336/336 [==============================] - 0s 408us/step - loss: 1.2123e-05 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "336/336 [==============================] - 0s 400us/step - loss: 9.9833e-06 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "336/336 [==============================] - 0s 365us/step - loss: 8.3140e-06 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "336/336 [==============================] - 0s 378us/step - loss: 7.0263e-06 - accuracy: 1.0000\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 0s 392us/step - loss: 5.9000e-06 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "336/336 [==============================] - 0s 388us/step - loss: 4.9633e-06 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "336/336 [==============================] - 0s 380us/step - loss: 4.1657e-06 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "336/336 [==============================] - 0s 387us/step - loss: 3.4921e-06 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "336/336 [==============================] - 0s 382us/step - loss: 2.9159e-06 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "336/336 [==============================] - 0s 379us/step - loss: 2.5332e-06 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "336/336 [==============================] - 0s 362us/step - loss: 2.0886e-06 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "336/336 [==============================] - 0s 370us/step - loss: 1.7566e-06 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "336/336 [==============================] - 0s 374us/step - loss: 1.4887e-06 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "336/336 [==============================] - 0s 377us/step - loss: 1.2553e-06 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "336/336 [==============================] - 0s 380us/step - loss: 1.0644e-06 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "336/336 [==============================] - 0s 377us/step - loss: 8.7375e-07 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "336/336 [==============================] - 0s 388us/step - loss: 7.6782e-07 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "336/336 [==============================] - 0s 359us/step - loss: 6.3829e-07 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 5.3970e-07 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 4.6373e-07 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 3.9114e-07 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 3.3767e-07 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "336/336 [==============================] - 0s 358us/step - loss: 2.7601e-07 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 2.3785e-07 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "336/336 [==============================] - 0s 365us/step - loss: 2.0480e-07 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "336/336 [==============================] - 0s 348us/step - loss: 1.7313e-07 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "336/336 [==============================] - 0s 348us/step - loss: 1.4871e-07 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 1.2212e-07 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "336/336 [==============================] - 0s 357us/step - loss: 1.1115e-07 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 9.2265e-08 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 7.8917e-08 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 6.6811e-08 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "336/336 [==============================] - 0s 347us/step - loss: 6.0386e-08 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "336/336 [==============================] - 0s 360us/step - loss: 4.9239e-08 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 4.6257e-08 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 4.0328e-08 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 3.5784e-08 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "336/336 [==============================] - 0s 358us/step - loss: 3.2696e-08 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 2.9110e-08 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "336/336 [==============================] - 0s 347us/step - loss: 2.3466e-08 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 2.3004e-08 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "336/336 [==============================] - 0s 357us/step - loss: 1.9703e-08 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 1.8034e-08 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 1.6508e-08 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.4413e-08 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 1.3668e-08 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 1.1964e-08 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "336/336 [==============================] - 0s 347us/step - loss: 1.1396e-08 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 1.0366e-08 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 9.2300e-09 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "336/336 [==============================] - 0s 347us/step - loss: 8.9460e-09 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 8.6620e-09 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 7.7390e-09 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 7.1355e-09 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 6.7450e-09 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 5.9285e-09 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 6.1770e-09 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 5.8930e-09 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 5.7155e-09 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 5.3250e-09 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 4.8280e-09 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 4.7215e-09 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "336/336 [==============================] - 0s 386us/step - loss: 4.4730e-09 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "336/336 [==============================] - 0s 485us/step - loss: 4.1535e-09 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 3.7275e-09 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "336/336 [==============================] - 0s 540us/step - loss: 3.6565e-09 - accuracy: 1.0000\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 150)               9150      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 13,263\n",
      "Trainable params: 13,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "336/336 [==============================] - 0s 524us/step - loss: 1.0308 - accuracy: 0.4504\n",
      "Epoch 2/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 0.7876 - accuracy: 0.9274\n",
      "Epoch 3/100\n",
      "336/336 [==============================] - 0s 522us/step - loss: 0.4018 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "336/336 [==============================] - 0s 522us/step - loss: 0.1506 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "336/336 [==============================] - 0s 541us/step - loss: 0.0613 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 0.0304 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "336/336 [==============================] - 0s 535us/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "336/336 [==============================] - 0s 533us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "336/336 [==============================] - 0s 522us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "336/336 [==============================] - 0s 528us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "336/336 [==============================] - 0s 535us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "336/336 [==============================] - 0s 532us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "336/336 [==============================] - 0s 531us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "336/336 [==============================] - 0s 544us/step - loss: 7.8812e-04 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "336/336 [==============================] - 0s 548us/step - loss: 6.0948e-04 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "336/336 [==============================] - 0s 546us/step - loss: 4.7330e-04 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "336/336 [==============================] - 0s 541us/step - loss: 3.7388e-04 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "336/336 [==============================] - 0s 535us/step - loss: 2.9523e-04 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "336/336 [==============================] - 0s 532us/step - loss: 2.3574e-04 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "336/336 [==============================] - 0s 530us/step - loss: 1.8947e-04 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "336/336 [==============================] - 0s 529us/step - loss: 1.5231e-04 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "336/336 [==============================] - 0s 533us/step - loss: 1.2334e-04 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "336/336 [==============================] - 0s 531us/step - loss: 1.0014e-04 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "336/336 [==============================] - 0s 563us/step - loss: 8.1641e-05 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "336/336 [==============================] - 0s 560us/step - loss: 6.6715e-05 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "336/336 [==============================] - 0s 541us/step - loss: 5.5258e-05 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "336/336 [==============================] - 0s 579us/step - loss: 4.5576e-05 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "336/336 [==============================] - 0s 619us/step - loss: 3.7604e-05 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "336/336 [==============================] - 0s 577us/step - loss: 3.1076e-05 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "336/336 [==============================] - 0s 555us/step - loss: 2.5778e-05 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "336/336 [==============================] - 0s 553us/step - loss: 2.1436e-05 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "336/336 [==============================] - 0s 572us/step - loss: 1.7840e-05 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "336/336 [==============================] - 0s 555us/step - loss: 1.4907e-05 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "336/336 [==============================] - 0s 568us/step - loss: 1.2347e-05 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "336/336 [==============================] - 0s 585us/step - loss: 1.0417e-05 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "336/336 [==============================] - 0s 561us/step - loss: 8.7014e-06 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "336/336 [==============================] - 0s 531us/step - loss: 7.3429e-06 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "336/336 [==============================] - 0s 550us/step - loss: 6.1567e-06 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "336/336 [==============================] - 0s 557us/step - loss: 5.2288e-06 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "336/336 [==============================] - 0s 610us/step - loss: 4.3538e-06 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "336/336 [==============================] - 0s 592us/step - loss: 3.5990e-06 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "336/336 [==============================] - 0s 561us/step - loss: 3.0584e-06 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "336/336 [==============================] - 0s 554us/step - loss: 2.5717e-06 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "336/336 [==============================] - 0s 551us/step - loss: 2.1735e-06 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "336/336 [==============================] - 0s 563us/step - loss: 1.8245e-06 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "336/336 [==============================] - 0s 575us/step - loss: 1.5233e-06 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "336/336 [==============================] - 0s 542us/step - loss: 1.2970e-06 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "336/336 [==============================] - 0s 572us/step - loss: 1.1158e-06 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "336/336 [==============================] - 0s 561us/step - loss: 9.0114e-07 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "336/336 [==============================] - 0s 562us/step - loss: 7.9077e-07 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "336/336 [==============================] - 0s 533us/step - loss: 6.6737e-07 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "336/336 [==============================] - 0s 546us/step - loss: 5.6591e-07 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "336/336 [==============================] - 0s 538us/step - loss: 4.7800e-07 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "336/336 [==============================] - 0s 531us/step - loss: 4.0969e-07 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "336/336 [==============================] - 0s 541us/step - loss: 3.4975e-07 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "336/336 [==============================] - 0s 551us/step - loss: 2.9013e-07 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "336/336 [==============================] - 0s 562us/step - loss: 2.5517e-07 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "336/336 [==============================] - 0s 610us/step - loss: 2.0286e-07 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "336/336 [==============================] - 0s 574us/step - loss: 1.7581e-07 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "336/336 [==============================] - 0s 553us/step - loss: 1.5555e-07 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "336/336 [==============================] - 0s 550us/step - loss: 1.2677e-07 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "336/336 [==============================] - 0s 564us/step - loss: 1.1293e-07 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "336/336 [==============================] - 0s 575us/step - loss: 9.7809e-08 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "336/336 [==============================] - 0s 596us/step - loss: 8.4713e-08 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "336/336 [==============================] - 0s 600us/step - loss: 7.0766e-08 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "336/336 [==============================] - 0s 615us/step - loss: 6.2994e-08 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "336/336 [==============================] - 0s 621us/step - loss: 5.2737e-08 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "336/336 [==============================] - 0s 604us/step - loss: 4.7024e-08 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "336/336 [==============================] - 0s 549us/step - loss: 4.1132e-08 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "336/336 [==============================] - 0s 558us/step - loss: 3.6377e-08 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "336/336 [==============================] - 0s 539us/step - loss: 3.1834e-08 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "336/336 [==============================] - 0s 554us/step - loss: 2.8108e-08 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "336/336 [==============================] - 0s 579us/step - loss: 2.5127e-08 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "336/336 [==============================] - 0s 864us/step - loss: 2.3423e-08 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "336/336 [==============================] - 0s 643us/step - loss: 2.0229e-08 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "336/336 [==============================] - 0s 605us/step - loss: 1.8809e-08 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "336/336 [==============================] - 0s 611us/step - loss: 1.6396e-08 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "336/336 [==============================] - 0s 637us/step - loss: 1.5225e-08 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "336/336 [==============================] - 0s 585us/step - loss: 1.3167e-08 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "336/336 [==============================] - 0s 587us/step - loss: 1.2066e-08 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "336/336 [==============================] - 0s 584us/step - loss: 1.1002e-08 - accuracy: 1.0000\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 0s 594us/step - loss: 1.0753e-08 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "336/336 [==============================] - 0s 576us/step - loss: 9.6886e-09 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "336/336 [==============================] - 0s 624us/step - loss: 8.5530e-09 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "336/336 [==============================] - 0s 631us/step - loss: 8.4465e-09 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "336/336 [==============================] - 0s 645us/step - loss: 8.6949e-09 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "336/336 [==============================] - 0s 762us/step - loss: 6.6365e-09 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "336/336 [==============================] - 0s 577us/step - loss: 6.5301e-09 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "336/336 [==============================] - 0s 566us/step - loss: 6.6010e-09 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "336/336 [==============================] - 0s 576us/step - loss: 5.8913e-09 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "336/336 [==============================] - 0s 575us/step - loss: 5.6428e-09 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "336/336 [==============================] - 0s 614us/step - loss: 5.2879e-09 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "336/336 [==============================] - 0s 588us/step - loss: 4.8266e-09 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "336/336 [==============================] - 0s 584us/step - loss: 4.6846e-09 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "336/336 [==============================] - 0s 584us/step - loss: 4.5427e-09 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "336/336 [==============================] - 0s 571us/step - loss: 4.4717e-09 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "336/336 [==============================] - 0s 587us/step - loss: 4.1878e-09 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "336/336 [==============================] - 0s 576us/step - loss: 3.5844e-09 - accuracy: 1.0000\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 150)               9150      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 13,263\n",
      "Trainable params: 13,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "336/336 [==============================] - 0s 578us/step - loss: 1.0235 - accuracy: 0.5022\n",
      "Epoch 2/100\n",
      "336/336 [==============================] - 0s 572us/step - loss: 0.7423 - accuracy: 0.9711\n",
      "Epoch 3/100\n",
      "336/336 [==============================] - 0s 564us/step - loss: 0.3292 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "336/336 [==============================] - 0s 576us/step - loss: 0.1136 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "336/336 [==============================] - 0s 582us/step - loss: 0.0465 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "336/336 [==============================] - 0s 564us/step - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "336/336 [==============================] - 0s 577us/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "336/336 [==============================] - 0s 691us/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "336/336 [==============================] - 0s 564us/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "336/336 [==============================] - 0s 553us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "336/336 [==============================] - 0s 560us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "336/336 [==============================] - 0s 539us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "336/336 [==============================] - 0s 545us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "336/336 [==============================] - 0s 552us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "336/336 [==============================] - 0s 569us/step - loss: 8.1715e-04 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "336/336 [==============================] - 0s 551us/step - loss: 6.3447e-04 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "336/336 [==============================] - 0s 552us/step - loss: 4.9801e-04 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "336/336 [==============================] - 0s 550us/step - loss: 3.9107e-04 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "336/336 [==============================] - 0s 554us/step - loss: 3.0840e-04 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "336/336 [==============================] - 0s 549us/step - loss: 2.4753e-04 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "336/336 [==============================] - 0s 547us/step - loss: 1.9917e-04 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "336/336 [==============================] - 0s 546us/step - loss: 1.6143e-04 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "336/336 [==============================] - 0s 550us/step - loss: 1.3255e-04 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "336/336 [==============================] - 0s 550us/step - loss: 1.0870e-04 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "336/336 [==============================] - 0s 545us/step - loss: 8.9106e-05 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "336/336 [==============================] - 0s 564us/step - loss: 7.3295e-05 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "336/336 [==============================] - 0s 551us/step - loss: 5.9166e-05 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "336/336 [==============================] - 0s 553us/step - loss: 4.8509e-05 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "336/336 [==============================] - 0s 554us/step - loss: 3.9771e-05 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "336/336 [==============================] - 0s 550us/step - loss: 3.3235e-05 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "336/336 [==============================] - 0s 570us/step - loss: 2.6798e-05 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "336/336 [==============================] - 0s 573us/step - loss: 2.2871e-05 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "336/336 [==============================] - 0s 530us/step - loss: 1.8679e-05 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "336/336 [==============================] - 0s 403us/step - loss: 1.5742e-05 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "336/336 [==============================] - 0s 380us/step - loss: 1.2774e-05 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "336/336 [==============================] - 0s 386us/step - loss: 1.0523e-05 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "336/336 [==============================] - 0s 384us/step - loss: 8.5969e-06 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "336/336 [==============================] - 0s 383us/step - loss: 7.1236e-06 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "336/336 [==============================] - 0s 380us/step - loss: 6.1816e-06 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "336/336 [==============================] - 0s 379us/step - loss: 5.1015e-06 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "336/336 [==============================] - 0s 379us/step - loss: 4.3337e-06 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "336/336 [==============================] - 0s 378us/step - loss: 3.5717e-06 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "336/336 [==============================] - 0s 376us/step - loss: 3.0333e-06 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "336/336 [==============================] - 0s 368us/step - loss: 2.4912e-06 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "336/336 [==============================] - 0s 373us/step - loss: 2.0737e-06 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "336/336 [==============================] - 0s 377us/step - loss: 1.7993e-06 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "336/336 [==============================] - 0s 371us/step - loss: 1.5273e-06 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "336/336 [==============================] - 0s 372us/step - loss: 1.2685e-06 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "336/336 [==============================] - 0s 373us/step - loss: 1.0691e-06 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "336/336 [==============================] - 0s 369us/step - loss: 9.0908e-07 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "336/336 [==============================] - 0s 373us/step - loss: 7.6759e-07 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "336/336 [==============================] - 0s 371us/step - loss: 6.4406e-07 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "336/336 [==============================] - 0s 376us/step - loss: 5.6648e-07 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "336/336 [==============================] - 0s 370us/step - loss: 4.6427e-07 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "336/336 [==============================] - 0s 368us/step - loss: 3.9297e-07 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "336/336 [==============================] - 0s 374us/step - loss: 3.5117e-07 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "336/336 [==============================] - 0s 374us/step - loss: 2.8278e-07 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "336/336 [==============================] - 0s 378us/step - loss: 2.3877e-07 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "336/336 [==============================] - 0s 380us/step - loss: 2.1663e-07 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "336/336 [==============================] - 0s 423us/step - loss: 1.8128e-07 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "336/336 [==============================] - 0s 385us/step - loss: 1.5175e-07 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "336/336 [==============================] - 0s 407us/step - loss: 1.3383e-07 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "336/336 [==============================] - 0s 502us/step - loss: 1.0764e-07 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "336/336 [==============================] - 0s 374us/step - loss: 9.5573e-08 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 8.3010e-08 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "336/336 [==============================] - 0s 415us/step - loss: 6.9595e-08 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "336/336 [==============================] - 0s 372us/step - loss: 6.1645e-08 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 5.2666e-08 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "336/336 [==============================] - 0s 382us/step - loss: 4.4433e-08 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "336/336 [==============================] - 0s 361us/step - loss: 4.1168e-08 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "336/336 [==============================] - 0s 362us/step - loss: 3.7370e-08 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 3.1231e-08 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 2.8640e-08 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "336/336 [==============================] - 0s 370us/step - loss: 2.5091e-08 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 2.3494e-08 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "336/336 [==============================] - 0s 357us/step - loss: 1.9661e-08 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 1.8384e-08 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 1.9874e-08 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "336/336 [==============================] - 0s 359us/step - loss: 1.5651e-08 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 1.4054e-08 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 1.1995e-08 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 1.1960e-08 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 1.0398e-08 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 1.0505e-08 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 9.3692e-09 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 7.9851e-09 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 8.5885e-09 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "336/336 [==============================] - 0s 357us/step - loss: 7.1689e-09 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "336/336 [==============================] - 0s 358us/step - loss: 6.8850e-09 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "336/336 [==============================] - 0s 362us/step - loss: 5.8203e-09 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "336/336 [==============================] - 0s 379us/step - loss: 6.3526e-09 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "336/336 [==============================] - 0s 370us/step - loss: 6.0332e-09 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "336/336 [==============================] - 0s 381us/step - loss: 5.4299e-09 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "336/336 [==============================] - 0s 371us/step - loss: 4.7911e-09 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "336/336 [==============================] - 0s 376us/step - loss: 4.7556e-09 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "336/336 [==============================] - 0s 367us/step - loss: 4.9685e-09 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "336/336 [==============================] - 0s 382us/step - loss: 4.5427e-09 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "336/336 [==============================] - 0s 371us/step - loss: 4.2233e-09 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 4.0103e-09 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 3.9748e-09 - accuracy: 1.0000\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 150)               9150      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 13,263\n",
      "Trainable params: 13,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 1.0538 - accuracy: 0.5076\n",
      "Epoch 2/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 0.8354 - accuracy: 0.9315\n",
      "Epoch 3/100\n",
      "336/336 [==============================] - 0s 346us/step - loss: 0.4881 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 0.1987 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 0.0785 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "336/336 [==============================] - 0s 348us/step - loss: 0.0374 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "336/336 [==============================] - 0s 348us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "336/336 [==============================] - 0s 348us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "336/336 [==============================] - 0s 346us/step - loss: 8.3046e-04 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 6.5703e-04 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 5.2131e-04 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 4.0763e-04 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "336/336 [==============================] - 0s 348us/step - loss: 3.2450e-04 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "336/336 [==============================] - 0s 346us/step - loss: 2.6021e-04 - accuracy: 1.0000\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 0s 351us/step - loss: 2.1146e-04 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.7148e-04 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 1.3982e-04 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "336/336 [==============================] - 0s 343us/step - loss: 1.1447e-04 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 9.4022e-05 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "336/336 [==============================] - 0s 347us/step - loss: 7.6972e-05 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 6.4068e-05 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 5.2437e-05 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "336/336 [==============================] - 0s 359us/step - loss: 4.3486e-05 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "336/336 [==============================] - 0s 439us/step - loss: 3.5950e-05 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "336/336 [==============================] - 0s 509us/step - loss: 3.0234e-05 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 2.5029e-05 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "336/336 [==============================] - 0s 521us/step - loss: 2.0642e-05 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "336/336 [==============================] - 0s 524us/step - loss: 1.7598e-05 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 1.4584e-05 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 1.2302e-05 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "336/336 [==============================] - 0s 544us/step - loss: 1.0224e-05 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "336/336 [==============================] - 0s 524us/step - loss: 8.6268e-06 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "336/336 [==============================] - 0s 524us/step - loss: 7.1911e-06 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "336/336 [==============================] - 0s 530us/step - loss: 6.1617e-06 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "336/336 [==============================] - 0s 529us/step - loss: 5.0814e-06 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "336/336 [==============================] - 0s 525us/step - loss: 4.2735e-06 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "336/336 [==============================] - 0s 520us/step - loss: 3.6575e-06 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "336/336 [==============================] - 0s 525us/step - loss: 3.1046e-06 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "336/336 [==============================] - 0s 523us/step - loss: 2.5485e-06 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 2.1684e-06 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "336/336 [==============================] - 0s 533us/step - loss: 1.8269e-06 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 1.5346e-06 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 1.2698e-06 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 1.1063e-06 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "336/336 [==============================] - 0s 523us/step - loss: 9.2530e-07 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "336/336 [==============================] - 0s 524us/step - loss: 7.7550e-07 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "336/336 [==============================] - 0s 530us/step - loss: 6.6460e-07 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 5.5800e-07 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "336/336 [==============================] - 0s 528us/step - loss: 4.7123e-07 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "336/336 [==============================] - 0s 525us/step - loss: 4.0064e-07 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "336/336 [==============================] - 0s 521us/step - loss: 3.4354e-07 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "336/336 [==============================] - 0s 529us/step - loss: 2.8718e-07 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 2.5222e-07 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "336/336 [==============================] - 0s 524us/step - loss: 2.1120e-07 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "336/336 [==============================] - 0s 516us/step - loss: 1.7486e-07 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 1.5314e-07 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "336/336 [==============================] - 0s 528us/step - loss: 1.3426e-07 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 1.0860e-07 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 9.3160e-08 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 8.6878e-08 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "336/336 [==============================] - 0s 528us/step - loss: 7.1192e-08 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 6.3987e-08 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "336/336 [==============================] - 0s 524us/step - loss: 5.5044e-08 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "336/336 [==============================] - 0s 525us/step - loss: 4.5178e-08 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "336/336 [==============================] - 0s 530us/step - loss: 4.1310e-08 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "336/336 [==============================] - 0s 530us/step - loss: 3.5312e-08 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 3.1550e-08 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "336/336 [==============================] - 0s 518us/step - loss: 2.6830e-08 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 2.4452e-08 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "336/336 [==============================] - 0s 536us/step - loss: 2.2607e-08 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "336/336 [==============================] - 0s 531us/step - loss: 1.9484e-08 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "336/336 [==============================] - 0s 521us/step - loss: 1.8596e-08 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "336/336 [==============================] - 0s 523us/step - loss: 1.6254e-08 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 1.4551e-08 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 1.2634e-08 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "336/336 [==============================] - 0s 517us/step - loss: 1.2492e-08 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "336/336 [==============================] - 0s 522us/step - loss: 1.1250e-08 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "336/336 [==============================] - 0s 528us/step - loss: 1.0185e-08 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "336/336 [==============================] - 0s 520us/step - loss: 9.2982e-09 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "336/336 [==============================] - 0s 522us/step - loss: 8.8369e-09 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "336/336 [==============================] - 0s 530us/step - loss: 8.3755e-09 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "336/336 [==============================] - 0s 483us/step - loss: 7.9851e-09 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "336/336 [==============================] - 0s 357us/step - loss: 7.0269e-09 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 6.8140e-09 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "336/336 [==============================] - 0s 348us/step - loss: 5.8913e-09 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "336/336 [==============================] - 0s 347us/step - loss: 5.9267e-09 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "336/336 [==============================] - 0s 348us/step - loss: 5.4654e-09 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 5.1815e-09 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "336/336 [==============================] - 0s 347us/step - loss: 4.8266e-09 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "336/336 [==============================] - 0s 366us/step - loss: 4.6491e-09 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 4.4007e-09 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "336/336 [==============================] - 0s 347us/step - loss: 4.2942e-09 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "336/336 [==============================] - 0s 346us/step - loss: 4.1168e-09 - accuracy: 1.0000\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 150)               9150      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 13,263\n",
      "Trainable params: 13,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 1.0227 - accuracy: 0.4975\n",
      "Epoch 2/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 0.7482 - accuracy: 0.9738\n",
      "Epoch 3/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 0.3442 - accuracy: 0.9997\n",
      "Epoch 4/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 0.1255 - accuracy: 0.9997\n",
      "Epoch 5/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 0.0538 - accuracy: 0.9997\n",
      "Epoch 6/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 0.0280 - accuracy: 0.9997\n",
      "Epoch 7/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 0.0163 - accuracy: 0.9997\n",
      "Epoch 8/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "336/336 [==============================] - 0s 348us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 9.3468e-04 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 7.4392e-04 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 5.6127e-04 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 4.1952e-04 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 3.2740e-04 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 2.6078e-04 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 2.0893e-04 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.7155e-04 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 1.3994e-04 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 1.1294e-04 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 9.4206e-05 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 7.7087e-05 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 6.3593e-05 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 5.2864e-05 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "336/336 [==============================] - 0s 346us/step - loss: 4.3830e-05 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 3.6496e-05 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 3.0543e-05 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 2.5201e-05 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 2.0959e-05 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 1.7644e-05 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "336/336 [==============================] - 0s 359us/step - loss: 1.4655e-05 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "336/336 [==============================] - 0s 370us/step - loss: 1.2296e-05 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "336/336 [==============================] - 0s 359us/step - loss: 1.0293e-05 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 8.5553e-06 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 7.2650e-06 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 6.0092e-06 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 5.0543e-06 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 4.2514e-06 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 3.5947e-06 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 2.9844e-06 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "336/336 [==============================] - 0s 358us/step - loss: 2.5516e-06 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 2.1046e-06 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 1.7797e-06 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 1.4916e-06 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.2349e-06 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 1.0590e-06 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 9.0603e-07 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 7.4478e-07 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 6.3955e-07 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 5.3599e-07 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 4.5494e-07 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 3.7594e-07 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 3.2643e-07 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 2.7089e-07 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 2.3139e-07 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 1.9824e-07 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "336/336 [==============================] - 0s 359us/step - loss: 1.6652e-07 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.4359e-07 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.1836e-07 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 1.0327e-07 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 8.8582e-08 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 7.6160e-08 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 6.4165e-08 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 5.7138e-08 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 4.9366e-08 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 4.2410e-08 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 3.7938e-08 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 3.2792e-08 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 2.9634e-08 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 2.6333e-08 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 2.3210e-08 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 2.0335e-08 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "336/336 [==============================] - 0s 364us/step - loss: 1.9093e-08 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "336/336 [==============================] - 0s 360us/step - loss: 1.6325e-08 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.5083e-08 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.3486e-08 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 1.2279e-08 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 1.0718e-08 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 1.0611e-08 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 9.6531e-09 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 8.1271e-09 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "336/336 [==============================] - 0s 358us/step - loss: 7.5948e-09 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "336/336 [==============================] - 0s 358us/step - loss: 7.5948e-09 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 6.5301e-09 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 7.2753e-09 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 6.2107e-09 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 5.7493e-09 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 5.6073e-09 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "336/336 [==============================] - 0s 357us/step - loss: 5.5009e-09 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 4.8621e-09 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 4.6491e-09 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 4.2942e-09 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 4.2233e-09 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 3.5844e-09 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 3.9038e-09 - accuracy: 1.0000\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 150)               9150      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 13,263\n",
      "Trainable params: 13,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 1.0255 - accuracy: 0.4966\n",
      "Epoch 2/100\n",
      "336/336 [==============================] - 0s 357us/step - loss: 0.7515 - accuracy: 0.9452\n",
      "Epoch 3/100\n",
      "336/336 [==============================] - 0s 357us/step - loss: 0.3555 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 0.1302 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 0.0550 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 0.0282 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "336/336 [==============================] - 0s 357us/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "336/336 [==============================] - 0s 348us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 8.1032e-04 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 6.4101e-04 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "336/336 [==============================] - 0s 362us/step - loss: 5.1364e-04 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "336/336 [==============================] - 0s 358us/step - loss: 4.1366e-04 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "336/336 [==============================] - 0s 360us/step - loss: 3.3325e-04 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 2.7300e-04 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 2.2186e-04 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 1.8184e-04 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 1.4914e-04 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 1.2297e-04 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 1.0045e-04 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 8.2549e-05 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 6.8078e-05 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 5.5924e-05 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 4.6121e-05 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 3.8313e-05 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 3.1519e-05 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 2.5907e-05 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 2.1490e-05 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 1.7785e-05 - accuracy: 1.0000\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 0s 359us/step - loss: 1.4524e-05 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 1.2321e-05 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.0242e-05 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 8.6508e-06 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 7.0358e-06 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "336/336 [==============================] - 0s 348us/step - loss: 6.0261e-06 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 5.0355e-06 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 4.1572e-06 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 3.5543e-06 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 2.8960e-06 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 2.3773e-06 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 2.1112e-06 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 1.7284e-06 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 1.4607e-06 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 1.2525e-06 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "336/336 [==============================] - 0s 347us/step - loss: 1.0413e-06 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 8.9535e-07 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 7.6386e-07 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 6.3068e-07 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "336/336 [==============================] - 0s 357us/step - loss: 5.6371e-07 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "336/336 [==============================] - 0s 347us/step - loss: 4.6544e-07 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 3.7746e-07 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 3.2689e-07 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 2.8086e-07 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "336/336 [==============================] - 0s 368us/step - loss: 2.3384e-07 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 1.9704e-07 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 1.7418e-07 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "336/336 [==============================] - 0s 360us/step - loss: 1.4551e-07 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 1.2254e-07 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 1.0590e-07 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 9.0285e-08 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 7.9106e-08 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 7.1760e-08 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 6.1645e-08 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 5.2063e-08 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 4.3226e-08 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 4.1061e-08 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 3.4851e-08 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 3.2260e-08 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 2.8001e-08 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 2.5162e-08 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 2.1933e-08 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 2.0371e-08 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "336/336 [==============================] - 0s 363us/step - loss: 1.9129e-08 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 1.6290e-08 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 1.5225e-08 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 1.3202e-08 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 1.1712e-08 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 1.2421e-08 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 1.0363e-08 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "336/336 [==============================] - 0s 357us/step - loss: 1.1641e-08 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "336/336 [==============================] - 0s 348us/step - loss: 8.6949e-09 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "336/336 [==============================] - 0s 358us/step - loss: 8.4110e-09 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 7.8077e-09 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 6.6365e-09 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 7.1689e-09 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 6.2462e-09 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 6.0687e-09 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 5.6428e-09 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 5.5719e-09 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 4.7911e-09 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "336/336 [==============================] - 0s 358us/step - loss: 4.7201e-09 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 4.7556e-09 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 4.3652e-09 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 3.9748e-09 - accuracy: 1.0000\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 150)               9150      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 13,263\n",
      "Trainable params: 13,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.0357 - accuracy: 0.4266\n",
      "Epoch 2/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 0.7864 - accuracy: 0.9494\n",
      "Epoch 3/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 0.3843 - accuracy: 0.9994\n",
      "Epoch 4/100\n",
      "336/336 [==============================] - 0s 361us/step - loss: 0.1392 - accuracy: 0.9997\n",
      "Epoch 5/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 0.0519 - accuracy: 0.9997\n",
      "Epoch 6/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "336/336 [==============================] - 0s 357us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 9.8698e-04 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 7.8347e-04 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 6.2592e-04 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 5.0181e-04 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 4.0705e-04 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 3.2984e-04 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 2.6513e-04 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 2.1704e-04 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 1.7713e-04 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "336/336 [==============================] - 0s 348us/step - loss: 1.4420e-04 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 1.1859e-04 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 9.7052e-05 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "336/336 [==============================] - 0s 425us/step - loss: 8.0019e-05 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "336/336 [==============================] - 0s 506us/step - loss: 6.5805e-05 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 5.4198e-05 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "336/336 [==============================] - 0s 517us/step - loss: 4.4548e-05 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 3.6951e-05 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "336/336 [==============================] - 0s 524us/step - loss: 3.0708e-05 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 2.5645e-05 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "336/336 [==============================] - 0s 530us/step - loss: 2.1029e-05 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "336/336 [==============================] - 0s 519us/step - loss: 1.7931e-05 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "336/336 [==============================] - 0s 533us/step - loss: 1.4445e-05 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "336/336 [==============================] - 0s 540us/step - loss: 1.2151e-05 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 1.0010e-05 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "336/336 [==============================] - 0s 531us/step - loss: 8.2263e-06 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 7.0949e-06 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "336/336 [==============================] - 0s 522us/step - loss: 5.8328e-06 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "336/336 [==============================] - 0s 528us/step - loss: 4.8366e-06 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "336/336 [==============================] - 0s 522us/step - loss: 4.1479e-06 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "336/336 [==============================] - 0s 528us/step - loss: 3.4293e-06 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 2.8959e-06 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "336/336 [==============================] - 0s 525us/step - loss: 2.3888e-06 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "336/336 [==============================] - 0s 524us/step - loss: 2.0288e-06 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "336/336 [==============================] - 0s 528us/step - loss: 1.6735e-06 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "336/336 [==============================] - 0s 529us/step - loss: 1.4192e-06 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "336/336 [==============================] - 0s 531us/step - loss: 1.2075e-06 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "336/336 [==============================] - 0s 538us/step - loss: 9.9745e-07 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 8.4524e-07 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "336/336 [==============================] - 0s 528us/step - loss: 7.0464e-07 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "336/336 [==============================] - 0s 521us/step - loss: 5.9160e-07 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "336/336 [==============================] - 0s 530us/step - loss: 5.0086e-07 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "336/336 [==============================] - 0s 528us/step - loss: 4.2811e-07 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "336/336 [==============================] - 0s 536us/step - loss: 3.6540e-07 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "336/336 [==============================] - 0s 529us/step - loss: 3.1160e-07 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "336/336 [==============================] - 0s 529us/step - loss: 2.5964e-07 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 2.1798e-07 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "336/336 [==============================] - 0s 524us/step - loss: 1.9469e-07 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 1.6364e-07 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "336/336 [==============================] - 0s 533us/step - loss: 1.3475e-07 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "336/336 [==============================] - 0s 531us/step - loss: 1.1332e-07 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "336/336 [==============================] - 0s 531us/step - loss: 9.8625e-08 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "336/336 [==============================] - 0s 523us/step - loss: 8.3791e-08 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "336/336 [==============================] - 0s 530us/step - loss: 7.3179e-08 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "336/336 [==============================] - 0s 524us/step - loss: 6.1077e-08 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "336/336 [==============================] - 0s 531us/step - loss: 5.5151e-08 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 4.8159e-08 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "336/336 [==============================] - 0s 529us/step - loss: 4.1913e-08 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 3.7051e-08 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "336/336 [==============================] - 0s 524us/step - loss: 3.2118e-08 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "336/336 [==============================] - 0s 537us/step - loss: 2.8498e-08 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "336/336 [==============================] - 0s 525us/step - loss: 2.5339e-08 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "336/336 [==============================] - 0s 525us/step - loss: 2.3707e-08 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "336/336 [==============================] - 0s 530us/step - loss: 2.0406e-08 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "336/336 [==============================] - 0s 528us/step - loss: 1.7922e-08 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 1.6645e-08 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "336/336 [==============================] - 0s 537us/step - loss: 1.4302e-08 - accuracy: 1.0000\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 0s 532us/step - loss: 1.4160e-08 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "336/336 [==============================] - 0s 522us/step - loss: 1.2492e-08 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "336/336 [==============================] - 0s 528us/step - loss: 1.3202e-08 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "336/336 [==============================] - 0s 523us/step - loss: 1.0363e-08 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "336/336 [==============================] - 0s 529us/step - loss: 1.0115e-08 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "336/336 [==============================] - 0s 532us/step - loss: 9.6177e-09 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 8.3400e-09 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "336/336 [==============================] - 0s 528us/step - loss: 7.9851e-09 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "336/336 [==============================] - 0s 520us/step - loss: 7.3463e-09 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "336/336 [==============================] - 0s 531us/step - loss: 7.2044e-09 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "336/336 [==============================] - 0s 536us/step - loss: 6.3171e-09 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 6.6365e-09 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "336/336 [==============================] - 0s 535us/step - loss: 5.8913e-09 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "336/336 [==============================] - 0s 525us/step - loss: 5.8203e-09 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "336/336 [==============================] - 0s 530us/step - loss: 5.3589e-09 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 4.9330e-09 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "336/336 [==============================] - 0s 533us/step - loss: 4.8621e-09 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "336/336 [==============================] - 0s 526us/step - loss: 4.7911e-09 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "336/336 [==============================] - 0s 533us/step - loss: 4.2587e-09 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 4.1523e-09 - accuracy: 1.0000\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 150)               9150      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 13,263\n",
      "Trainable params: 13,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "336/336 [==============================] - 0s 541us/step - loss: 1.0016 - accuracy: 0.5511\n",
      "Epoch 2/100\n",
      "336/336 [==============================] - 0s 535us/step - loss: 0.6797 - accuracy: 0.9815\n",
      "Epoch 3/100\n",
      "336/336 [==============================] - 0s 531us/step - loss: 0.3207 - accuracy: 0.9997\n",
      "Epoch 4/100\n",
      "336/336 [==============================] - 0s 537us/step - loss: 0.1284 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "336/336 [==============================] - 0s 535us/step - loss: 0.0555 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "336/336 [==============================] - 0s 535us/step - loss: 0.0274 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "336/336 [==============================] - 0s 544us/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "336/336 [==============================] - 0s 536us/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "336/336 [==============================] - 0s 535us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "336/336 [==============================] - 0s 534us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "336/336 [==============================] - 0s 537us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "336/336 [==============================] - 0s 548us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "336/336 [==============================] - 0s 534us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "336/336 [==============================] - 0s 536us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "336/336 [==============================] - 0s 533us/step - loss: 8.7145e-04 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "336/336 [==============================] - 0s 537us/step - loss: 6.7264e-04 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "336/336 [==============================] - 0s 536us/step - loss: 5.2142e-04 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "336/336 [==============================] - 0s 544us/step - loss: 3.9855e-04 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "336/336 [==============================] - 0s 538us/step - loss: 3.1263e-04 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "336/336 [==============================] - 0s 534us/step - loss: 2.4789e-04 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "336/336 [==============================] - 0s 539us/step - loss: 1.9808e-04 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "336/336 [==============================] - 0s 537us/step - loss: 1.6045e-04 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "336/336 [==============================] - 0s 542us/step - loss: 1.3051e-04 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "336/336 [==============================] - 0s 543us/step - loss: 1.0637e-04 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "336/336 [==============================] - 0s 534us/step - loss: 8.7544e-05 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "336/336 [==============================] - 0s 534us/step - loss: 7.1476e-05 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "336/336 [==============================] - 0s 532us/step - loss: 5.9510e-05 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "336/336 [==============================] - 0s 533us/step - loss: 4.9110e-05 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "336/336 [==============================] - 0s 532us/step - loss: 4.0838e-05 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "336/336 [==============================] - 0s 531us/step - loss: 3.3697e-05 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "336/336 [==============================] - 0s 527us/step - loss: 2.8562e-05 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "336/336 [==============================] - 0s 537us/step - loss: 2.3620e-05 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "336/336 [==============================] - 0s 533us/step - loss: 1.9807e-05 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "336/336 [==============================] - 0s 538us/step - loss: 1.6505e-05 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "336/336 [==============================] - 0s 534us/step - loss: 1.3753e-05 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "336/336 [==============================] - 0s 532us/step - loss: 1.1568e-05 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "336/336 [==============================] - 0s 534us/step - loss: 9.7797e-06 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "336/336 [==============================] - 0s 536us/step - loss: 8.1299e-06 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "336/336 [==============================] - 0s 537us/step - loss: 6.8485e-06 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "336/336 [==============================] - 0s 535us/step - loss: 5.7991e-06 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "336/336 [==============================] - 0s 533us/step - loss: 4.9032e-06 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "336/336 [==============================] - 0s 533us/step - loss: 4.0024e-06 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "336/336 [==============================] - 0s 534us/step - loss: 3.4330e-06 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "336/336 [==============================] - 0s 534us/step - loss: 2.8206e-06 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "336/336 [==============================] - 0s 556us/step - loss: 2.4727e-06 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "336/336 [==============================] - 0s 535us/step - loss: 2.0882e-06 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "336/336 [==============================] - 0s 542us/step - loss: 1.7448e-06 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "336/336 [==============================] - 0s 532us/step - loss: 1.4546e-06 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "336/336 [==============================] - 0s 537us/step - loss: 1.2268e-06 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "336/336 [==============================] - 0s 539us/step - loss: 1.0289e-06 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "336/336 [==============================] - 0s 542us/step - loss: 8.8545e-07 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "336/336 [==============================] - 0s 535us/step - loss: 7.4978e-07 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "336/336 [==============================] - 0s 539us/step - loss: 6.2844e-07 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "336/336 [==============================] - 0s 534us/step - loss: 5.1665e-07 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "336/336 [==============================] - 0s 537us/step - loss: 4.4560e-07 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "336/336 [==============================] - 0s 532us/step - loss: 3.8808e-07 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "336/336 [==============================] - 0s 531us/step - loss: 3.3314e-07 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "336/336 [==============================] - 0s 533us/step - loss: 2.7749e-07 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "336/336 [==============================] - 0s 528us/step - loss: 2.3998e-07 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "336/336 [==============================] - 0s 491us/step - loss: 2.0484e-07 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "336/336 [==============================] - 0s 365us/step - loss: 1.7550e-07 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.4515e-07 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "336/336 [==============================] - 0s 359us/step - loss: 1.2329e-07 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 1.0821e-07 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 9.0463e-08 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 8.0561e-08 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 6.7891e-08 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 5.9835e-08 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 5.1566e-08 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 4.3084e-08 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 3.9855e-08 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "336/336 [==============================] - 0s 359us/step - loss: 3.3786e-08 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 3.0556e-08 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "336/336 [==============================] - 0s 359us/step - loss: 2.8995e-08 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 2.6156e-08 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 2.1720e-08 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.9377e-08 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 1.8064e-08 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "336/336 [==============================] - 0s 371us/step - loss: 1.5899e-08 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "336/336 [==============================] - 0s 358us/step - loss: 1.5048e-08 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 1.3060e-08 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 1.2102e-08 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 1.1357e-08 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.0115e-08 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.0647e-08 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 9.0143e-09 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 7.8077e-09 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "336/336 [==============================] - 0s 359us/step - loss: 7.5948e-09 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "336/336 [==============================] - 0s 360us/step - loss: 7.2753e-09 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 6.8495e-09 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "336/336 [==============================] - 0s 357us/step - loss: 6.3881e-09 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "336/336 [==============================] - 0s 360us/step - loss: 5.8913e-09 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 5.4299e-09 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 5.2879e-09 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 4.8976e-09 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "336/336 [==============================] - 0s 358us/step - loss: 4.7911e-09 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "336/336 [==============================] - 0s 360us/step - loss: 4.5427e-09 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 4.0813e-09 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "336/336 [==============================] - 0s 358us/step - loss: 4.0813e-09 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 3.8329e-09 - accuracy: 1.0000\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 150)               9150      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 13,263\n",
      "Trainable params: 13,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 1.0355 - accuracy: 0.4314\n",
      "Epoch 2/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 0.7441 - accuracy: 0.9783\n",
      "Epoch 3/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 0.3654 - accuracy: 0.9991\n",
      "Epoch 4/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 0.1417 - accuracy: 0.9994\n",
      "Epoch 5/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 0.0546 - accuracy: 0.9997\n",
      "Epoch 6/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "336/336 [==============================] - 0s 364us/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "336/336 [==============================] - 0s 362us/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "336/336 [==============================] - 0s 360us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 9.2284e-04 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "336/336 [==============================] - 0s 386us/step - loss: 7.2379e-04 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "336/336 [==============================] - 0s 369us/step - loss: 5.7699e-04 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "336/336 [==============================] - 0s 361us/step - loss: 4.6099e-04 - accuracy: 1.0000\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 0s 372us/step - loss: 3.7209e-04 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 3.0262e-04 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "336/336 [==============================] - 0s 375us/step - loss: 2.4464e-04 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 2.0017e-04 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "336/336 [==============================] - 0s 357us/step - loss: 1.6329e-04 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "336/336 [==============================] - 0s 363us/step - loss: 1.3481e-04 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 1.1073e-04 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "336/336 [==============================] - 0s 357us/step - loss: 9.1328e-05 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 7.5424e-05 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 6.2686e-05 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 5.1537e-05 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 4.3067e-05 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 3.5851e-05 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 2.9663e-05 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "336/336 [==============================] - 0s 346us/step - loss: 2.4664e-05 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 2.0395e-05 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 1.7164e-05 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 1.4088e-05 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 1.1899e-05 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 9.7657e-06 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "336/336 [==============================] - 0s 346us/step - loss: 7.9415e-06 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "336/336 [==============================] - 0s 347us/step - loss: 6.8024e-06 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 5.5991e-06 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 4.6354e-06 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 3.9253e-06 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 3.2532e-06 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 2.7292e-06 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 2.3050e-06 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 1.9473e-06 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.6336e-06 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.3648e-06 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "336/336 [==============================] - 0s 365us/step - loss: 1.1326e-06 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 9.8443e-07 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 8.0649e-07 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 6.7763e-07 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 5.8103e-07 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 4.9255e-07 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 4.1466e-07 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 3.5291e-07 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "336/336 [==============================] - 0s 358us/step - loss: 2.8651e-07 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 2.5155e-07 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "336/336 [==============================] - 0s 363us/step - loss: 2.1308e-07 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "336/336 [==============================] - 0s 357us/step - loss: 1.8078e-07 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 1.5498e-07 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "336/336 [==============================] - 0s 364us/step - loss: 1.2592e-07 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.1257e-07 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 9.3053e-08 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 8.1058e-08 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 6.8388e-08 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 6.0013e-08 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 5.1566e-08 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 4.5391e-08 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 3.7974e-08 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 3.5383e-08 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 3.1444e-08 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 2.7256e-08 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 2.4772e-08 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 2.1791e-08 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 1.9732e-08 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 1.7248e-08 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 1.5651e-08 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.4480e-08 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 1.3344e-08 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 1.1250e-08 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 1.1995e-08 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "336/336 [==============================] - 0s 356us/step - loss: 1.0115e-08 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 8.4820e-09 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 8.7304e-09 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 7.3463e-09 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 7.7012e-09 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "336/336 [==============================] - 0s 363us/step - loss: 6.4946e-09 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "336/336 [==============================] - 0s 359us/step - loss: 6.3881e-09 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "336/336 [==============================] - 0s 349us/step - loss: 6.1752e-09 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 5.7848e-09 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 5.6428e-09 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "336/336 [==============================] - 0s 350us/step - loss: 5.2170e-09 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 4.9685e-09 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "336/336 [==============================] - 0s 355us/step - loss: 4.5072e-09 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "336/336 [==============================] - 0s 352us/step - loss: 4.4007e-09 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "336/336 [==============================] - 0s 354us/step - loss: 4.0458e-09 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "336/336 [==============================] - 0s 351us/step - loss: 3.9393e-09 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "336/336 [==============================] - 0s 353us/step - loss: 3.5490e-09 - accuracy: 1.0000\n",
      "Average score of Cross Validation: 1.0\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 10\n",
    "num_folds = 10\n",
    "\n",
    "kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "cv_results = np.array([])\n",
    "for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "    X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "    X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "    model = getNetwork()\n",
    "    model.fit(X_cross_train, y_cross_train, epochs=EPOCHS, batch_size=BATCH_SIZE)  \n",
    "    y_pred = model.predict(X_cross_test)\n",
    "    predictions_categorical = np.argmax(y_pred, axis=1)\n",
    "    f1s = f1_score(y_cross_test, predictions_categorical, average=\"weighted\")\n",
    "    cv_results = np.append(cv_results, [f1s])\n",
    "\n",
    "print(f'Average score of Cross Validation: {cv_results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 150)               9150      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 13,263\n",
      "Trainable params: 13,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "280/280 [==============================] - 0s 614us/step - loss: 0.9995 - accuracy: 0.5634 - val_loss: 0.8942 - val_accuracy: 0.6956\n",
      "Epoch 2/100\n",
      "280/280 [==============================] - 0s 452us/step - loss: 0.7557 - accuracy: 0.9603 - val_loss: 0.5893 - val_accuracy: 0.9989\n",
      "Epoch 3/100\n",
      "280/280 [==============================] - 0s 446us/step - loss: 0.4387 - accuracy: 0.9996 - val_loss: 0.2991 - val_accuracy: 0.9989\n",
      "Epoch 4/100\n",
      "280/280 [==============================] - 0s 591us/step - loss: 0.2062 - accuracy: 1.0000 - val_loss: 0.1345 - val_accuracy: 0.9989\n",
      "Epoch 5/100\n",
      "280/280 [==============================] - 0s 446us/step - loss: 0.0924 - accuracy: 1.0000 - val_loss: 0.0617 - val_accuracy: 0.9989\n",
      "Epoch 6/100\n",
      "280/280 [==============================] - 0s 440us/step - loss: 0.0458 - accuracy: 1.0000 - val_loss: 0.0331 - val_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "280/280 [==============================] - 0s 463us/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 0.0200 - val_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "280/280 [==============================] - 0s 466us/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "280/280 [==============================] - 0s 463us/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "280/280 [==============================] - 0s 453us/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "280/280 [==============================] - 0s 461us/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "280/280 [==============================] - 0s 462us/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "280/280 [==============================] - 0s 461us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "280/280 [==============================] - 0s 446us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "280/280 [==============================] - 0s 443us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "280/280 [==============================] - 0s 443us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "280/280 [==============================] - 0s 444us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "280/280 [==============================] - 0s 441us/step - loss: 9.7981e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "280/280 [==============================] - 0s 445us/step - loss: 7.9692e-04 - accuracy: 1.0000 - val_loss: 8.7526e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "280/280 [==============================] - 0s 445us/step - loss: 6.5532e-04 - accuracy: 1.0000 - val_loss: 7.2388e-04 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "280/280 [==============================] - 0s 444us/step - loss: 5.3929e-04 - accuracy: 1.0000 - val_loss: 5.9396e-04 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "280/280 [==============================] - 0s 441us/step - loss: 4.4524e-04 - accuracy: 1.0000 - val_loss: 5.2750e-04 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "280/280 [==============================] - 0s 445us/step - loss: 3.7094e-04 - accuracy: 1.0000 - val_loss: 4.3456e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "280/280 [==============================] - 0s 461us/step - loss: 3.0963e-04 - accuracy: 1.0000 - val_loss: 3.9997e-04 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "280/280 [==============================] - 0s 443us/step - loss: 2.6090e-04 - accuracy: 1.0000 - val_loss: 3.2580e-04 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "280/280 [==============================] - 0s 443us/step - loss: 2.1859e-04 - accuracy: 1.0000 - val_loss: 2.7756e-04 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "280/280 [==============================] - 0s 445us/step - loss: 1.8085e-04 - accuracy: 1.0000 - val_loss: 2.5028e-04 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "280/280 [==============================] - 0s 448us/step - loss: 1.5106e-04 - accuracy: 1.0000 - val_loss: 2.1304e-04 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "280/280 [==============================] - 0s 442us/step - loss: 1.2680e-04 - accuracy: 1.0000 - val_loss: 1.8364e-04 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "280/280 [==============================] - 0s 448us/step - loss: 1.0805e-04 - accuracy: 1.0000 - val_loss: 1.6249e-04 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "280/280 [==============================] - 0s 438us/step - loss: 8.9755e-05 - accuracy: 1.0000 - val_loss: 1.4206e-04 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "280/280 [==============================] - 0s 443us/step - loss: 7.6938e-05 - accuracy: 1.0000 - val_loss: 1.2673e-04 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "280/280 [==============================] - 0s 444us/step - loss: 6.4920e-05 - accuracy: 1.0000 - val_loss: 1.1630e-04 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "280/280 [==============================] - 0s 444us/step - loss: 5.4826e-05 - accuracy: 1.0000 - val_loss: 9.7102e-05 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "280/280 [==============================] - 0s 442us/step - loss: 4.7653e-05 - accuracy: 1.0000 - val_loss: 8.1368e-05 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "280/280 [==============================] - 0s 458us/step - loss: 4.0267e-05 - accuracy: 1.0000 - val_loss: 8.4620e-05 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "280/280 [==============================] - 0s 446us/step - loss: 3.4384e-05 - accuracy: 1.0000 - val_loss: 7.1248e-05 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "280/280 [==============================] - 0s 439us/step - loss: 2.9468e-05 - accuracy: 1.0000 - val_loss: 5.9933e-05 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "280/280 [==============================] - 0s 466us/step - loss: 2.5070e-05 - accuracy: 1.0000 - val_loss: 6.0939e-05 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "280/280 [==============================] - 0s 448us/step - loss: 2.1629e-05 - accuracy: 1.0000 - val_loss: 5.0111e-05 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "280/280 [==============================] - 0s 448us/step - loss: 1.8336e-05 - accuracy: 1.0000 - val_loss: 4.9305e-05 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "280/280 [==============================] - 0s 444us/step - loss: 1.5850e-05 - accuracy: 1.0000 - val_loss: 4.3591e-05 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "280/280 [==============================] - 0s 449us/step - loss: 1.3598e-05 - accuracy: 1.0000 - val_loss: 3.8134e-05 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "280/280 [==============================] - 0s 452us/step - loss: 1.1567e-05 - accuracy: 1.0000 - val_loss: 3.1518e-05 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "280/280 [==============================] - 0s 444us/step - loss: 9.9674e-06 - accuracy: 1.0000 - val_loss: 3.0720e-05 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "280/280 [==============================] - 0s 441us/step - loss: 8.5159e-06 - accuracy: 1.0000 - val_loss: 3.1618e-05 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "280/280 [==============================] - 0s 445us/step - loss: 7.3542e-06 - accuracy: 1.0000 - val_loss: 2.7752e-05 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "280/280 [==============================] - 0s 442us/step - loss: 6.1710e-06 - accuracy: 1.0000 - val_loss: 2.9476e-05 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "280/280 [==============================] - 0s 445us/step - loss: 5.6172e-06 - accuracy: 1.0000 - val_loss: 2.0038e-05 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "280/280 [==============================] - 0s 445us/step - loss: 4.8345e-06 - accuracy: 1.0000 - val_loss: 1.7746e-05 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "280/280 [==============================] - 0s 441us/step - loss: 4.1428e-06 - accuracy: 1.0000 - val_loss: 1.7736e-05 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "280/280 [==============================] - 0s 451us/step - loss: 3.5104e-06 - accuracy: 1.0000 - val_loss: 1.8723e-05 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "280/280 [==============================] - 0s 441us/step - loss: 3.0073e-06 - accuracy: 1.0000 - val_loss: 1.2718e-05 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "280/280 [==============================] - 0s 446us/step - loss: 2.5582e-06 - accuracy: 1.0000 - val_loss: 1.5124e-05 - val_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "280/280 [==============================] - 0s 454us/step - loss: 2.2724e-06 - accuracy: 1.0000 - val_loss: 1.2555e-05 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "280/280 [==============================] - 0s 447us/step - loss: 2.0307e-06 - accuracy: 1.0000 - val_loss: 1.1161e-05 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "280/280 [==============================] - 0s 442us/step - loss: 1.7628e-06 - accuracy: 1.0000 - val_loss: 1.0050e-05 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "280/280 [==============================] - 0s 443us/step - loss: 1.4851e-06 - accuracy: 1.0000 - val_loss: 9.2968e-06 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "280/280 [==============================] - 0s 444us/step - loss: 1.2753e-06 - accuracy: 1.0000 - val_loss: 7.4569e-06 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "280/280 [==============================] - 0s 449us/step - loss: 1.1354e-06 - accuracy: 1.0000 - val_loss: 7.8534e-06 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "280/280 [==============================] - 0s 441us/step - loss: 9.7657e-07 - accuracy: 1.0000 - val_loss: 6.8676e-06 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "280/280 [==============================] - 0s 435us/step - loss: 8.3492e-07 - accuracy: 1.0000 - val_loss: 7.1128e-06 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "280/280 [==============================] - 0s 444us/step - loss: 7.3820e-07 - accuracy: 1.0000 - val_loss: 7.2806e-06 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "280/280 [==============================] - 0s 442us/step - loss: 6.1508e-07 - accuracy: 1.0000 - val_loss: 5.8045e-06 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "280/280 [==============================] - 0s 443us/step - loss: 5.4268e-07 - accuracy: 1.0000 - val_loss: 6.6577e-06 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "280/280 [==============================] - 0s 438us/step - loss: 4.8049e-07 - accuracy: 1.0000 - val_loss: 6.3097e-06 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "280/280 [==============================] - 0s 441us/step - loss: 4.2304e-07 - accuracy: 1.0000 - val_loss: 3.8161e-06 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "280/280 [==============================] - 0s 451us/step - loss: 3.5567e-07 - accuracy: 1.0000 - val_loss: 3.2541e-06 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "280/280 [==============================] - 0s 443us/step - loss: 3.1550e-07 - accuracy: 1.0000 - val_loss: 3.7937e-06 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "280/280 [==============================] - 0s 446us/step - loss: 2.6797e-07 - accuracy: 1.0000 - val_loss: 2.8405e-06 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "280/280 [==============================] - 0s 444us/step - loss: 2.4557e-07 - accuracy: 1.0000 - val_loss: 2.7750e-06 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "280/280 [==============================] - 0s 442us/step - loss: 2.0992e-07 - accuracy: 1.0000 - val_loss: 3.5159e-06 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "280/280 [==============================] - 0s 442us/step - loss: 1.8088e-07 - accuracy: 1.0000 - val_loss: 3.1831e-06 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "280/280 [==============================] - 0s 443us/step - loss: 1.6201e-07 - accuracy: 1.0000 - val_loss: 2.9082e-06 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "280/280 [==============================] - 0s 440us/step - loss: 1.4089e-07 - accuracy: 1.0000 - val_loss: 2.3567e-06 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "280/280 [==============================] - 0s 448us/step - loss: 1.2866e-07 - accuracy: 1.0000 - val_loss: 2.3549e-06 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "280/280 [==============================] - 0s 440us/step - loss: 1.0741e-07 - accuracy: 1.0000 - val_loss: 2.2569e-06 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "280/280 [==============================] - 0s 476us/step - loss: 9.1227e-08 - accuracy: 1.0000 - val_loss: 2.2155e-06 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "280/280 [==============================] - 0s 442us/step - loss: 7.9600e-08 - accuracy: 1.0000 - val_loss: 1.4377e-06 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "280/280 [==============================] - 0s 444us/step - loss: 7.5384e-08 - accuracy: 1.0000 - val_loss: 2.0115e-06 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "280/280 [==============================] - 0s 444us/step - loss: 6.2139e-08 - accuracy: 1.0000 - val_loss: 1.9620e-06 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "280/280 [==============================] - 0s 440us/step - loss: 5.7283e-08 - accuracy: 1.0000 - val_loss: 1.2317e-06 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "280/280 [==============================] - 0s 445us/step - loss: 5.1747e-08 - accuracy: 1.0000 - val_loss: 1.8045e-06 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "280/280 [==============================] - 0s 449us/step - loss: 4.5145e-08 - accuracy: 1.0000 - val_loss: 1.8200e-06 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "280/280 [==============================] - 0s 441us/step - loss: 3.9396e-08 - accuracy: 1.0000 - val_loss: 1.2396e-06 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "280/280 [==============================] - 0s 447us/step - loss: 3.6713e-08 - accuracy: 1.0000 - val_loss: 1.2387e-06 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "280/280 [==============================] - 0s 441us/step - loss: 3.2070e-08 - accuracy: 1.0000 - val_loss: 1.6802e-06 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "280/280 [==============================] - 0s 444us/step - loss: 2.9770e-08 - accuracy: 1.0000 - val_loss: 1.1645e-06 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "280/280 [==============================] - 0s 442us/step - loss: 2.7939e-08 - accuracy: 1.0000 - val_loss: 1.0684e-06 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "280/280 [==============================] - 0s 442us/step - loss: 2.4106e-08 - accuracy: 1.0000 - val_loss: 1.2853e-06 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "280/280 [==============================] - 0s 444us/step - loss: 2.1338e-08 - accuracy: 1.0000 - val_loss: 8.8574e-07 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "280/280 [==============================] - 0s 450us/step - loss: 2.1210e-08 - accuracy: 1.0000 - val_loss: 8.7105e-07 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "280/280 [==============================] - 0s 442us/step - loss: 1.8782e-08 - accuracy: 1.0000 - val_loss: 6.9039e-07 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "280/280 [==============================] - 0s 445us/step - loss: 1.7121e-08 - accuracy: 1.0000 - val_loss: 9.2582e-07 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "280/280 [==============================] - 0s 450us/step - loss: 1.4992e-08 - accuracy: 1.0000 - val_loss: 5.9027e-07 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "280/280 [==============================] - 0s 450us/step - loss: 1.4949e-08 - accuracy: 1.0000 - val_loss: 9.2161e-07 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "280/280 [==============================] - 0s 443us/step - loss: 1.2436e-08 - accuracy: 1.0000 - val_loss: 1.2928e-06 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "280/280 [==============================] - 0s 445us/step - loss: 1.2947e-08 - accuracy: 1.0000 - val_loss: 7.8972e-07 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "280/280 [==============================] - 0s 439us/step - loss: 1.1329e-08 - accuracy: 1.0000 - val_loss: 1.0360e-06 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "280/280 [==============================] - 0s 444us/step - loss: 1.0988e-08 - accuracy: 1.0000 - val_loss: 9.3258e-07 - val_accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       292\n",
      "           1       1.00      1.00      1.00       282\n",
      "           2       1.00      1.00      1.00       360\n",
      "\n",
      "    accuracy                           1.00       934\n",
      "   macro avg       1.00      1.00      1.00       934\n",
      "weighted avg       1.00      1.00      1.00       934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 10\n",
    "model = getNetwork()\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.25)\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "report = classification_report(y_test, pred)\n",
    "classification_report_csv(report, \"NN\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Models in C code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpkophdomo/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpkophdomo/assets\n"
     ]
    }
   ],
   "source": [
    "# Neural network with TinyMLGen\n",
    "with open(tasks[choosenIndex] + '/exportedModels/NNmodel.h', 'w') as f:\n",
    "    f.write(tiny.port(model, optimize=False))\n",
    "\n",
    "# Classifiers with MicroMLGen\n",
    "for name, model in models:\n",
    "    prepath = tasks[choosenIndex] + '/exportedModels/'\n",
    "    path = prepath + name + '.h'\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(port(model, optimize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione Inferance Rate medio (Intensit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = read_csv(\"InfTimeReport.csv\")\n",
    "g = sbs.barplot(x=csv['Algoritmo'], y=csv['InfTime'])\n",
    "g.set_yscale(\"log\")\n",
    "plt.ylabel(\"Inference Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoria occupata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = read_csv(\"MemOccupationReport.csv\")\n",
    "g = sbs.barplot(x=csv['Algoritmo'], y=csv['MemOccupata'])\n",
    "g.set_yscale('log')\n",
    "plt.ylabel(\"MemOccupata in Byte\")\n",
    "plt.show()\n",
    "# SVC in overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
