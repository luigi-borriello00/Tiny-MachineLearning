{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Comparison for TinyML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "import pickle\n",
    "from pandas import read_csv\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,  classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold,StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dense, Input, concatenate, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from micromlgen import port\n",
    "import tinymlgen as tiny\n",
    "\n",
    "import warnings\n",
    "import seaborn as sbs\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['../data/X-intensity.pkl', '../data/X-all.pkl', '../data/X-10-25.pkl', '../data/X-1-2.pkl', '../data/X-25_50-50_25.pkl']\n",
    "labels = ['../data/y-intensity.pkl', '../data/y-all.pkl', '../data/y-10-25.pkl', '../data/y-1-2.pkl', '../data/y-25_50-50_25.pkl']\n",
    "choosenIndex = 1\n",
    "tasks = ['intensity', 'all','10-25','1-2', '25-50']\n",
    "with open(data[choosenIndex], 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open(labels[choosenIndex], 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if choosenIndex == 1:\n",
    "    X = X[165:-13]\n",
    "    y = y[165:-13]\n",
    "if choosenIndex == 2:\n",
    "    X = X[146:-13]\n",
    "    y = y[146:-13]\n",
    "if choosenIndex == 3:\n",
    "    X = X[101:-13]\n",
    "    y = y[101:-13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8329, 60)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)\n",
    "uniques = np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 3 0 3 0 4 0 5 3 1 2 0 5 1 0 1 5 1 0 2 5 5 5 5 5 3 2 0 4 0 0 1 3 2 4 2 2\n",
      " 3 5 3 1 1 3 5 3 3 5 5 1 3 5 0 2 2 3 3 1 0 4 0 0 0 1 0 5 4 0 0 1 4 0 3 3 4\n",
      " 5 4 3 0 4 5 0 3 0 1 0 1 1 0 4 2 0 5 4 3 2 3 4 0 0 0 3 1 2 0 2 0 4 5 4 3 2\n",
      " 5 3 1 4 2 3 0 1 1 5 5 5 1 3 2 4 3 5 4 2 2 2 3 4 0 5 0 5 4 3 3 3 1 0 5 1 4\n",
      " 2 5 1 3 1 3 4 1 0 0 3 5 5 4 4 1 0 1 5 4 2 3 5 2 2 5 2 1 5 5 3 1 3 1 0 5 5\n",
      " 4 0 1 3 0 2 2 1 5 3 2 3 4 3 3 3 5 3 2 4 0 1 3 1 0 4 0 4 5 5 1 2 5 3 2 1 0\n",
      " 2 3 3 3 2 5 4 0 2 5 4 1 2 3 5 1 4 1 3 3 3 1 3 5 4 4 0 4 2 1 4 1 2 4 3 5 4\n",
      " 4 3 3 4 0 3 4 1 5 2 4 4 0 3 4 3 2 5 3 5 5 2 2 0 3 2 3 3 1 2 3 5 0 0 3 1 2\n",
      " 4 5 1 5 1 3 5 0 3 2 1 2 5 5 2 5 3 2 4 4 1 5 3 5 5 0 3 2 4 2 4 4 3 1 4 2 3\n",
      " 0 5 1 2 5 1 3 4 4 5 1 2 4 2 5 3 2 5 1 5 3 5 4 5 5 1 0 0 3 5 2 5 0 0 0 3 0\n",
      " 1 3 5 1 4 5 4 1 3 4 4 4 5 2 3 4 4 4 3 5 4 2 5 2 2 3 3 3 4 1 2 4 1 4 5 2 0\n",
      " 0 3 4 5 0 1 5 5 1 5 1 5 2 5 4 2 1 4 3 1 3 2 3 3 5 1 5 1 5 1 5 1 5 3 1 0 1\n",
      " 5 3 1 3 3 2 5 5 3 4 5 3 0 2 0 1 0 0 5 3 2 1 1 5 4 4 1 0 5 2 0 4 2 1 4 3 5\n",
      " 0 1 4 2 1 1 1 5 3 2 1 2 1 0 4 0 1 2 5 3 0 1 5 1 0 1 3 3 3 2 5 3 5 2 3 0 5\n",
      " 4 1 0 4 1 2 5 2 0 3 5 1 5 0 3 2 2 0 5 5 2 3 5 1 1 4 3 3 5 1 0 5 2 4 4 4 1\n",
      " 4 3 4 2 2 0 1 0 2 2 2 4 3 5 5 4 2 3 5 0 0 4 1 3 2 1 0 0 4 1 3 3 5 2 5 3 4\n",
      " 4 3 3 5 4 1 1 0 3 1 2 3 0 0 3 4 2 3 2 2 0 4 4 5 3 5 1 3 4 5 4 5 4 0 4 5 4\n",
      " 1 0 5 4 3 5 0 5 2 3 5 3 2 2 3 5 5 5 0 2 4 3 1 3 0 4 5 0 3 3 3 1 5 1 1 4 5\n",
      " 3 0 4 4 4 5 5 5 2 5 4 1 1 3 4 0 3 1 3 1 5 4 4 5 4 4 1 0 0 5 0 5 2 1 3 2 2\n",
      " 0 5 5 1 1 0 4 5 5 1 3 1 1 4 1 4 4 2 3 4 0 2 3 2 5 2 5 0 3 5 0 0 3 3 0 4 3\n",
      " 0 5 4 0 4 5 1 5 4 3 5 5 4 1 3 4 0 0 0 3 2 0 3 5 3 5 4 5 4 5 2 0 5 1 0 2 0\n",
      " 4 1 5 0 2 0 4 2 1 2 0 5 1 1 3 5 4 5 4 5 0 5 4 5 5 1 2 2 3 4 1 1 1 5 4 4 0\n",
      " 5 4 2 2 2 0 1 1 5 4 2 2 4 1 3 2 5 3 5]\n"
     ]
    }
   ],
   "source": [
    "y_test = y_test[:half]\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(833, 60)\n",
      "[[ 0.03  0.02  0.99 -0.03 -0.01  0.96 -0.   -0.01  0.96  0.05  0.01  1.\n",
      "   0.02  0.03  1.03 -0.01 -0.02  0.96  0.02  0.    0.98  0.02  0.01  0.98\n",
      "   0.04 -0.02  0.92  0.07  0.02  1.01  0.08  0.02  0.97 -0.03 -0.01  0.97\n",
      "   0.08 -0.    0.9   0.04  0.02  1.01  0.02  0.01  0.98  0.02  0.01  0.99\n",
      "   0.09  0.01  0.94  0.08  0.01  0.96  0.02  0.01  1.01  0.1   0.03  0.99]\n",
      " [ 0.03  0.11  0.97  0.02  0.11  1.    0.01  0.09  1.01  0.02  0.08  1.04\n",
      "  -0.04  0.06  0.98 -0.04  0.11  0.94 -0.    0.11  1.02 -0.08  0.04  0.99\n",
      "  -0.03  0.07  0.98  0.05  0.15  0.99 -0.    0.09  1.01  0.04  0.13  1.03\n",
      "  -0.03  0.07  0.99 -0.07  0.05  0.96  0.    0.08  0.96  0.03  0.14  0.96\n",
      "   0.03  0.12  0.96  0.02  0.06  1.01 -0.02  0.09  0.98  0.01  0.13  0.99]\n",
      " [-0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.99 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98]\n",
      " [-0.03  0.1   0.95 -0.    0.14  0.98 -0.    0.11  1.   -0.01  0.08  0.95\n",
      "   0.03  0.1   0.99  0.03  0.08  1.01 -0.03  0.06  1.   -0.03  0.04  1.\n",
      "   0.01  0.1   1.02 -0.02  0.06  1.04  0.02  0.11  0.95 -0.01  0.15  0.95\n",
      "  -0.01  0.11  1.01 -0.08  0.1   0.99  0.03  0.11  0.98  0.06  0.1   1.01\n",
      "  -0.    0.11  1.01  0.02  0.11  1.04 -0.05  0.06  0.99  0.05  0.09  1.  ]\n",
      " [-0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.01  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98]\n",
      " [-0.06 -0.    0.98 -0.06  0.    0.99 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.06 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.05  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98]\n",
      " [ 0.02 -0.01  1.    0.09  0.    0.98  0.05  0.01  1.02 -0.01 -0.02  0.94\n",
      "   0.06  0.01  1.    0.04 -0.04  0.97  0.02 -0.01  0.98  0.02 -0.    0.97\n",
      "   0.02  0.03  1.   -0.05 -0.02  0.95  0.03 -0.01  0.95 -0.02 -0.02  0.95\n",
      "   0.03  0.02  0.99  0.02 -0.04  0.99  0.02  0.    0.99  0.02 -0.    0.99\n",
      "  -0.06 -0.    0.98  0.02 -0.    1.    0.05  0.01  1.03 -0.01 -0.02  0.95]\n",
      " [ 0.02  0.12  0.99  0.01  0.11  0.95  0.02  0.08  1.    0.02  0.08  1.03\n",
      "   0.01  0.08  0.98 -0.03  0.05  1.    0.    0.1   1.02 -0.07  0.03  0.99\n",
      "  -0.04  0.06  1.    0.03  0.16  0.97 -0.01  0.1   1.    0.01  0.18  1.\n",
      "   0.04  0.11  0.97  0.02  0.05  1.01 -0.01  0.1   1.   -0.04  0.04  0.99\n",
      "   0.02  0.11  0.98 -0.04  0.05  0.98 -0.01  0.07  0.97  0.02  0.14  0.97]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98]\n",
      " [-0.06  0.01  0.99 -0.04 -0.04  0.98 -0.05  0.    0.99 -0.09 -0.    0.99\n",
      "  -0.06 -0.02  0.98 -0.07  0.01  0.99 -0.05 -0.04  0.97 -0.06 -0.01  0.98\n",
      "  -0.05 -0.04  0.97 -0.03 -0.05  0.97 -0.06 -0.02  0.98 -0.05 -0.04  0.98\n",
      "  -0.06 -0.02  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.    0.99\n",
      "  -0.06  0.01  0.99 -0.06 -0.    0.99 -0.06  0.01  0.98 -0.06 -0.    0.98]\n",
      " [-0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.01  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98]\n",
      " [ 0.07  0.01  1.01 -0.03 -0.02  0.95  0.04  0.01  1.03  0.05 -0.02  0.98\n",
      "   0.02 -0.06  0.98  0.02  0.01  0.98  0.02 -0.    0.98  0.08  0.03  0.98\n",
      "   0.01 -0.01  0.96  0.01 -0.02  0.94 -0.01 -0.02  0.94  0.09  0.02  0.99\n",
      "   0.02  0.01  0.99  0.02  0.    0.99  0.02 -0.    0.97  0.07  0.02  1.\n",
      "   0.01 -0.    1.    0.02 -0.01  0.94  0.08  0.01  1.    0.06  0.04  0.98]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97]\n",
      " [-0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.01  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98]\n",
      " [ 0.03  0.02  0.98  0.02  0.01  0.98  0.07 -0.04  0.86 -0.04 -0.    1.01\n",
      "  -0.01  0.01  1.02  0.09  0.02  0.98  0.    0.04  1.09  0.03  0.01  0.99\n",
      "   0.02  0.01  0.99  0.03  0.03  0.99  0.1   0.02  0.97  0.03 -0.    0.95\n",
      "   0.05  0.02  1.    0.1   0.02  0.96  0.04 -0.04  0.87  0.02  0.01  0.98\n",
      "   0.02  0.01  0.98 -0.    0.04  1.04  0.05  0.03  1.02  0.06  0.01  0.95]\n",
      " [-0.06 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.06 -0.02  0.99\n",
      "  -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.02  0.98]\n",
      " [-0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98]\n",
      " [-0.07  0.    0.99 -0.06  0.01  0.99 -0.06  0.01  0.99 -0.06 -0.03  0.98\n",
      "  -0.07  0.01  0.98 -0.06 -0.03  0.98 -0.06 -0.    0.98 -0.06  0.01  0.99\n",
      "  -0.05 -0.02  0.97 -0.06 -0.03  0.98 -0.05 -0.02  0.97 -0.05 -0.04  0.97\n",
      "  -0.05 -0.03  0.97 -0.05  0.01  1.   -0.04 -0.05  0.98 -0.05 -0.02  0.99\n",
      "  -0.05 -0.03  0.99 -0.05  0.01  0.99 -0.05 -0.02  0.99 -0.05  0.01  0.99]\n",
      " [-0.01 -0.02  0.95  0.01 -0.01  0.94  0.04  0.01  1.01  0.08  0.03  0.98\n",
      "   0.02 -0.    0.97  0.02 -0.    0.99  0.03  0.01  0.97  0.1   0.01  1.\n",
      "  -0.02 -0.01  0.97  0.07  0.01  0.99  0.04  0.01  0.99 -0.01 -0.05  0.99\n",
      "   0.03 -0.    0.98  0.02 -0.    0.99  0.06  0.04  0.96  0.03  0.    1.\n",
      "   0.07  0.01  1.02  0.06  0.    0.98 -0.01  0.01  0.98  0.02 -0.04  0.99]\n",
      " [ 0.03  0.03  1.01  0.02  0.01  0.98  0.02  0.    0.99  0.02 -0.04  0.86\n",
      "  -0.04 -0.01  0.99 -0.04 -0.    0.99  0.08  0.01  0.95  0.01 -0.04  0.9\n",
      "   0.02  0.03  0.98  0.02  0.01  0.98  0.02 -0.01  0.98  0.07  0.    0.94\n",
      "   0.05  0.    0.95 -0.01  0.01  1.01 -0.06 -0.01  1.01  0.02 -0.04  0.88\n",
      "   0.02  0.    0.98  0.02  0.    0.99  0.05 -0.    0.95  0.09  0.02  0.96]\n",
      " [ 0.02  0.01  0.97  0.02  0.01  0.98  0.02  0.01  0.98 -0.01  0.02  1.01\n",
      "   0.08  0.01  0.95  0.02  0.01  1.01  0.1   0.02  0.97  0.04 -0.03  0.87\n",
      "   0.02  0.01  0.98  0.02  0.01  0.98  0.02  0.03  1.02  0.1   0.02  0.96\n",
      "  -0.03 -0.01  0.98  0.01  0.01  1.01  0.04  0.04  1.03  0.02 -0.03  0.94\n",
      "   0.02  0.01  0.98  0.02  0.01  0.99 -0.01  0.05  1.11 -0.01 -0.01  0.96]\n",
      " [ 0.02  0.01  0.98  0.08  0.03  0.99  0.09  0.02  0.98  0.07  0.02  0.99\n",
      "  -0.05 -0.01  0.99  0.07  0.05  1.02  0.02  0.02  0.98  0.02  0.01  0.99\n",
      "   0.03  0.03  0.99 -0.06 -0.01  0.99 -0.02  0.01  1.02  0.03  0.    0.95\n",
      "   0.07  0.03  1.01 -0.01 -0.05  0.98  0.02  0.    0.98  0.02  0.    0.99\n",
      "   0.09  0.03  0.97  0.09  0.03  1.    0.04  0.    0.95 -0.03 -0.    1.  ]\n",
      " [ 0.02  0.01  0.98  0.02 -0.    0.99  0.06 -0.01  0.97 -0.04 -0.02  0.95\n",
      "   0.07  0.01  1.02  0.05 -0.    0.96  0.06  0.03  0.99  0.02 -0.    0.99\n",
      "   0.02 -0.01  0.98  0.02 -0.01  0.98  0.06 -0.01  0.99  0.08  0.01  1.02\n",
      "   0.03  0.    1.02 -0.04 -0.02  0.95  0.08 -0.02  0.97  0.02  0.01  0.97\n",
      "   0.02 -0.    0.97  0.   -0.04  0.99 -0.05 -0.02  0.96  0.05 -0.    0.97]\n",
      " [ 0.02  0.11  0.96 -0.03  0.06  1.    0.05  0.12  0.98  0.02  0.15  0.99\n",
      "  -0.04  0.08  1.   -0.    0.1   0.95  0.02  0.11  0.96  0.05  0.14  1.02\n",
      "  -0.04  0.06  1.   -0.02  0.04  1.    0.01  0.11  0.99  0.01  0.06  1.02\n",
      "   0.03  0.12  0.96 -0.05  0.11  0.95 -0.01  0.08  0.96 -0.08  0.07  0.98\n",
      "   0.01  0.09  0.98  0.04  0.15  0.98 -0.01  0.09  0.99  0.02  0.09  1.01]\n",
      " [-0.05 -0.    0.99 -0.03 -0.05  0.97 -0.06 -0.05  0.97 -0.05 -0.02  0.99\n",
      "  -0.05 -0.03  0.98 -0.07  0.02  0.99 -0.05 -0.03  0.98 -0.07  0.01  0.99\n",
      "  -0.04 -0.02  0.99 -0.04 -0.02  0.97 -0.06 -0.05  0.97 -0.04 -0.05  0.98\n",
      "  -0.05 -0.05  0.98 -0.05 -0.04  0.98 -0.05 -0.05  0.98 -0.06 -0.02  0.99\n",
      "  -0.06 -0.02  0.99 -0.06 -0.02  0.99 -0.07  0.01  0.98 -0.06 -0.02  0.98]\n",
      " [-0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.01  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.01  0.98]\n",
      " [-0.09  0.03  0.98 -0.04  0.    0.99 -0.07 -0.01  0.98 -0.07  0.03  0.99\n",
      "  -0.05  0.01  0.99 -0.06  0.01  0.98 -0.02  0.    0.99 -0.04 -0.01  0.98\n",
      "  -0.07 -0.01  0.98 -0.07  0.03  0.99 -0.06  0.    0.99 -0.03 -0.01  0.98\n",
      "  -0.07  0.01  0.99 -0.05 -0.01  0.98 -0.04 -0.02  0.98 -0.09 -0.02  0.97\n",
      "  -0.05 -0.    0.99 -0.1   0.03  0.99 -0.03  0.01  0.99 -0.07  0.    0.98]\n",
      " [-0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.03  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.04  0.04  0.99 -0.03  0.01  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.01  0.98]\n",
      " [-0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.06  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98]\n",
      " [-0.06 -0.02  0.99 -0.06 -0.02  0.99 -0.06 -0.02  0.99 -0.07 -0.02  0.98\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.01  0.98\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [ 0.02  0.17  1.    0.04  0.11  0.96  0.02  0.04  1.01 -0.01  0.11  1.\n",
      "  -0.06  0.04  1.    0.03  0.12  0.97 -0.01  0.05  1.   -0.    0.07  0.98\n",
      "  -0.    0.11  0.96 -0.03  0.06  1.   -0.01  0.06  1.   -0.01  0.06  0.97\n",
      "   0.01  0.13  0.96  0.03  0.13  0.98 -0.03  0.05  0.99 -0.04  0.05  0.99\n",
      "  -0.01  0.08  0.99 -0.04  0.04  0.98 -0.01  0.09  1.02 -0.07  0.04  0.93]\n",
      " [-0.06 -0.03  0.99 -0.07  0.01  0.99 -0.06  0.02  0.99 -0.03 -0.05  0.98\n",
      "  -0.05 -0.    0.99 -0.05 -0.03  0.99 -0.05 -0.06  0.98 -0.07  0.01  0.99\n",
      "  -0.05 -0.04  0.98 -0.05 -0.02  0.97 -0.05 -0.01  0.99 -0.04 -0.04  0.97\n",
      "  -0.05 -0.04  0.98 -0.06 -0.    1.   -0.06 -0.03  0.98 -0.06 -0.    0.98\n",
      "  -0.06 -0.04  0.98 -0.   -0.06  0.98 -0.05  0.02  0.99 -0.07  0.01  0.99]\n",
      " [-0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.99 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98]\n",
      " [-0.04 -0.05  0.98 -0.06 -0.03  0.99 -0.06 -0.02  0.99 -0.06 -0.02  0.99\n",
      "  -0.06 -0.03  0.99 -0.07  0.01  0.99 -0.06  0.02  0.99 -0.03 -0.05  0.98\n",
      "  -0.05 -0.    0.99 -0.05 -0.03  0.99 -0.05 -0.06  0.98 -0.07  0.01  0.99\n",
      "  -0.05 -0.04  0.98 -0.05 -0.02  0.97 -0.05 -0.01  0.99 -0.04 -0.04  0.97\n",
      "  -0.05 -0.04  0.98 -0.06 -0.    1.   -0.06 -0.03  0.98 -0.06 -0.    0.98]\n",
      " [-0.04 -0.05  0.98 -0.05 -0.04  0.98 -0.05 -0.04  0.99 -0.05 -0.05  0.98\n",
      "  -0.07 -0.01  0.99 -0.05 -0.04  0.97 -0.07  0.    0.98 -0.05 -0.04  0.97\n",
      "  -0.05 -0.03  0.97 -0.06 -0.03  0.98 -0.04 -0.04  0.98 -0.06 -0.04  0.97\n",
      "  -0.05 -0.03  0.99 -0.06 -0.04  0.97 -0.07  0.01  0.99 -0.05 -0.01  0.98\n",
      "  -0.06  0.    0.98 -0.05 -0.05  0.97 -0.04 -0.02  0.98 -0.05 -0.04  0.98]\n",
      " [-0.03  0.08  0.93  0.02  0.1   0.98 -0.02  0.11  0.95  0.01  0.09  0.98\n",
      "  -0.03  0.02  0.99 -0.04  0.05  0.98  0.04  0.1   1.    0.01  0.06  0.98\n",
      "  -0.03  0.06  0.97  0.03  0.13  0.98  0.01  0.1   1.01 -0.03  0.02  0.99\n",
      "  -0.01  0.08  0.99 -0.03  0.11  0.95 -0.02  0.08  1.   -0.05  0.07  0.91\n",
      "  -0.04  0.06  0.98  0.04  0.09  1.01 -0.01  0.09  0.98 -0.02 -0.    0.95]\n",
      " [ 0.08  0.01  1.02  0.01  0.    1.01  0.04  0.01  1.    0.03  0.05  0.99\n",
      "   0.02 -0.01  0.99  0.02 -0.    0.98  0.02 -0.03  0.98  0.09  0.01  1.\n",
      "  -0.03 -0.02  0.97  0.06  0.01  1.03 -0.04 -0.01  0.98  0.02  0.04  0.98\n",
      "   0.02 -0.01  0.99  0.02 -0.    0.99  0.1   0.    0.97 -0.03  0.01  1.02\n",
      "   0.05  0.01  1.03  0.07  0.    0.99  0.01  0.03  0.99  0.03  0.01  0.97]\n",
      " [ 0.04  0.12  0.97  0.03  0.11  0.99  0.    0.11  0.99  0.03  0.14  0.98\n",
      "   0.03  0.1   1.    0.03  0.1   1.    0.03  0.1   0.96  0.03  0.14  0.99\n",
      "  -0.    0.09  0.94  0.05  0.11  1.03 -0.04  0.06  0.99 -0.05  0.09  0.94\n",
      "  -0.    0.11  1.01 -0.05  0.13  0.97  0.02  0.1   0.96  0.06  0.12  1.\n",
      "  -0.01  0.1   1.    0.01  0.07  1.04  0.01  0.1   0.99 -0.05  0.05  0.98]\n",
      " [-0.07 -0.02  0.98 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.03  0.98 -0.06 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.    0.99\n",
      "  -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.02  0.98]\n",
      " [ 0.    0.07  0.98 -0.    0.09  1.   -0.01  0.08  0.95  0.03  0.09  1.\n",
      "   0.05  0.13  1.01 -0.01  0.07  0.96  0.03  0.13  0.98  0.02  0.11  0.99\n",
      "   0.    0.06  1.03 -0.03  0.07  0.99  0.01  0.15  0.96 -0.02  0.07  0.96\n",
      "   0.05  0.07  0.98  0.02  0.1   0.98 -0.07  0.06  0.95 -0.01  0.07  0.95\n",
      "  -0.05  0.08  0.94 -0.04  0.06  0.99  0.04  0.13  0.98  0.01  0.09  0.98]\n",
      " [-0.03 -0.02  0.96  0.05  0.01  1.03 -0.03 -0.03  0.97  0.   -0.03  1.\n",
      "   0.02 -0.01  0.98  0.03 -0.    0.98  0.09  0.01  0.97  0.06  0.    0.98\n",
      "   0.06  0.    0.97 -0.02 -0.02  0.94  0.04  0.04  0.98  0.03  0.01  0.97\n",
      "   0.02 -0.01  0.98  0.02 -0.01  1.    0.04  0.02  0.99 -0.   -0.02  0.94\n",
      "   0.03  0.01  1.03  0.07  0.01  1.01  0.06  0.01  0.97  0.02 -0.01  0.98]\n",
      " [-0.    0.12  0.96 -0.01  0.12  0.98 -0.01  0.06  0.96 -0.04  0.05  0.96\n",
      "   0.02  0.12  0.96  0.03  0.13  1.    0.03  0.11  0.97  0.02  0.11  1.\n",
      "   0.01  0.09  1.01  0.02  0.08  1.04 -0.04  0.06  0.98 -0.04  0.11  0.94\n",
      "  -0.    0.11  1.02 -0.08  0.04  0.99 -0.03  0.07  0.98  0.05  0.15  0.99\n",
      "  -0.    0.09  1.01  0.04  0.13  1.03 -0.03  0.07  0.99 -0.07  0.05  0.96]\n",
      " [-0.05  0.06  1.   -0.05  0.02  0.97 -0.    0.07  0.95 -0.02  0.09  0.91\n",
      "   0.01  0.09  0.98 -0.01  0.12  0.96  0.01  0.09  0.98  0.01  0.08  1.\n",
      "  -0.    0.09  1.    0.05  0.13  0.99  0.01  0.08  0.98 -0.01  0.1   1.\n",
      "  -0.02  0.08  0.95  0.03  0.11  0.97  0.04  0.11  1.01  0.01  0.08  0.95\n",
      "  -0.03  0.03  1.    0.01  0.1   1.02 -0.06  0.05  1.03  0.03  0.11  0.97]\n",
      " [-0.03 -0.02  0.96  0.02  0.01  0.98  0.03  0.01  0.98  0.02  0.    0.99\n",
      "  -0.05  0.    1.    0.    0.01  1.02  0.02  0.01  0.96  0.1   0.04  0.99\n",
      "  -0.02 -0.03  0.94  0.02 -0.    0.98  0.02  0.01  1.    0.04 -0.02  0.93\n",
      "   0.05  0.    0.94 -0.03 -0.01  0.98 -0.01  0.01  1.02  0.08  0.    0.91\n",
      "   0.02  0.03  1.    0.02  0.01  0.99  0.02 -0.    0.98 -0.04  0.    1.03]\n",
      " [ 0.07  0.02  1.    0.01 -0.    1.    0.02 -0.01  0.94  0.08  0.01  1.\n",
      "   0.06  0.04  0.98  0.02  0.01  0.97  0.02 -0.    0.99  0.03  0.03  0.99\n",
      "   0.05  0.01  1.    0.02  0.    1.02  0.07  0.01  1.    0.09  0.02  1.\n",
      "   0.04  0.05  0.97  0.02 -0.01  0.99  0.02 -0.    0.97 -0.   -0.07  0.98\n",
      "   0.08  0.01  1.02  0.07  0.01  0.99  0.   -0.02  0.94  0.09  0.02  0.98]\n",
      " [-0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.98\n",
      "  -0.06 -0.02  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98]\n",
      " [-0.    0.07  0.97 -0.04  0.04  0.99 -0.04  0.05  0.98  0.02  0.08  1.\n",
      "   0.    0.06  0.97 -0.04  0.07  0.99 -0.    0.1   0.95  0.02  0.08  0.97\n",
      "   0.04  0.11  1.02  0.01  0.09  0.95 -0.01  0.04  1.    0.01  0.11  0.99\n",
      "  -0.    0.06  1.03  0.03  0.12  0.97 -0.05  0.09  0.95 -0.    0.11  1.02\n",
      "  -0.07  0.1   0.99 -0.04  0.07  0.99  0.06  0.12  1.   -0.01  0.1   1.  ]\n",
      " [ 0.02 -0.    0.98  0.02 -0.    0.98  0.05 -0.01  0.98 -0.02 -0.02  0.94\n",
      "   0.07  0.01  0.99 -0.02 -0.01  0.97  0.07 -0.    0.97  0.03  0.01  0.97\n",
      "   0.02 -0.03  0.99  0.02  0.01  0.99  0.1   0.01  1.    0.07  0.01  0.98\n",
      "  -0.01 -0.02  0.94  0.08  0.    0.99  0.04  0.05  0.97  0.02 -0.02  0.99\n",
      "   0.02 -0.    0.99 -0.01 -0.07  0.98  0.06  0.    0.98 -0.03 -0.02  0.94]\n",
      " [-0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.06  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98]\n",
      " [-0.05 -0.03  0.99 -0.05 -0.06  0.98 -0.06 -0.02  1.   -0.05 -0.03  0.99\n",
      "  -0.06 -0.01  1.   -0.05  0.01  1.   -0.07  0.01  0.99 -0.05 -0.02  0.99\n",
      "  -0.06 -0.    0.98 -0.05  0.01  1.   -0.05 -0.03  0.97 -0.05  0.    0.99\n",
      "  -0.04 -0.04  0.99 -0.05  0.01  0.99 -0.07  0.01  1.   -0.06 -0.05  0.97\n",
      "  -0.06 -0.    0.98 -0.05 -0.05  0.97 -0.04 -0.04  0.97 -0.05 -0.04  0.98]\n",
      " [-0.05 -0.05  0.98 -0.07  0.    0.99 -0.05 -0.05  0.97 -0.06  0.    0.98\n",
      "  -0.05 -0.04  0.98 -0.05 -0.02  0.97 -0.05  0.    0.99 -0.03 -0.05  0.97\n",
      "  -0.06 -0.04  0.97 -0.07  0.    0.99 -0.06 -0.05  0.97 -0.07  0.01  0.99\n",
      "  -0.05 -0.05  0.97 -0.04 -0.03  0.97 -0.05 -0.05  0.97 -0.04 -0.05  0.98\n",
      "  -0.06 -0.02  0.98 -0.06 -0.01  0.99 -0.06  0.01  0.99 -0.07  0.01  0.98]\n",
      " [ 0.05  0.08  1.01 -0.01  0.11  1.   -0.03  0.03  0.99 -0.03  0.06  1.\n",
      "  -0.07  0.08  0.95 -0.01  0.1   0.98 -0.02  0.08  1.    0.03  0.12  0.96\n",
      "  -0.01  0.11  0.95  0.02  0.13  0.99  0.01  0.12  0.99  0.03  0.14  0.98\n",
      "   0.03  0.1   1.    0.01  0.05  1.   -0.04  0.06  0.99 -0.05  0.03  0.97\n",
      "  -0.03  0.07  1.   -0.07  0.07  0.95  0.    0.08  0.99  0.05  0.09  1.01]\n",
      " [-0.03  0.09  1.    0.02  0.09  1.   -0.04  0.06  0.99 -0.05  0.08  0.93\n",
      "   0.03  0.11  0.97  0.04  0.15  0.98 -0.    0.09  0.93  0.04  0.13  0.93\n",
      "  -0.04  0.06  1.   -0.05  0.01  0.98 -0.01  0.07  0.93 -0.    0.04  1.01\n",
      "   0.01  0.1   0.96  0.01  0.16  0.96 -0.    0.07  0.97  0.03  0.14  1.01\n",
      "  -0.05  0.06  0.99  0.03  0.07  1.01 -0.01  0.09  0.98 -0.    0.09  0.96]\n",
      " [-0.07 -0.03  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.06 -0.02  0.98 -0.07 -0.01  0.99]\n",
      " [-0.04  0.01  0.98 -0.05  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98]\n",
      " [-0.07 -0.01  0.98 -0.06  0.03  0.99 -0.04  0.    0.98 -0.05  0.01  0.98\n",
      "  -0.02 -0.01  0.98 -0.05 -0.02  0.98 -0.07  0.02  0.99 -0.01  0.01  0.99\n",
      "  -0.06  0.    0.98 -0.08 -0.01  0.98 -0.04  0.01  0.99 -0.07  0.01  0.98\n",
      "  -0.06  0.02  0.99 -0.03 -0.03  0.98 -0.05 -0.    0.98 -0.09  0.03  0.99\n",
      "  -0.07 -0.01  0.98 -0.06  0.01  0.99 -0.08  0.02  0.99 -0.05  0.01  0.98]\n",
      " [-0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.03  0.99 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.01  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.01  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98]\n",
      " [-0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98]\n",
      " [-0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.02  0.98 -0.04  0.04  0.99 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98]\n",
      " [-0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.06 -0.02  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98]\n",
      " [-0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98]\n",
      " [ 0.01 -0.06  0.86  0.09  0.02  0.96 -0.03 -0.01  0.97  0.02  0.01  1.02\n",
      "  -0.01 -0.03  0.94  0.02  0.    0.96  0.02  0.01  0.98  0.02  0.01  0.99\n",
      "  -0.05  0.    1.02  0.09  0.02  0.98 -0.01  0.01  1.01  0.07  0.03  1.01\n",
      "   0.07  0.03  0.97  0.02  0.    0.99  0.02  0.    0.99  0.02  0.03  1.01\n",
      "  -0.04 -0.01  0.97  0.08  0.02  0.98  0.   -0.    0.96  0.1   0.02  0.96]\n",
      " [-0.11  0.02  0.99 -0.06 -0.02  0.98 -0.07 -0.    0.98 -0.08 -0.01  0.98\n",
      "  -0.06  0.02  0.99 -0.05 -0.    0.99 -0.08  0.02  0.98 -0.05 -0.01  0.98\n",
      "  -0.03  0.01  0.99 -0.07  0.03  0.99 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "   0.    0.01  1.   -0.07 -0.02  0.98 -0.04 -0.01  0.98 -0.05  0.03  0.99\n",
      "  -0.06  0.01  0.99 -0.05 -0.    0.98 -0.08  0.    0.99 -0.04 -0.01  0.98]\n",
      " [-0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98]\n",
      " [-0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.06 -0.02  0.99 -0.06 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98]\n",
      " [-0.05 -0.    0.98 -0.04  0.02  0.99 -0.07  0.01  0.98 -0.07  0.    0.98\n",
      "  -0.01  0.    0.99 -0.05  0.    0.98 -0.05 -0.05  0.96 -0.05  0.03  0.98\n",
      "  -0.04  0.01  0.98 -0.04  0.01  0.99 -0.08  0.02  0.99 -0.06  0.    0.98\n",
      "  -0.05  0.03  0.99 -0.06 -0.02  0.98 -0.06  0.02  0.99 -0.04  0.02  0.99\n",
      "  -0.05  0.01  0.98 -0.06  0.01  0.98 -0.03 -0.01  0.98 -0.07 -0.01  0.98]\n",
      " [-0.05  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98]\n",
      " [-0.01  0.1   0.99 -0.01  0.02  0.99 -0.04  0.07  0.98 -0.05  0.12  0.95\n",
      "   0.    0.08  0.96  0.04  0.13  0.99  0.03  0.11  0.97  0.05  0.09  1.\n",
      "  -0.    0.06  0.97 -0.02  0.1   1.    0.    0.1   0.95 -0.    0.07  1.01\n",
      "  -0.02  0.04  0.98  0.01  0.07  0.95 -0.04  0.03  0.99 -0.02  0.08  1.01\n",
      "  -0.08  0.07  0.95  0.    0.08  0.99  0.04  0.16  0.98  0.    0.09  0.99]\n",
      " [ 0.04  0.07  1.   -0.02  0.07  0.99 -0.05  0.04  0.98 -0.01  0.1   0.99\n",
      "  -0.04  0.02  0.97 -0.04  0.06  1.   -0.06  0.1   0.95 -0.03  0.11  0.98\n",
      "  -0.03  0.05  0.99 -0.03  0.06  1.   -0.01  0.11  0.95 -0.01  0.13  0.98\n",
      "  -0.04  0.08  1.    0.01  0.11  0.95  0.03  0.09  1.    0.02  0.08  1.04\n",
      "  -0.03  0.07  0.99 -0.03  0.11  0.94 -0.    0.1   1.03 -0.08  0.04  0.95]\n",
      " [-0.06 -0.    0.98 -0.1  -0.02  0.98 -0.04 -0.01  0.98 -0.07 -0.    0.98\n",
      "  -0.03  0.    0.99 -0.06  0.03  1.   -0.06 -0.    0.98 -0.05 -0.04  0.97\n",
      "  -0.06 -0.02  0.98 -0.04  0.    0.98 -0.08 -0.02  0.98 -0.05 -0.01  0.98\n",
      "  -0.06 -0.    0.99 -0.03  0.    0.98 -0.07  0.01  0.99 -0.06 -0.02  0.98\n",
      "  -0.1   0.01  0.98 -0.06  0.    0.99 -0.03  0.03  1.   -0.04 -0.01  0.98]\n",
      " [-0.02  0.01  1.01 -0.02  0.    1.01  0.08  0.02  0.95 -0.01  0.03  1.08\n",
      "   0.03  0.01  1.    0.02  0.    0.99  0.03  0.02  0.98  0.1   0.01  0.95\n",
      "  -0.01  0.01  1.02  0.05  0.01  0.95 -0.05 -0.01  0.98  0.04  0.06  1.09\n",
      "   0.02  0.    0.98  0.02  0.01  0.98 -0.02 -0.05  0.92  0.    0.01  1.02\n",
      "   0.01 -0.    0.96 -0.04 -0.    1.   -0.05 -0.01  1.01  0.02 -0.01  0.97]\n",
      " [-0.05 -0.    0.98 -0.03  0.01  0.98 -0.07 -0.    0.98 -0.06 -0.02  0.98\n",
      "  -0.01 -0.    0.98 -0.05 -0.    0.98 -0.06  0.01  0.99 -0.04 -0.01  0.98\n",
      "  -0.04  0.    0.98 -0.07 -0.02  0.98 -0.01  0.    0.99 -0.05 -0.    0.98\n",
      "  -0.09 -0.02  0.97 -0.04 -0.01  0.98 -0.07 -0.    0.98 -0.04 -0.03  0.98\n",
      "  -0.05  0.04  1.   -0.05  0.    0.98 -0.08  0.    0.98 -0.07 -0.01  0.98]\n",
      " [-0.07  0.03  0.99 -0.05  0.06  0.99  0.05  0.15  0.99 -0.01  0.08  0.98\n",
      "   0.05  0.09  1.01 -0.03  0.07  0.99 -0.07  0.06  0.96 -0.    0.09  0.98\n",
      "  -0.04  0.05  0.91 -0.03  0.07  0.98  0.    0.13  0.96  0.    0.12  0.98\n",
      "  -0.02  0.05  0.97 -0.02  0.07  1.    0.01  0.06  1.   -0.04  0.05  0.95\n",
      "  -0.04  0.07  0.99 -0.02  0.09  0.95 -0.    0.1   0.94  0.04  0.14  0.94]\n",
      " [-0.03  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.06  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98]\n",
      " [-0.05 -0.    0.99 -0.08  0.05  1.   -0.04 -0.01  0.98 -0.05  0.02  0.99\n",
      "  -0.09  0.02  0.99 -0.06  0.02  0.99 -0.05  0.    0.98 -0.02  0.    0.98\n",
      "  -0.06 -0.01  0.98 -0.04 -0.    0.98 -0.07  0.03  0.99 -0.05  0.01  0.98\n",
      "  -0.06 -0.01  0.98 -0.06  0.03  0.99 -0.07  0.01  0.99 -0.05 -0.02  0.98\n",
      "  -0.1  -0.01  0.97 -0.05  0.01  0.98 -0.07  0.05  1.   -0.07  0.01  0.98]\n",
      " [ 0.04 -0.03  0.98  0.03  0.01  0.98  0.02 -0.    0.99  0.02 -0.    0.97\n",
      "  -0.03 -0.02  0.96 -0.03 -0.02  0.96  0.07  0.    0.98 -0.05 -0.02  0.97\n",
      "   0.06  0.04  0.98  0.03 -0.02  0.99  0.02 -0.    0.99 -0.   -0.05  0.99\n",
      "  -0.04 -0.02  0.96  0.06  0.01  1.03  0.06  0.    0.98 -0.02 -0.03  0.98\n",
      "   0.03  0.04  0.97  0.02 -0.    0.98  0.02 -0.01  0.98 -0.03  0.01  0.99]\n",
      " [-0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98\n",
      "  -0.04  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.04  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98]\n",
      " [ 0.01  0.1   0.96 -0.04  0.13  0.94 -0.    0.08  0.97 -0.04  0.05  0.91\n",
      "   0.04  0.11  0.96 -0.02  0.12  0.95  0.01  0.08  0.97 -0.03  0.04  0.98\n",
      "   0.01  0.1   0.99  0.05  0.1   1.    0.03  0.11  1.   -0.01  0.11  1.\n",
      "   0.    0.12  1.   -0.02  0.07  1.01 -0.03  0.03  0.96 -0.04  0.07  0.98\n",
      "   0.02  0.13  0.96  0.03  0.1   0.96 -0.04  0.05  1.03  0.02  0.11  0.95]\n",
      " [-0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.06 -0.02  0.98\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.01  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.    0.98 -0.05  0.02  0.98 -0.03  0.04  0.99 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.99 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.04  0.01  0.98]\n",
      " [-0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.02  0.98 -0.06 -0.02  0.98 -0.07 -0.02  0.98 -0.06 -0.02  0.99]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.06 -0.02  0.98 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.01  0.98]\n",
      " [-0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98 -0.    0.03  0.99\n",
      "  -0.05  0.02  0.98 -0.03  0.04  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98\n",
      "  -0.04  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98]\n",
      " [-0.05  0.01  0.98 -0.05 -0.    0.98 -0.03  0.01  0.98 -0.07 -0.    0.98\n",
      "  -0.06 -0.02  0.98 -0.01 -0.    0.98 -0.05 -0.    0.98 -0.06  0.01  0.99\n",
      "  -0.04 -0.01  0.98 -0.04  0.    0.98 -0.07 -0.02  0.98 -0.01  0.    0.99\n",
      "  -0.05 -0.    0.98 -0.09 -0.02  0.97 -0.04 -0.01  0.98 -0.07 -0.    0.98\n",
      "  -0.04 -0.03  0.98 -0.05  0.04  1.   -0.05  0.    0.98 -0.08  0.    0.98]\n",
      " [-0.03 -0.05  0.97 -0.04 -0.04  0.98 -0.04 -0.04  0.98 -0.05 -0.05  0.98\n",
      "  -0.06 -0.03  0.99 -0.07  0.01  0.99 -0.05 -0.    0.98 -0.05 -0.01  0.98\n",
      "  -0.04 -0.02  0.98 -0.04 -0.05  0.98 -0.04 -0.04  0.98 -0.05 -0.04  0.99\n",
      "  -0.05 -0.04  0.98 -0.06 -0.01  0.99 -0.05 -0.05  0.98 -0.07  0.    0.99\n",
      "  -0.05 -0.05  0.97 -0.06  0.    0.98 -0.05 -0.04  0.98 -0.05 -0.02  0.97]\n",
      " [-0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98]\n",
      " [ 0.09  0.02  0.95  0.05 -0.02  0.89  0.03  0.01  0.98  0.02  0.01  0.99\n",
      "   0.04  0.03  1.   -0.02  0.03  1.   -0.04 -0.    0.99  0.04  0.02  1.01\n",
      "  -0.02 -0.02  0.96 -0.   -0.02  0.95  0.02  0.    0.98  0.03  0.01  0.98\n",
      "   0.1   0.02  0.96  0.02  0.02  1.02  0.08  0.02  0.97  0.01 -0.01  0.95\n",
      "   0.06  0.05  1.05  0.03  0.01  1.    0.02  0.01  0.98  0.03  0.02  0.98]\n",
      " [-0.06 -0.02  0.98 -0.04  0.    0.98 -0.08 -0.02  0.98 -0.05 -0.01  0.98\n",
      "  -0.06 -0.    0.99 -0.03  0.    0.98 -0.07  0.01  0.99 -0.06 -0.02  0.98\n",
      "  -0.1   0.01  0.98 -0.06  0.    0.99 -0.03  0.03  1.   -0.04 -0.01  0.98\n",
      "  -0.05 -0.02  0.98 -0.08  0.01  0.98 -0.1  -0.    0.98 -0.05  0.    0.98\n",
      "  -0.02  0.02  1.   -0.07 -0.02  0.98 -0.04  0.01  0.99 -0.01 -0.02  0.98]\n",
      " [-0.01  0.07  0.95 -0.05  0.08  0.94 -0.04  0.06  0.99  0.04  0.13  0.98\n",
      "   0.01  0.09  0.98 -0.03  0.02  0.99 -0.04  0.05  0.98 -0.    0.06  1.\n",
      "  -0.01  0.07  0.97 -0.04  0.06  0.99  0.02  0.12  0.97 -0.    0.08  1.01\n",
      "  -0.03  0.02  0.98  0.02  0.1   0.97 -0.03  0.08  0.95 -0.03  0.07  0.98\n",
      "  -0.05  0.07  0.92 -0.    0.08  0.99  0.03  0.11  1.01 -0.01  0.07  0.93]\n",
      " [-0.04 -0.04  0.99 -0.06  0.    0.99 -0.06 -0.01  1.   -0.05 -0.    0.99\n",
      "  -0.06 -0.    0.98 -0.05 -0.02  0.98 -0.04 -0.03  0.97 -0.06 -0.03  0.98\n",
      "  -0.04 -0.05  0.98 -0.05  0.01  1.   -0.06 -0.03  0.99 -0.06  0.01  0.99\n",
      "  -0.08  0.01  0.99 -0.05 -0.    0.99 -0.04 -0.02  0.97 -0.04 -0.01  0.98\n",
      "  -0.04 -0.03  0.98 -0.04 -0.05  0.98 -0.04 -0.05  0.98 -0.06 -0.03  0.99]\n",
      " [-0.01  0.07  0.95  0.02  0.14  0.93 -0.03  0.07  0.99  0.06  0.11  0.99\n",
      "   0.    0.07  0.97 -0.01  0.06  1.03 -0.01  0.08  1.01  0.03  0.08  1.\n",
      "   0.03  0.09  0.99  0.    0.11  0.99 -0.01  0.13  0.99 -0.01  0.07  1.01\n",
      "  -0.04  0.03  0.96 -0.04  0.07  0.98 -0.04  0.06  0.95 -0.03  0.07  1.\n",
      "  -0.08  0.03  0.95  0.03  0.11  0.96  0.05  0.16  0.98 -0.    0.09  1.01]\n",
      " [-0.04 -0.01  0.98 -0.06  0.03  0.99 -0.06  0.    0.99 -0.06 -0.01  0.98\n",
      "  -0.08 -0.    0.98 -0.04  0.    0.98 -0.06 -0.02  0.98 -0.02  0.03  0.99\n",
      "  -0.05  0.    0.98 -0.02 -0.04  0.97 -0.07  0.    0.98 -0.07  0.01  0.99\n",
      "  -0.03 -0.    0.98 -0.03  0.02  0.99 -0.05 -0.    0.99 -0.07 -0.03  0.97\n",
      "  -0.07 -0.    0.98 -0.06 -0.02  0.98 -0.1  -0.    0.98 -0.05  0.    0.98]\n",
      " [-0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98]\n",
      " [-0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.03  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98]\n",
      " [-0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98]\n",
      " [-0.01  0.1   0.99 -0.    0.09  0.96 -0.01  0.08  1.    0.03  0.08  1.\n",
      "   0.03  0.1   0.99  0.02  0.12  0.98  0.    0.11  0.95  0.02  0.11  0.96\n",
      "   0.02  0.14  0.98  0.02  0.11  0.96  0.03  0.14  0.97 -0.01  0.07  0.93\n",
      "   0.05  0.14  0.95 -0.01  0.07  1.    0.01  0.04  1.01 -0.    0.07  0.94\n",
      "   0.07  0.12  0.94 -0.02  0.07  0.99 -0.04  0.02  0.99 -0.    0.08  0.99]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.06  0.    0.99 -0.07  0.02  0.99 -0.05 -0.    0.99 -0.04 -0.04  0.97\n",
      "  -0.04 -0.04  0.98 -0.04 -0.04  0.98 -0.05 -0.05  0.98 -0.04 -0.04  0.98\n",
      "  -0.05 -0.04  0.99 -0.05 -0.05  0.98 -0.05 -0.03  0.99 -0.05 -0.05  0.98\n",
      "  -0.06 -0.01  1.   -0.05 -0.05  0.98 -0.07  0.01  0.98 -0.05 -0.04  0.98\n",
      "  -0.06 -0.01  0.97 -0.06 -0.05  0.97 -0.04 -0.04  0.97 -0.05 -0.05  0.97]\n",
      " [-0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.01  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.03  0.99 -0.05  0.02  0.98\n",
      "  -0.03  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98]\n",
      " [-0.05  0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98 -0.03 -0.04  0.98\n",
      "  -0.05  0.01  0.99 -0.03 -0.05  0.98 -0.04 -0.05  0.98 -0.04 -0.04  0.99\n",
      "  -0.04 -0.05  0.98 -0.06 -0.01  1.   -0.05 -0.01  0.99 -0.07  0.01  0.99\n",
      "  -0.05  0.    0.99 -0.06 -0.01  0.97 -0.05 -0.05  0.97 -0.03 -0.05  0.97\n",
      "  -0.05 -0.01  0.99 -0.03 -0.05  0.97 -0.05 -0.01  0.99 -0.03 -0.06  0.97]\n",
      " [-0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98\n",
      "  -0.04  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98]\n",
      " [-0.1  -0.    0.98 -0.05  0.    0.98 -0.06 -0.    0.98 -0.04  0.03  0.99\n",
      "  -0.04  0.01  0.99 -0.08 -0.    0.98 -0.08  0.02  0.98 -0.06 -0.    0.99\n",
      "  -0.04 -0.02  0.97 -0.06 -0.02  0.98 -0.07  0.01  0.99 -0.08  0.01  0.98\n",
      "  -0.08  0.03  0.99 -0.06  0.    0.98 -0.01 -0.01  0.99 -0.07 -0.    0.98\n",
      "  -0.05 -0.01  0.98 -0.03 -0.03  0.98 -0.05 -0.02  0.98 -0.06  0.    0.98]\n",
      " [ 0.02  0.01  1.01  0.1   0.02  0.97  0.04 -0.03  0.87  0.02  0.01  0.98\n",
      "   0.02  0.01  0.98  0.02  0.03  1.02  0.1   0.02  0.96 -0.03 -0.01  0.98\n",
      "   0.01  0.01  1.01  0.04  0.04  1.03  0.02 -0.03  0.94  0.02  0.01  0.98\n",
      "   0.02  0.01  0.99 -0.01  0.05  1.11 -0.01 -0.01  0.96  0.05  0.    0.95\n",
      "  -0.02  0.01  0.95 -0.01  0.04  1.09  0.02 -0.    0.99  0.02  0.01  0.98]\n",
      " [-0.04  0.    0.98 -0.05  0.02  0.99 -0.06  0.01  0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.01  0.98 -0.07 -0.    0.98 -0.06 -0.01  0.98 -0.06  0.02  0.99\n",
      "  -0.09 -0.02  0.97 -0.05  0.    0.98 -0.04  0.04  1.   -0.07 -0.02  0.98\n",
      "  -0.05  0.02  0.99 -0.09 -0.    0.98 -0.03 -0.01  0.98 -0.05  0.    0.98\n",
      "  -0.07  0.02  0.98 -0.07  0.01  0.99 -0.06 -0.02  0.98 -0.06  0.03  0.99]\n",
      " [-0.02  0.09  0.98  0.01  0.13  0.99 -0.01  0.09  0.96 -0.06  0.05  0.99\n",
      "  -0.04  0.06  0.96 -0.    0.07  0.96 -0.04  0.04  0.97 -0.03  0.09  0.95\n",
      "  -0.01  0.13  0.96  0.03  0.11  0.97  0.03  0.15  0.98  0.    0.09  0.94\n",
      "   0.07  0.13  0.95 -0.04  0.06  0.99 -0.05  0.01  0.98 -0.01  0.07  0.93\n",
      "   0.07  0.1   0.98 -0.05  0.06  0.99 -0.07  0.04  0.97 -0.    0.07  0.94]\n",
      " [-0.04 -0.05  0.98 -0.05 -0.05  0.98 -0.05 -0.04  0.99 -0.04 -0.05  0.98\n",
      "  -0.05 -0.05  0.98 -0.05 -0.03  0.99 -0.06 -0.04  0.98 -0.07  0.01  0.98\n",
      "  -0.06 -0.04  0.98 -0.07  0.01  0.98 -0.06 -0.01  0.98 -0.06 -0.    0.97\n",
      "  -0.06  0.01  0.99 -0.04 -0.05  0.98 -0.06  0.    0.99 -0.04 -0.04  0.99\n",
      "  -0.06 -0.04  0.97 -0.06 -0.01  1.   -0.05 -0.05  0.98 -0.04 -0.04  0.97]\n",
      " [-0.06 -0.02  0.99 -0.01 -0.04  0.92  0.02  0.01  0.98  0.02  0.01  0.98\n",
      "   0.07  0.01  0.95 -0.04 -0.01  1.   -0.02  0.    1.01  0.05  0.02  1.\n",
      "   0.06  0.04  1.02  0.02  0.02  1.    0.02  0.01  0.98  0.02  0.    0.99\n",
      "  -0.05  0.    1.05 -0.02 -0.    1.02 -0.01 -0.    0.96  0.03 -0.01  0.94\n",
      "   0.06  0.01  0.95  0.02  0.    0.98  0.02  0.01  0.98  0.04  0.03  1.  ]\n",
      " [-0.02  0.05  0.97 -0.02  0.07  1.    0.01  0.06  1.   -0.04  0.05  0.95\n",
      "  -0.04  0.07  0.99 -0.02  0.09  0.95 -0.    0.1   0.94  0.04  0.14  0.94\n",
      "  -0.01  0.08  1.    0.02  0.09  1.01 -0.01  0.08  0.93  0.05  0.09  1.01\n",
      "  -0.05  0.06  0.99  0.05  0.16  0.98 -0.01  0.08  1.   -0.05  0.05  0.94\n",
      "   0.04  0.11  0.96  0.01  0.13  0.96  0.01  0.07  0.97 -0.03  0.03  1.01]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97]\n",
      " [-0.05 -0.    0.98 -0.06  0.    0.99 -0.06 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05  0.    0.99 -0.05 -0.    0.99 -0.06 -0.01  0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.06 -0.04  0.97 -0.05 -0.03  0.99 -0.06 -0.04  0.97 -0.07  0.01  0.99\n",
      "  -0.05 -0.01  0.98 -0.06  0.    0.98 -0.05 -0.05  0.97 -0.04 -0.02  0.98\n",
      "  -0.05 -0.04  0.98 -0.04 -0.04  0.97 -0.04 -0.05  0.98 -0.05 -0.04  0.98\n",
      "  -0.05 -0.04  0.98 -0.05 -0.05  0.98 -0.06 -0.02  0.98 -0.05 -0.05  0.98\n",
      "  -0.07 -0.    0.99 -0.06 -0.02  0.98 -0.06 -0.    0.98 -0.06 -0.03  0.97]\n",
      " [ 0.04  0.06  1.01 -0.01  0.11  1.   -0.01  0.06  1.03 -0.    0.1   0.97\n",
      "   0.06  0.09  1.   -0.    0.07  0.97 -0.04  0.04  0.99 -0.04  0.05  0.98\n",
      "   0.02  0.08  1.    0.    0.06  0.97 -0.04  0.07  0.99 -0.    0.1   0.95\n",
      "   0.02  0.08  0.97  0.04  0.11  1.02  0.01  0.09  0.95 -0.01  0.04  1.\n",
      "   0.01  0.11  0.99 -0.    0.06  1.03  0.03  0.12  0.97 -0.05  0.09  0.95]\n",
      " [-0.05  0.02  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.07 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99]\n",
      " [-0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97]\n",
      " [ 0.02 -0.01  0.98  0.02 -0.    0.98 -0.01 -0.02  0.98  0.06  0.01  0.99\n",
      "   0.07  0.01  1.02 -0.   -0.    0.98  0.03 -0.04  0.97  0.02 -0.01  0.98\n",
      "   0.02 -0.01  0.97  0.02 -0.    1.    0.07 -0.    0.98  0.08  0.01  1.02\n",
      "  -0.01 -0.02  0.94 -0.01 -0.02  0.97  0.05 -0.01  0.97  0.02  0.01  0.98\n",
      "   0.02 -0.    0.98 -0.02 -0.02  0.99  0.08  0.01  1.    0.04 -0.01  0.95]\n",
      " [ 0.07  0.01  1.    0.09  0.02  1.    0.04  0.05  0.97  0.02 -0.01  0.99\n",
      "   0.02 -0.    0.97 -0.   -0.07  0.98  0.08  0.01  1.02  0.07  0.01  0.99\n",
      "   0.   -0.02  0.94  0.09  0.02  0.98  0.02 -0.01  0.98  0.02 -0.01  0.98\n",
      "   0.02 -0.01  0.98 -0.02 -0.03  0.98  0.02  0.    1.01 -0.01 -0.02  0.94\n",
      "  -0.04 -0.02  0.96  0.01  0.04  0.99  0.02 -0.01  0.98  0.02 -0.01  0.99]\n",
      " [ 0.1   0.02  0.97  0.04 -0.03  0.87  0.02  0.01  0.98  0.02  0.01  0.98\n",
      "   0.02  0.03  1.02  0.1   0.02  0.96 -0.03 -0.01  0.98  0.01  0.01  1.01\n",
      "   0.04  0.04  1.03  0.02 -0.03  0.94  0.02  0.01  0.98  0.02  0.01  0.99\n",
      "  -0.01  0.05  1.11 -0.01 -0.01  0.96  0.05  0.    0.95 -0.02  0.01  0.95\n",
      "  -0.01  0.04  1.09  0.02 -0.    0.99  0.02  0.01  0.98  0.03  0.    0.97]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.06 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.99\n",
      "  -0.06 -0.02  0.99 -0.07 -0.02  0.98 -0.06 -0.02  0.99 -0.07 -0.01  0.99]\n",
      " [-0.03  0.06  0.97  0.03  0.13  0.98  0.01  0.1   1.01 -0.03  0.02  0.99\n",
      "  -0.01  0.08  0.99 -0.03  0.11  0.95 -0.02  0.08  1.   -0.05  0.07  0.91\n",
      "  -0.04  0.06  0.98  0.04  0.09  1.01 -0.01  0.09  0.98 -0.02 -0.    0.95\n",
      "  -0.05  0.06  0.99 -0.03  0.13  0.95 -0.    0.07  0.96 -0.    0.07  1.\n",
      "   0.02  0.11  0.95  0.06  0.1   1.    0.02  0.07  0.99  0.01  0.13  0.99]\n",
      " [-0.07  0.01  0.98 -0.05  0.    0.99 -0.04 -0.04  0.97 -0.05 -0.01  0.99\n",
      "  -0.03 -0.05  0.98 -0.06 -0.01  0.98 -0.06 -0.01  0.99 -0.05  0.01  0.99\n",
      "  -0.07  0.01  0.99 -0.05  0.    0.99 -0.04  0.01  0.98 -0.04 -0.02  0.98\n",
      "  -0.04 -0.02  0.98 -0.04 -0.02  0.98 -0.04 -0.02  0.98 -0.03 -0.04  0.98\n",
      "  -0.04 -0.03  0.98 -0.05 -0.04  0.99 -0.05 -0.05  0.98 -0.07 -0.    0.99]\n",
      " [-0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05  0.    0.98\n",
      "  -0.05  0.    0.99 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98]\n",
      " [-0.    0.09  0.96 -0.01  0.08  1.    0.03  0.08  1.    0.03  0.1   0.99\n",
      "   0.02  0.12  0.98  0.    0.11  0.95  0.02  0.11  0.96  0.02  0.14  0.98\n",
      "   0.02  0.11  0.96  0.03  0.14  0.97 -0.01  0.07  0.93  0.05  0.14  0.95\n",
      "  -0.01  0.07  1.    0.01  0.04  1.01 -0.    0.07  0.94  0.07  0.12  0.94\n",
      "  -0.02  0.07  0.99 -0.04  0.02  0.99 -0.    0.08  0.99  0.03  0.09  1.03]\n",
      " [ 0.02 -0.    0.99  0.1   0.    0.98  0.08  0.01  1.02  0.05  0.01  1.03\n",
      "  -0.04 -0.02  0.95  0.05  0.04  0.98  0.02 -0.01  0.99  0.02 -0.    0.99\n",
      "   0.02  0.    0.97  0.08  0.    0.99  0.04 -0.    0.96  0.02  0.    1.02\n",
      "  -0.03 -0.02  0.97  0.05  0.05  0.98  0.02  0.    0.98  0.02 -0.    0.98\n",
      "  -0.01 -0.02  1.   -0.03 -0.01  0.97  0.07  0.01  1.03  0.05 -0.    0.96]\n",
      " [-0.07  0.01  0.98 -0.04 -0.01  0.98 -0.09  0.02  0.98 -0.05  0.    0.98\n",
      "  -0.04 -0.01  0.98 -0.05 -0.02  0.97 -0.04  0.    0.98 -0.03  0.    0.99\n",
      "  -0.1   0.01  0.98 -0.06 -0.    0.98 -0.03  0.04  1.   -0.06 -0.02  0.98\n",
      "  -0.07 -0.01  0.98 -0.07  0.03  0.99 -0.09 -0.01  0.97 -0.06  0.    0.98\n",
      "  -0.04  0.03  0.99 -0.07 -0.01  0.98 -0.04  0.01  0.99 -0.01  0.01  0.99]\n",
      " [-0.06 -0.01  0.99 -0.06  0.01  0.99 -0.07  0.01  0.98 -0.06  0.01  0.99\n",
      "  -0.05  0.01  0.98 -0.06  0.01  0.98 -0.05  0.01  0.98 -0.04 -0.04  0.98\n",
      "  -0.05  0.01  0.99 -0.03 -0.05  0.98 -0.04 -0.04  0.98 -0.04 -0.04  0.99\n",
      "  -0.05 -0.02  0.99 -0.05 -0.02  1.   -0.05  0.01  1.   -0.06 -0.01  1.\n",
      "  -0.05 -0.01  0.98 -0.07  0.01  0.99 -0.05  0.01  1.   -0.07  0.01  0.98]\n",
      " [-0.05 -0.02  0.97 -0.05  0.01  1.   -0.04 -0.03  0.97 -0.05  0.01  0.99\n",
      "  -0.04 -0.05  0.98 -0.06  0.01  0.99 -0.07  0.01  1.   -0.05 -0.04  0.97\n",
      "  -0.06 -0.    0.98 -0.05 -0.03  0.98 -0.04 -0.03  0.97 -0.04 -0.03  0.98\n",
      "  -0.04 -0.04  0.98 -0.04 -0.01  0.98 -0.05  0.01  0.98 -0.05  0.01  0.99\n",
      "  -0.05  0.01  0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98]\n",
      " [-0.04 -0.04  0.99 -0.05 -0.01  0.99 -0.07  0.01  0.99 -0.05  0.01  1.\n",
      "  -0.06  0.01  0.98 -0.05 -0.    0.99 -0.03 -0.05  0.97 -0.06 -0.05  0.97\n",
      "  -0.05 -0.02  0.99 -0.05 -0.03  0.98 -0.07  0.02  0.99 -0.05 -0.03  0.98\n",
      "  -0.07  0.01  0.99 -0.04 -0.02  0.99 -0.04 -0.02  0.97 -0.06 -0.05  0.97\n",
      "  -0.04 -0.05  0.98 -0.05 -0.05  0.98 -0.05 -0.04  0.98 -0.05 -0.05  0.98]\n",
      " [ 0.01  0.13  0.99 -0.01  0.09  0.96 -0.06  0.05  0.99 -0.04  0.06  0.96\n",
      "  -0.    0.07  0.96 -0.04  0.04  0.97 -0.03  0.09  0.95 -0.01  0.13  0.96\n",
      "   0.03  0.11  0.97  0.03  0.15  0.98  0.    0.09  0.94  0.07  0.13  0.95\n",
      "  -0.04  0.06  0.99 -0.05  0.01  0.98 -0.01  0.07  0.93  0.07  0.1   0.98\n",
      "  -0.05  0.06  0.99 -0.07  0.04  0.97 -0.    0.07  0.94 -0.04  0.06  0.92]\n",
      " [-0.08 -0.02  0.98 -0.04  0.03  0.99 -0.06  0.    0.98 -0.08  0.02  0.99\n",
      "  -0.07  0.01  0.98 -0.04 -0.01  0.98 -0.06  0.03  0.99 -0.06  0.    0.99\n",
      "  -0.06 -0.01  0.98 -0.08 -0.    0.98 -0.04  0.    0.98 -0.06 -0.02  0.98\n",
      "  -0.02  0.03  0.99 -0.05  0.    0.98 -0.02 -0.04  0.97 -0.07  0.    0.98\n",
      "  -0.07  0.01  0.99 -0.03 -0.    0.98 -0.03  0.02  0.99 -0.05 -0.    0.99]\n",
      " [-0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.03  0.98 -0.04  0.01  0.98 -0.04  0.04  0.99 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98]\n",
      " [ 0.01 -0.    1.01  0.05 -0.02  0.98  0.02 -0.01  1.    0.02 -0.01  0.99\n",
      "   0.02 -0.    0.99  0.1   0.01  0.98  0.   -0.01  0.95 -0.02 -0.02  0.94\n",
      "   0.04  0.01  1.01  0.01  0.02  0.99  0.02 -0.01  0.98  0.02 -0.01  0.98\n",
      "   0.03  0.02  0.96  0.06  0.01  1.    0.08  0.01  1.01 -0.01 -0.01  0.99\n",
      "   0.09  0.01  1.    0.02 -0.05  0.98  0.02  0.01  0.98  0.02 -0.    0.97]\n",
      " [-0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98]\n",
      " [ 0.02  0.01  0.98  0.08  0.08  1.08 -0.01  0.01  1.02  0.06  0.02  1.\n",
      "   0.06  0.01  0.95 -0.02  0.03  1.07  0.01 -0.03  0.95  0.02  0.01  0.99\n",
      "   0.02  0.01  0.98  0.07  0.02  0.98  0.02 -0.    0.95  0.07  0.02  0.99\n",
      "   0.1   0.02  0.97  0.07  0.02  0.95  0.03  0.02  0.98  0.02  0.    0.99\n",
      "   0.02  0.02  0.99  0.1   0.03  0.99  0.08  0.02  0.98  0.04  0.    0.95]\n",
      " [-0.07 -0.02  0.98 -0.05  0.02  0.99 -0.09 -0.    0.98 -0.03 -0.01  0.98\n",
      "  -0.05  0.    0.98 -0.07  0.02  0.98 -0.07  0.01  0.99 -0.06 -0.02  0.98\n",
      "  -0.06  0.03  0.99 -0.06  0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.04  0.01  0.98 -0.04 -0.01  0.99 -0.06  0.04  0.99 -0.06  0.    0.99\n",
      "  -0.    0.    0.98 -0.05 -0.02  0.98 -0.05 -0.01  0.98 -0.08  0.02  0.99]\n",
      " [-0.01  0.08  0.98 -0.04  0.07  0.98 -0.    0.09  0.96 -0.03  0.1   0.95\n",
      "  -0.    0.14  0.98 -0.    0.11  1.   -0.01  0.08  0.95  0.03  0.1   0.99\n",
      "   0.03  0.08  1.01 -0.03  0.06  1.   -0.03  0.04  1.    0.01  0.1   1.02\n",
      "  -0.02  0.06  1.04  0.02  0.11  0.95 -0.01  0.15  0.95 -0.01  0.11  1.01\n",
      "  -0.08  0.1   0.99  0.03  0.11  0.98  0.06  0.1   1.01 -0.    0.11  1.01]\n",
      " [-0.    0.07  0.96 -0.    0.07  1.    0.02  0.11  0.95  0.06  0.1   1.\n",
      "   0.02  0.07  0.99  0.01  0.13  0.99 -0.02  0.08  0.96 -0.02  0.06  1.\n",
      "  -0.03  0.04  0.96 -0.01  0.08  0.99 -0.04  0.04  0.98 -0.04  0.07  0.98\n",
      "  -0.08  0.03  0.94  0.01  0.1   0.96  0.04  0.16  0.98 -0.01  0.09  1.01\n",
      "   0.03  0.13  0.93 -0.01  0.09  0.99 -0.03  0.01  0.99 -0.01  0.1   0.99]\n",
      " [-0.01  0.09  0.98 -0.    0.09  0.96 -0.03  0.06  1.   -0.02  0.05  0.99\n",
      "   0.    0.06  0.98 -0.01  0.1   1.    0.01  0.11  0.95 -0.04  0.06  0.99\n",
      "  -0.04  0.05  0.96 -0.03  0.07  0.99 -0.04  0.04  0.97 -0.04  0.07  0.96\n",
      "  -0.06  0.06  0.91  0.03  0.11  0.96  0.05  0.11  1.01 -0.01  0.07  0.95\n",
      "   0.02  0.13  0.92  0.04  0.12  0.96  0.01  0.03  1.01 -0.    0.1   0.99]\n",
      " [-0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.99 -0.07 -0.02  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99]\n",
      " [-0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98]\n",
      " [ 0.05 -0.01  0.98 -0.02 -0.02  0.94  0.07  0.01  0.99 -0.02 -0.01  0.97\n",
      "   0.07 -0.    0.97  0.03  0.01  0.97  0.02 -0.03  0.99  0.02  0.01  0.99\n",
      "   0.1   0.01  1.    0.07  0.01  0.98 -0.01 -0.02  0.94  0.08  0.    0.99\n",
      "   0.04  0.05  0.97  0.02 -0.02  0.99  0.02 -0.    0.99 -0.01 -0.07  0.98\n",
      "   0.06  0.    0.98 -0.03 -0.02  0.94  0.   -0.    1.    0.07 -0.01  0.98]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.02  0.98 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.11  0.    0.98 -0.06 -0.02  0.98 -0.07  0.01  0.98 -0.09 -0.    0.98\n",
      "  -0.03 -0.02  0.98 -0.05 -0.    0.98 -0.08  0.02  0.98 -0.07 -0.01  0.98\n",
      "  -0.04  0.01  0.99 -0.06  0.03  0.99 -0.05  0.    0.98 -0.06 -0.    0.99\n",
      "  -0.05 -0.02  0.98 -0.07  0.01  0.98 -0.03  0.    0.98 -0.03  0.03  0.99\n",
      "  -0.05 -0.    0.99 -0.03 -0.04  0.97 -0.05  0.02  0.99 -0.05 -0.01  0.98]\n",
      " [-0.06 -0.    0.98 -0.06 -0.03  0.97 -0.05 -0.05  0.97 -0.06 -0.04  0.97\n",
      "  -0.04 -0.05  0.98 -0.06 -0.03  0.98 -0.04 -0.04  0.98 -0.06 -0.05  0.97\n",
      "  -0.06 -0.01  0.99 -0.06 -0.03  0.98 -0.07  0.01  0.99 -0.05 -0.02  0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.01  0.98 -0.04 -0.03  0.98 -0.05 -0.05  0.97\n",
      "  -0.05 -0.04  0.98 -0.05 -0.05  0.98 -0.06 -0.02  0.99 -0.06 -0.02  0.99]\n",
      " [-0.06 -0.01  0.99 -0.04 -0.    0.99  0.08  0.02  0.98  0.05  0.03  1.02\n",
      "  -0.01 -0.03  0.95  0.02  0.02  0.98  0.02  0.01  0.98  0.07  0.02  0.98\n",
      "   0.02  0.02  1.02  0.07  0.02  1.    0.08  0.01  0.96  0.08  0.03  0.98\n",
      "   0.03  0.03  1.01  0.02  0.01  0.98  0.03  0.01  0.99  0.07  0.03  0.99\n",
      "   0.09  0.02  0.96  0.02  0.01  1.01 -0.05 -0.    1.    0.03 -0.04  0.86]\n",
      " [-0.06 -0.02  0.99 -0.06 -0.02  0.99 -0.07 -0.02  0.98 -0.06 -0.02  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.03  0.98]\n",
      " [-0.02  0.08  0.96 -0.05  0.05  0.95 -0.    0.11  1.02 -0.02  0.14  0.96\n",
      "  -0.04  0.07  0.98 -0.03  0.01  0.99 -0.    0.08  0.95  0.04  0.04  0.97\n",
      "   0.02  0.11  0.96 -0.07  0.1   0.95 -0.01  0.1   0.98  0.01  0.14  0.92\n",
      "  -0.03  0.06  1.   -0.    0.12  0.96  0.01  0.1   0.98  0.01  0.1   0.96\n",
      "  -0.03  0.06  1.    0.05  0.12  0.99  0.02  0.14  0.99 -0.03  0.09  1.  ]\n",
      " [-0.07 -0.02  0.98 -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.    0.99\n",
      "  -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.02  0.98]\n",
      " [ 0.05  0.15  0.99 -0.01  0.08  0.98  0.05  0.09  1.01 -0.03  0.07  0.99\n",
      "  -0.07  0.06  0.96 -0.    0.09  0.98 -0.04  0.05  0.91 -0.03  0.07  0.98\n",
      "   0.    0.13  0.96  0.    0.12  0.98 -0.02  0.05  0.97 -0.02  0.07  1.\n",
      "   0.01  0.06  1.   -0.04  0.05  0.95 -0.04  0.07  0.99 -0.02  0.09  0.95\n",
      "  -0.    0.1   0.94  0.04  0.14  0.94 -0.01  0.08  1.    0.02  0.09  1.01]\n",
      " [-0.05 -0.    0.98 -0.06 -0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.99\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.02  0.98\n",
      "  -0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.06 -0.02  0.98 -0.06 -0.02  0.99\n",
      "  -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98]\n",
      " [-0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.03  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.04  0.04  0.99 -0.03  0.01  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98]\n",
      " [-0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98]\n",
      " [ 0.02  0.08  1.03 -0.05  0.06  0.99 -0.04  0.12  0.94 -0.01  0.1   0.99\n",
      "  -0.01  0.14  0.95  0.03  0.11  0.97  0.06  0.11  1.   -0.    0.07  0.98\n",
      "   0.03  0.12  1.03  0.02  0.11  0.98  0.03  0.08  1.01 -0.01  0.08  0.98\n",
      "   0.02  0.12  0.96 -0.04  0.06  1.    0.02  0.08  1.    0.02  0.07  0.98\n",
      "  -0.    0.11  1.    0.02  0.11  1.   -0.04  0.06  0.99 -0.07  0.03  0.94]\n",
      " [ 0.04  0.04  1.03  0.02 -0.03  0.94  0.02  0.01  0.98  0.02  0.01  0.99\n",
      "  -0.01  0.05  1.11 -0.01 -0.01  0.96  0.05  0.    0.95 -0.02  0.01  0.95\n",
      "  -0.01  0.04  1.09  0.02 -0.    0.99  0.02  0.01  0.98  0.03  0.    0.97\n",
      "   0.1   0.02  0.97 -0.01 -0.01  0.96  0.03  0.01  1.01 -0.01 -0.02  0.94\n",
      "   0.06  0.06  1.06  0.02 -0.    0.99  0.02  0.    0.99 -0.01  0.03  1.04]\n",
      " [ 0.02 -0.01  0.99  0.02 -0.01  0.98  0.04  0.04  0.98  0.1   0.01  1.\n",
      "   0.04  0.01  1.03  0.04 -0.01  0.95  0.03 -0.02  0.98  0.05  0.    0.97\n",
      "   0.02  0.01  0.98  0.02 -0.    0.99 -0.02 -0.05  0.99  0.   -0.01  0.99\n",
      "   0.02  0.    1.02  0.06  0.    0.98  0.09  0.02  0.98  0.03  0.02  0.97\n",
      "   0.02 -0.01  0.97  0.02 -0.01  1.    0.09  0.    0.98  0.05  0.01  1.02]\n",
      " [-0.05  0.    0.98 -0.08  0.02  0.99 -0.06  0.02  0.98 -0.07 -0.01  0.98\n",
      "  -0.09 -0.01  0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.04  0.02  0.99\n",
      "  -0.07  0.01  0.98 -0.07  0.    0.98 -0.01  0.    0.99 -0.05  0.    0.98\n",
      "  -0.05 -0.05  0.96 -0.05  0.03  0.98 -0.04  0.01  0.98 -0.04  0.01  0.99\n",
      "  -0.08  0.02  0.99 -0.06  0.    0.98 -0.05  0.03  0.99 -0.06 -0.02  0.98]\n",
      " [-0.06 -0.    0.99 -0.06  0.01  0.99 -0.03 -0.    0.98 -0.04  0.01  0.99\n",
      "  -0.04  0.03  0.99 -0.01 -0.01  0.99 -0.06 -0.    0.98 -0.11 -0.03  0.97\n",
      "  -0.06 -0.02  0.98 -0.06  0.01  0.99 -0.07 -0.02  0.97 -0.03  0.01  0.98\n",
      "  -0.05  0.    0.98 -0.08  0.01  0.98 -0.07 -0.02  0.98 -0.05  0.02  0.99\n",
      "  -0.09  0.01  0.98 -0.05  0.    0.99 -0.05 -0.01  0.98 -0.03 -0.01  0.98]\n",
      " [-0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.03  0.98 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.03  0.98 -0.06 -0.02  0.98\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98]\n",
      " [ 0.02  0.01  0.98  0.07  0.02  0.98  0.02 -0.    0.95  0.07  0.02  0.99\n",
      "   0.1   0.02  0.97  0.07  0.02  0.95  0.03  0.02  0.98  0.02  0.    0.99\n",
      "   0.02  0.02  0.99  0.1   0.03  0.99  0.08  0.02  0.98  0.04  0.    0.95\n",
      "   0.08  0.03  1.    0.05  0.01  0.99  0.03  0.02  0.98  0.02  0.01  0.98\n",
      "   0.07 -0.04  0.86 -0.04 -0.    1.01 -0.01  0.01  1.02  0.09  0.02  0.98]\n",
      " [-0.06  0.    0.98 -0.03 -0.02  0.98 -0.06  0.02  0.99 -0.07 -0.01  0.98\n",
      "  -0.02 -0.02  0.98 -0.06 -0.    0.99 -0.06  0.01  0.99 -0.08  0.    0.99\n",
      "  -0.05  0.02  0.99 -0.05 -0.02  0.98 -0.05  0.04  1.   -0.05  0.    0.99\n",
      "  -0.02 -0.03  0.97 -0.04  0.02  0.99 -0.05 -0.01  0.98 -0.07  0.03  0.99\n",
      "  -0.07 -0.03  0.97 -0.05  0.    0.98 -0.03  0.    0.98 -0.04  0.02  0.99]\n",
      " [-0.06  0.01  0.99 -0.07  0.01  0.98 -0.05 -0.03  0.98 -0.06  0.01  0.98\n",
      "  -0.05 -0.    0.99 -0.04  0.02  0.99 -0.05 -0.    0.98 -0.04 -0.01  0.98\n",
      "  -0.04 -0.02  0.98 -0.05 -0.    0.98 -0.03 -0.05  0.98 -0.05 -0.    0.99\n",
      "  -0.04 -0.04  0.99 -0.04 -0.05  0.98 -0.07 -0.    1.   -0.05 -0.03  0.98\n",
      "  -0.07  0.    0.98 -0.05 -0.01  0.99 -0.05 -0.02  0.97 -0.05  0.01  1.  ]\n",
      " [ 0.02  0.11  0.96  0.05  0.14  1.02 -0.04  0.06  1.   -0.02  0.04  1.\n",
      "   0.01  0.11  0.99  0.01  0.06  1.02  0.03  0.12  0.96 -0.05  0.11  0.95\n",
      "  -0.01  0.08  0.96 -0.08  0.07  0.98  0.01  0.09  0.98  0.04  0.15  0.98\n",
      "  -0.01  0.09  0.99  0.02  0.09  1.01 -0.02  0.09  0.97  0.01  0.07  1.\n",
      "  -0.01  0.08  0.98 -0.01  0.12  1.   -0.01  0.09  0.95 -0.03  0.05  0.99]\n",
      " [-0.02  0.    1.01  0.03 -0.    0.94 -0.03 -0.01  1.    0.02  0.02  0.98\n",
      "   0.02  0.01  0.98  0.03  0.01  1.    0.1   0.02  0.94 -0.    0.01  1.02\n",
      "   0.05  0.    0.95  0.08  0.03  1.   -0.02 -0.03  0.93  0.02  0.01  0.98\n",
      "   0.02  0.    0.99  0.06  0.02  0.97 -0.04 -0.    1.01  0.02 -0.    0.95\n",
      "  -0.03 -0.    0.99  0.07 -0.01  0.9   0.04  0.04  1.02  0.02  0.01  0.98]\n",
      " [-0.06 -0.    0.99 -0.06  0.01  0.98 -0.06 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.06  0.01  0.99 -0.04 -0.03  0.98 -0.06  0.01  0.99 -0.04 -0.05  0.98\n",
      "  -0.05  0.01  1.   -0.05 -0.03  0.99 -0.06 -0.02  0.98 -0.07 -0.    0.99\n",
      "  -0.05  0.01  1.   -0.06 -0.    0.98 -0.05  0.    0.99 -0.05 -0.03  0.97\n",
      "  -0.05  0.    0.99 -0.04 -0.05  0.98 -0.05 -0.02  0.98 -0.06 -0.02  0.99]\n",
      " [-0.04 -0.04  0.97 -0.05 -0.01  0.99 -0.03 -0.05  0.98 -0.06 -0.01  0.98\n",
      "  -0.06 -0.01  0.99 -0.05  0.01  0.99 -0.07  0.01  0.99 -0.05  0.    0.99\n",
      "  -0.04  0.01  0.98 -0.04 -0.02  0.98 -0.04 -0.02  0.98 -0.04 -0.02  0.98\n",
      "  -0.04 -0.02  0.98 -0.03 -0.04  0.98 -0.04 -0.03  0.98 -0.05 -0.04  0.99\n",
      "  -0.05 -0.05  0.98 -0.07 -0.    0.99 -0.05 -0.05  0.98 -0.06  0.    0.98]\n",
      " [ 0.02 -0.    0.98 -0.02 -0.02  0.99  0.08  0.01  1.    0.04 -0.01  0.95\n",
      "  -0.01 -0.02  0.94  0.05 -0.02  0.98  0.03  0.02  0.97  0.02 -0.01  0.97\n",
      "   0.03 -0.    0.99 -0.03 -0.01  0.97 -0.02 -0.01  0.98  0.03 -0.01  0.95\n",
      "   0.08  0.02  1.02 -0.02 -0.02  1.    0.02 -0.02  0.99  0.02 -0.    0.99\n",
      "   0.04  0.02  0.97  0.07  0.    0.99  0.07  0.01  1.    0.   -0.02  0.94]\n",
      " [-0.05  0.01  0.98 -0.06  0.01  0.98 -0.05  0.01  0.98 -0.04 -0.04  0.98\n",
      "  -0.05  0.01  0.99 -0.03 -0.05  0.98 -0.04 -0.04  0.98 -0.04 -0.04  0.99\n",
      "  -0.05 -0.02  0.99 -0.05 -0.02  1.   -0.05  0.01  1.   -0.06 -0.01  1.\n",
      "  -0.05 -0.01  0.98 -0.07  0.01  0.99 -0.05  0.01  1.   -0.07  0.01  0.98\n",
      "  -0.06 -0.03  0.98 -0.06 -0.01  0.98 -0.05  0.01  1.   -0.05 -0.03  0.99]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.06 -0.03  0.98 -0.06 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98]\n",
      " [-0.02 -0.01  0.97  0.02  0.01  0.99  0.01  0.03  0.99  0.02 -0.    0.98\n",
      "   0.02 -0.    0.97 -0.01 -0.06  0.98  0.02  0.    1.    0.03  0.01  1.02\n",
      "   0.08  0.01  1.    0.04  0.03  0.99  0.02  0.    0.97  0.02 -0.    0.99\n",
      "   0.02 -0.01  0.98 -0.03 -0.    0.97  0.05  0.01  1.02 -0.01 -0.02  0.94\n",
      "   0.02  0.    0.99  0.07  0.01  0.97  0.03 -0.01  0.99  0.02 -0.    0.99]\n",
      " [ 0.02  0.    0.99  0.02 -0.    0.98  0.05 -0.01  0.98  0.02  0.    1.01\n",
      "  -0.02 -0.02  0.94  0.   -0.01  0.96  0.07 -0.01  0.97  0.02 -0.01  0.98\n",
      "   0.02 -0.01  0.98  0.02 -0.02  0.98  0.09  0.01  1.01 -0.03 -0.02  0.96\n",
      "   0.05  0.01  1.03 -0.03 -0.03  0.97  0.   -0.03  1.    0.02 -0.01  0.98\n",
      "   0.03 -0.    0.98  0.09  0.01  0.97  0.06  0.    0.98  0.06  0.    0.97]\n",
      " [-0.03  0.06  1.   -0.01  0.11  0.95 -0.01  0.13  0.98 -0.04  0.08  1.\n",
      "   0.01  0.11  0.95  0.03  0.09  1.    0.02  0.08  1.04 -0.03  0.07  0.99\n",
      "  -0.03  0.11  0.94 -0.    0.1   1.03 -0.08  0.04  0.95 -0.04  0.06  0.99\n",
      "   0.05  0.16  0.99 -0.01  0.1   1.    0.02  0.17  1.    0.04  0.11  0.96\n",
      "   0.02  0.04  1.01 -0.01  0.11  1.   -0.06  0.04  1.    0.03  0.12  0.97]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.02  0.98 -0.07 -0.01  0.98\n",
      "  -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97]\n",
      " [ 0.02  0.15  0.99 -0.04  0.08  1.   -0.    0.1   0.95  0.02  0.11  0.96\n",
      "   0.05  0.14  1.02 -0.04  0.06  1.   -0.02  0.04  1.    0.01  0.11  0.99\n",
      "   0.01  0.06  1.02  0.03  0.12  0.96 -0.05  0.11  0.95 -0.01  0.08  0.96\n",
      "  -0.08  0.07  0.98  0.01  0.09  0.98  0.04  0.15  0.98 -0.01  0.09  0.99\n",
      "   0.02  0.09  1.01 -0.02  0.09  0.97  0.01  0.07  1.   -0.01  0.08  0.98]\n",
      " [-0.07 -0.01  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.99 -0.06 -0.02  0.99]\n",
      " [-0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98]\n",
      " [ 0.08  0.01  1.02  0.05  0.01  1.03 -0.04 -0.02  0.95  0.05  0.04  0.98\n",
      "   0.02 -0.01  0.99  0.02 -0.    0.99  0.02  0.    0.97  0.08  0.    0.99\n",
      "   0.04 -0.    0.96  0.02  0.    1.02 -0.03 -0.02  0.97  0.05  0.05  0.98\n",
      "   0.02  0.    0.98  0.02 -0.    0.98 -0.01 -0.02  1.   -0.03 -0.01  0.97\n",
      "   0.07  0.01  1.03  0.05 -0.    0.96 -0.04 -0.01  0.98  0.02 -0.03  0.98]\n",
      " [ 0.03 -0.02  0.99  0.02 -0.    0.99  0.03  0.03  0.97  0.07  0.01  1.01\n",
      "  -0.03 -0.02  0.95  0.04  0.01  1.03  0.05 -0.02  0.98  0.02 -0.06  0.98\n",
      "   0.02  0.01  0.98  0.02 -0.    0.98  0.08  0.03  0.98  0.01 -0.01  0.96\n",
      "   0.01 -0.02  0.94 -0.01 -0.02  0.94  0.09  0.02  0.99  0.02  0.01  0.99\n",
      "   0.02  0.    0.99  0.02 -0.    0.97  0.07  0.02  1.    0.01 -0.    1.  ]\n",
      " [-0.03  0.01  0.98 -0.05 -0.    0.98 -0.04  0.02  0.99 -0.04  0.02  0.99\n",
      "  -0.06 -0.02  0.98 -0.02  0.02  0.99 -0.05  0.    0.98 -0.07 -0.    0.99\n",
      "  -0.08  0.01  0.99 -0.04 -0.    0.98 -0.07 -0.02  0.98 -0.01  0.01  0.99\n",
      "  -0.05  0.    0.99 -0.05 -0.05  0.96 -0.07 -0.02  0.98 -0.07  0.01  0.99\n",
      "  -0.03  0.01  0.99 -0.07  0.01  0.99 -0.05 -0.    0.98 -0.05 -0.04  0.98]\n",
      " [-0.03  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98 -0.04  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.99 -0.03  0.01  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.03  0.98 -0.07 -0.01  0.99\n",
      "  -0.07 -0.    0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97]\n",
      " [-0.01  0.02  0.99 -0.04  0.07  0.98 -0.05  0.12  0.95  0.    0.08  0.96\n",
      "   0.04  0.13  0.99  0.03  0.11  0.97  0.05  0.09  1.   -0.    0.06  0.97\n",
      "  -0.02  0.1   1.    0.    0.1   0.95 -0.    0.07  1.01 -0.02  0.04  0.98\n",
      "   0.01  0.07  0.95 -0.04  0.03  0.99 -0.02  0.08  1.01 -0.08  0.07  0.95\n",
      "   0.    0.08  0.99  0.04  0.16  0.98  0.    0.09  0.99  0.04  0.14  0.95]\n",
      " [-0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98]\n",
      " [-0.05 -0.03  0.99 -0.05  0.01  1.   -0.07  0.01  0.99 -0.05  0.01  1.\n",
      "  -0.04 -0.04  0.97 -0.05 -0.02  0.99 -0.04 -0.04  0.99 -0.05 -0.    0.99\n",
      "  -0.06 -0.01  0.99 -0.05 -0.01  0.99 -0.06 -0.    0.98 -0.05 -0.05  0.98\n",
      "  -0.04 -0.05  0.98 -0.06 -0.03  0.98 -0.05 -0.04  0.98 -0.06 -0.02  0.98\n",
      "  -0.07  0.    0.99 -0.07  0.    0.99 -0.06  0.01  0.99 -0.06  0.01  0.99]\n",
      " [-0.06 -0.05  0.97 -0.07  0.    1.   -0.06 -0.04  0.97 -0.07  0.02  0.99\n",
      "  -0.06 -0.04  0.97 -0.07  0.01  0.99 -0.05  0.01  0.99 -0.08  0.02  0.99\n",
      "  -0.06  0.01  0.99 -0.06  0.01  0.98 -0.04 -0.02  0.98 -0.05 -0.01  0.98\n",
      "  -0.04 -0.04  0.98 -0.04 -0.05  0.98 -0.04 -0.05  0.98 -0.05 -0.05  0.98\n",
      "  -0.06 -0.02  0.99 -0.05 -0.04  0.98 -0.07  0.01  0.98 -0.05 -0.05  0.97]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.02  0.98\n",
      "  -0.06 -0.02  0.99 -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98]\n",
      " [ 0.   -0.01  0.95 -0.02 -0.02  0.94  0.04  0.01  1.01  0.01  0.02  0.99\n",
      "   0.02 -0.01  0.98  0.02 -0.01  0.98  0.03  0.02  0.96  0.06  0.01  1.\n",
      "   0.08  0.01  1.01 -0.01 -0.01  0.99  0.09  0.01  1.    0.02 -0.05  0.98\n",
      "   0.02  0.01  0.98  0.02 -0.    0.97  0.03 -0.06  0.99  0.05  0.01  1.01\n",
      "  -0.01 -0.02  0.94 -0.01 -0.01  0.99 -0.01 -0.03  0.98  0.01 -0.02  1.  ]\n",
      " [-0.03  0.07  0.96  0.02  0.12  0.96  0.04  0.16  0.97  0.03  0.11  0.98\n",
      "  -0.01  0.03  1.   -0.    0.09  0.93  0.04  0.15  0.96 -0.01  0.09  0.97\n",
      "   0.04  0.08  1.01 -0.    0.1   0.99  0.05  0.1   1.    0.01  0.09  0.98\n",
      "  -0.03  0.03  0.99 -0.01  0.11  0.99 -0.03  0.07  0.97  0.01  0.1   0.99\n",
      "  -0.03  0.05  0.99  0.01  0.07  0.97 -0.03  0.05  0.97  0.    0.09  0.99]\n",
      " [-0.05 -0.04  0.97 -0.05 -0.03  0.97 -0.05  0.01  1.   -0.04 -0.05  0.98\n",
      "  -0.05 -0.02  0.99 -0.05 -0.03  0.99 -0.05  0.01  0.99 -0.05 -0.02  0.99\n",
      "  -0.05  0.01  0.99 -0.07  0.01  0.99 -0.05  0.01  0.99 -0.05 -0.    0.98\n",
      "  -0.04 -0.03  0.98 -0.04 -0.01  0.98 -0.04 -0.04  0.98 -0.04 -0.05  0.98\n",
      "  -0.04 -0.05  0.98 -0.07 -0.02  0.99 -0.07  0.    0.99 -0.06  0.    0.99]\n",
      " [ 0.06  0.1   1.   -0.01  0.09  1.    0.02  0.14  0.98 -0.01  0.08  0.96\n",
      "  -0.    0.12  0.96 -0.01  0.12  0.98 -0.01  0.06  0.96 -0.04  0.05  0.96\n",
      "   0.02  0.12  0.96  0.03  0.13  1.    0.03  0.11  0.97  0.02  0.11  1.\n",
      "   0.01  0.09  1.01  0.02  0.08  1.04 -0.04  0.06  0.98 -0.04  0.11  0.94\n",
      "  -0.    0.11  1.02 -0.08  0.04  0.99 -0.03  0.07  0.98  0.05  0.15  0.99]\n",
      " [-0.09 -0.02  0.98 -0.05 -0.    0.98 -0.06 -0.    0.99 -0.05 -0.02  0.98\n",
      "  -0.04  0.01  0.99 -0.07  0.01  0.98 -0.08  0.03  0.99 -0.05 -0.    0.99\n",
      "  -0.04 -0.03  0.97 -0.07 -0.02  0.98 -0.07  0.01  0.99 -0.06  0.03  0.99\n",
      "  -0.09  0.01  0.98 -0.06  0.    0.98 -0.02  0.01  1.   -0.07 -0.01  0.98\n",
      "  -0.05 -0.01  0.98 -0.02 -0.02  0.98 -0.05 -0.02  0.98 -0.06  0.    0.98]\n",
      " [ 0.03  0.07  1.02 -0.04  0.07  0.98 -0.05  0.12  0.94  0.    0.07  0.96\n",
      "   0.03  0.15  0.97 -0.05  0.06  1.    0.05  0.09  1.   -0.01  0.06  0.97\n",
      "  -0.02  0.08  1.    0.02  0.12  0.96 -0.05  0.07  0.96 -0.05  0.07  0.94\n",
      "   0.    0.11  1.   -0.03  0.15  0.96 -0.01  0.09  0.93  0.06  0.15  1.01\n",
      "  -0.01  0.09  0.97 -0.05  0.02  0.97  0.01  0.11  1.   -0.06  0.05  1.  ]\n",
      " [ 0.02  0.12  0.98  0.    0.11  0.95  0.02  0.11  0.96  0.02  0.14  0.98\n",
      "   0.02  0.11  0.96  0.03  0.14  0.97 -0.01  0.07  0.93  0.05  0.14  0.95\n",
      "  -0.01  0.07  1.    0.01  0.04  1.01 -0.    0.07  0.94  0.07  0.12  0.94\n",
      "  -0.02  0.07  0.99 -0.04  0.02  0.99 -0.    0.08  0.99  0.03  0.09  1.03\n",
      "  -0.04  0.06  0.99  0.03  0.06  1.01 -0.01  0.09  0.98 -0.04  0.04  1.02]\n",
      " [ 0.03  0.11  0.96  0.05  0.09  1.01 -0.01  0.11  1.    0.04  0.15  1.02\n",
      "  -0.    0.09  0.99 -0.05  0.04  0.98 -0.    0.1   0.99 -0.04  0.06  0.92\n",
      "  -0.03  0.07  0.97  0.01  0.13  0.96 -0.    0.12  0.99 -0.02  0.05  0.96\n",
      "  -0.    0.09  1.    0.04  0.12  0.97  0.03  0.11  0.99  0.    0.11  0.99\n",
      "   0.03  0.14  0.98  0.03  0.1   1.    0.03  0.1   1.    0.03  0.1   0.96]\n",
      " [ 0.06  0.05  1.03  0.02  0.01  0.98  0.02  0.    0.99  0.02 -0.    0.97\n",
      "   0.09  0.01  0.95  0.05  0.    0.95  0.01  0.01  1.01 -0.06 -0.01  0.99\n",
      "   0.05  0.01  0.95  0.02  0.01  0.98  0.02  0.    0.98  0.02  0.04  1.04\n",
      "  -0.03  0.    1.01  0.06  0.02  1.    0.07  0.01  0.96 -0.02  0.02  1.06\n",
      "   0.01 -0.03  0.95  0.02  0.    0.99  0.02  0.01  0.98 -0.02 -0.02  0.95]\n",
      " [-0.02  0.11  0.99 -0.03  0.05  1.    0.02  0.11  0.96 -0.05  0.07  0.97\n",
      "  -0.03  0.07  0.97  0.03  0.1   0.97 -0.    0.07  1.   -0.03  0.07  1.01\n",
      "   0.    0.06  1.03 -0.03  0.07  0.99 -0.03  0.04  1.    0.    0.1   0.99\n",
      "   0.03  0.09  1.04 -0.04  0.1   0.98 -0.07  0.03  0.96 -0.01  0.08  0.94\n",
      "  -0.    0.01  0.96 -0.04  0.06  0.99 -0.07  0.04  0.96 -0.    0.09  0.97]\n",
      " [-0.06 -0.02  0.98 -0.05 -0.05  0.98 -0.07 -0.    0.99 -0.06 -0.02  0.98\n",
      "  -0.06 -0.    0.98 -0.06 -0.03  0.97 -0.05 -0.05  0.97 -0.06 -0.04  0.97\n",
      "  -0.04 -0.05  0.98 -0.06 -0.03  0.98 -0.04 -0.04  0.98 -0.06 -0.05  0.97\n",
      "  -0.06 -0.01  0.99 -0.06 -0.03  0.98 -0.07  0.01  0.99 -0.05 -0.02  0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.01  0.98 -0.04 -0.03  0.98 -0.05 -0.05  0.97]\n",
      " [-0.04  0.    0.99 -0.07 -0.01  0.98 -0.03 -0.02  0.98 -0.02 -0.01  0.98\n",
      "  -0.05 -0.    0.98 -0.11  0.    0.98 -0.05 -0.02  0.98 -0.04  0.01  0.99\n",
      "  -0.09  0.03  0.99 -0.06  0.01  0.99 -0.05  0.    0.98 -0.03 -0.01  0.98\n",
      "  -0.07  0.01  0.98 -0.07 -0.02  0.98 -0.02  0.02  0.99 -0.05  0.    0.98\n",
      "  -0.07  0.    0.99 -0.04 -0.    0.98 -0.04 -0.01  0.98 -0.06  0.03  0.99]\n",
      " [-0.03  0.01  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.    0.98 -0.05  0.02  0.98 -0.03  0.04  0.99 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.99 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98]\n",
      " [-0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.02  0.98\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.03  0.03  0.97  0.01  0.1   0.97  0.04  0.12  0.98  0.03  0.09  0.99\n",
      "  -0.01  0.08  0.96  0.03  0.13  0.98  0.01  0.09  1.01  0.04  0.1   1.04\n",
      "  -0.01  0.07  0.99 -0.03  0.04  0.99 -0.    0.11  1.02 -0.07  0.03  0.99\n",
      "  -0.05  0.06  0.99  0.05  0.15  0.99 -0.01  0.08  0.98  0.05  0.09  1.01\n",
      "  -0.03  0.07  0.99 -0.07  0.06  0.96 -0.    0.09  0.98 -0.04  0.05  0.91]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.06 -0.02  0.98\n",
      "  -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98]\n",
      " [-0.07 -0.02  0.98 -0.07  0.01  0.99 -0.03  0.01  0.99 -0.07  0.01  0.99\n",
      "  -0.05 -0.    0.98 -0.05 -0.04  0.98 -0.07 -0.01  0.98 -0.05 -0.02  0.98\n",
      "  -0.09 -0.02  0.98 -0.05 -0.    0.98 -0.06 -0.    0.99 -0.05 -0.02  0.98\n",
      "  -0.04  0.01  0.99 -0.07  0.01  0.98 -0.08  0.03  0.99 -0.05 -0.    0.99\n",
      "  -0.04 -0.03  0.97 -0.07 -0.02  0.98 -0.07  0.01  0.99 -0.06  0.03  0.99]\n",
      " [-0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.03  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98]\n",
      " [-0.09  0.01  0.98 -0.06  0.    0.98  0.   -0.    0.99 -0.06 -0.02  0.98\n",
      "  -0.04  0.01  0.99 -0.04  0.02  0.99 -0.05 -0.02  0.97 -0.05  0.    0.98\n",
      "  -0.07 -0.02  0.98 -0.07  0.    0.98 -0.06 -0.02  0.98 -0.02 -0.01  0.99\n",
      "  -0.06 -0.    0.99 -0.06  0.01  0.99 -0.03 -0.    0.98 -0.04  0.01  0.99\n",
      "  -0.04  0.03  0.99 -0.01 -0.01  0.99 -0.06 -0.    0.98 -0.11 -0.03  0.97]\n",
      " [-0.04 -0.02  0.97 -0.02 -0.02  0.94  0.02 -0.01  0.94  0.07  0.01  1.02\n",
      "   0.06 -0.03  0.97  0.02  0.01  0.97  0.02 -0.    0.98  0.02 -0.02  1.\n",
      "   0.01 -0.01  0.97  0.02 -0.01  0.95  0.02  0.    1.02 -0.05 -0.02  0.96\n",
      "   0.06 -0.    0.97  0.02  0.01  0.98  0.02 -0.    0.98  0.    0.01  1.\n",
      "  -0.04 -0.02  0.94 -0.01 -0.01  0.99  0.05  0.01  1.03 -0.04 -0.01  0.98]\n",
      " [ 0.07 -0.    0.93  0.06  0.06  1.06  0.03  0.02  0.98  0.02  0.01  0.98\n",
      "   0.08  0.08  1.08 -0.01  0.01  1.02  0.06  0.02  1.    0.06  0.01  0.95\n",
      "  -0.02  0.03  1.07  0.01 -0.03  0.95  0.02  0.01  0.99  0.02  0.01  0.98\n",
      "   0.07  0.02  0.98  0.02 -0.    0.95  0.07  0.02  0.99  0.1   0.02  0.97\n",
      "   0.07  0.02  0.95  0.03  0.02  0.98  0.02  0.    0.99  0.02  0.02  0.99]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.02  0.98 -0.07 -0.02  0.98]\n",
      " [-0.04 -0.02  0.98 -0.05  0.    0.98 -0.04 -0.02  0.98 -0.04 -0.02  0.98\n",
      "  -0.05 -0.01  0.98 -0.04 -0.02  0.98 -0.04 -0.05  0.98 -0.05 -0.02  0.98\n",
      "  -0.03 -0.05  0.98 -0.05  0.01  0.99 -0.03 -0.05  0.98 -0.04 -0.03  0.98\n",
      "  -0.05 -0.02  0.99 -0.05 -0.01  0.99 -0.05 -0.02  0.99 -0.05 -0.    0.99\n",
      "  -0.05 -0.02  1.   -0.06 -0.05  0.97 -0.07  0.    1.   -0.06 -0.04  0.97]\n",
      " [-0.    0.01  1.01  0.04 -0.01  0.93  0.01  0.05  1.08  0.02 -0.    0.98\n",
      "   0.02  0.01  0.99  0.02 -0.03  0.92  0.04 -0.    0.94 -0.04 -0.    0.99\n",
      "   0.03  0.02  1.02  0.08 -0.    0.91  0.02  0.03  1.    0.02  0.    0.99\n",
      "   0.02  0.01  0.98  0.03  0.04  1.08  0.08  0.01  0.95  0.05  0.02  1.\n",
      "   0.02  0.02  1.03 -0.03 -0.    1.02  0.02 -0.    0.98  0.02  0.01  0.98]\n",
      " [ 0.01  0.11  1.   -0.06  0.05  1.   -0.03  0.07  1.   -0.    0.15  0.95\n",
      "  -0.01  0.09  0.99  0.04  0.16  1.02  0.02  0.11  0.96  0.01  0.05  1.01\n",
      "  -0.01  0.1   1.   -0.03  0.06  0.98  0.    0.09  0.99  0.01  0.07  1.\n",
      "   0.    0.07  0.98 -0.    0.09  1.   -0.01  0.08  0.95  0.03  0.09  1.\n",
      "   0.05  0.13  1.01 -0.01  0.07  0.96  0.03  0.13  0.98  0.02  0.11  0.99]\n",
      " [-0.06 -0.    0.98 -0.06  0.01  0.98 -0.06 -0.01  0.98 -0.05 -0.02  0.97\n",
      "  -0.06  0.02  0.99 -0.04 -0.04  0.97 -0.06  0.01  0.99 -0.04 -0.05  0.98\n",
      "  -0.06 -0.01  0.99 -0.06 -0.02  0.99 -0.06 -0.04  0.97 -0.07  0.01  0.99\n",
      "  -0.05  0.01  0.99 -0.04 -0.03  0.97 -0.05 -0.01  0.99 -0.03 -0.05  0.97\n",
      "  -0.06  0.01  0.99 -0.06 -0.01  0.99 -0.06  0.01  0.99 -0.07  0.01  0.98]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.97 -0.06 -0.02  0.98 -0.07 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98]\n",
      " [-0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.01  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.99 -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.01  0.98]\n",
      " [-0.05 -0.02  0.97 -0.06  0.02  0.99 -0.04 -0.04  0.97 -0.06  0.01  0.99\n",
      "  -0.04 -0.05  0.98 -0.06 -0.01  0.99 -0.06 -0.02  0.99 -0.06 -0.04  0.97\n",
      "  -0.07  0.01  0.99 -0.05  0.01  0.99 -0.04 -0.03  0.97 -0.05 -0.01  0.99\n",
      "  -0.03 -0.05  0.97 -0.06  0.01  0.99 -0.06 -0.01  0.99 -0.06  0.01  0.99\n",
      "  -0.07  0.01  0.98 -0.05 -0.03  0.98 -0.06  0.01  0.98 -0.05 -0.    0.99]\n",
      " [-0.02  0.04  0.97  0.    0.09  1.    0.05  0.1   1.    0.    0.1   0.97\n",
      "  -0.02  0.04  0.97 -0.02  0.07  1.   -0.03  0.05  0.99 -0.04  0.08  0.95\n",
      "  -0.04  0.08  0.99  0.01  0.12  0.96  0.03  0.11  0.98  0.06  0.13  1.03\n",
      "  -0.02  0.07  1.   -0.06  0.06  0.95  0.    0.11  1.01 -0.07  0.08  0.98\n",
      "  -0.04  0.07  0.98  0.05  0.15  0.99 -0.01  0.11  1.01  0.04  0.16  1.01]\n",
      " [-0.05  0.08  0.94 -0.04  0.06  0.99  0.04  0.13  0.98  0.01  0.09  0.98\n",
      "  -0.03  0.02  0.99 -0.04  0.05  0.98 -0.    0.06  1.   -0.01  0.07  0.97\n",
      "  -0.04  0.06  0.99  0.02  0.12  0.97 -0.    0.08  1.01 -0.03  0.02  0.98\n",
      "   0.02  0.1   0.97 -0.03  0.08  0.95 -0.03  0.07  0.98 -0.05  0.07  0.92\n",
      "  -0.    0.08  0.99  0.03  0.11  1.01 -0.01  0.07  0.93 -0.    0.03  0.99]\n",
      " [ 0.06  0.15  0.98 -0.01  0.08  1.   -0.04  0.01  0.99 -0.    0.09  0.95\n",
      "   0.02  0.08  1.03 -0.05  0.06  0.99 -0.04  0.12  0.94 -0.01  0.1   0.99\n",
      "  -0.01  0.14  0.95  0.03  0.11  0.97  0.06  0.11  1.   -0.    0.07  0.98\n",
      "   0.03  0.12  1.03  0.02  0.11  0.98  0.03  0.08  1.01 -0.01  0.08  0.98\n",
      "   0.02  0.12  0.96 -0.04  0.06  1.    0.02  0.08  1.    0.02  0.07  0.98]\n",
      " [-0.06 -0.01  0.98 -0.06 -0.01  0.99 -0.05  0.01  0.99 -0.07  0.01  0.99\n",
      "  -0.05  0.    0.99 -0.04  0.01  0.98 -0.04 -0.02  0.98 -0.04 -0.02  0.98\n",
      "  -0.04 -0.02  0.98 -0.04 -0.02  0.98 -0.03 -0.04  0.98 -0.04 -0.03  0.98\n",
      "  -0.05 -0.04  0.99 -0.05 -0.05  0.98 -0.07 -0.    0.99 -0.05 -0.05  0.98\n",
      "  -0.06  0.    0.98 -0.06 -0.02  0.98 -0.04 -0.04  0.97 -0.06 -0.03  0.97]\n",
      " [ 0.01 -0.02  1.    0.02 -0.    0.98  0.02 -0.    0.98 -0.02  0.04  0.99\n",
      "  -0.03 -0.02  0.94  0.03 -0.01  0.95  0.08  0.01  1.02 -0.01 -0.05  0.99\n",
      "   0.02  0.01  0.98  0.02 -0.01  0.97  0.01 -0.03  1.    0.07 -0.01  0.99\n",
      "  -0.03 -0.02  0.94  0.04  0.01  1.03  0.05  0.02  0.99 -0.    0.01  1.\n",
      "   0.02  0.01  0.98  0.02 -0.    0.98  0.06 -0.01  0.97  0.02 -0.01  0.96]\n",
      " [-0.09  0.    0.96 -0.05  0.    0.97 -0.09  0.02  0.98 -0.06  0.01  0.97\n",
      "  -0.06 -0.01  0.97 -0.04  0.    0.98 -0.06  0.03  0.98 -0.06  0.    0.98\n",
      "  -0.01 -0.03  0.97 -0.05  0.02  0.98 -0.07  0.    0.97 -0.02 -0.01  0.97\n",
      "  -0.05 -0.01  0.97 -0.05 -0.    0.97 -0.05 -0.02  0.97 -0.06  0.02  0.98\n",
      "  -0.06 -0.01  0.97 -0.05  0.03  0.98 -0.05  0.01  0.97 -0.05 -0.01  0.97]\n",
      " [-0.03  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98\n",
      "  -0.04  0.01  0.98 -0.04  0.04  0.99 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98]\n",
      " [-0.07  0.01  0.99 -0.05 -0.04  0.98 -0.05 -0.02  0.97 -0.05 -0.01  0.99\n",
      "  -0.04 -0.04  0.97 -0.05 -0.04  0.98 -0.06 -0.    1.   -0.06 -0.03  0.98\n",
      "  -0.06 -0.    0.98 -0.06 -0.04  0.98 -0.   -0.06  0.98 -0.05  0.02  0.99\n",
      "  -0.07  0.01  0.99 -0.05  0.    0.99 -0.05 -0.01  0.98 -0.05 -0.05  0.97\n",
      "  -0.03 -0.05  0.98 -0.06 -0.03  0.98 -0.07  0.01  0.99 -0.07  0.01  0.99]\n",
      " [-0.05  0.    1.    0.    0.01  1.02  0.02  0.01  0.96  0.1   0.04  0.99\n",
      "  -0.02 -0.03  0.94  0.02 -0.    0.98  0.02  0.01  1.    0.04 -0.02  0.93\n",
      "   0.05  0.    0.94 -0.03 -0.01  0.98 -0.01  0.01  1.02  0.08  0.    0.91\n",
      "   0.02  0.03  1.    0.02  0.01  0.99  0.02 -0.    0.98 -0.04  0.    1.03\n",
      "  -0.02  0.01  1.02  0.01 -0.    0.96  0.09  0.03  0.99 -0.02 -0.05  0.89]\n",
      " [-0.05 -0.02  0.98 -0.04  0.01  0.99 -0.09  0.03  0.99 -0.06  0.01  0.99\n",
      "  -0.05  0.    0.98 -0.03 -0.01  0.98 -0.07  0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.02  0.02  0.99 -0.05  0.    0.98 -0.07  0.    0.99 -0.04 -0.    0.98\n",
      "  -0.04 -0.01  0.98 -0.06  0.03  0.99 -0.01  0.02  1.   -0.05 -0.    0.98\n",
      "  -0.04 -0.04  0.97 -0.06 -0.02  0.98 -0.05  0.02  0.99 -0.08 -0.02  0.98]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.02  0.98 -0.06 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.02  0.98]\n",
      " [-0.05  0.    0.98 -0.04 -0.02  0.98 -0.04 -0.03  0.98 -0.04 -0.04  0.98\n",
      "  -0.06 -0.03  0.99 -0.05 -0.05  0.98 -0.07  0.01  0.98 -0.06 -0.01  0.98\n",
      "  -0.04 -0.04  0.97 -0.05 -0.    0.99 -0.04 -0.05  0.98 -0.05  0.01  0.99\n",
      "  -0.05 -0.03  0.99 -0.05  0.01  1.   -0.07  0.01  0.99 -0.05  0.01  1.\n",
      "  -0.04 -0.04  0.97 -0.05 -0.02  0.99 -0.04 -0.04  0.99 -0.05 -0.    0.99]\n",
      " [ 0.01  0.03  1.01 -0.    0.1   0.99 -0.02  0.01  0.97 -0.02  0.08  0.97\n",
      "   0.05  0.13  0.98 -0.01  0.08  0.98  0.01  0.11  0.96 -0.04  0.06  0.99\n",
      "  -0.03  0.11  0.95 -0.01  0.12  0.98  0.    0.08  0.96 -0.03  0.06  0.95\n",
      "   0.04  0.1   0.99  0.03  0.08  0.99  0.03  0.1   0.96  0.01  0.09  1.\n",
      "  -0.01  0.08  1.01 -0.03  0.04  1.03 -0.04  0.07  0.97 -0.01  0.14  0.95]\n",
      " [ 0.07  0.01  0.97  0.03 -0.01  0.99  0.02 -0.    0.99 -0.   -0.03  1.\n",
      "   0.02  0.    1.    0.07  0.01  0.99  0.02 -0.01  0.94 -0.03 -0.03  0.98\n",
      "   0.03  0.04  0.97  0.02 -0.    0.98  0.02 -0.01  0.98  0.06 -0.04  0.98\n",
      "   0.03  0.    1.01 -0.02 -0.01  0.97  0.08  0.02  1.02  0.08  0.03  0.98\n",
      "   0.02 -0.02  0.99  0.02 -0.01  0.98  0.02 -0.01  0.98  0.06  0.01  1.  ]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.06 -0.02  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99]\n",
      " [-0.05  0.    0.99 -0.02 -0.    0.98 -0.07  0.02  0.99 -0.06 -0.01  0.98\n",
      "  -0.05 -0.02  0.98 -0.06 -0.03  0.97 -0.05 -0.    0.99 -0.08  0.05  1.\n",
      "  -0.04 -0.01  0.98 -0.05  0.02  0.99 -0.09  0.02  0.99 -0.06  0.02  0.99\n",
      "  -0.05  0.    0.98 -0.02  0.    0.98 -0.06 -0.01  0.98 -0.04 -0.    0.98\n",
      "  -0.07  0.03  0.99 -0.05  0.01  0.98 -0.06 -0.01  0.98 -0.06  0.03  0.99]\n",
      " [-0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.    0.99 -0.07 -0.    0.99]\n",
      " [ 0.    0.08  1.01  0.02  0.08  1.04 -0.02  0.07  0.99 -0.04  0.04  0.95\n",
      "  -0.    0.1   1.03 -0.08  0.03  0.97 -0.05  0.06  0.99  0.05  0.16  0.98\n",
      "  -0.    0.09  1.    0.02  0.17  1.    0.04  0.11  0.96  0.03  0.05  1.01\n",
      "  -0.01  0.1   1.   -0.    0.05  1.02  0.    0.09  0.99 -0.07  0.06  0.97\n",
      "  -0.03  0.08  0.98  0.02  0.14  0.99 -0.01  0.08  0.96 -0.05  0.07  0.96]\n",
      " [-0.05  0.06  0.99  0.02  0.07  1.01 -0.    0.07  0.95  0.02  0.02  0.97\n",
      "  -0.    0.09  0.99 -0.05  0.12  0.94 -0.01  0.07  0.95 -0.05  0.08  0.94\n",
      "  -0.04  0.06  0.99  0.05  0.13  0.98  0.01  0.08  0.98 -0.01  0.04  1.\n",
      "   0.03  0.12  0.96  0.03  0.08  1.   -0.01  0.06  0.97  0.01  0.07  0.99\n",
      "   0.02  0.12  0.96 -0.05  0.05  0.98 -0.02  0.03  0.98  0.02  0.09  0.95]\n",
      " [-0.06  0.05  0.97 -0.02  0.08  0.99  0.01  0.14  1.    0.    0.1   0.95\n",
      "  -0.01  0.06  1.    0.04  0.11  1.01 -0.03  0.06  0.97  0.03  0.13  0.98\n",
      "   0.01  0.1   1.01  0.04  0.1   1.01 -0.01  0.07  0.96 -0.02  0.04  1.\n",
      "   0.    0.09  1.02 -0.08  0.05  1.01  0.02  0.11  0.97  0.01  0.15  0.95\n",
      "  -0.    0.11  1.02 -0.08  0.1   0.98  0.03  0.11  0.97  0.05  0.15  0.98]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.06 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.98\n",
      "  -0.06 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98]\n",
      " [-0.04  0.09  0.97 -0.02  0.06  0.97  0.02  0.11  0.99 -0.03  0.09  0.95\n",
      "  -0.02  0.13  0.96 -0.    0.1   0.97  0.02  0.14  0.96  0.01  0.1   0.94\n",
      "   0.06  0.14  0.96 -0.04  0.06  1.   -0.04  0.03  0.98  0.    0.11  1.01\n",
      "  -0.03  0.13  0.96 -0.    0.09  0.99 -0.    0.02  1.01 -0.    0.09  0.97\n",
      "  -0.    0.05  1.02 -0.04  0.07  0.98 -0.06  0.05  0.98 -0.01  0.11  0.99]\n",
      " [ 0.02  0.05  1.06  0.   -0.01  0.94 -0.03 -0.    0.98 -0.02  0.01  1.01\n",
      "   0.    0.04  1.08  0.02  0.01  0.98  0.02  0.01  0.98  0.02  0.01  0.99\n",
      "  -0.05  0.01  1.06 -0.03  0.01  1.01  0.02 -0.    0.94  0.07  0.    0.95\n",
      "   0.07  0.04  1.02  0.03  0.01  0.99  0.02  0.01  0.98  0.    0.01  1.\n",
      "  -0.06 -0.01  0.98  0.05 -0.    0.96 -0.03  0.    1.    0.06  0.03  1.02]\n",
      " [-0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.06 -0.02  0.98 -0.07  0.    0.98 -0.04  0.02  0.99 -0.02 -0.01  0.99\n",
      "  -0.06 -0.    0.99 -0.07 -0.02  0.98 -0.05  0.02  0.99 -0.07  0.    0.98\n",
      "  -0.07  0.    0.98 -0.04  0.03  1.   -0.05  0.    0.98 -0.01 -0.02  0.98\n",
      "  -0.04  0.02  0.99 -0.04 -0.    0.98 -0.02 -0.    0.99 -0.08 -0.02  0.97\n",
      "  -0.06  0.    0.98 -0.03 -0.02  0.98 -0.06  0.02  0.99 -0.07 -0.01  0.98]\n",
      " [-0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98]\n",
      " [-0.05  0.01  0.98 -0.06 -0.02  0.98 -0.04  0.01  0.99 -0.03  0.    0.99\n",
      "  -0.07  0.03  0.99 -0.06  0.    0.99 -0.03  0.03  1.   -0.04  0.01  0.99\n",
      "  -0.04 -0.    0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.06  0.03  0.98\n",
      "  -0.09  0.03  0.98 -0.04  0.    0.99 -0.07 -0.01  0.98 -0.07  0.03  0.99\n",
      "  -0.05  0.01  0.99 -0.06  0.01  0.98 -0.02  0.    0.99 -0.04 -0.01  0.98]\n",
      " [-0.04 -0.03  0.98 -0.04 -0.04  0.98 -0.05 -0.05  0.97 -0.05 -0.04  0.98\n",
      "  -0.05 -0.04  0.98 -0.07 -0.02  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.07  0.01  0.98 -0.06 -0.    0.98 -0.05 -0.01  0.98 -0.06 -0.03  0.98\n",
      "  -0.04 -0.03  0.98 -0.06  0.01  0.99 -0.04 -0.04  0.98 -0.05  0.    0.99\n",
      "  -0.09 -0.    0.99 -0.06 -0.02  0.98 -0.07  0.01  0.99 -0.05 -0.04  0.97]\n",
      " [-0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.99\n",
      "  -0.06 -0.02  0.99 -0.06 -0.02  0.99 -0.07 -0.02  0.98 -0.06 -0.01  0.99]\n",
      " [-0.06  0.01  0.99 -0.08  0.    0.99 -0.05  0.02  0.99 -0.05 -0.02  0.98\n",
      "  -0.05  0.04  1.   -0.05  0.    0.99 -0.02 -0.03  0.97 -0.04  0.02  0.99\n",
      "  -0.05 -0.01  0.98 -0.07  0.03  0.99 -0.07 -0.03  0.97 -0.05  0.    0.98\n",
      "  -0.03  0.    0.98 -0.04  0.02  0.99 -0.07  0.01  0.99 -0.02  0.02  0.99\n",
      "  -0.06  0.    0.99 -0.05  0.01  0.98 -0.06 -0.02  0.98 -0.04  0.01  0.99]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.02  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.06 -0.02  0.99 -0.06 -0.03  0.99 -0.07  0.01  0.99 -0.06  0.02  0.99\n",
      "  -0.03 -0.05  0.98 -0.05 -0.    0.99 -0.05 -0.03  0.99 -0.05 -0.06  0.98\n",
      "  -0.07  0.01  0.99 -0.05 -0.04  0.98 -0.05 -0.02  0.97 -0.05 -0.01  0.99\n",
      "  -0.04 -0.04  0.97 -0.05 -0.04  0.98 -0.06 -0.    1.   -0.06 -0.03  0.98\n",
      "  -0.06 -0.    0.98 -0.06 -0.04  0.98 -0.   -0.06  0.98 -0.05  0.02  0.99]\n",
      " [-0.06  0.03  0.99 -0.07  0.03  0.99 -0.05  0.    0.98 -0.   -0.01  0.99\n",
      "  -0.08 -0.02  0.98 -0.04  0.02  0.99 -0.02 -0.02  0.98 -0.05 -0.02  0.98\n",
      "  -0.05 -0.01  0.98 -0.09 -0.    0.98 -0.05  0.02  0.99 -0.04 -0.02  0.98\n",
      "  -0.08 -0.02  0.98 -0.06 -0.    0.98 -0.06  0.02  0.99 -0.04 -0.02  0.98\n",
      "  -0.04  0.01  0.99 -0.03  0.01  0.99 -0.06 -0.04  0.97 -0.05 -0.    0.99]\n",
      " [-0.04  0.13  0.97  0.03  0.11  0.96  0.04  0.07  1.01 -0.01  0.11  1.\n",
      "  -0.04  0.02  0.97  0.03  0.11  0.96 -0.04  0.11  0.95  0.    0.08  0.97\n",
      "  -0.02  0.04  0.97 -0.    0.09  1.    0.04  0.09  1.    0.03  0.09  0.99\n",
      "  -0.01  0.1   1.    0.03  0.13  0.99  0.    0.08  1.01 -0.02  0.03  0.98\n",
      "  -0.04  0.06  0.98 -0.04  0.05  0.96 -0.    0.1   1.03 -0.07  0.02  0.98]\n",
      " [-0.02  0.01  1.02  0.03  0.    0.95  0.07  0.03  1.01 -0.01 -0.05  0.98\n",
      "   0.02  0.    0.98  0.02  0.    0.99  0.09  0.03  0.97  0.09  0.03  1.\n",
      "   0.04  0.    0.95 -0.03 -0.    1.    0.09  0.03  0.97  0.02  0.01  0.98\n",
      "   0.02  0.    0.99  0.02  0.    0.98 -0.05 -0.    1.04  0.06  0.01  0.95\n",
      "   0.07  0.02  0.99  0.09  0.03  0.99  0.03  0.06  1.1   0.02 -0.    0.99]\n",
      " [-0.05  0.    0.99 -0.06 -0.01  0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.06 -0.    0.98 -0.06  0.    0.99 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.05 -0.01  0.98 -0.05  0.02  0.99 -0.02  0.    0.99 -0.05  0.    0.98\n",
      "  -0.06 -0.02  0.98 -0.06  0.02  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.09  0.01  0.98 -0.06  0.    0.98  0.   -0.    0.99 -0.06 -0.02  0.98\n",
      "  -0.04  0.01  0.99 -0.04  0.02  0.99 -0.05 -0.02  0.97 -0.05  0.    0.98\n",
      "  -0.07 -0.02  0.98 -0.07  0.    0.98 -0.06 -0.02  0.98 -0.02 -0.01  0.99]\n",
      " [ 0.01  0.09  0.98  0.01  0.08  1.   -0.    0.09  1.    0.05  0.13  0.99\n",
      "   0.01  0.08  0.98 -0.01  0.1   1.   -0.02  0.08  0.95  0.03  0.11  0.97\n",
      "   0.04  0.11  1.01  0.01  0.08  0.95 -0.03  0.03  1.    0.01  0.1   1.02\n",
      "  -0.06  0.05  1.03  0.03  0.11  0.97  0.03  0.16  0.97 -0.01  0.11  1.01\n",
      "   0.01  0.17  0.99 -0.04  0.07  0.98 -0.    0.03  1.   -0.01  0.09  0.98]\n",
      " [-0.    0.09  0.93  0.04  0.15  0.96 -0.01  0.09  0.97  0.04  0.08  1.01\n",
      "  -0.    0.1   0.99  0.05  0.1   1.    0.01  0.09  0.98 -0.03  0.03  0.99\n",
      "  -0.01  0.11  0.99 -0.03  0.07  0.97  0.01  0.1   0.99 -0.03  0.05  0.99\n",
      "   0.01  0.07  0.97 -0.03  0.05  0.97  0.    0.09  0.99  0.03  0.09  1.\n",
      "   0.01  0.07  0.98  0.02  0.12  0.99  0.01  0.11  0.95  0.02  0.08  1.  ]\n",
      " [-0.06 -0.    0.99 -0.05 -0.02  0.98 -0.07  0.    0.98 -0.03  0.01  0.99\n",
      "  -0.09  0.02  0.98 -0.05  0.    0.99 -0.03  0.02  0.99 -0.06 -0.02  0.98\n",
      "  -0.06  0.01  0.99 -0.07 -0.02  0.98 -0.06 -0.04  0.97 -0.05 -0.    0.98\n",
      "  -0.04  0.02  0.99 -0.07  0.01  0.99 -0.06 -0.01  0.98 -0.01 -0.02  0.98\n",
      "  -0.05 -0.01  0.98 -0.06 -0.01  0.98 -0.09  0.    0.99 -0.03  0.    0.98]\n",
      " [-0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98]\n",
      " [ 0.06  0.12  1.   -0.01  0.1   1.    0.02  0.18  1.01 -0.02  0.08  0.97\n",
      "   0.06  0.09  1.01 -0.01  0.09  0.99 -0.04  0.02  0.99  0.03  0.12  0.97\n",
      "   0.04  0.08  1.01  0.02  0.08  0.98 -0.01  0.07  0.96 -0.02  0.07  0.95\n",
      "   0.04  0.1   0.99  0.02  0.08  0.99  0.    0.09  0.96  0.03  0.13  0.99\n",
      "   0.01  0.08  1.01  0.    0.05  1.04  0.    0.08  0.99 -0.01  0.06  1.  ]\n",
      " [-0.06 -0.02  0.98 -0.1  -0.    0.98 -0.05  0.    0.98 -0.06 -0.    0.98\n",
      "  -0.04  0.03  0.99 -0.04  0.01  0.99 -0.08 -0.    0.98 -0.08  0.02  0.98\n",
      "  -0.06 -0.    0.99 -0.04 -0.02  0.97 -0.06 -0.02  0.98 -0.07  0.01  0.99\n",
      "  -0.08  0.01  0.98 -0.08  0.03  0.99 -0.06  0.    0.98 -0.01 -0.01  0.99\n",
      "  -0.07 -0.    0.98 -0.05 -0.01  0.98 -0.03 -0.03  0.98 -0.05 -0.02  0.98]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98]\n",
      " [ 0.02  0.01  0.98  0.02  0.01  0.99  0.09  0.01  0.94  0.08  0.01  0.96\n",
      "   0.02  0.01  1.01  0.1   0.03  0.99  0.07  0.02  0.95  0.03  0.02  0.98\n",
      "   0.02  0.01  0.99  0.02  0.02  1.    0.09  0.03  1.    0.08  0.02  0.98\n",
      "   0.02  0.    0.96  0.06  0.03  0.98  0.03 -0.01  0.96  0.02  0.01  0.98\n",
      "   0.02  0.01  0.98 -0.05  0.    1.03  0.07  0.01  0.95 -0.   -0.    0.96]\n",
      " [-0.07  0.01  0.99 -0.05 -0.05  0.97 -0.04 -0.01  0.97 -0.05 -0.05  0.97\n",
      "  -0.04 -0.03  0.97 -0.05 -0.03  0.98 -0.04 -0.04  0.98 -0.06 -0.03  0.97\n",
      "  -0.07  0.01  0.99 -0.06 -0.05  0.97 -0.07  0.01  0.98 -0.05 -0.05  0.97\n",
      "  -0.05 -0.02  0.97 -0.04 -0.05  0.98 -0.03 -0.05  0.97 -0.06 -0.05  0.98\n",
      "  -0.04 -0.05  0.98 -0.05 -0.05  0.98 -0.05 -0.04  0.99 -0.04 -0.05  0.98]\n",
      " [-0.04  0.    0.99 -0.05 -0.01  0.98 -0.08 -0.    0.98 -0.08 -0.03  0.97\n",
      "  -0.06  0.    0.99 -0.06  0.04  0.99 -0.05 -0.02  0.98 -0.05  0.01  0.99\n",
      "  -0.01  0.01  0.99 -0.05 -0.02  0.97 -0.05  0.    0.98 -0.05 -0.03  0.98\n",
      "  -0.07 -0.    0.98 -0.04  0.02  0.99 -0.01 -0.01  0.99 -0.06 -0.    0.99\n",
      "  -0.08 -0.01  0.98 -0.04  0.01  0.99 -0.05  0.01  0.99 -0.07  0.02  0.99]\n",
      " [-0.05 -0.    0.99 -0.03  0.01  0.98 -0.04  0.    0.99 -0.05 -0.01  0.98\n",
      "  -0.08 -0.    0.98 -0.08 -0.03  0.97 -0.06  0.    0.99 -0.06  0.04  0.99\n",
      "  -0.05 -0.02  0.98 -0.05  0.01  0.99 -0.01  0.01  0.99 -0.05 -0.02  0.97\n",
      "  -0.05  0.    0.98 -0.05 -0.03  0.98 -0.07 -0.    0.98 -0.04  0.02  0.99\n",
      "  -0.01 -0.01  0.99 -0.06 -0.    0.99 -0.08 -0.01  0.98 -0.04  0.01  0.99]\n",
      " [-0.05  0.02  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98]\n",
      " [ 0.03  0.11  0.97  0.04  0.15  0.98 -0.    0.09  0.93  0.04  0.13  0.93\n",
      "  -0.04  0.06  1.   -0.05  0.01  0.98 -0.01  0.07  0.93 -0.    0.04  1.01\n",
      "   0.01  0.1   0.96  0.01  0.16  0.96 -0.    0.07  0.97  0.03  0.14  1.01\n",
      "  -0.05  0.06  0.99  0.03  0.07  1.01 -0.01  0.09  0.98 -0.    0.09  0.96\n",
      "  -0.03  0.06  1.   -0.02  0.05  0.99  0.    0.06  0.98 -0.01  0.1   1.  ]\n",
      " [-0.05 -0.01  0.98 -0.04 -0.02  0.98 -0.09 -0.02  0.97 -0.05 -0.    0.99\n",
      "  -0.1   0.03  0.99 -0.03  0.01  0.99 -0.07  0.    0.98 -0.08 -0.01  0.98\n",
      "  -0.06  0.02  1.   -0.05 -0.    0.99 -0.08  0.01  0.98 -0.04  0.    0.98\n",
      "  -0.05  0.02  0.99 -0.06  0.01  0.98 -0.05  0.    0.98 -0.05 -0.01  0.98\n",
      "  -0.07 -0.    0.98 -0.06 -0.01  0.98 -0.06  0.02  0.99 -0.09 -0.02  0.97]\n",
      " [-0.    0.07  0.98  0.02  0.13  0.97  0.    0.1   0.95  0.03  0.1   0.99\n",
      "   0.02  0.11  0.97 -0.03  0.08  1.01 -0.04  0.05  0.96 -0.02  0.07  0.98\n",
      "  -0.06  0.03  0.94 -0.05  0.07  0.99 -0.03  0.12  0.95 -0.    0.1   1.03\n",
      "  -0.06  0.03  1.01 -0.04  0.06  1.    0.02  0.16  0.96 -0.01  0.09  0.99\n",
      "  -0.06  0.11  0.97 -0.02  0.08  0.97  0.01  0.14  0.96 -0.    0.07  0.97]\n",
      " [-0.07  0.02  0.99 -0.05 -0.03  0.98 -0.07  0.01  0.99 -0.04 -0.02  0.99\n",
      "  -0.04 -0.02  0.97 -0.06 -0.05  0.97 -0.04 -0.05  0.98 -0.05 -0.05  0.98\n",
      "  -0.05 -0.04  0.98 -0.05 -0.05  0.98 -0.06 -0.02  0.99 -0.06 -0.02  0.99\n",
      "  -0.06 -0.02  0.99 -0.07  0.01  0.98 -0.06 -0.02  0.98 -0.07  0.01  0.98\n",
      "  -0.06  0.01  0.99 -0.06 -0.01  0.97 -0.06  0.01  0.99 -0.04 -0.04  0.97]\n",
      " [ 0.02  0.01  0.98  0.02  0.01  0.98  0.1   0.01  0.93 -0.03 -0.01  0.97\n",
      "  -0.03 -0.01  0.98  0.08  0.02  0.98  0.07  0.03  0.99  0.03  0.02  1.01\n",
      "   0.02  0.01  0.98  0.03  0.01  1.    0.1   0.02  0.97  0.05  0.    0.95\n",
      "   0.02  0.01  1.01 -0.01 -0.02  0.95  0.02  0.06  1.09  0.02  0.01  0.98\n",
      "   0.02  0.01  0.99  0.01 -0.04  0.92 -0.02 -0.01  0.95 -0.    0.01  1.02]\n",
      " [ 0.02  0.06  0.99  0.03  0.1   0.97 -0.    0.08  1.01 -0.01  0.08  1.01\n",
      "  -0.04  0.04  1.02 -0.02  0.08  0.96  0.02  0.16  0.96 -0.    0.1   1.02\n",
      "   0.    0.12  0.93  0.02  0.1   0.98  0.01  0.03  1.01 -0.01  0.11  1.\n",
      "  -0.03  0.02  0.99  0.    0.1   0.96 -0.03  0.12  0.95  0.    0.08  0.97\n",
      "   0.01  0.08  1.01 -0.01  0.08  1.    0.02  0.07  1.    0.02  0.07  0.99]\n",
      " [ 0.06  0.04  0.97  0.02 -0.01  0.98  0.02 -0.    0.98  0.01 -0.03  0.99\n",
      "  -0.03 -0.02  0.95  0.08  0.01  1.   -0.01 -0.02  0.94  0.08  0.02  0.99\n",
      "   0.04  0.01  0.97  0.02 -0.    0.99  0.02 -0.01  0.98 -0.01  0.03  0.99\n",
      "   0.05  0.01  1.01  0.04  0.01  1.03  0.07  0.01  0.94 -0.04 -0.01  0.99\n",
      "   0.02  0.01  0.98  0.02 -0.01  0.97  0.03  0.    0.99  0.07 -0.    0.98]\n",
      " [ 0.02  0.01  0.98  0.02 -0.    0.99 -0.02 -0.05  0.99  0.   -0.01  0.99\n",
      "   0.02  0.    1.02  0.06  0.    0.98  0.09  0.02  0.98  0.03  0.02  0.97\n",
      "   0.02 -0.01  0.97  0.02 -0.01  1.    0.09  0.    0.98  0.05  0.01  1.02\n",
      "  -0.01 -0.02  0.94  0.06  0.01  1.    0.04 -0.04  0.97  0.02 -0.01  0.98\n",
      "   0.02 -0.    0.97  0.02  0.03  1.   -0.05 -0.02  0.95  0.03 -0.01  0.95]\n",
      " [-0.04 -0.02  0.98 -0.03 -0.03  0.98 -0.04 -0.02  0.98 -0.04 -0.05  0.99\n",
      "  -0.05 -0.06  0.98 -0.08  0.01  0.99 -0.06  0.01  0.98 -0.06 -0.01  0.97\n",
      "  -0.06  0.02  0.99 -0.03 -0.05  0.98 -0.05  0.02  0.99 -0.05 -0.02  1.\n",
      "  -0.05  0.01  1.   -0.07  0.01  0.99 -0.06 -0.03  0.98 -0.03 -0.05  0.98\n",
      "  -0.06 -0.01  0.98 -0.05  0.01  1.   -0.06 -0.02  0.98 -0.07  0.01  0.99]\n",
      " [-0.05 -0.02  0.97 -0.06  0.01  0.99 -0.04 -0.04  0.98 -0.06 -0.05  0.97\n",
      "  -0.05 -0.02  0.97 -0.06 -0.02  0.98 -0.04 -0.04  0.97 -0.06 -0.05  0.97\n",
      "  -0.04 -0.04  0.99 -0.06 -0.05  0.97 -0.06 -0.    1.   -0.06 -0.05  0.97\n",
      "  -0.07  0.01  0.99 -0.06 -0.03  0.98 -0.05 -0.02  0.97 -0.06 -0.04  0.98\n",
      "  -0.04 -0.05  0.98 -0.06 -0.02  0.98 -0.05 -0.04  0.99 -0.06 -0.04  0.98]\n",
      " [-0.03  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98]\n",
      " [ 0.04  0.08  1.01 -0.01  0.07  0.96 -0.03  0.01  0.96  0.01  0.1   0.96\n",
      "  -0.04  0.13  0.94 -0.    0.08  0.97 -0.04  0.05  0.91  0.04  0.11  0.96\n",
      "  -0.02  0.12  0.95  0.01  0.08  0.97 -0.03  0.04  0.98  0.01  0.1   0.99\n",
      "   0.05  0.1   1.    0.03  0.11  1.   -0.01  0.11  1.    0.    0.12  1.\n",
      "  -0.02  0.07  1.01 -0.03  0.03  0.96 -0.04  0.07  0.98  0.02  0.13  0.96]\n",
      " [-0.06 -0.01  0.99 -0.05 -0.02  1.   -0.05 -0.04  0.98 -0.07  0.01  0.99\n",
      "  -0.05 -0.01  0.99 -0.06 -0.01  0.97 -0.05 -0.05  0.97 -0.04 -0.04  0.98\n",
      "  -0.05 -0.04  0.97 -0.07  0.01  1.   -0.06 -0.04  0.97 -0.07  0.01  0.98\n",
      "  -0.05 -0.05  0.97 -0.05 -0.02  0.97 -0.05 -0.05  0.97 -0.03 -0.05  0.97\n",
      "  -0.04 -0.05  0.98 -0.04 -0.05  0.98 -0.06 -0.03  0.98 -0.06 -0.03  0.99]\n",
      " [-0.02  0.07  0.96 -0.03  0.04  0.99 -0.01  0.08  1.02 -0.05  0.02  0.97\n",
      "   0.02  0.11  0.95 -0.02  0.11  0.95 -0.    0.1   1.02 -0.03  0.12  0.94\n",
      "   0.01  0.1   0.98 -0.07  0.03  0.96 -0.01  0.08  0.95  0.04  0.14  0.99\n",
      "  -0.05  0.06  0.99 -0.04  0.05  0.99 -0.01  0.11  0.99 -0.02  0.11  0.93\n",
      "  -0.01  0.09  0.96 -0.04  0.09  0.95 -0.04  0.09  0.97 -0.02  0.06  0.97]\n",
      " [ 0.02  0.08  0.97  0.04  0.11  1.02  0.01  0.09  0.95 -0.01  0.04  1.\n",
      "   0.01  0.11  0.99 -0.    0.06  1.03  0.03  0.12  0.97 -0.05  0.09  0.95\n",
      "  -0.    0.11  1.02 -0.07  0.1   0.99 -0.04  0.07  0.99  0.06  0.12  1.\n",
      "  -0.01  0.1   1.    0.02  0.18  1.01 -0.02  0.08  0.97  0.06  0.09  1.01\n",
      "  -0.01  0.09  0.99 -0.04  0.02  0.99  0.03  0.12  0.97  0.04  0.08  1.01]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.02  0.98\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.99]\n",
      " [-0.05 -0.05  0.98 -0.05 -0.03  0.99 -0.05 -0.05  0.98 -0.06 -0.01  1.\n",
      "  -0.05 -0.05  0.98 -0.07  0.01  0.98 -0.05 -0.04  0.98 -0.06 -0.01  0.97\n",
      "  -0.06 -0.05  0.97 -0.04 -0.04  0.97 -0.05 -0.05  0.97 -0.04 -0.05  0.98\n",
      "  -0.05 -0.02  0.98 -0.05 -0.02  0.99 -0.05 -0.03  0.98 -0.07  0.01  0.99\n",
      "  -0.06 -0.04  0.97 -0.05 -0.01  0.98 -0.05 -0.05  0.97 -0.05 -0.02  0.97]\n",
      " [-0.01  0.08  0.97 -0.02  0.14  0.96 -0.04  0.07  0.98  0.06  0.12  0.99\n",
      "  -0.    0.07  0.98 -0.03  0.03  0.99  0.03  0.12  0.96  0.05  0.11  0.96\n",
      "   0.01  0.11  0.98 -0.02  0.05  0.96 -0.04  0.05  0.97  0.01  0.12  0.95\n",
      "  -0.02  0.11  0.97 -0.02  0.09  1.   -0.03  0.07  0.96  0.02  0.12  0.96\n",
      "   0.04  0.16  0.97  0.03  0.11  0.98 -0.01  0.03  1.   -0.    0.09  0.93]\n",
      " [ 0.02  0.04  1.02  0.02  0.    0.99  0.02  0.01  0.98 -0.04 -0.06  0.89\n",
      "   0.05  0.    0.94 -0.04 -0.    0.99  0.04  0.02  1.01 -0.01 -0.03  0.95\n",
      "   0.02  0.    0.97  0.02  0.01  0.98  0.02  0.01  0.99  0.09  0.03  0.96\n",
      "  -0.02 -0.01  0.97  0.08  0.01  0.95  0.03 -0.01  0.94 -0.   -0.05  0.86\n",
      "   0.02  0.01  0.98  0.02  0.01  0.98  0.04  0.02  0.99  0.05  0.02  1.01]\n",
      " [-0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.99 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98]\n",
      " [-0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.04  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.04  0.98 -0.04  0.02  0.98 -0.05  0.05  0.98 -0.03  0.01  0.98]\n",
      " [-0.01  0.07  0.93 -0.    0.13  0.93  0.02  0.11  0.96  0.03  0.09  1.01\n",
      "  -0.01  0.08  0.92  0.06  0.07  0.97  0.04  0.12  0.96 -0.06  0.02  0.98\n",
      "  -0.    0.07  0.95 -0.04  0.    0.95 -0.    0.08  0.99 -0.    0.14  0.96\n",
      "  -0.    0.07  0.96  0.02  0.09  0.99  0.02  0.11  0.96 -0.04  0.05  0.98\n",
      "  -0.02  0.1   0.98  0.01  0.15  0.95  0.02  0.12  0.96 -0.02  0.1   0.96]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97]\n",
      " [-0.04 -0.03  0.98 -0.05 -0.02  0.99 -0.05 -0.01  0.99 -0.05 -0.02  0.99\n",
      "  -0.05 -0.    0.99 -0.05 -0.02  1.   -0.06 -0.05  0.97 -0.07  0.    1.\n",
      "  -0.06 -0.04  0.97 -0.07  0.02  0.99 -0.06 -0.04  0.97 -0.07  0.01  0.99\n",
      "  -0.05  0.01  0.99 -0.08  0.02  0.99 -0.06  0.01  0.99 -0.06  0.01  0.98\n",
      "  -0.04 -0.02  0.98 -0.05 -0.01  0.98 -0.04 -0.04  0.98 -0.04 -0.05  0.98]\n",
      " [-0.05  0.    0.98 -0.08 -0.05  0.96 -0.07  0.01  0.98 -0.05 -0.01  0.98\n",
      "  -0.07 -0.03  0.98 -0.06  0.01  0.99 -0.05  0.    0.98 -0.07  0.03  0.99\n",
      "  -0.04  0.01  0.99 -0.06  0.02  0.99 -0.1  -0.    0.98 -0.05  0.    0.99\n",
      "  -0.04  0.02  0.99 -0.07 -0.    0.98 -0.06 -0.02  0.98 -0.03 -0.01  0.98\n",
      "  -0.08 -0.03  0.97 -0.05  0.    0.98 -0.09  0.04  1.   -0.07 -0.01  0.98]\n",
      " [-0.03 -0.02  0.98  0.02 -0.02  0.97  0.03  0.01  0.98  0.02  0.01  0.98\n",
      "   0.07 -0.    0.91  0.08  0.02  1.    0.01  0.01  1.01 -0.03  0.01  1.01\n",
      "  -0.02 -0.04  0.92  0.02  0.01  0.98  0.02  0.    0.99  0.04  0.03  0.98\n",
      "   0.07  0.    0.94  0.07  0.02  0.99  0.01  0.    0.96 -0.03  0.02  1.03\n",
      "   0.02  0.05  1.04  0.02 -0.    0.99  0.02  0.    0.98  0.02  0.05  1.06]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.06 -0.03  0.98 -0.06 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.02  0.98]\n",
      " [ 0.07  0.01  0.95 -0.02  0.01  1.02  0.03  0.05  1.07  0.03  0.02  1.\n",
      "   0.02  0.01  0.98  0.03  0.02  0.99  0.   -0.01  0.97  0.09  0.02  0.97\n",
      "  -0.04  0.    1.    0.07  0.    0.94  0.01  0.04  1.07  0.03  0.01  0.98\n",
      "   0.02  0.01  0.99  0.01 -0.04  0.92  0.08  0.03  1.   -0.03  0.    1.01\n",
      "   0.02  0.01  1.01  0.04 -0.02  0.91  0.04  0.04  1.02  0.02  0.01  0.98]\n",
      " [-0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.04  0.07  0.99  0.02  0.12  0.96 -0.06  0.08  1.01 -0.03  0.03  0.97\n",
      "   0.03  0.1   0.96 -0.05  0.05  0.96 -0.03  0.07  0.98 -0.06  0.06  0.92\n",
      "   0.03  0.11  0.97  0.04  0.14  0.99 -0.01  0.07  0.94  0.04  0.05  0.99\n",
      "  -0.02  0.08  0.97 -0.05  0.12  0.94 -0.01  0.08  0.96  0.02  0.14  0.94\n",
      "  -0.02  0.07  1.    0.05  0.14  0.98  0.01  0.08  0.97  0.03  0.1   1.  ]\n",
      " [ 0.08  0.    0.91  0.02  0.03  1.    0.02  0.01  0.99  0.02 -0.    0.98\n",
      "  -0.04  0.    1.03 -0.02  0.01  1.02  0.01 -0.    0.96  0.09  0.03  0.99\n",
      "  -0.02 -0.05  0.89  0.02 -0.    0.98  0.02  0.01  0.98  0.04  0.02  0.99\n",
      "  -0.05 -0.01  0.96 -0.03  0.    1.    0.06  0.02  0.99  0.02  0.03  1.03\n",
      "   0.02  0.04  1.02  0.02  0.    0.99  0.02  0.01  0.98 -0.04 -0.06  0.89]\n",
      " [-0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.06  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98]\n",
      " [-0.04  0.06  0.99 -0.07  0.03  0.94 -0.02  0.09  0.97  0.    0.12  0.95\n",
      "  -0.01  0.07  0.96  0.04  0.13  0.93  0.04  0.11  0.96 -0.01  0.02  1.01\n",
      "  -0.    0.1   0.98  0.04  0.07  1.   -0.02  0.07  0.99 -0.05  0.04  0.98\n",
      "  -0.01  0.1   0.99 -0.04  0.02  0.97 -0.04  0.06  1.   -0.06  0.1   0.95\n",
      "  -0.03  0.11  0.98 -0.03  0.05  0.99 -0.03  0.06  1.   -0.01  0.11  0.95]\n",
      " [-0.05  0.    0.99 -0.04  0.01  0.98 -0.04 -0.02  0.98 -0.04 -0.02  0.98\n",
      "  -0.04 -0.02  0.98 -0.04 -0.02  0.98 -0.03 -0.04  0.98 -0.04 -0.03  0.98\n",
      "  -0.05 -0.04  0.99 -0.05 -0.05  0.98 -0.07 -0.    0.99 -0.05 -0.05  0.98\n",
      "  -0.06  0.    0.98 -0.06 -0.02  0.98 -0.04 -0.04  0.97 -0.06 -0.03  0.97\n",
      "  -0.04 -0.04  0.99 -0.06  0.    0.99 -0.06 -0.01  1.   -0.05 -0.    0.99]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98]\n",
      " [-0.05 -0.05  0.98 -0.05 -0.04  0.98 -0.05 -0.05  0.98 -0.06 -0.02  0.99\n",
      "  -0.06 -0.02  0.99 -0.06 -0.02  0.99 -0.07  0.01  0.98 -0.06 -0.02  0.98\n",
      "  -0.07  0.01  0.98 -0.06  0.01  0.99 -0.06 -0.01  0.97 -0.06  0.01  0.99\n",
      "  -0.04 -0.04  0.97 -0.06 -0.01  0.98 -0.04 -0.04  0.99 -0.06 -0.03  0.97\n",
      "  -0.06 -0.    1.   -0.05  0.01  1.   -0.07  0.01  0.98 -0.05  0.    0.99]\n",
      " [-0.02  0.    0.98  0.02 -0.02  0.98  0.02 -0.    0.99  0.02 -0.    0.99\n",
      "   0.1   0.    0.98  0.08  0.01  1.02  0.05  0.01  1.03 -0.04 -0.02  0.95\n",
      "   0.05  0.04  0.98  0.02 -0.01  0.99  0.02 -0.    0.99  0.02  0.    0.97\n",
      "   0.08  0.    0.99  0.04 -0.    0.96  0.02  0.    1.02 -0.03 -0.02  0.97\n",
      "   0.05  0.05  0.98  0.02  0.    0.98  0.02 -0.    0.98 -0.01 -0.02  1.  ]\n",
      " [-0.01 -0.02  0.94 -0.04 -0.02  0.95 -0.01  0.01  0.99  0.02  0.01  0.97\n",
      "   0.02 -0.01  0.99  0.02 -0.05  0.97  0.   -0.    0.98 -0.   -0.02  0.93\n",
      "  -0.02 -0.01  0.97  0.06 -0.02  0.98  0.03  0.03  0.97  0.02  0.    0.98\n",
      "   0.02 -0.    0.98  0.04 -0.03  0.97 -0.01 -0.02  0.95 -0.   -0.02  0.93\n",
      "   0.08  0.01  1.02  0.04 -0.03  0.97  0.02  0.01  0.97  0.02 -0.    0.97]\n",
      " [-0.04 -0.03  0.98 -0.04 -0.04  0.98 -0.04 -0.01  0.98 -0.05  0.01  0.98\n",
      "  -0.05  0.01  0.99 -0.05  0.01  0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06 -0.    0.98 -0.04 -0.01  0.98 -0.04 -0.04  0.98 -0.05 -0.04  0.98\n",
      "  -0.05 -0.03  0.99 -0.05 -0.05  0.98 -0.07  0.    0.99 -0.06 -0.01  0.98\n",
      "  -0.07  0.    0.99 -0.05 -0.05  0.98 -0.07  0.01  0.99 -0.05 -0.05  0.97]\n",
      " [-0.02 -0.02  0.99  0.08  0.01  1.    0.04 -0.01  0.95 -0.01 -0.02  0.94\n",
      "   0.05 -0.02  0.98  0.03  0.02  0.97  0.02 -0.01  0.97  0.03 -0.    0.99\n",
      "  -0.03 -0.01  0.97 -0.02 -0.01  0.98  0.03 -0.01  0.95  0.08  0.02  1.02\n",
      "  -0.02 -0.02  1.    0.02 -0.02  0.99  0.02 -0.    0.99  0.04  0.02  0.97\n",
      "   0.07  0.    0.99  0.07  0.01  1.    0.   -0.02  0.94 -0.01 -0.02  0.97]\n",
      " [ 0.01  0.08  0.95  0.02  0.12  1.   -0.    0.09  1.02 -0.02  0.03  0.99\n",
      "   0.01  0.1   0.95 -0.05  0.07  0.95 -0.01  0.09  1.02 -0.08  0.09  0.99\n",
      "  -0.03  0.07  0.99  0.06  0.14  0.99 -0.01  0.11  1.01  0.01  0.17  0.99\n",
      "  -0.04  0.07  0.99  0.04  0.06  1.01 -0.01  0.11  1.   -0.01  0.06  1.03\n",
      "  -0.    0.1   0.97  0.06  0.09  1.   -0.    0.07  0.97 -0.04  0.04  0.99]\n",
      " [-0.05  0.01  0.98 -0.05  0.01  0.99 -0.05  0.01  0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.06 -0.    0.98 -0.04 -0.01  0.98 -0.04 -0.04  0.98\n",
      "  -0.05 -0.04  0.98 -0.05 -0.03  0.99 -0.05 -0.05  0.98 -0.07  0.    0.99\n",
      "  -0.06 -0.01  0.98 -0.07  0.    0.99 -0.05 -0.05  0.98 -0.07  0.01  0.99\n",
      "  -0.05 -0.05  0.97 -0.06  0.    0.98 -0.06 -0.05  0.97 -0.07  0.01  0.99]\n",
      " [-0.02  0.    0.98 -0.06 -0.01  0.98 -0.04 -0.    0.98 -0.07  0.03  0.99\n",
      "  -0.05  0.01  0.98 -0.06 -0.01  0.98 -0.06  0.03  0.99 -0.07  0.01  0.99\n",
      "  -0.05 -0.02  0.98 -0.1  -0.01  0.97 -0.05  0.01  0.98 -0.07  0.05  1.\n",
      "  -0.07  0.01  0.98 -0.04  0.    0.98 -0.09  0.02  0.99 -0.04 -0.02  0.98\n",
      "  -0.05  0.    0.98 -0.06  0.02  0.99 -0.06  0.02  0.99 -0.05 -0.01  0.98]\n",
      " [-0.04 -0.02  0.98 -0.1  -0.01  0.97 -0.06 -0.    0.99 -0.02  0.04  1.\n",
      "  -0.04  0.02  0.99 -0.05 -0.01  0.98 -0.02  0.    0.99 -0.08 -0.01  0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.02  0.98 -0.04  0.01  0.99 -0.06  0.02  0.99\n",
      "  -0.08 -0.02  0.98 -0.06 -0.    0.99 -0.05  0.01  0.98 -0.06  0.02  0.99\n",
      "  -0.04 -0.01  0.98 -0.08 -0.01  0.98 -0.01 -0.02  0.98 -0.05 -0.    0.98]\n",
      " [-0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.02  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.01  0.98]\n",
      " [-0.02 -0.01  0.95 -0.    0.01  1.02  0.07  0.02  0.98 -0.02 -0.02  0.96\n",
      "   0.03  0.02  1.    0.02  0.    0.98  0.02  0.01  0.99  0.07  0.02  0.99\n",
      "   0.06  0.02  1.01  0.05  0.02  1.   -0.05 -0.01  0.99  0.06  0.05  1.03\n",
      "   0.02  0.01  0.98  0.02  0.    0.99  0.02 -0.    0.97  0.09  0.01  0.95\n",
      "   0.05  0.    0.95  0.01  0.01  1.01 -0.06 -0.01  0.99  0.05  0.01  0.95]\n",
      " [-0.03  0.07  0.96 -0.05  0.08  0.91  0.02  0.1   0.98  0.04  0.15  0.98\n",
      "  -0.01  0.08  0.97  0.07  0.11  0.95 -0.05  0.06  0.99 -0.07  0.07  0.95\n",
      "  -0.01  0.07  0.95  0.02  0.14  0.93 -0.03  0.07  0.99  0.06  0.11  0.99\n",
      "   0.    0.07  0.97 -0.01  0.06  1.03 -0.01  0.08  1.01  0.03  0.08  1.\n",
      "   0.03  0.09  0.99  0.    0.11  0.99 -0.01  0.13  0.99 -0.01  0.07  1.01]\n",
      " [-0.02 -0.02  0.97  0.06 -0.02  0.97  0.02 -0.01  0.99  0.02 -0.    0.98\n",
      "   0.02 -0.02  0.99 -0.01 -0.02  0.97  0.03  0.    1.02  0.05 -0.    0.96\n",
      "  -0.01 -0.02  0.96 -0.01  0.01  1.    0.02  0.01  0.98  0.02 -0.    0.99\n",
      "   0.06 -0.01  0.97 -0.04 -0.02  0.95  0.07  0.01  1.02  0.05 -0.    0.96\n",
      "   0.06  0.03  0.99  0.02 -0.    0.99  0.02 -0.01  0.98  0.02 -0.01  0.98]\n",
      " [-0.   -0.    0.96 -0.03  0.    1.01  0.04 -0.02  0.9   0.02 -0.    0.97\n",
      "   0.02  0.01  0.98  0.02 -0.01  0.98  0.01  0.02  1.01  0.05  0.    0.95\n",
      "   0.06  0.02  0.99  0.1   0.02  0.97  0.07  0.    0.93  0.02 -0.    0.98\n",
      "   0.02  0.    0.99  0.04 -0.    0.96  0.09  0.03  0.99 -0.03  0.    1.\n",
      "   0.07  0.02  0.99 -0.02 -0.02  0.96 -0.   -0.03  0.93  0.02  0.01  0.98]\n",
      " [-0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.99 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98]\n",
      " [ 0.01  0.11  1.    0.    0.07  1.04  0.02  0.11  0.96 -0.05  0.07  0.95\n",
      "  -0.01  0.1   1.   -0.07  0.12  0.99 -0.01  0.08  0.99  0.05  0.08  1.01\n",
      "  -0.    0.11  1.01  0.02  0.11  1.04 -0.04  0.07  0.99  0.02  0.07  1.\n",
      "  -0.01  0.08  0.98 -0.04  0.05  0.99 -0.04  0.06  0.98 -0.03  0.05  0.99\n",
      "   0.01  0.06  0.98 -0.03  0.07  0.99  0.02  0.12  0.96 -0.01  0.07  1.01]\n",
      " [-0.07  0.01  0.99 -0.06 -0.03  0.98 -0.05 -0.02  0.97 -0.06 -0.04  0.98\n",
      "  -0.04 -0.05  0.98 -0.06 -0.02  0.98 -0.05 -0.04  0.99 -0.06 -0.04  0.98\n",
      "  -0.06 -0.02  0.99 -0.06 -0.    0.99 -0.06 -0.02  0.99 -0.07  0.    0.98\n",
      "  -0.06  0.01  0.99 -0.07  0.01  0.98 -0.06  0.    0.99 -0.07  0.01  0.98\n",
      "  -0.06 -0.01  0.98 -0.06 -0.    0.98 -0.06 -0.04  0.97 -0.05 -0.01  0.97]\n",
      " [-0.09  0.02  0.98 -0.06  0.    0.99 -0.02  0.03  1.   -0.04 -0.01  0.99\n",
      "  -0.05 -0.02  0.98 -0.08  0.01  0.98 -0.1  -0.01  0.97 -0.05  0.    0.98\n",
      "  -0.04  0.04  1.   -0.05 -0.02  0.98 -0.06  0.01  0.99 -0.02  0.02  0.99\n",
      "  -0.05 -0.01  0.97 -0.05  0.    0.98 -0.06 -0.03  0.98 -0.06 -0.01  0.98\n",
      "  -0.03  0.    0.99 -0.04 -0.03  0.98 -0.05  0.    0.98 -0.08  0.02  0.99]\n",
      " [-0.05  0.01  0.99 -0.06 -0.01  0.99 -0.05 -0.    0.99 -0.07  0.01  0.99\n",
      "  -0.05 -0.    0.99 -0.06  0.    0.98 -0.05 -0.    0.99 -0.04 -0.02  0.98\n",
      "  -0.05 -0.01  0.98 -0.04 -0.04  0.98 -0.04 -0.05  0.98 -0.04 -0.05  0.98\n",
      "  -0.05 -0.04  0.99 -0.05 -0.05  0.99 -0.05 -0.03  0.99 -0.05 -0.06  0.98\n",
      "  -0.06 -0.02  1.   -0.05 -0.03  0.99 -0.06 -0.01  1.   -0.05  0.01  1.  ]\n",
      " [-0.04  0.04  1.   -0.07 -0.02  0.98 -0.05  0.02  0.99 -0.09 -0.    0.98\n",
      "  -0.03 -0.01  0.98 -0.05  0.    0.98 -0.07  0.02  0.98 -0.07  0.01  0.99\n",
      "  -0.06 -0.02  0.98 -0.06  0.03  0.99 -0.06  0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.04  0.01  0.98 -0.04 -0.01  0.99 -0.06  0.04  0.99\n",
      "  -0.06  0.    0.99 -0.    0.    0.98 -0.05 -0.02  0.98 -0.05 -0.01  0.98]\n",
      " [-0.06  0.    0.98 -0.08  0.01  0.98 -0.05 -0.02  0.98 -0.04 -0.    0.98\n",
      "  -0.04  0.03  0.99 -0.05  0.01  0.98 -0.07  0.    0.99 -0.04 -0.01  0.98\n",
      "  -0.04 -0.    0.98 -0.06 -0.02  0.98 -0.04  0.04  1.   -0.05  0.    0.98\n",
      "  -0.01 -0.02  1.   -0.04  0.02  0.99 -0.06 -0.01  0.98 -0.05  0.02  0.99\n",
      "  -0.05 -0.03  0.97 -0.05  0.    0.98 -0.03  0.01  1.   -0.03  0.02  0.99]\n",
      " [ 0.    0.1   0.96  0.06  0.11  1.    0.01  0.09  0.98 -0.03  0.03  0.97\n",
      "   0.01  0.1   0.97  0.04  0.12  0.98  0.03  0.09  0.99 -0.01  0.08  0.96\n",
      "   0.03  0.13  0.98  0.01  0.09  1.01  0.04  0.1   1.04 -0.01  0.07  0.99\n",
      "  -0.03  0.04  0.99 -0.    0.11  1.02 -0.07  0.03  0.99 -0.05  0.06  0.99\n",
      "   0.05  0.15  0.99 -0.01  0.08  0.98  0.05  0.09  1.01 -0.03  0.07  0.99]\n",
      " [-0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.07 -0.02  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.06 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.04  0.02  0.99 -0.05 -0.01  0.98 -0.02  0.    0.99 -0.08 -0.01  0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.02  0.98 -0.04  0.01  0.99 -0.06  0.02  0.99\n",
      "  -0.08 -0.02  0.98 -0.06 -0.    0.99 -0.05  0.01  0.98 -0.06  0.02  0.99\n",
      "  -0.04 -0.01  0.98 -0.08 -0.01  0.98 -0.01 -0.02  0.98 -0.05 -0.    0.98\n",
      "  -0.1  -0.01  0.98 -0.05 -0.01  0.98 -0.07  0.    0.98 -0.03 -0.02  0.98]\n",
      " [-0.05 -0.05  0.98 -0.07  0.01  0.98 -0.06 -0.01  0.98 -0.04 -0.04  0.97\n",
      "  -0.05 -0.    0.99 -0.04 -0.05  0.98 -0.05  0.01  0.99 -0.05 -0.03  0.99\n",
      "  -0.05  0.01  1.   -0.07  0.01  0.99 -0.05  0.01  1.   -0.04 -0.04  0.97\n",
      "  -0.05 -0.02  0.99 -0.04 -0.04  0.99 -0.05 -0.    0.99 -0.06 -0.01  0.99\n",
      "  -0.05 -0.01  0.99 -0.06 -0.    0.98 -0.05 -0.05  0.98 -0.04 -0.05  0.98]\n",
      " [-0.06  0.05  0.95 -0.01  0.1   0.99 -0.08  0.09  0.99 -0.04  0.06  1.\n",
      "   0.06  0.13  0.99 -0.01  0.11  1.01  0.02  0.09  1.04 -0.03  0.08  0.98\n",
      "   0.03  0.07  1.01 -0.01  0.08  0.98 -0.04  0.07  0.98 -0.    0.09  0.96\n",
      "  -0.03  0.1   0.95 -0.    0.14  0.98 -0.    0.11  1.   -0.01  0.08  0.95\n",
      "   0.03  0.1   0.99  0.03  0.08  1.01 -0.03  0.06  1.   -0.03  0.04  1.  ]\n",
      " [-0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98]\n",
      " [-0.03 -0.    0.98 -0.02  0.01  1.01  0.    0.04  1.08  0.02  0.01  0.98\n",
      "   0.02  0.01  0.98  0.02  0.01  0.99 -0.05  0.01  1.06 -0.03  0.01  1.01\n",
      "   0.02 -0.    0.94  0.07  0.    0.95  0.07  0.04  1.02  0.03  0.01  0.99\n",
      "   0.02  0.01  0.98  0.    0.01  1.   -0.06 -0.01  0.98  0.05 -0.    0.96\n",
      "  -0.03  0.    1.    0.06  0.03  1.02  0.04  0.01  0.99  0.02  0.01  0.98]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.03  0.98 -0.06 -0.02  0.98\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.05  0.01  1.   -0.07  0.01  0.99 -0.05  0.01  1.   -0.04 -0.04  0.97\n",
      "  -0.05 -0.02  0.99 -0.04 -0.04  0.99 -0.05 -0.    0.99 -0.06 -0.01  0.99\n",
      "  -0.05 -0.01  0.99 -0.06 -0.    0.98 -0.05 -0.05  0.98 -0.04 -0.05  0.98\n",
      "  -0.06 -0.03  0.98 -0.05 -0.04  0.98 -0.06 -0.02  0.98 -0.07  0.    0.99\n",
      "  -0.07  0.    0.99 -0.06  0.01  0.99 -0.06  0.01  0.99 -0.06 -0.03  0.98]\n",
      " [ 0.02 -0.    0.99  0.03  0.01  0.98  0.04 -0.02  0.98  0.   -0.01  0.99\n",
      "  -0.01 -0.02  0.94 -0.05 -0.02  0.96  0.01  0.04  0.99  0.02  0.01  0.97\n",
      "   0.02 -0.    0.97  0.01 -0.06  0.98  0.05  0.01  1.   -0.   -0.02  0.94\n",
      "  -0.   -0.01  1.   -0.02 -0.03  0.98  0.03  0.03  0.97  0.02  0.    0.98\n",
      "   0.02 -0.    0.99  0.04  0.01  0.99  0.01 -0.    1.   -0.02 -0.01  0.98]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.06 -0.03  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.06 -0.02  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99]\n",
      " [ 0.    0.08  0.96  0.03  0.14  0.96  0.03  0.12  0.96  0.02  0.06  1.01\n",
      "  -0.02  0.09  0.98  0.01  0.13  0.99 -0.01  0.09  0.96 -0.06  0.05  0.99\n",
      "  -0.04  0.06  0.96 -0.    0.07  0.96 -0.04  0.04  0.97 -0.03  0.09  0.95\n",
      "  -0.01  0.13  0.96  0.03  0.11  0.97  0.03  0.15  0.98  0.    0.09  0.94\n",
      "   0.07  0.13  0.95 -0.04  0.06  0.99 -0.05  0.01  0.98 -0.01  0.07  0.93]\n",
      " [-0.01 -0.01  0.99 -0.07 -0.    0.98 -0.05 -0.01  0.98 -0.03 -0.03  0.98\n",
      "  -0.05 -0.02  0.98 -0.06  0.    0.98 -0.08 -0.01  0.98 -0.06  0.02  0.99\n",
      "  -0.07 -0.01  0.98 -0.05 -0.03  0.98 -0.06 -0.    0.99 -0.07  0.02  1.\n",
      "  -0.06 -0.02  0.98 -0.06 -0.02  0.98 -0.03 -0.01  0.98 -0.07 -0.03  0.97\n",
      "  -0.05  0.    0.98 -0.11  0.    0.98 -0.06 -0.02  0.98 -0.07  0.01  0.98]\n",
      " [-0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [ 0.02 -0.    1.    0.07 -0.    0.98  0.08  0.01  1.02 -0.01 -0.02  0.94\n",
      "  -0.01 -0.02  0.97  0.05 -0.01  0.97  0.02  0.01  0.98  0.02 -0.    0.98\n",
      "  -0.02 -0.02  0.99  0.08  0.01  1.    0.04 -0.01  0.95 -0.01 -0.02  0.94\n",
      "   0.05 -0.02  0.98  0.03  0.02  0.97  0.02 -0.01  0.97  0.03 -0.    0.99\n",
      "  -0.03 -0.01  0.97 -0.02 -0.01  0.98  0.03 -0.01  0.95  0.08  0.02  1.02]\n",
      " [-0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97]\n",
      " [-0.06 -0.01  0.99 -0.06 -0.05  0.97 -0.06 -0.01  0.98 -0.05 -0.05  0.97\n",
      "  -0.03 -0.06  0.98 -0.05 -0.    0.99 -0.06 -0.03  0.99 -0.06  0.    0.99\n",
      "  -0.07  0.02  0.99 -0.05 -0.    0.99 -0.04 -0.04  0.97 -0.04 -0.04  0.98\n",
      "  -0.04 -0.04  0.98 -0.05 -0.05  0.98 -0.04 -0.04  0.98 -0.05 -0.04  0.99\n",
      "  -0.05 -0.05  0.98 -0.05 -0.03  0.99 -0.05 -0.05  0.98 -0.06 -0.01  1.  ]\n",
      " [-0.06  0.03  0.99 -0.04  0.    0.98 -0.05  0.01  0.98 -0.02 -0.01  0.98\n",
      "  -0.05 -0.02  0.98 -0.07  0.02  0.99 -0.01  0.01  0.99 -0.06  0.    0.98\n",
      "  -0.08 -0.01  0.98 -0.04  0.01  0.99 -0.07  0.01  0.98 -0.06  0.02  0.99\n",
      "  -0.03 -0.03  0.98 -0.05 -0.    0.98 -0.09  0.03  0.99 -0.07 -0.01  0.98\n",
      "  -0.06  0.01  0.99 -0.08  0.02  0.99 -0.05  0.01  0.98 -0.05 -0.    0.98]\n",
      " [-0.05 -0.01  0.99 -0.07  0.    0.97 -0.06 -0.05  0.97 -0.04 -0.04  0.98\n",
      "  -0.06 -0.05  0.97 -0.07  0.01  0.99 -0.05  0.    0.99 -0.05 -0.02  0.97\n",
      "  -0.05 -0.03  0.98 -0.04 -0.03  0.97 -0.05 -0.05  0.97 -0.03 -0.06  0.98\n",
      "  -0.05 -0.05  0.98 -0.04 -0.05  0.98 -0.06 -0.04  0.98 -0.05 -0.04  0.98\n",
      "  -0.06 -0.03  0.98 -0.06 -0.03  0.99 -0.05 -0.04  0.98 -0.05 -0.05  0.98]\n",
      " [ 0.06  0.06  1.06  0.03  0.    1.    0.02  0.01  0.99  0.02  0.    0.97\n",
      "  -0.02 -0.01  0.99  0.09  0.02  0.97 -0.04 -0.    0.99  0.02 -0.01  0.94\n",
      "   0.06  0.07  1.08  0.02  0.    0.99  0.02  0.01  0.97 -0.03 -0.04  0.94\n",
      "   0.06  0.03  1.01 -0.03 -0.    0.98 -0.03  0.    1.01 -0.03 -0.02  0.91\n",
      "   0.04  0.03  1.01  0.02  0.01  0.98  0.02  0.01  0.99  0.04  0.03  1.03]\n",
      " [-0.03  0.08  0.98 -0.04  0.11  0.94 -0.    0.11  1.01 -0.04  0.13  0.97\n",
      "   0.03  0.11  0.96  0.04  0.07  1.01 -0.01  0.11  1.   -0.04  0.02  0.97\n",
      "   0.03  0.11  0.96 -0.04  0.11  0.95  0.    0.08  0.97 -0.02  0.04  0.97\n",
      "  -0.    0.09  1.    0.04  0.09  1.    0.03  0.09  0.99 -0.01  0.1   1.\n",
      "   0.03  0.13  0.99  0.    0.08  1.01 -0.02  0.03  0.98 -0.04  0.06  0.98]\n",
      " [-0.06 -0.03  0.98 -0.05 -0.02  0.97 -0.05 -0.04  0.97 -0.05 -0.03  0.97\n",
      "  -0.05  0.01  1.   -0.04 -0.05  0.98 -0.05 -0.02  0.99 -0.05 -0.03  0.99\n",
      "  -0.05  0.01  0.99 -0.05 -0.02  0.99 -0.05  0.01  0.99 -0.07  0.01  0.99\n",
      "  -0.05  0.01  0.99 -0.05 -0.    0.98 -0.04 -0.03  0.98 -0.04 -0.01  0.98\n",
      "  -0.04 -0.04  0.98 -0.04 -0.05  0.98 -0.04 -0.05  0.98 -0.07 -0.02  0.99]\n",
      " [ 0.02  0.    0.99  0.02  0.    0.98 -0.01  0.05  1.11  0.06  0.02  1.01\n",
      "   0.07  0.02  0.99 -0.02 -0.01  0.96 -0.04 -0.    1.02  0.02  0.    0.98\n",
      "   0.02  0.01  0.98  0.02  0.01  0.98  0.09  0.02  0.99  0.08  0.01  0.95\n",
      "  -0.    0.01  1.01  0.04 -0.01  0.93  0.01  0.05  1.08  0.02 -0.    0.98\n",
      "   0.02  0.01  0.99  0.02 -0.03  0.92  0.04 -0.    0.94 -0.04 -0.    0.99]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.06 -0.03  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98]\n",
      " [-0.03 -0.02  0.95  0.07  0.01  1.01 -0.02 -0.    0.98  0.03 -0.05  1.\n",
      "   0.02  0.01  0.98  0.02 -0.    0.98  0.06  0.06  0.99 -0.02 -0.01  0.98\n",
      "   0.06  0.01  1.03  0.06  0.    0.98 -0.05 -0.02  0.98  0.03 -0.01  0.97\n",
      "   0.02 -0.01  0.98  0.02 -0.    0.99 -0.05 -0.02  0.97  0.08  0.01  1.02\n",
      "   0.01  0.    1.01  0.04  0.01  1.    0.03  0.05  0.99  0.02 -0.01  0.99]\n",
      " [-0.05  0.07  0.99 -0.03  0.12  0.95 -0.    0.1   1.03 -0.06  0.03  1.01\n",
      "  -0.04  0.06  1.    0.02  0.16  0.96 -0.01  0.09  0.99 -0.06  0.11  0.97\n",
      "  -0.02  0.08  0.97  0.01  0.14  0.96 -0.    0.07  0.97 -0.    0.07  1.\n",
      "   0.01  0.1   0.95  0.03  0.12  0.97 -0.02  0.11  0.99 -0.03  0.05  1.\n",
      "   0.02  0.11  0.96 -0.05  0.07  0.97 -0.03  0.07  0.97  0.03  0.1   0.97]\n",
      " [ 0.02 -0.01  0.99  0.02 -0.    0.97 -0.   -0.07  0.98  0.08  0.01  1.02\n",
      "   0.07  0.01  0.99  0.   -0.02  0.94  0.09  0.02  0.98  0.02 -0.01  0.98\n",
      "   0.02 -0.01  0.98  0.02 -0.01  0.98 -0.02 -0.03  0.98  0.02  0.    1.01\n",
      "  -0.01 -0.02  0.94 -0.04 -0.02  0.96  0.01  0.04  0.99  0.02 -0.01  0.98\n",
      "   0.02 -0.01  0.99  0.01 -0.04  0.99  0.05 -0.    0.98  0.01 -0.    1.  ]\n",
      " [-0.01  0.    0.99 -0.05 -0.    0.98 -0.09 -0.02  0.97 -0.04 -0.01  0.98\n",
      "  -0.07 -0.    0.98 -0.04 -0.03  0.98 -0.05  0.04  1.   -0.05  0.    0.98\n",
      "  -0.08  0.    0.98 -0.07 -0.01  0.98 -0.05 -0.01  0.98 -0.1   0.01  0.98\n",
      "  -0.05  0.    0.98 -0.06 -0.03  0.98 -0.04 -0.01  0.97 -0.05  0.02  0.99\n",
      "  -0.04  0.02  0.99 -0.09  0.02  0.98 -0.06  0.    0.99 -0.02  0.03  1.  ]\n",
      " [ 0.08  0.    0.99  0.04  0.05  0.97  0.02 -0.02  0.99  0.02 -0.    0.99\n",
      "  -0.01 -0.07  0.98  0.06  0.    0.98 -0.03 -0.02  0.94  0.   -0.    1.\n",
      "   0.07 -0.01  0.98  0.02 -0.01  0.98  0.02 -0.01  0.98  0.03  0.01  0.96\n",
      "   0.01 -0.02  0.98  0.01 -0.    1.   -0.01 -0.02  0.94 -0.05 -0.02  0.96\n",
      "  -0.03 -0.03  1.    0.02  0.01  0.97  0.02  0.    0.97  0.02 -0.05  0.99]\n",
      " [ 0.08 -0.02  0.97  0.02  0.01  0.97  0.02 -0.    0.97  0.   -0.04  0.99\n",
      "  -0.05 -0.02  0.96  0.05 -0.    0.97 -0.02 -0.01  0.97  0.02  0.01  0.99\n",
      "   0.01  0.03  0.99  0.02 -0.    0.98  0.02 -0.    0.97 -0.01 -0.06  0.98\n",
      "   0.02  0.    1.    0.03  0.01  1.02  0.08  0.01  1.    0.04  0.03  0.99\n",
      "   0.02  0.    0.97  0.02 -0.    0.99  0.02 -0.01  0.98 -0.03 -0.    0.97]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.06 -0.02  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.01  0.99]\n",
      " [-0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98]\n",
      " [-0.03  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98]\n",
      " [ 0.03  0.11  0.97  0.04  0.1   1.01  0.01  0.09  0.99 -0.02  0.05  1.\n",
      "   0.    0.11  1.01 -0.06  0.03  1.01  0.01  0.1   0.95  0.05  0.12  1.\n",
      "  -0.01  0.07  0.94  0.07  0.09  0.99 -0.05  0.06  0.99 -0.03  0.14  0.94\n",
      "  -0.01  0.07  0.97  0.03  0.16  1.01 -0.03  0.07  1.    0.04  0.07  1.01\n",
      "  -0.01  0.09  0.99 -0.02  0.05  1.03 -0.02  0.07  1.01 -0.01  0.06  1.  ]\n",
      " [-0.02  0.01  0.99 -0.04 -0.02  0.96  0.06  0.01  1.03  0.05 -0.    0.97\n",
      "  -0.01 -0.03  0.98  0.02 -0.01  0.98  0.02 -0.01  0.99  0.02 -0.01  0.98\n",
      "   0.03 -0.01  0.98  0.07  0.01  0.99  0.07  0.01  1.02  0.01 -0.    0.99\n",
      "   0.02 -0.04  0.98  0.02 -0.01  0.98  0.02 -0.01  0.97  0.02 -0.02  1.\n",
      "   0.07  0.01  1.    0.   -0.    1.    0.06  0.    0.97 -0.03 -0.01  0.97]\n",
      " [-0.06 -0.    0.97 -0.05  0.02  0.99 -0.03 -0.04  0.98 -0.05  0.02  0.99\n",
      "  -0.03 -0.05  0.98 -0.05 -0.    0.99 -0.04 -0.04  0.99 -0.05 -0.03  0.99\n",
      "  -0.07  0.01  0.99 -0.05 -0.01  0.99 -0.07  0.    0.97 -0.06 -0.05  0.97\n",
      "  -0.04 -0.04  0.98 -0.06 -0.05  0.97 -0.07  0.01  0.99 -0.05  0.    0.99\n",
      "  -0.05 -0.02  0.97 -0.05 -0.03  0.98 -0.04 -0.03  0.97 -0.05 -0.05  0.97]\n",
      " [ 0.06  0.06  1.03 -0.06 -0.01  0.98  0.08  0.01  0.96 -0.02 -0.    0.98\n",
      "   0.07 -0.01  0.92  0.03  0.03  1.01  0.02  0.01  0.98  0.02  0.    0.99\n",
      "   0.02 -0.04  0.86 -0.04 -0.01  0.99 -0.04 -0.    0.99  0.08  0.01  0.95\n",
      "   0.01 -0.04  0.9   0.02  0.03  0.98  0.02  0.01  0.98  0.02 -0.01  0.98\n",
      "   0.07  0.    0.94  0.05  0.    0.95 -0.01  0.01  1.01 -0.06 -0.01  1.01]\n",
      " [-0.03  0.03  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.04  0.04  0.99 -0.03  0.01  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98 -0.04  0.01  0.98]\n",
      " [-0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98]\n",
      " [-0.04  0.04  0.99 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98]\n",
      " [-0.01  0.08  0.95  0.03  0.1   0.99  0.03  0.08  1.01 -0.03  0.06  1.\n",
      "  -0.03  0.04  1.    0.01  0.1   1.02 -0.02  0.06  1.04  0.02  0.11  0.95\n",
      "  -0.01  0.15  0.95 -0.01  0.11  1.01 -0.08  0.1   0.99  0.03  0.11  0.98\n",
      "   0.06  0.1   1.01 -0.    0.11  1.01  0.02  0.11  1.04 -0.05  0.06  0.99\n",
      "   0.05  0.09  1.    0.    0.07  0.98  0.    0.08  0.96 -0.02  0.07  1.01]\n",
      " [-0.03  0.02  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98\n",
      "  -0.04  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.99 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.07 -0.02  0.98 -0.06 -0.02  0.99 -0.07 -0.02  0.99\n",
      "  -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [ 0.    0.09  1.02 -0.08  0.05  1.01  0.02  0.11  0.97  0.01  0.15  0.95\n",
      "  -0.    0.11  1.02 -0.08  0.1   0.98  0.03  0.11  0.97  0.05  0.15  0.98\n",
      "  -0.01  0.1   1.    0.02  0.09  1.02 -0.03  0.08  0.98  0.05  0.07  1.01\n",
      "   0.01  0.07  0.97 -0.03  0.03  1.01  0.03  0.06  1.    0.05  0.11  0.99\n",
      "   0.02  0.08  0.98 -0.02  0.1   1.   -0.02  0.08  0.95  0.03  0.1   1.  ]\n",
      " [-0.03 -0.01  0.98  0.01  0.01  1.01  0.04  0.04  1.03  0.02 -0.03  0.94\n",
      "   0.02  0.01  0.98  0.02  0.01  0.99 -0.01  0.05  1.11 -0.01 -0.01  0.96\n",
      "   0.05  0.    0.95 -0.02  0.01  0.95 -0.01  0.04  1.09  0.02 -0.    0.99\n",
      "   0.02  0.01  0.98  0.03  0.    0.97  0.1   0.02  0.97 -0.01 -0.01  0.96\n",
      "   0.03  0.01  1.01 -0.01 -0.02  0.94  0.06  0.06  1.06  0.02 -0.    0.99]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.02  0.99\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.02  0.98]\n",
      " [-0.05 -0.    0.98 -0.04  0.02  0.99 -0.04  0.02  0.99 -0.06 -0.02  0.98\n",
      "  -0.02  0.02  0.99 -0.05  0.    0.98 -0.07 -0.    0.99 -0.08  0.01  0.99\n",
      "  -0.04 -0.    0.98 -0.07 -0.02  0.98 -0.01  0.01  0.99 -0.05  0.    0.99\n",
      "  -0.05 -0.05  0.96 -0.07 -0.02  0.98 -0.07  0.01  0.99 -0.03  0.01  0.99\n",
      "  -0.07  0.01  0.99 -0.05 -0.    0.98 -0.05 -0.04  0.98 -0.07 -0.01  0.98]\n",
      " [ 0.02 -0.02  0.97  0.05  0.04  0.97  0.02  0.    0.98  0.02  0.    0.97\n",
      "  -0.05 -0.01  0.99  0.06  0.    0.98 -0.   -0.02  0.93  0.01 -0.    1.01\n",
      "  -0.04 -0.03  0.98  0.02 -0.03  0.99  0.02  0.    0.99  0.02 -0.01  0.98\n",
      "   0.1   0.02  1.   -0.02 -0.02  0.94  0.07  0.01  1.    0.05 -0.    0.98\n",
      "   0.01 -0.05  0.98  0.02 -0.01  0.98  0.02 -0.    0.98  0.04  0.03  0.97]\n",
      " [-0.05 -0.    0.98 -0.05 -0.02  0.98 -0.05 -0.02  0.98 -0.04  0.01  0.99\n",
      "  -0.02 -0.02  0.98 -0.06  0.    0.98 -0.06 -0.    0.98 -0.03 -0.01  0.97\n",
      "  -0.07  0.02  0.98 -0.05  0.03  0.99 -0.09 -0.01  0.97 -0.05 -0.    0.99\n",
      "  -0.09  0.03  0.99 -0.06  0.02  0.99 -0.06  0.01  0.98 -0.03 -0.01  0.98\n",
      "  -0.02  0.01  0.99 -0.06  0.    0.98 -0.04 -0.04  0.98 -0.07  0.    0.98]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.02  0.98]\n",
      " [ 0.    0.11  1.01 -0.06  0.03  1.01  0.01  0.1   0.95  0.05  0.12  1.\n",
      "  -0.01  0.07  0.94  0.07  0.09  0.99 -0.05  0.06  0.99 -0.03  0.14  0.94\n",
      "  -0.01  0.07  0.97  0.03  0.16  1.01 -0.03  0.07  1.    0.04  0.07  1.01\n",
      "  -0.01  0.09  0.99 -0.02  0.05  1.03 -0.02  0.07  1.01 -0.01  0.06  1.\n",
      "  -0.01  0.09  0.98  0.01  0.13  0.99 -0.03  0.07  0.96  0.01  0.08  1.01]\n",
      " [-0.05  0.    0.98 -0.05 -0.03  0.98 -0.07 -0.    0.98 -0.04  0.02  0.99\n",
      "  -0.01 -0.01  0.99 -0.06 -0.    0.99 -0.08 -0.01  0.98 -0.04  0.01  0.99\n",
      "  -0.05  0.01  0.99 -0.07  0.02  0.99 -0.01 -0.01  0.99 -0.05  0.    0.98\n",
      "  -0.09 -0.04  0.96 -0.04 -0.01  0.98 -0.07 -0.01  0.98 -0.09 -0.01  0.98\n",
      "  -0.03 -0.02  0.98 -0.05 -0.    0.98 -0.07  0.02  0.98 -0.05 -0.02  0.98]\n",
      " [-0.07  0.01  0.99 -0.05 -0.    0.98 -0.05 -0.04  0.98 -0.07 -0.01  0.98\n",
      "  -0.05 -0.02  0.98 -0.09 -0.02  0.98 -0.05 -0.    0.98 -0.06 -0.    0.99\n",
      "  -0.05 -0.02  0.98 -0.04  0.01  0.99 -0.07  0.01  0.98 -0.08  0.03  0.99\n",
      "  -0.05 -0.    0.99 -0.04 -0.03  0.97 -0.07 -0.02  0.98 -0.07  0.01  0.99\n",
      "  -0.06  0.03  0.99 -0.09  0.01  0.98 -0.06  0.    0.98 -0.02  0.01  1.  ]\n",
      " [-0.04 -0.01  0.98 -0.07  0.    0.98 -0.08 -0.02  0.98 -0.02 -0.01  0.98\n",
      "  -0.05 -0.    0.98 -0.09  0.02  0.98 -0.07 -0.02  0.98 -0.05 -0.01  0.98\n",
      "  -0.03  0.03  0.99 -0.06  0.01  0.99 -0.05  0.03  0.98 -0.07 -0.02  0.98\n",
      "  -0.04  0.02  0.99 -0.04 -0.01  0.98 -0.01  0.02  0.99 -0.05  0.    0.98\n",
      "  -0.04 -0.03  0.97 -0.04 -0.01  0.98 -0.04 -0.01  0.98 -0.07  0.02  0.99]\n",
      " [ 0.07  0.01  0.99  0.02 -0.01  0.94 -0.03 -0.03  0.98  0.03  0.04  0.97\n",
      "   0.02 -0.    0.98  0.02 -0.01  0.98  0.06 -0.04  0.98  0.03  0.    1.01\n",
      "  -0.02 -0.01  0.97  0.08  0.02  1.02  0.08  0.03  0.98  0.02 -0.02  0.99\n",
      "   0.02 -0.01  0.98  0.02 -0.01  0.98  0.06  0.01  1.    0.07  0.01  0.99\n",
      "  -0.02 -0.02  0.95  0.1   0.01  1.    0.01  0.02  0.99  0.02  0.    0.98]\n",
      " [-0.04 -0.03  0.97 -0.06 -0.01  0.99 -0.04 -0.04  0.99 -0.06 -0.02  0.98\n",
      "  -0.06 -0.01  0.99 -0.06  0.01  0.99 -0.07  0.01  0.99 -0.05  0.01  0.99\n",
      "  -0.06  0.01  0.98 -0.07  0.01  0.99 -0.06  0.01  0.99 -0.07  0.01  0.98\n",
      "  -0.06  0.01  0.99 -0.06  0.01  0.98 -0.06 -0.    0.98 -0.05 -0.01  0.98\n",
      "  -0.05 -0.    0.98 -0.04 -0.05  0.98 -0.05 -0.03  0.99 -0.06 -0.02  1.  ]\n",
      " [-0.01  0.07  0.98  0.07  0.12  0.95 -0.02  0.09  0.97 -0.06  0.08  0.95\n",
      "  -0.01  0.09  0.97  0.03  0.07  1.02 -0.04  0.07  0.98 -0.05  0.12  0.94\n",
      "   0.    0.07  0.96  0.03  0.15  0.97 -0.05  0.06  1.    0.05  0.09  1.\n",
      "  -0.01  0.06  0.97 -0.02  0.08  1.    0.02  0.12  0.96 -0.05  0.07  0.96\n",
      "  -0.05  0.07  0.94  0.    0.11  1.   -0.03  0.15  0.96 -0.01  0.09  0.93]\n",
      " [-0.05 -0.    0.99 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06 -0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98]\n",
      " [-0.05 -0.01  0.98 -0.06  0.03  0.99 -0.06  0.04  0.99 -0.05 -0.    0.99\n",
      "  -0.01 -0.    0.99 -0.05 -0.02  0.98 -0.07  0.01  0.98 -0.02  0.    0.98\n",
      "  -0.05  0.    0.99 -0.05  0.    0.98 -0.03 -0.02  0.98 -0.04  0.01  0.99\n",
      "  -0.07  0.    0.98 -0.03  0.02  0.99 -0.06  0.    0.98 -0.05 -0.02  0.98\n",
      "  -0.08  0.01  0.98 -0.04 -0.01  0.98 -0.04 -0.02  0.98 -0.08 -0.02  0.97]\n",
      " [ 0.03  0.14  0.95  0.03  0.12  0.97 -0.    0.12  0.96  0.    0.1   0.98\n",
      "   0.03  0.11  0.97 -0.04  0.05  0.99 -0.01  0.1   0.94  0.02  0.15  0.98\n",
      "  -0.04  0.08  0.99  0.03  0.11  1.01  0.02  0.11  0.97  0.04  0.1   1.\n",
      "   0.    0.1   0.96 -0.06  0.05  0.95 -0.01  0.1   0.99 -0.08  0.09  0.99\n",
      "  -0.04  0.06  1.    0.06  0.13  0.99 -0.01  0.11  1.01  0.02  0.09  1.04]\n",
      " [ 0.09  0.02  0.98 -0.02 -0.04  0.92  0.02  0.    0.98  0.02  0.01  0.98\n",
      "   0.04  0.03  1.   -0.04 -0.02  0.96 -0.04 -0.    1.01  0.07  0.02  0.98\n",
      "   0.03  0.03  1.03  0.02  0.03  1.02  0.02 -0.    0.98  0.02  0.01  0.99\n",
      "   0.01 -0.06  0.86  0.09  0.02  0.96 -0.03 -0.01  0.97  0.02  0.01  1.02\n",
      "  -0.01 -0.03  0.94  0.02  0.    0.96  0.02  0.01  0.98  0.02  0.01  0.99]\n",
      " [-0.06 -0.    0.98 -0.05 -0.02  0.98 -0.04  0.01  0.99 -0.06  0.02  0.99\n",
      "  -0.08 -0.02  0.98 -0.06 -0.    0.99 -0.05  0.01  0.98 -0.06  0.02  0.99\n",
      "  -0.04 -0.01  0.98 -0.08 -0.01  0.98 -0.01 -0.02  0.98 -0.05 -0.    0.98\n",
      "  -0.1  -0.01  0.98 -0.05 -0.01  0.98 -0.07  0.    0.98 -0.03 -0.02  0.98\n",
      "  -0.03  0.02  0.99 -0.05  0.    0.98 -0.11  0.03  0.98 -0.08 -0.    0.98]\n",
      " [-0.07  0.01  0.99 -0.05  0.    0.99 -0.05 -0.01  0.98 -0.05 -0.05  0.97\n",
      "  -0.03 -0.05  0.98 -0.06 -0.03  0.98 -0.07  0.01  0.99 -0.07  0.01  0.99\n",
      "  -0.05  0.    0.98 -0.06  0.    0.98 -0.04 -0.04  0.98 -0.05 -0.04  0.98\n",
      "  -0.05 -0.05  0.98 -0.07  0.    0.99 -0.06 -0.01  0.99 -0.07  0.01  0.98\n",
      "  -0.06  0.02  0.98 -0.04 -0.04  0.97 -0.06  0.01  0.99 -0.04 -0.05  0.98]\n",
      " [ 0.04  0.06  1.09  0.02  0.    0.98  0.02  0.01  0.98 -0.02 -0.05  0.92\n",
      "   0.    0.01  1.02  0.01 -0.    0.96 -0.04 -0.    1.   -0.05 -0.01  1.01\n",
      "   0.02 -0.01  0.97  0.02  0.    0.99  0.02  0.    0.98 -0.05 -0.    1.03\n",
      "  -0.04 -0.    0.99 -0.03 -0.    1.    0.07  0.01  0.95 -0.03 -0.01  1.01\n",
      "   0.02 -0.01  0.98  0.02  0.01  0.98  0.03  0.02  0.99 -0.03 -0.01  0.96]\n",
      " [-0.03 -0.04  0.98 -0.05  0.01  0.99 -0.03 -0.05  0.98 -0.04 -0.05  0.98\n",
      "  -0.04 -0.04  0.99 -0.04 -0.05  0.98 -0.06 -0.01  1.   -0.05 -0.01  0.99\n",
      "  -0.07  0.01  0.99 -0.05  0.    0.99 -0.06 -0.01  0.97 -0.05 -0.05  0.97\n",
      "  -0.03 -0.05  0.97 -0.05 -0.01  0.99 -0.03 -0.05  0.97 -0.05 -0.01  0.99\n",
      "  -0.03 -0.06  0.97 -0.05 -0.05  0.97 -0.04 -0.05  0.98 -0.06  0.01  0.99]\n",
      " [-0.06 -0.    0.98 -0.05 -0.01  0.99 -0.04 -0.04  0.98 -0.06 -0.04  0.97\n",
      "  -0.07  0.01  1.   -0.06 -0.04  0.97 -0.07  0.01  0.98 -0.06 -0.05  0.97\n",
      "  -0.04 -0.04  0.97 -0.05 -0.05  0.97 -0.04 -0.05  0.98 -0.06 -0.02  0.98\n",
      "  -0.06 -0.02  0.99 -0.06  0.01  0.99 -0.06  0.    0.99 -0.05  0.    0.98\n",
      "  -0.04 -0.02  0.98 -0.04 -0.03  0.98 -0.04 -0.04  0.98 -0.06 -0.03  0.99]\n",
      " [-0.    0.08  0.99  0.03  0.11  1.01 -0.01  0.07  0.93 -0.    0.03  0.99\n",
      "   0.01  0.1   0.98  0.03  0.16  0.97 -0.01  0.1   1.01 -0.07  0.11  0.99\n",
      "  -0.01  0.08  0.99  0.06  0.12  1.    0.    0.07  0.97 -0.01  0.05  1.03\n",
      "  -0.01  0.08  1.    0.05  0.11  1.    0.01  0.07  0.97  0.02  0.12  0.97\n",
      "  -0.04  0.05  0.98  0.03  0.09  1.    0.03  0.13  0.99 -0.03  0.09  1.  ]\n",
      " [ 0.03  0.08  1.   -0.    0.08  0.98 -0.03  0.04  1.03 -0.03  0.07  1.01\n",
      "   0.03  0.08  1.   -0.01  0.06  0.97 -0.04  0.07  0.99  0.02  0.12  0.96\n",
      "  -0.06  0.08  1.01 -0.03  0.03  0.97  0.03  0.1   0.96 -0.05  0.05  0.96\n",
      "  -0.03  0.07  0.98 -0.06  0.06  0.92  0.03  0.11  0.97  0.04  0.14  0.99\n",
      "  -0.01  0.07  0.94  0.04  0.05  0.99 -0.02  0.08  0.97 -0.05  0.12  0.94]\n",
      " [-0.02  0.07  1.    0.05  0.14  0.98  0.01  0.08  0.97  0.03  0.1   1.\n",
      "   0.02  0.11  0.99  0.05  0.13  0.98  0.01  0.08  0.98  0.02  0.12  0.98\n",
      "  -0.03  0.06  0.97  0.03  0.12  0.97  0.04  0.12  1.01 -0.03  0.06  0.97\n",
      "  -0.    0.07  1.01  0.    0.09  1.02 -0.01  0.03  0.99 -0.02  0.08  0.96\n",
      "  -0.05  0.05  0.95 -0.    0.11  1.02 -0.02  0.14  0.96 -0.04  0.07  0.98]\n",
      " [-0.06  0.03  0.99 -0.06  0.    0.99 -0.06 -0.01  0.98 -0.08 -0.    0.98\n",
      "  -0.04  0.    0.98 -0.06 -0.02  0.98 -0.02  0.03  0.99 -0.05  0.    0.98\n",
      "  -0.02 -0.04  0.97 -0.07  0.    0.98 -0.07  0.01  0.99 -0.03 -0.    0.98\n",
      "  -0.03  0.02  0.99 -0.05 -0.    0.99 -0.07 -0.03  0.97 -0.07 -0.    0.98\n",
      "  -0.06 -0.02  0.98 -0.1  -0.    0.98 -0.05  0.    0.98 -0.06 -0.    0.98]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98]\n",
      " [-0.07  0.    0.98 -0.06  0.01  0.99 -0.07  0.01  0.98 -0.06  0.    0.99\n",
      "  -0.07  0.01  0.98 -0.06 -0.01  0.98 -0.06 -0.    0.98 -0.06 -0.04  0.97\n",
      "  -0.05 -0.01  0.97 -0.05  0.    0.99 -0.04 -0.04  0.99 -0.05 -0.01  0.99\n",
      "  -0.07  0.01  0.99 -0.05  0.01  1.   -0.06  0.01  0.98 -0.05 -0.    0.99\n",
      "  -0.03 -0.05  0.97 -0.06 -0.05  0.97 -0.05 -0.02  0.99 -0.05 -0.03  0.98]\n",
      " [-0.06  0.01  0.98 -0.03 -0.01  0.98 -0.02  0.01  0.99 -0.06  0.    0.98\n",
      "  -0.04 -0.04  0.98 -0.07  0.    0.98 -0.04  0.01  0.98 -0.03 -0.03  0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.02  0.98 -0.05 -0.01  0.98\n",
      "  -0.05  0.02  0.99 -0.01 -0.    0.99 -0.06  0.    0.98 -0.06 -0.02  0.99\n",
      "  -0.06  0.03  0.99 -0.07 -0.01  0.98 -0.06 -0.02  0.98 -0.09 -0.01  0.97]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.06 -0.02  0.98]\n",
      " [-0.04  0.01  0.99 -0.04  0.02  0.99 -0.06 -0.04  0.97 -0.05 -0.    0.98\n",
      "  -0.11  0.02  0.99 -0.06 -0.02  0.98 -0.07 -0.    0.98 -0.08 -0.01  0.98\n",
      "  -0.06  0.02  0.99 -0.05 -0.    0.99 -0.08  0.02  0.98 -0.05 -0.01  0.98\n",
      "  -0.03  0.01  0.99 -0.07  0.03  0.99 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "   0.    0.01  1.   -0.07 -0.02  0.98 -0.04 -0.01  0.98 -0.05  0.03  0.99]\n",
      " [-0.03 -0.    0.98  0.02 -0.02  0.98  0.02 -0.01  0.97  0.02 -0.    0.98\n",
      "  -0.02  0.    0.99  0.04  0.01  1.01 -0.02 -0.02  0.96 -0.03 -0.02  0.95\n",
      "   0.04 -0.04  0.97  0.03  0.01  0.98  0.02 -0.01  0.99  0.01 -0.03  0.99\n",
      "  -0.05 -0.02  0.96  0.05 -0.    0.97 -0.02 -0.02  0.96 -0.03 -0.    0.98\n",
      "  -0.   -0.03  1.    0.02  0.01  0.97  0.02 -0.    0.98  0.06  0.06  0.99]\n",
      " [-0.05 -0.05  0.97 -0.04 -0.01  0.97 -0.05 -0.05  0.97 -0.04 -0.03  0.97\n",
      "  -0.05 -0.03  0.98 -0.04 -0.04  0.98 -0.06 -0.03  0.97 -0.07  0.01  0.99\n",
      "  -0.06 -0.05  0.97 -0.07  0.01  0.98 -0.05 -0.05  0.97 -0.05 -0.02  0.97\n",
      "  -0.04 -0.05  0.98 -0.03 -0.05  0.97 -0.06 -0.05  0.98 -0.04 -0.05  0.98\n",
      "  -0.05 -0.05  0.98 -0.05 -0.04  0.99 -0.04 -0.05  0.98 -0.05 -0.05  0.98]\n",
      " [-0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.99 -0.05  0.02  0.98 -0.04  0.04  0.98]\n",
      " [-0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98]\n",
      " [-0.03  0.04  1.    0.01  0.1   1.02 -0.02  0.06  1.04  0.02  0.11  0.95\n",
      "  -0.01  0.15  0.95 -0.01  0.11  1.01 -0.08  0.1   0.99  0.03  0.11  0.98\n",
      "   0.06  0.1   1.01 -0.    0.11  1.01  0.02  0.11  1.04 -0.05  0.06  0.99\n",
      "   0.05  0.09  1.    0.    0.07  0.98  0.    0.08  0.96 -0.02  0.07  1.01\n",
      "   0.04  0.1   1.    0.03  0.11  1.   -0.02  0.09  1.   -0.01  0.09  0.95]\n",
      " [-0.04  0.01  0.99 -0.06  0.03  0.99 -0.05  0.    0.98 -0.03  0.03  0.99\n",
      "  -0.05  0.03  0.99 -0.06  0.01  0.99 -0.04 -0.02  0.98 -0.04 -0.03  0.97\n",
      "  -0.05  0.    0.98 -0.1  -0.01  0.97 -0.06 -0.02  0.98 -0.04  0.    0.98\n",
      "  -0.09 -0.01  0.98 -0.05  0.    0.99 -0.06  0.    0.98 -0.08  0.01  0.98\n",
      "  -0.05 -0.02  0.98 -0.04 -0.    0.98 -0.04  0.03  0.99 -0.05  0.01  0.98]\n",
      " [ 0.08  0.01  1.    0.04 -0.01  0.95 -0.01 -0.02  0.94  0.05 -0.02  0.98\n",
      "   0.03  0.02  0.97  0.02 -0.01  0.97  0.03 -0.    0.99 -0.03 -0.01  0.97\n",
      "  -0.02 -0.01  0.98  0.03 -0.01  0.95  0.08  0.02  1.02 -0.02 -0.02  1.\n",
      "   0.02 -0.02  0.99  0.02 -0.    0.99  0.04  0.02  0.97  0.07  0.    0.99\n",
      "   0.07  0.01  1.    0.   -0.02  0.94 -0.01 -0.02  0.97  0.01 -0.05  0.99]\n",
      " [-0.03  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98\n",
      "  -0.04  0.02  0.98 -0.05  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.03  0.98\n",
      "  -0.07 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97]\n",
      " [ 0.02 -0.04  0.88  0.02  0.    0.98  0.02  0.01  0.99  0.06  0.06  1.03\n",
      "  -0.06 -0.01  0.98  0.08  0.01  0.96 -0.02 -0.    0.98  0.07 -0.01  0.92\n",
      "   0.03  0.03  1.01  0.02  0.01  0.98  0.02  0.    0.99  0.02 -0.04  0.86\n",
      "  -0.04 -0.01  0.99 -0.04 -0.    0.99  0.08  0.01  0.95  0.01 -0.04  0.9\n",
      "   0.02  0.03  0.98  0.02  0.01  0.98  0.02 -0.01  0.98  0.07  0.    0.94]\n",
      " [-0.04 -0.01  0.99 -0.04 -0.    0.99  0.08  0.01  0.95  0.01 -0.04  0.9\n",
      "   0.02  0.03  0.98  0.02  0.01  0.98  0.02 -0.01  0.98  0.07  0.    0.94\n",
      "   0.05  0.    0.95 -0.01  0.01  1.01 -0.06 -0.01  1.01  0.02 -0.04  0.88\n",
      "   0.02  0.    0.98  0.02  0.    0.99  0.05 -0.    0.95  0.09  0.02  0.96\n",
      "   0.   -0.    0.96 -0.04 -0.    1.    0.09  0.02  0.95  0.02  0.01  0.98]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.98\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.99\n",
      "  -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [ 0.02 -0.03  0.99  0.02 -0.01  0.99  0.02  0.    0.98  0.09  0.02  0.99\n",
      "   0.07  0.01  1.03 -0.02 -0.01  0.97 -0.01 -0.02  0.96  0.07  0.01  0.97\n",
      "   0.03  0.01  0.97  0.02 -0.    0.97  0.01 -0.01  0.99 -0.01 -0.    0.98\n",
      "  -0.01 -0.01  0.99  0.06  0.01  1.02  0.09  0.02  1.    0.04  0.03  0.97\n",
      "   0.02 -0.    0.98  0.02 -0.    0.97  0.01 -0.06  0.98  0.04  0.    1.01]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.02  0.98 -0.07 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.06 -0.02  0.98 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.99]\n",
      " [ 0.04  0.05  0.97  0.02 -0.01  0.99  0.02 -0.    0.97 -0.   -0.07  0.98\n",
      "   0.08  0.01  1.02  0.07  0.01  0.99  0.   -0.02  0.94  0.09  0.02  0.98\n",
      "   0.02 -0.01  0.98  0.02 -0.01  0.98  0.02 -0.01  0.98 -0.02 -0.03  0.98\n",
      "   0.02  0.    1.01 -0.01 -0.02  0.94 -0.04 -0.02  0.96  0.01  0.04  0.99\n",
      "   0.02 -0.01  0.98  0.02 -0.01  0.99  0.01 -0.04  0.99  0.05 -0.    0.98]\n",
      " [-0.03 -0.05  0.98 -0.06 -0.01  0.98 -0.05  0.01  1.   -0.06 -0.02  0.98\n",
      "  -0.07  0.01  0.99 -0.05  0.01  0.99 -0.03 -0.05  0.97 -0.05 -0.01  0.99\n",
      "  -0.05 -0.02  0.99 -0.05  0.01  0.99 -0.07  0.    1.   -0.05  0.01  0.99\n",
      "  -0.07  0.02  0.98 -0.06  0.01  0.99 -0.04 -0.03  0.97 -0.05 -0.06  0.98\n",
      "  -0.04 -0.05  0.98 -0.06 -0.03  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [ 0.02 -0.    0.98  0.02 -0.01  0.98  0.06 -0.04  0.98  0.03  0.    1.01\n",
      "  -0.02 -0.01  0.97  0.08  0.02  1.02  0.08  0.03  0.98  0.02 -0.02  0.99\n",
      "   0.02 -0.01  0.98  0.02 -0.01  0.98  0.06  0.01  1.    0.07  0.01  0.99\n",
      "  -0.02 -0.02  0.95  0.1   0.01  1.    0.01  0.02  0.99  0.02  0.    0.98\n",
      "   0.02 -0.    0.99  0.03 -0.05  0.98  0.02 -0.01  0.96 -0.02 -0.01  1.  ]\n",
      " [-0.05 -0.    0.98 -0.06 -0.    0.98 -0.06 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05  0.    0.99 -0.05 -0.    0.99 -0.06 -0.01  0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.05 -0.    0.99 -0.03 -0.05  0.97 -0.05 -0.04  0.98 -0.03 -0.05  0.98\n",
      "  -0.05 -0.05  0.97 -0.04 -0.05  0.98 -0.06 -0.05  0.98 -0.04 -0.04  0.98\n",
      "  -0.05 -0.05  0.98 -0.05 -0.05  0.98 -0.06 -0.04  0.98 -0.06 -0.06  0.98\n",
      "  -0.06 -0.02  0.99 -0.06 -0.03  0.98 -0.07  0.01  0.98 -0.06  0.    0.99\n",
      "  -0.04 -0.02  0.98 -0.06 -0.    0.98 -0.04 -0.05  0.98 -0.05 -0.02  0.99]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.02  0.98\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.02  0.98]\n",
      " [-0.1   0.01  0.98 -0.06  0.    0.99 -0.03  0.03  1.   -0.04 -0.01  0.98\n",
      "  -0.05 -0.02  0.98 -0.08  0.01  0.98 -0.1  -0.    0.98 -0.05  0.    0.98\n",
      "  -0.02  0.02  1.   -0.07 -0.02  0.98 -0.04  0.01  0.99 -0.01 -0.02  0.98\n",
      "  -0.06 -0.02  0.98 -0.05 -0.    0.98 -0.07 -0.02  0.98 -0.07  0.    0.98\n",
      "  -0.03  0.01  0.99 -0.04 -0.03  0.98 -0.06 -0.    0.99 -0.07  0.02  0.99]\n",
      " [-0.03  0.05  1.    0.    0.1   1.02 -0.07  0.03  0.99 -0.04  0.06  1.\n",
      "   0.03  0.16  0.97 -0.01  0.1   1.    0.01  0.18  1.    0.04  0.11  0.97\n",
      "   0.02  0.05  1.01 -0.01  0.1   1.   -0.04  0.04  0.99  0.02  0.11  0.98\n",
      "  -0.04  0.05  0.98 -0.01  0.07  0.97  0.02  0.14  0.97 -0.04  0.06  0.99\n",
      "  -0.06  0.05  0.97 -0.02  0.08  0.99  0.01  0.14  1.    0.    0.1   0.95]\n",
      " [-0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.    0.11  1.02  0.03  0.17  1.02 -0.01  0.09  0.99  0.02  0.03  1.01\n",
      "  -0.01  0.1   0.99 -0.    0.06  1.03 -0.03  0.08  0.98 -0.03  0.05  0.99\n",
      "  -0.02  0.1   0.98 -0.04  0.14  0.99 -0.01  0.08  0.96 -0.05  0.09  0.95\n",
      "  -0.03  0.09  0.97 -0.03  0.05  0.98  0.02  0.11  0.99 -0.03  0.1   0.95\n",
      "  -0.02  0.13  0.97  0.01  0.12  0.99  0.02  0.14  0.97  0.02  0.11  0.95]\n",
      " [-0.05  0.    0.99 -0.03 -0.05  0.97 -0.06 -0.04  0.97 -0.07  0.    0.99\n",
      "  -0.06 -0.05  0.97 -0.07  0.01  0.99 -0.05 -0.05  0.97 -0.04 -0.03  0.97\n",
      "  -0.05 -0.05  0.97 -0.04 -0.05  0.98 -0.06 -0.02  0.98 -0.06 -0.01  0.99\n",
      "  -0.06  0.01  0.99 -0.07  0.01  0.98 -0.06  0.01  0.99 -0.05  0.01  0.98\n",
      "  -0.06  0.01  0.98 -0.05  0.01  0.98 -0.04 -0.04  0.98 -0.05  0.01  0.99]\n",
      " [ 0.01  0.07  0.97 -0.03  0.03  1.01  0.03  0.06  1.    0.05  0.11  0.99\n",
      "   0.02  0.08  0.98 -0.02  0.1   1.   -0.02  0.08  0.95  0.03  0.1   1.\n",
      "   0.05  0.14  1.01 -0.01  0.08  0.96  0.02  0.08  1.01  0.01  0.11  0.96\n",
      "   0.06  0.15  1.01 -0.04  0.07  0.99 -0.02  0.03  1.    0.01  0.11  0.98\n",
      "  -0.01  0.04  1.    0.02  0.11  0.98 -0.04  0.13  0.94 -0.01  0.07  0.96]\n",
      " [-0.05  0.06  0.99 -0.07  0.04  0.97 -0.    0.07  0.94 -0.04  0.06  0.92\n",
      "   0.04  0.11  0.96 -0.06  0.12  0.94 -0.01  0.1   0.98  0.03  0.14  0.95\n",
      "   0.03  0.12  0.97 -0.    0.12  0.96  0.    0.1   0.98  0.03  0.11  0.97\n",
      "  -0.04  0.05  0.99 -0.01  0.1   0.94  0.02  0.15  0.98 -0.04  0.08  0.99\n",
      "   0.03  0.11  1.01  0.02  0.11  0.97  0.04  0.1   1.    0.    0.1   0.96]\n",
      " [ 0.02 -0.01  0.98 -0.01  0.    0.99  0.06  0.01  1.02 -0.02 -0.02  0.96\n",
      "   0.08  0.01  1.   -0.03 -0.01  1.    0.02 -0.02  0.99  0.02 -0.    0.98\n",
      "   0.04  0.02  0.97 -0.02 -0.02  0.96 -0.02 -0.02  0.94  0.01  0.    1.01\n",
      "   0.04 -0.02  0.98  0.01 -0.06  0.99  0.02  0.01  0.97  0.02 -0.    0.98\n",
      "  -0.05 -0.01  1.    0.07  0.01  0.99 -0.   -0.02  0.94 -0.02 -0.01  0.98]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.98\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.06 -0.02  0.98 -0.06 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.98]\n",
      " [ 0.04 -0.01  0.98 -0.04 -0.02  0.95  0.06  0.01  1.03 -0.04 -0.02  0.96\n",
      "   0.04 -0.01  0.97  0.02 -0.01  0.99  0.02 -0.    0.98 -0.02  0.01  0.99\n",
      "  -0.04 -0.02  0.96  0.06  0.01  1.03  0.05 -0.    0.97 -0.01 -0.03  0.98\n",
      "   0.02 -0.01  0.98  0.02 -0.01  0.99  0.02 -0.01  0.98  0.03 -0.01  0.98\n",
      "   0.07  0.01  0.99  0.07  0.01  1.02  0.01 -0.    0.99  0.02 -0.04  0.98]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.06 -0.02  0.98 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98]\n",
      " [ 0.02  0.01  0.98 -0.02 -0.02  0.95 -0.02  0.01  1.01 -0.01 -0.    0.97\n",
      "   0.09  0.02  0.98 -0.02 -0.04  0.92  0.02  0.    0.98  0.02  0.01  0.98\n",
      "   0.04  0.03  1.   -0.04 -0.02  0.96 -0.04 -0.    1.01  0.07  0.02  0.98\n",
      "   0.03  0.03  1.03  0.02  0.03  1.02  0.02 -0.    0.98  0.02  0.01  0.99\n",
      "   0.01 -0.06  0.86  0.09  0.02  0.96 -0.03 -0.01  0.97  0.02  0.01  1.02]\n",
      " [-0.06 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.06 -0.02  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.99]\n",
      " [ 0.02 -0.05  0.97  0.   -0.    0.98 -0.   -0.02  0.93 -0.02 -0.01  0.97\n",
      "   0.06 -0.02  0.98  0.03  0.03  0.97  0.02  0.    0.98  0.02 -0.    0.98\n",
      "   0.04 -0.03  0.97 -0.01 -0.02  0.95 -0.   -0.02  0.93  0.08  0.01  1.02\n",
      "   0.04 -0.03  0.97  0.02  0.01  0.97  0.02 -0.    0.97  0.03  0.02  0.98\n",
      "   0.09  0.01  1.    0.08  0.01  1.02 -0.01 -0.02  0.94  0.02 -0.02  0.97]\n",
      " [-0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.01  0.98\n",
      "  -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.01  0.98\n",
      "  -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.04  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.99\n",
      "  -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [ 0.01 -0.01  0.95 -0.01 -0.01  0.96  0.05  0.02  1.   -0.06 -0.01  0.99\n",
      "   0.07  0.04  1.01  0.02  0.    0.99  0.02  0.01  0.98  0.01  0.03  1.03\n",
      "  -0.02 -0.01  0.96  0.08  0.01  0.95 -0.   -0.    0.96  0.04  0.04  1.03\n",
      "   0.01 -0.02  0.95  0.02  0.    0.99  0.02  0.    0.98 -0.01  0.05  1.11\n",
      "   0.06  0.02  1.01  0.07  0.02  0.99 -0.02 -0.01  0.96 -0.04 -0.    1.02]\n",
      " [-0.01  0.06  1.03 -0.01  0.08  1.01  0.03  0.08  1.    0.03  0.09  0.99\n",
      "   0.    0.11  0.99 -0.01  0.13  0.99 -0.01  0.07  1.01 -0.04  0.03  0.96\n",
      "  -0.04  0.07  0.98 -0.04  0.06  0.95 -0.03  0.07  1.   -0.08  0.03  0.95\n",
      "   0.03  0.11  0.96  0.05  0.16  0.98 -0.    0.09  1.01 -0.    0.17  0.99\n",
      "   0.03  0.11  0.96  0.05  0.09  1.01 -0.01  0.11  1.    0.04  0.15  1.02]\n",
      " [-0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.99\n",
      "  -0.07 -0.    0.99 -0.07 -0.    0.99 -0.07 -0.    0.99 -0.07 -0.    0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99]\n",
      " [-0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.03  0.98]\n",
      " [-0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.04  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99]\n",
      " [ 0.09  0.01  1.01 -0.   -0.01  1.    0.04  0.01  1.03  0.07  0.02  0.99\n",
      "   0.01 -0.05  1.    0.02 -0.    0.99  0.02  0.    0.99  0.01 -0.03  0.97\n",
      "  -0.03 -0.02  0.95 -0.01 -0.02  0.94  0.08  0.01  1.02  0.07 -0.01  0.97\n",
      "   0.03  0.01  0.98  0.02  0.    0.99  0.01 -0.03  1.    0.09  0.01  1.01\n",
      "   0.08  0.01  1.02 -0.01 -0.02  0.94  0.07 -0.    0.99  0.02  0.05  0.99]\n",
      " [ 0.01  0.08  0.97 -0.03  0.03  1.   -0.02  0.07  1.    0.02  0.13  0.96\n",
      "   0.01  0.11  0.98 -0.01  0.1   1.   -0.03  0.06  0.96  0.02  0.12  0.95\n",
      "  -0.01  0.13  0.94 -0.01  0.09  0.97  0.05  0.14  0.99 -0.01  0.07  0.94\n",
      "  -0.01  0.12  0.92 -0.04  0.06  0.98 -0.02  0.03  1.    0.    0.1   0.95\n",
      "  -0.06  0.03  0.96 -0.05  0.06  0.99  0.03  0.15  0.97 -0.01  0.08  0.97]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.98 -0.07 -0.03  0.98]\n",
      " [-0.04  0.05  0.98 -0.01  0.07  0.97  0.02  0.14  0.97 -0.04  0.06  0.99\n",
      "  -0.06  0.05  0.97 -0.02  0.08  0.99  0.01  0.14  1.    0.    0.1   0.95\n",
      "  -0.01  0.06  1.    0.04  0.11  1.01 -0.03  0.06  0.97  0.03  0.13  0.98\n",
      "   0.01  0.1   1.01  0.04  0.1   1.01 -0.01  0.07  0.96 -0.02  0.04  1.\n",
      "   0.    0.09  1.02 -0.08  0.05  1.01  0.02  0.11  0.97  0.01  0.15  0.95]\n",
      " [-0.    0.1   0.95  0.02  0.08  0.97  0.04  0.11  1.02  0.01  0.09  0.95\n",
      "  -0.01  0.04  1.    0.01  0.11  0.99 -0.    0.06  1.03  0.03  0.12  0.97\n",
      "  -0.05  0.09  0.95 -0.    0.11  1.02 -0.07  0.1   0.99 -0.04  0.07  0.99\n",
      "   0.06  0.12  1.   -0.01  0.1   1.    0.02  0.18  1.01 -0.02  0.08  0.97\n",
      "   0.06  0.09  1.01 -0.01  0.09  0.99 -0.04  0.02  0.99  0.03  0.12  0.97]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.    0.99 -0.07  0.01  0.99\n",
      "  -0.06 -0.    0.98 -0.06  0.01  0.98 -0.06 -0.01  0.98 -0.05 -0.02  0.97\n",
      "  -0.06  0.02  0.99 -0.04 -0.04  0.97 -0.06  0.01  0.99 -0.04 -0.05  0.98\n",
      "  -0.06 -0.01  0.99 -0.06 -0.02  0.99 -0.06 -0.04  0.97 -0.07  0.01  0.99\n",
      "  -0.05  0.01  0.99 -0.04 -0.03  0.97 -0.05 -0.01  0.99 -0.03 -0.05  0.97]\n",
      " [ 0.02 -0.02  0.99  0.02 -0.    0.99  0.04  0.02  0.97  0.07  0.    0.99\n",
      "   0.07  0.01  1.    0.   -0.02  0.94 -0.01 -0.02  0.97  0.01 -0.05  0.99\n",
      "   0.02  0.    0.98  0.02 -0.    0.99  0.01  0.05  0.99  0.02 -0.01  0.96\n",
      "  -0.02 -0.02  0.94  0.04  0.01  1.02 -0.04 -0.01  0.99  0.02  0.01  0.98\n",
      "   0.02  0.    0.99  0.02 -0.01  0.98  0.04 -0.01  0.98 -0.01 -0.01  0.98]\n",
      " [-0.01  0.03  1.08  0.03  0.01  1.    0.02  0.    0.99  0.03  0.02  0.98\n",
      "   0.1   0.01  0.95 -0.01  0.01  1.02  0.05  0.01  0.95 -0.05 -0.01  0.98\n",
      "   0.04  0.06  1.09  0.02  0.    0.98  0.02  0.01  0.98 -0.02 -0.05  0.92\n",
      "   0.    0.01  1.02  0.01 -0.    0.96 -0.04 -0.    1.   -0.05 -0.01  1.01\n",
      "   0.02 -0.01  0.97  0.02  0.    0.99  0.02  0.    0.98 -0.05 -0.    1.03]\n",
      " [ 0.04  0.14  0.97  0.03  0.12  0.97  0.05  0.12  0.99  0.03  0.11  1.\n",
      "  -0.01  0.1   1.    0.02  0.11  0.95 -0.05  0.06  0.98 -0.03  0.11  0.94\n",
      "   0.03  0.11  0.96  0.04  0.14  0.99  0.02  0.11  0.97  0.06  0.12  1.01\n",
      "  -0.03  0.08  0.98 -0.04  0.11  0.94 -0.    0.11  1.01 -0.04  0.13  0.97\n",
      "   0.03  0.11  0.96  0.04  0.07  1.01 -0.01  0.11  1.   -0.04  0.02  0.97]\n",
      " [-0.05 -0.01  0.97 -0.05  0.    0.98 -0.06 -0.03  0.98 -0.06 -0.01  0.98\n",
      "  -0.03  0.    0.99 -0.04 -0.03  0.98 -0.05  0.    0.98 -0.08  0.02  0.99\n",
      "  -0.05 -0.02  0.98 -0.04  0.    0.98 -0.03  0.01  0.99 -0.04 -0.    0.99\n",
      "  -0.05 -0.    0.98 -0.11  0.02  0.98 -0.05 -0.02  0.98 -0.04  0.01  0.99\n",
      "  -0.08  0.02  0.99 -0.06  0.01  0.99 -0.05 -0.    0.98 -0.04  0.01  0.98]\n",
      " [ 0.05  0.    0.95 -0.02 -0.    0.97 -0.02  0.02  1.06  0.01 -0.    0.96\n",
      "   0.02  0.01  0.98  0.02 -0.    0.98  0.09  0.01  0.91  0.01  0.02  1.02\n",
      "  -0.03  0.    1.   -0.04 -0.    1.01 -0.03 -0.01  0.99  0.02  0.02  0.98\n",
      "   0.03  0.01  0.98  0.03 -0.    0.91  0.09  0.01  0.95  0.02  0.01  1.02\n",
      "   0.08  0.01  0.96 -0.   -0.02  0.95  0.01 -0.05  0.89  0.02  0.    0.98]\n",
      " [-0.04  0.05  0.98  0.02  0.08  1.    0.    0.06  0.97 -0.04  0.07  0.99\n",
      "  -0.    0.1   0.95  0.02  0.08  0.97  0.04  0.11  1.02  0.01  0.09  0.95\n",
      "  -0.01  0.04  1.    0.01  0.11  0.99 -0.    0.06  1.03  0.03  0.12  0.97\n",
      "  -0.05  0.09  0.95 -0.    0.11  1.02 -0.07  0.1   0.99 -0.04  0.07  0.99\n",
      "   0.06  0.12  1.   -0.01  0.1   1.    0.02  0.18  1.01 -0.02  0.08  0.97]\n",
      " [-0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.03  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.04  0.03  0.99 -0.03  0.03  0.99 -0.04  0.01  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.99 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98]\n",
      " [-0.05 -0.05  0.97 -0.04 -0.05  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.99\n",
      "  -0.06  0.01  0.99 -0.06  0.    0.99 -0.05  0.    0.98 -0.04 -0.02  0.98\n",
      "  -0.04 -0.03  0.98 -0.04 -0.04  0.98 -0.06 -0.03  0.99 -0.05 -0.05  0.98\n",
      "  -0.07  0.01  0.98 -0.06 -0.01  0.98 -0.04 -0.04  0.97 -0.05 -0.    0.99\n",
      "  -0.04 -0.05  0.98 -0.05  0.01  0.99 -0.05 -0.03  0.99 -0.05  0.01  1.  ]\n",
      " [-0.03  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.03  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98]\n",
      " [-0.05  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98]\n",
      " [-0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98]\n",
      " [ 0.07  0.01  0.99 -0.02 -0.01  0.97  0.07 -0.    0.97  0.03  0.01  0.97\n",
      "   0.02 -0.03  0.99  0.02  0.01  0.99  0.1   0.01  1.    0.07  0.01  0.98\n",
      "  -0.01 -0.02  0.94  0.08  0.    0.99  0.04  0.05  0.97  0.02 -0.02  0.99\n",
      "   0.02 -0.    0.99 -0.01 -0.07  0.98  0.06  0.    0.98 -0.03 -0.02  0.94\n",
      "   0.   -0.    1.    0.07 -0.01  0.98  0.02 -0.01  0.98  0.02 -0.01  0.98]\n",
      " [-0.03  0.07  1.01  0.    0.06  1.03 -0.03  0.07  0.99 -0.03  0.04  1.\n",
      "   0.    0.1   0.99  0.03  0.09  1.04 -0.04  0.1   0.98 -0.07  0.03  0.96\n",
      "  -0.01  0.08  0.94 -0.    0.01  0.96 -0.04  0.06  0.99 -0.07  0.04  0.96\n",
      "  -0.    0.09  0.97 -0.02  0.01  0.97 -0.04  0.06  0.98 -0.02  0.05  0.99\n",
      "  -0.01  0.1   0.99 -0.    0.09  0.96 -0.01  0.08  1.    0.03  0.08  1.  ]\n",
      " [-0.05 -0.03  0.99 -0.06 -0.04  0.97 -0.07  0.01  0.99 -0.05  0.01  1.\n",
      "  -0.06 -0.01  0.98 -0.05 -0.01  0.99 -0.04 -0.03  0.97 -0.06 -0.01  0.99\n",
      "  -0.04 -0.04  0.99 -0.06 -0.02  0.98 -0.06 -0.01  0.99 -0.06  0.01  0.99\n",
      "  -0.07  0.01  0.99 -0.05  0.01  0.99 -0.06  0.01  0.98 -0.07  0.01  0.99\n",
      "  -0.06  0.01  0.99 -0.07  0.01  0.98 -0.06  0.01  0.99 -0.06  0.01  0.98]\n",
      " [-0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.99 -0.06 -0.02  0.99 -0.06 -0.02  0.98\n",
      "  -0.06 -0.02  0.99 -0.07 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.06 -0.02  0.98\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [ 0.02  0.01  0.98  0.02  0.01  0.98  0.02  0.03  1.02  0.1   0.02  0.96\n",
      "  -0.03 -0.01  0.98  0.01  0.01  1.01  0.04  0.04  1.03  0.02 -0.03  0.94\n",
      "   0.02  0.01  0.98  0.02  0.01  0.99 -0.01  0.05  1.11 -0.01 -0.01  0.96\n",
      "   0.05  0.    0.95 -0.02  0.01  0.95 -0.01  0.04  1.09  0.02 -0.    0.99\n",
      "   0.02  0.01  0.98  0.03  0.    0.97  0.1   0.02  0.97 -0.01 -0.01  0.96]\n",
      " [-0.05 -0.    0.98 -0.04 -0.04  0.97 -0.06 -0.02  0.98 -0.05  0.02  0.99\n",
      "  -0.08 -0.02  0.98 -0.04  0.03  0.99 -0.06  0.    0.98 -0.08  0.02  0.99\n",
      "  -0.07  0.01  0.98 -0.04 -0.01  0.98 -0.06  0.03  0.99 -0.06  0.    0.99\n",
      "  -0.06 -0.01  0.98 -0.08 -0.    0.98 -0.04  0.    0.98 -0.06 -0.02  0.98\n",
      "  -0.02  0.03  0.99 -0.05  0.    0.98 -0.02 -0.04  0.97 -0.07  0.    0.98]\n",
      " [-0.04 -0.01  0.98 -0.05 -0.02  0.98 -0.08  0.01  0.98 -0.1  -0.    0.98\n",
      "  -0.05  0.    0.98 -0.02  0.02  1.   -0.07 -0.02  0.98 -0.04  0.01  0.99\n",
      "  -0.01 -0.02  0.98 -0.06 -0.02  0.98 -0.05 -0.    0.98 -0.07 -0.02  0.98\n",
      "  -0.07  0.    0.98 -0.03  0.01  0.99 -0.04 -0.03  0.98 -0.06 -0.    0.99\n",
      "  -0.07  0.02  0.99 -0.04 -0.02  0.98 -0.04  0.01  0.99 -0.04  0.02  0.99]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.02  0.98 -0.07 -0.03  0.98]\n",
      " [-0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.99 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98]\n",
      " [ 0.02  0.    0.99 -0.05  0.    1.05 -0.02 -0.    1.02 -0.01 -0.    0.96\n",
      "   0.03 -0.01  0.94  0.06  0.01  0.95  0.02  0.    0.98  0.02  0.01  0.98\n",
      "   0.04  0.03  1.    0.11  0.02  0.96  0.07  0.02  0.96 -0.01 -0.01  0.97\n",
      "  -0.05 -0.    1.02  0.03  0.04  1.03  0.02  0.01  0.98  0.02  0.01  0.98\n",
      "   0.1   0.01  0.93 -0.03 -0.01  0.97 -0.03 -0.01  0.98  0.08  0.02  0.98]\n",
      " [-0.08  0.01  0.99 -0.05 -0.    0.99 -0.04 -0.02  0.97 -0.04 -0.01  0.98\n",
      "  -0.04 -0.03  0.98 -0.04 -0.05  0.98 -0.04 -0.05  0.98 -0.06 -0.03  0.99\n",
      "  -0.06 -0.02  0.99 -0.06 -0.02  0.99 -0.06 -0.03  0.99 -0.07  0.01  0.99\n",
      "  -0.06  0.02  0.99 -0.03 -0.05  0.98 -0.05 -0.    0.99 -0.05 -0.03  0.99\n",
      "  -0.05 -0.06  0.98 -0.07  0.01  0.99 -0.05 -0.04  0.98 -0.05 -0.02  0.97]\n",
      " [-0.04  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98]\n",
      " [-0.05 -0.    0.97 -0.05 -0.02  0.97 -0.06  0.02  0.98 -0.06 -0.01  0.97\n",
      "  -0.05  0.03  0.98 -0.05  0.01  0.97 -0.05 -0.01  0.97 -0.06  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.06 -0.02  0.97 -0.09 -0.02  0.97 -0.05  0.    0.98\n",
      "  -0.1   0.02  0.97 -0.07  0.01  0.98 -0.04  0.    0.98 -0.09  0.01  0.98\n",
      "  -0.03  0.    0.98 -0.05  0.    0.98 -0.07 -0.    0.98 -0.05 -0.02  0.97]\n",
      " [-0.07  0.01  0.99 -0.05  0.01  0.99 -0.06  0.01  0.98 -0.07  0.01  0.99\n",
      "  -0.06  0.01  0.99 -0.07  0.01  0.98 -0.06  0.01  0.99 -0.06  0.01  0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.01  0.98 -0.05 -0.    0.98 -0.04 -0.05  0.98\n",
      "  -0.05 -0.03  0.99 -0.06 -0.02  1.   -0.05 -0.02  0.99 -0.07  0.01  0.98\n",
      "  -0.05 -0.02  0.98 -0.05 -0.01  0.97 -0.05 -0.04  0.97 -0.05 -0.02  0.97]\n",
      " [-0.06 -0.02  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98]\n",
      " [-0.07 -0.01  0.98 -0.05 -0.01  0.98 -0.1   0.01  0.98 -0.05  0.    0.98\n",
      "  -0.06 -0.03  0.98 -0.04 -0.01  0.97 -0.05  0.02  0.99 -0.04  0.02  0.99\n",
      "  -0.09  0.02  0.98 -0.06  0.    0.99 -0.02  0.03  1.   -0.04 -0.01  0.99\n",
      "  -0.05 -0.02  0.98 -0.08  0.01  0.98 -0.1  -0.01  0.97 -0.05  0.    0.98\n",
      "  -0.04  0.04  1.   -0.05 -0.02  0.98 -0.06  0.01  0.99 -0.02  0.02  0.99]\n",
      " [ 0.03  0.17  0.97 -0.    0.11  1.02  0.03  0.17  1.02 -0.01  0.09  0.99\n",
      "   0.02  0.03  1.01 -0.01  0.1   0.99 -0.    0.06  1.03 -0.03  0.08  0.98\n",
      "  -0.03  0.05  0.99 -0.02  0.1   0.98 -0.04  0.14  0.99 -0.01  0.08  0.96\n",
      "  -0.05  0.09  0.95 -0.03  0.09  0.97 -0.03  0.05  0.98  0.02  0.11  0.99\n",
      "  -0.03  0.1   0.95 -0.02  0.13  0.97  0.01  0.12  0.99  0.02  0.14  0.97]\n",
      " [ 0.02 -0.    0.99 -0.    0.01  0.99  0.09  0.01  1.01 -0.02 -0.02  0.94\n",
      "   0.01  0.    1.02 -0.04 -0.01  0.98  0.   -0.02  0.98  0.02 -0.    0.99\n",
      "   0.02 -0.    0.98 -0.04 -0.02  0.97 -0.02 -0.02  0.94  0.02 -0.01  0.94\n",
      "   0.07  0.01  1.02  0.06 -0.03  0.97  0.02  0.01  0.97  0.02 -0.    0.98\n",
      "   0.02 -0.02  1.    0.01 -0.01  0.97  0.02 -0.01  0.95  0.02  0.    1.02]\n",
      " [-0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.03  0.99 -0.04  0.01  0.98 -0.05  0.04  0.98]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.03  0.98\n",
      "  -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.03  0.98 -0.07 -0.02  0.99]\n",
      " [-0.09  0.02  0.98 -0.06  0.01  0.97 -0.06 -0.01  0.97 -0.04  0.    0.98\n",
      "  -0.06  0.03  0.98 -0.06  0.    0.98 -0.01 -0.03  0.97 -0.05  0.02  0.98\n",
      "  -0.07  0.    0.97 -0.02 -0.01  0.97 -0.05 -0.01  0.97 -0.05 -0.    0.97\n",
      "  -0.05 -0.02  0.97 -0.06  0.02  0.98 -0.06 -0.01  0.97 -0.05  0.03  0.98\n",
      "  -0.05  0.01  0.97 -0.05 -0.01  0.97 -0.06  0.03  0.98 -0.05  0.02  0.98]\n",
      " [-0.05 -0.03  0.97 -0.05  0.    0.99 -0.04 -0.05  0.98 -0.05 -0.02  0.98\n",
      "  -0.06 -0.02  0.99 -0.05  0.01  0.99 -0.07  0.    0.99 -0.06  0.01  0.99\n",
      "  -0.06  0.01  0.98 -0.06  0.01  0.99 -0.05  0.    0.98 -0.04 -0.02  0.98\n",
      "  -0.05  0.    0.98 -0.04 -0.04  0.98 -0.05 -0.    0.99 -0.04 -0.05  0.99\n",
      "  -0.05 -0.05  0.98 -0.06 -0.02  0.99 -0.05 -0.05  0.98 -0.07  0.01  0.98]\n",
      " [-0.06 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.98\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.06 -0.02  0.99\n",
      "  -0.06 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.99]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.06 -0.02  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97]\n",
      " [ 0.04 -0.02  0.98  0.   -0.01  0.99 -0.01 -0.02  0.94 -0.05 -0.02  0.96\n",
      "   0.01  0.04  0.99  0.02  0.01  0.97  0.02 -0.    0.97  0.01 -0.06  0.98\n",
      "   0.05  0.01  1.   -0.   -0.02  0.94 -0.   -0.01  1.   -0.02 -0.03  0.98\n",
      "   0.03  0.03  0.97  0.02  0.    0.98  0.02 -0.    0.99  0.04  0.01  0.99\n",
      "   0.01 -0.    1.   -0.02 -0.01  0.98  0.04 -0.    0.98 -0.04 -0.01  0.99]\n",
      " [-0.02  0.1   0.98  0.01  0.15  0.95  0.02  0.12  0.96 -0.02  0.1   0.96\n",
      "  -0.01  0.07  0.97 -0.    0.07  0.96 -0.    0.09  1.   -0.03  0.09  0.95\n",
      "  -0.04  0.1   0.95  0.02  0.12  0.99 -0.01  0.1   0.95 -0.02  0.08  0.94\n",
      "  -0.01  0.13  0.95  0.01  0.09  0.99  0.04  0.13  1.   -0.01  0.07  0.94\n",
      "   0.06  0.08  0.98  0.03  0.12  0.96 -0.06  0.09  0.94 -0.01  0.07  0.95]\n",
      " [-0.04 -0.05  0.98 -0.06 -0.03  0.98 -0.04 -0.04  0.98 -0.06 -0.05  0.97\n",
      "  -0.06 -0.01  0.99 -0.06 -0.03  0.98 -0.07  0.01  0.99 -0.05 -0.02  0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.01  0.98 -0.04 -0.03  0.98 -0.05 -0.05  0.97\n",
      "  -0.05 -0.04  0.98 -0.05 -0.05  0.98 -0.06 -0.02  0.99 -0.06 -0.02  0.99\n",
      "  -0.06 -0.03  0.98 -0.07  0.    0.99 -0.06 -0.04  0.98 -0.07  0.01  0.98]\n",
      " [-0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.06 -0.02  0.99 -0.06 -0.02  0.99 -0.06 -0.02  0.99\n",
      "  -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.06 -0.02  0.99 -0.06 -0.02  0.98\n",
      "  -0.06 -0.02  0.98 -0.07 -0.    0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97]\n",
      " [-0.03 -0.06  0.98 -0.04 -0.05  0.98 -0.05 -0.05  0.98 -0.06 -0.04  0.98\n",
      "  -0.07 -0.02  0.99 -0.07  0.01  0.99 -0.06  0.01  0.98 -0.07  0.01  0.98\n",
      "  -0.06  0.01  0.98 -0.06 -0.    0.97 -0.05  0.02  0.99 -0.03 -0.04  0.98\n",
      "  -0.05  0.02  0.99 -0.03 -0.05  0.98 -0.05 -0.    0.99 -0.04 -0.04  0.99\n",
      "  -0.05 -0.03  0.99 -0.07  0.01  0.99 -0.05 -0.01  0.99 -0.07  0.    0.97]\n",
      " [-0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.98]\n",
      " [-0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.01  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98]\n",
      " [-0.05  0.    0.98 -0.08  0.02  0.99 -0.05 -0.02  0.98 -0.04  0.    0.98\n",
      "  -0.03  0.01  0.99 -0.04 -0.    0.99 -0.05 -0.    0.98 -0.11  0.02  0.98\n",
      "  -0.05 -0.02  0.98 -0.04  0.01  0.99 -0.08  0.02  0.99 -0.06  0.01  0.99\n",
      "  -0.05 -0.    0.98 -0.04  0.01  0.98 -0.07  0.    0.98 -0.06 -0.02  0.98\n",
      "  -0.03  0.03  0.99 -0.05  0.    0.98 -0.07 -0.    0.99 -0.04  0.    0.99]\n",
      " [-0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.99 -0.03  0.01  0.98]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98]\n",
      " [-0.05 -0.02  0.98 -0.04 -0.03  0.97 -0.06 -0.03  0.98 -0.04 -0.05  0.98\n",
      "  -0.05  0.01  1.   -0.06 -0.03  0.99 -0.06  0.01  0.99 -0.08  0.01  0.99\n",
      "  -0.05 -0.    0.99 -0.04 -0.02  0.97 -0.04 -0.01  0.98 -0.04 -0.03  0.98\n",
      "  -0.04 -0.05  0.98 -0.04 -0.05  0.98 -0.06 -0.03  0.99 -0.06 -0.02  0.99\n",
      "  -0.06 -0.02  0.99 -0.06 -0.03  0.99 -0.07  0.01  0.99 -0.06  0.02  0.99]\n",
      " [ 0.03  0.03  0.97  0.02  0.    0.98  0.02 -0.    0.98  0.04 -0.03  0.97\n",
      "  -0.01 -0.02  0.95 -0.   -0.02  0.93  0.08  0.01  1.02  0.04 -0.03  0.97\n",
      "   0.02  0.01  0.97  0.02 -0.    0.97  0.03  0.02  0.98  0.09  0.01  1.\n",
      "   0.08  0.01  1.02 -0.01 -0.02  0.94  0.02 -0.02  0.97  0.05  0.04  0.97\n",
      "   0.02  0.    0.98  0.02  0.    0.97 -0.05 -0.01  0.99  0.06  0.    0.98]\n",
      " [-0.01  0.11  1.    0.04  0.15  1.02 -0.    0.09  0.99 -0.05  0.04  0.98\n",
      "  -0.    0.1   0.99 -0.04  0.06  0.92 -0.03  0.07  0.97  0.01  0.13  0.96\n",
      "  -0.    0.12  0.99 -0.02  0.05  0.96 -0.    0.09  1.    0.04  0.12  0.97\n",
      "   0.03  0.11  0.99  0.    0.11  0.99  0.03  0.14  0.98  0.03  0.1   1.\n",
      "   0.03  0.1   1.    0.03  0.1   0.96  0.03  0.14  0.99 -0.    0.09  0.94]\n",
      " [-0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.03  0.99 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98]\n",
      " [-0.07 -0.01  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.    0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.02  0.98 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.02  0.99]\n",
      " [ 0.08  0.01  0.95 -0.   -0.    0.96  0.04  0.04  1.03  0.01 -0.02  0.95\n",
      "   0.02  0.    0.99  0.02  0.    0.98 -0.01  0.05  1.11  0.06  0.02  1.01\n",
      "   0.07  0.02  0.99 -0.02 -0.01  0.96 -0.04 -0.    1.02  0.02  0.    0.98\n",
      "   0.02  0.01  0.98  0.02  0.01  0.98  0.09  0.02  0.99  0.08  0.01  0.95\n",
      "  -0.    0.01  1.01  0.04 -0.01  0.93  0.01  0.05  1.08  0.02 -0.    0.98]\n",
      " [-0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97]\n",
      " [-0.03  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.01  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98]\n",
      " [-0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98]\n",
      " [-0.    0.11  1.02 -0.08  0.04  0.99 -0.03  0.07  0.98  0.05  0.15  0.99\n",
      "  -0.    0.09  1.01  0.04  0.13  1.03 -0.03  0.07  0.99 -0.07  0.05  0.96\n",
      "   0.    0.08  0.96  0.03  0.14  0.96  0.03  0.12  0.96  0.02  0.06  1.01\n",
      "  -0.02  0.09  0.98  0.01  0.13  0.99 -0.01  0.09  0.96 -0.06  0.05  0.99\n",
      "  -0.04  0.06  0.96 -0.    0.07  0.96 -0.04  0.04  0.97 -0.03  0.09  0.95]\n",
      " [-0.01  0.12  0.98 -0.01  0.06  0.96 -0.04  0.05  0.96  0.02  0.12  0.96\n",
      "   0.03  0.13  1.    0.03  0.11  0.97  0.02  0.11  1.    0.01  0.09  1.01\n",
      "   0.02  0.08  1.04 -0.04  0.06  0.98 -0.04  0.11  0.94 -0.    0.11  1.02\n",
      "  -0.08  0.04  0.99 -0.03  0.07  0.98  0.05  0.15  0.99 -0.    0.09  1.01\n",
      "   0.04  0.13  1.03 -0.03  0.07  0.99 -0.07  0.05  0.96  0.    0.08  0.96]\n",
      " [ 0.07  0.12  0.95 -0.02  0.09  0.97 -0.06  0.08  0.95 -0.01  0.09  0.97\n",
      "   0.03  0.07  1.02 -0.04  0.07  0.98 -0.05  0.12  0.94  0.    0.07  0.96\n",
      "   0.03  0.15  0.97 -0.05  0.06  1.    0.05  0.09  1.   -0.01  0.06  0.97\n",
      "  -0.02  0.08  1.    0.02  0.12  0.96 -0.05  0.07  0.96 -0.05  0.07  0.94\n",
      "   0.    0.11  1.   -0.03  0.15  0.96 -0.01  0.09  0.93  0.06  0.15  1.01]\n",
      " [-0.06 -0.03  0.98 -0.07  0.    0.99 -0.06  0.    0.99 -0.05 -0.02  0.97\n",
      "  -0.06  0.01  0.99 -0.04 -0.04  0.98 -0.06 -0.05  0.97 -0.05 -0.02  0.97\n",
      "  -0.06 -0.02  0.98 -0.04 -0.04  0.97 -0.06 -0.05  0.97 -0.04 -0.04  0.99\n",
      "  -0.06 -0.05  0.97 -0.06 -0.    1.   -0.06 -0.05  0.97 -0.07  0.01  0.99\n",
      "  -0.06 -0.03  0.98 -0.05 -0.02  0.97 -0.06 -0.04  0.98 -0.04 -0.05  0.98]\n",
      " [ 0.03 -0.05  0.98  0.02 -0.01  0.96 -0.02 -0.01  1.    0.04  0.01  1.02\n",
      "   0.06  0.02  0.99  0.02 -0.01  0.99  0.02 -0.01  0.98  0.02 -0.    0.98\n",
      "  -0.01 -0.02  0.98  0.06  0.01  0.99  0.07  0.01  1.02 -0.   -0.    0.98\n",
      "   0.03 -0.04  0.97  0.02 -0.01  0.98  0.02 -0.01  0.97  0.02 -0.    1.\n",
      "   0.07 -0.    0.98  0.08  0.01  1.02 -0.01 -0.02  0.94 -0.01 -0.02  0.97]\n",
      " [-0.03  0.1   0.98 -0.03  0.05  0.99 -0.03  0.05  0.99 -0.02  0.1   0.95\n",
      "  -0.02  0.09  0.93 -0.04  0.08  1.   -0.01  0.1   0.95 -0.01  0.07  0.93\n",
      "  -0.    0.13  0.93  0.02  0.11  0.96  0.03  0.09  1.01 -0.01  0.08  0.92\n",
      "   0.06  0.07  0.97  0.04  0.12  0.96 -0.06  0.02  0.98 -0.    0.07  0.95\n",
      "  -0.04  0.    0.95 -0.    0.08  0.99 -0.    0.14  0.96 -0.    0.07  0.96]\n",
      " [ 0.02 -0.    0.97 -0.02  0.02  1.03  0.05  0.    0.95  0.07  0.02  0.98\n",
      "   0.1   0.02  0.96  0.02 -0.04  0.88  0.02  0.    0.98  0.02  0.01  0.99\n",
      "   0.06  0.06  1.03 -0.06 -0.01  0.98  0.08  0.01  0.96 -0.02 -0.    0.98\n",
      "   0.07 -0.01  0.92  0.03  0.03  1.01  0.02  0.01  0.98  0.02  0.    0.99\n",
      "   0.02 -0.04  0.86 -0.04 -0.01  0.99 -0.04 -0.    0.99  0.08  0.01  0.95]\n",
      " [-0.06 -0.04  0.97 -0.07  0.01  1.   -0.06 -0.04  0.97 -0.07  0.01  0.98\n",
      "  -0.06 -0.05  0.97 -0.04 -0.04  0.97 -0.05 -0.05  0.97 -0.04 -0.05  0.98\n",
      "  -0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.06  0.01  0.99 -0.06  0.    0.99\n",
      "  -0.05  0.    0.98 -0.04 -0.02  0.98 -0.04 -0.03  0.98 -0.04 -0.04  0.98\n",
      "  -0.06 -0.03  0.99 -0.05 -0.05  0.98 -0.07  0.01  0.98 -0.06 -0.01  0.98]\n",
      " [ 0.03  0.1   1.04  0.03  0.11  0.98 -0.07  0.09  0.95  0.    0.09  0.97\n",
      "  -0.03  0.04  0.98  0.01  0.1   0.99  0.05  0.12  0.98  0.03  0.1   0.99\n",
      "   0.02  0.12  0.98  0.03  0.13  0.97  0.02  0.08  1.    0.02  0.06  0.99\n",
      "   0.03  0.1   0.97 -0.    0.08  1.01 -0.01  0.08  1.01 -0.04  0.04  1.02\n",
      "  -0.02  0.08  0.96  0.02  0.16  0.96 -0.    0.1   1.02  0.    0.12  0.93]\n",
      " [-0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98 -0.04  0.04  0.99\n",
      "  -0.04  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.03  0.99 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98]\n",
      " [-0.01 -0.01  0.99  0.06  0.01  1.02  0.09  0.02  1.    0.04  0.03  0.97\n",
      "   0.02 -0.    0.98  0.02 -0.    0.97  0.01 -0.06  0.98  0.04  0.    1.01\n",
      "   0.07  0.01  1.01 -0.01 -0.02  0.94  0.07 -0.02  0.98  0.02 -0.01  0.99\n",
      "   0.02 -0.01  0.98  0.03  0.01  0.99  0.03 -0.02  0.98 -0.04 -0.02  0.95\n",
      "   0.07  0.01  0.99 -0.02 -0.02  0.96 -0.02  0.01  1.    0.02  0.    0.98]\n",
      " [-0.06 -0.    0.98 -0.06 -0.    0.98 -0.05  0.    0.98 -0.05  0.    0.99\n",
      "  -0.05 -0.    0.99 -0.06 -0.01  0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.02  0.98\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97]\n",
      " [-0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98 -0.04  0.04  0.99\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.04  0.03  0.99]\n",
      " [-0.04  0.01  0.99 -0.08 -0.03  0.98 -0.06 -0.    0.98 -0.06  0.02  0.98\n",
      "  -0.06 -0.01  0.97 -0.06 -0.01  0.98 -0.07  0.01  0.98 -0.03  0.03  0.99\n",
      "  -0.05  0.    0.98 -0.01 -0.01  1.   -0.05  0.02  0.98 -0.04 -0.01  0.98\n",
      "  -0.02 -0.01  0.98 -0.06  0.03  0.99 -0.05  0.    0.98 -0.04 -0.02  0.98\n",
      "  -0.04  0.01  0.98 -0.06  0.01  0.98 -0.04 -0.02  0.98 -0.05 -0.01  0.98]\n",
      " [-0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.04 -0.05  0.99 -0.05 -0.05  0.98 -0.06 -0.02  0.99 -0.05 -0.05  0.98\n",
      "  -0.07  0.01  0.98 -0.06 -0.04  0.97 -0.04 -0.04  0.97 -0.06 -0.05  0.97\n",
      "  -0.05 -0.03  0.99 -0.06 -0.    0.99 -0.07  0.01  0.99 -0.05 -0.03  0.98\n",
      "  -0.07  0.01  0.98 -0.05 -0.05  0.97 -0.04 -0.03  0.97 -0.05 -0.05  0.97\n",
      "  -0.04 -0.05  0.98 -0.06 -0.01  0.99 -0.06 -0.04  0.99 -0.06 -0.02  0.98]\n",
      " [ 0.02 -0.03  0.92  0.04 -0.    0.94 -0.04 -0.    0.99  0.03  0.02  1.02\n",
      "   0.08 -0.    0.91  0.02  0.03  1.    0.02  0.    0.99  0.02  0.01  0.98\n",
      "   0.03  0.04  1.08  0.08  0.01  0.95  0.05  0.02  1.    0.02  0.02  1.03\n",
      "  -0.03 -0.    1.02  0.02 -0.    0.98  0.02  0.01  0.98  0.04  0.04  1.01\n",
      "   0.04 -0.01  0.94 -0.01 -0.    0.96 -0.    0.01  1.01 -0.06 -0.01  0.93]\n",
      " [-0.06 -0.05  0.97 -0.05 -0.03  0.99 -0.05 -0.01  0.99 -0.06 -0.    0.99\n",
      "  -0.05 -0.04  0.98 -0.06 -0.    0.98 -0.04 -0.03  0.98 -0.04 -0.02  0.98\n",
      "  -0.05 -0.05  0.98 -0.03 -0.04  0.98 -0.05 -0.06  0.97 -0.04 -0.05  0.98\n",
      "  -0.04 -0.06  0.98 -0.05 -0.05  0.98 -0.04 -0.05  0.99 -0.05 -0.06  0.98\n",
      "  -0.06 -0.03  0.99 -0.05 -0.06  0.98 -0.07 -0.01  0.99 -0.06 -0.04  0.98]\n",
      " [-0.04  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.01  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98]\n",
      " [ 0.05  0.11  0.96  0.01  0.11  0.98 -0.02  0.05  0.96 -0.04  0.05  0.97\n",
      "   0.01  0.12  0.95 -0.02  0.11  0.97 -0.02  0.09  1.   -0.03  0.07  0.96\n",
      "   0.02  0.12  0.96  0.04  0.16  0.97  0.03  0.11  0.98 -0.01  0.03  1.\n",
      "  -0.    0.09  0.93  0.04  0.15  0.96 -0.01  0.09  0.97  0.04  0.08  1.01\n",
      "  -0.    0.1   0.99  0.05  0.1   1.    0.01  0.09  0.98 -0.03  0.03  0.99]\n",
      " [ 0.01 -0.    1.    0.06  0.01  1.03  0.08 -0.01  0.98  0.03  0.01  0.97\n",
      "   0.02  0.    0.99  0.02 -0.    0.98  0.05 -0.01  0.98  0.02  0.    1.01\n",
      "  -0.02 -0.02  0.94  0.   -0.01  0.96  0.07 -0.01  0.97  0.02 -0.01  0.98\n",
      "   0.02 -0.01  0.98  0.02 -0.02  0.98  0.09  0.01  1.01 -0.03 -0.02  0.96\n",
      "   0.05  0.01  1.03 -0.03 -0.03  0.97  0.   -0.03  1.    0.02 -0.01  0.98]\n",
      " [-0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98]\n",
      " [ 0.02  0.01  0.98  0.02  0.    0.98  0.02  0.04  1.04 -0.03  0.    1.01\n",
      "   0.06  0.02  1.    0.07  0.01  0.96 -0.02  0.02  1.06  0.01 -0.03  0.95\n",
      "   0.02  0.    0.99  0.02  0.01  0.98 -0.02 -0.02  0.95 -0.02  0.01  1.01\n",
      "  -0.01 -0.    0.97  0.09  0.02  0.98 -0.02 -0.04  0.92  0.02  0.    0.98\n",
      "   0.02  0.01  0.98  0.04  0.03  1.   -0.04 -0.02  0.96 -0.04 -0.    1.01]\n",
      " [-0.04  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98]\n",
      " [ 0.04  0.11  1.01  0.01  0.08  0.95 -0.03  0.03  1.    0.01  0.1   1.02\n",
      "  -0.06  0.05  1.03  0.03  0.11  0.97  0.03  0.16  0.97 -0.01  0.11  1.01\n",
      "   0.01  0.17  0.99 -0.04  0.07  0.98 -0.    0.03  1.   -0.01  0.09  0.98\n",
      "   0.02  0.15  0.96 -0.04  0.06  0.99 -0.05  0.08  0.95 -0.01  0.12  0.99\n",
      "  -0.01  0.06  1.02 -0.03  0.06  0.96  0.04  0.11  0.97  0.04  0.16  0.98]\n",
      " [-0.06  0.01  0.98 -0.05  0.01  0.98 -0.04 -0.04  0.98 -0.05  0.01  0.99\n",
      "  -0.03 -0.05  0.98 -0.04 -0.04  0.98 -0.04 -0.04  0.99 -0.05 -0.02  0.99\n",
      "  -0.05 -0.02  1.   -0.05  0.01  1.   -0.06 -0.01  1.   -0.05 -0.01  0.98\n",
      "  -0.07  0.01  0.99 -0.05  0.01  1.   -0.07  0.01  0.98 -0.06 -0.03  0.98\n",
      "  -0.06 -0.01  0.98 -0.05  0.01  1.   -0.05 -0.03  0.99 -0.05  0.01  0.99]\n",
      " [-0.06 -0.04  0.98 -0.06 -0.02  0.99 -0.06 -0.    0.99 -0.06 -0.02  0.99\n",
      "  -0.07  0.    0.98 -0.06  0.01  0.99 -0.07  0.01  0.98 -0.06  0.    0.99\n",
      "  -0.07  0.01  0.98 -0.06 -0.01  0.98 -0.06 -0.    0.98 -0.06 -0.04  0.97\n",
      "  -0.05 -0.01  0.97 -0.05  0.    0.99 -0.04 -0.04  0.99 -0.05 -0.01  0.99\n",
      "  -0.07  0.01  0.99 -0.05  0.01  1.   -0.06  0.01  0.98 -0.05 -0.    0.99]\n",
      " [-0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98]\n",
      " [-0.02 -0.03  0.94  0.02 -0.    0.98  0.02  0.01  1.    0.04 -0.02  0.93\n",
      "   0.05  0.    0.94 -0.03 -0.01  0.98 -0.01  0.01  1.02  0.08  0.    0.91\n",
      "   0.02  0.03  1.    0.02  0.01  0.99  0.02 -0.    0.98 -0.04  0.    1.03\n",
      "  -0.02  0.01  1.02  0.01 -0.    0.96  0.09  0.03  0.99 -0.02 -0.05  0.89\n",
      "   0.02 -0.    0.98  0.02  0.01  0.98  0.04  0.02  0.99 -0.05 -0.01  0.96]\n",
      " [-0.04 -0.    0.99  0.02 -0.01  0.94  0.06  0.07  1.08  0.02  0.    0.99\n",
      "   0.02  0.01  0.97 -0.03 -0.04  0.94  0.06  0.03  1.01 -0.03 -0.    0.98\n",
      "  -0.03  0.    1.01 -0.03 -0.02  0.91  0.04  0.03  1.01  0.02  0.01  0.98\n",
      "   0.02  0.01  0.99  0.04  0.03  1.03  0.    0.01  1.02 -0.02  0.    1.01\n",
      "  -0.   -0.01  0.95  0.06  0.06  1.06  0.03  0.02  0.99  0.03  0.01  0.98]\n",
      " [-0.05  0.    0.99 -0.06 -0.01  0.97 -0.05 -0.05  0.97 -0.03 -0.05  0.97\n",
      "  -0.05 -0.01  0.99 -0.03 -0.05  0.97 -0.05 -0.01  0.99 -0.03 -0.06  0.97\n",
      "  -0.05 -0.05  0.97 -0.04 -0.05  0.98 -0.06  0.01  0.99 -0.06 -0.03  0.99\n",
      "  -0.07 -0.    0.99 -0.08  0.01  0.99 -0.07  0.    0.99 -0.07  0.01  0.99\n",
      "  -0.08  0.01  0.98 -0.06  0.02  0.98 -0.05 -0.    0.98 -0.06  0.02  0.99]\n",
      " [-0.02  0.1   0.95 -0.02  0.09  0.93 -0.04  0.08  1.   -0.01  0.1   0.95\n",
      "  -0.01  0.07  0.93 -0.    0.13  0.93  0.02  0.11  0.96  0.03  0.09  1.01\n",
      "  -0.01  0.08  0.92  0.06  0.07  0.97  0.04  0.12  0.96 -0.06  0.02  0.98\n",
      "  -0.    0.07  0.95 -0.04  0.    0.95 -0.    0.08  0.99 -0.    0.14  0.96\n",
      "  -0.    0.07  0.96  0.02  0.09  0.99  0.02  0.11  0.96 -0.04  0.05  0.98]\n",
      " [ 0.02  0.    0.97  0.02 -0.    0.99  0.02 -0.01  0.98 -0.03 -0.    0.97\n",
      "   0.05  0.01  1.02 -0.01 -0.02  0.94  0.02  0.    0.99  0.07  0.01  0.97\n",
      "   0.03 -0.01  0.99  0.02 -0.    0.99 -0.   -0.03  1.    0.02  0.    1.\n",
      "   0.07  0.01  0.99  0.02 -0.01  0.94 -0.03 -0.03  0.98  0.03  0.04  0.97\n",
      "   0.02 -0.    0.98  0.02 -0.01  0.98  0.06 -0.04  0.98  0.03  0.    1.01]\n",
      " [-0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.98]\n",
      " [-0.06 -0.02  0.98 -0.06 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.97 -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.04 -0.01  0.98 -0.06  0.03  0.99 -0.01  0.02  1.   -0.05 -0.    0.98\n",
      "  -0.04 -0.04  0.97 -0.06 -0.02  0.98 -0.05  0.02  0.99 -0.08 -0.02  0.98\n",
      "  -0.04  0.03  0.99 -0.06  0.    0.98 -0.08  0.02  0.99 -0.07  0.01  0.98\n",
      "  -0.04 -0.01  0.98 -0.06  0.03  0.99 -0.06  0.    0.99 -0.06 -0.01  0.98\n",
      "  -0.08 -0.    0.98 -0.04  0.    0.98 -0.06 -0.02  0.98 -0.02  0.03  0.99]\n",
      " [-0.02  0.11  0.95  0.01  0.09  0.98 -0.03  0.02  0.99 -0.04  0.05  0.98\n",
      "   0.04  0.1   1.    0.01  0.06  0.98 -0.03  0.06  0.97  0.03  0.13  0.98\n",
      "   0.01  0.1   1.01 -0.03  0.02  0.99 -0.01  0.08  0.99 -0.03  0.11  0.95\n",
      "  -0.02  0.08  1.   -0.05  0.07  0.91 -0.04  0.06  0.98  0.04  0.09  1.01\n",
      "  -0.01  0.09  0.98 -0.02 -0.    0.95 -0.05  0.06  0.99 -0.03  0.13  0.95]\n",
      " [ 0.04  0.1   0.99  0.02  0.08  0.99  0.    0.09  0.96  0.03  0.13  0.99\n",
      "   0.01  0.08  1.01  0.    0.05  1.04  0.    0.08  0.99 -0.01  0.06  1.\n",
      "   0.    0.11  0.98  0.06  0.13  1.03 -0.02  0.07  0.99 -0.03  0.1   0.95\n",
      "  -0.01  0.09  1.02 -0.07  0.05  0.93 -0.04  0.06  0.99  0.05  0.16  0.98\n",
      "  -0.01  0.1   1.    0.01  0.17  1.    0.04  0.11  0.96  0.04  0.06  1.01]\n",
      " [-0.01 -0.02  0.96  0.02  0.    0.98  0.02  0.01  0.98  0.04 -0.02  0.92\n",
      "   0.07  0.02  1.01  0.08  0.02  0.97 -0.03 -0.01  0.97  0.08 -0.    0.9\n",
      "   0.04  0.02  1.01  0.02  0.01  0.98  0.02  0.01  0.99  0.09  0.01  0.94\n",
      "   0.08  0.01  0.96  0.02  0.01  1.01  0.1   0.03  0.99  0.07  0.02  0.95\n",
      "   0.03  0.02  0.98  0.02  0.01  0.99  0.02  0.02  1.    0.09  0.03  1.  ]\n",
      " [-0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.07 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.02  0.98]\n",
      " [-0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.99\n",
      "  -0.03  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98]\n",
      " [ 0.03 -0.02  0.98 -0.04 -0.02  0.95  0.07  0.01  0.99 -0.02 -0.02  0.96\n",
      "  -0.02  0.01  1.    0.02  0.    0.98  0.02 -0.    0.98  0.01 -0.02  1.\n",
      "  -0.05 -0.02  0.96  0.05  0.    0.97 -0.02 -0.02  0.96  0.09  0.01  0.99\n",
      "   0.03 -0.03  0.98  0.02 -0.01  0.99  0.02 -0.01  0.98 -0.04 -0.01  1.\n",
      "  -0.04 -0.02  0.95 -0.01 -0.02  0.94  0.08  0.02  1.02  0.06 -0.02  0.97]\n",
      " [-0.05 -0.03  0.99 -0.05 -0.05  0.98 -0.07  0.    0.99 -0.06 -0.01  0.98\n",
      "  -0.07  0.    0.99 -0.05 -0.05  0.98 -0.07  0.01  0.99 -0.05 -0.05  0.97\n",
      "  -0.06  0.    0.98 -0.06 -0.05  0.97 -0.07  0.01  0.99 -0.05  0.01  1.\n",
      "  -0.04 -0.05  0.98 -0.06 -0.03  0.98 -0.04 -0.04  0.99 -0.06 -0.05  0.97\n",
      "  -0.04 -0.05  0.97 -0.05 -0.01  0.99 -0.04 -0.04  0.97 -0.05 -0.    0.99]\n",
      " [-0.05 -0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05  0.    0.98 -0.05  0.    0.99 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98]\n",
      " [-0.09  0.03  0.98 -0.04 -0.01  0.98 -0.07  0.    0.98 -0.08  0.03  0.99\n",
      "  -0.06  0.    0.99 -0.06  0.    0.99 -0.03  0.01  0.99 -0.04  0.01  0.99\n",
      "  -0.08  0.    0.98 -0.08  0.03  0.99 -0.05  0.    0.99 -0.02 -0.    0.98\n",
      "  -0.07  0.02  0.99 -0.06 -0.01  0.98 -0.05 -0.02  0.98 -0.06 -0.03  0.97\n",
      "  -0.05 -0.    0.99 -0.08  0.05  1.   -0.04 -0.01  0.98 -0.05  0.02  0.99]\n",
      " [-0.03  0.01  0.98 -0.04  0.    0.99 -0.05 -0.01  0.98 -0.08 -0.    0.98\n",
      "  -0.08 -0.03  0.97 -0.06  0.    0.99 -0.06  0.04  0.99 -0.05 -0.02  0.98\n",
      "  -0.05  0.01  0.99 -0.01  0.01  0.99 -0.05 -0.02  0.97 -0.05  0.    0.98\n",
      "  -0.05 -0.03  0.98 -0.07 -0.    0.98 -0.04  0.02  0.99 -0.01 -0.01  0.99\n",
      "  -0.06 -0.    0.99 -0.08 -0.01  0.98 -0.04  0.01  0.99 -0.05  0.01  0.99]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.99\n",
      "  -0.06 -0.02  0.99 -0.07 -0.02  0.98 -0.06 -0.02  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97]\n",
      " [-0.05 -0.    0.99 -0.02 -0.03  0.98 -0.04 -0.01  0.98 -0.04  0.01  0.99\n",
      "  -0.06 -0.03  0.98 -0.05 -0.    0.98 -0.06 -0.    0.99 -0.03  0.01  0.99\n",
      "  -0.07  0.    0.99 -0.05 -0.02  0.98 -0.08 -0.02  0.97 -0.05 -0.    0.99\n",
      "  -0.03  0.01  0.98 -0.04  0.    0.99 -0.05 -0.01  0.98 -0.08 -0.    0.98\n",
      "  -0.08 -0.03  0.97 -0.06  0.    0.99 -0.06  0.04  0.99 -0.05 -0.02  0.98]\n",
      " [ 0.03  0.15  0.97 -0.05  0.06  1.    0.05  0.09  1.   -0.01  0.06  0.97\n",
      "  -0.02  0.08  1.    0.02  0.12  0.96 -0.05  0.07  0.96 -0.05  0.07  0.94\n",
      "   0.    0.11  1.   -0.03  0.15  0.96 -0.01  0.09  0.93  0.06  0.15  1.01\n",
      "  -0.01  0.09  0.97 -0.05  0.02  0.97  0.01  0.11  1.   -0.06  0.05  1.\n",
      "  -0.03  0.07  1.   -0.    0.15  0.95 -0.01  0.09  0.99  0.04  0.16  1.02]\n",
      " [-0.03  0.    0.98 -0.03  0.03  0.99 -0.05 -0.    0.99 -0.03 -0.04  0.97\n",
      "  -0.05  0.02  0.99 -0.05 -0.01  0.98 -0.06  0.03  0.99 -0.08  0.    0.98\n",
      "  -0.05 -0.    0.99 -0.02 -0.03  0.98 -0.04 -0.01  0.98 -0.04  0.01  0.99\n",
      "  -0.06 -0.03  0.98 -0.05 -0.    0.98 -0.06 -0.    0.99 -0.03  0.01  0.99\n",
      "  -0.07  0.    0.99 -0.05 -0.02  0.98 -0.08 -0.02  0.97 -0.05 -0.    0.99]\n",
      " [-0.05 -0.    0.98 -0.05 -0.    0.98 -0.04 -0.02  0.98 -0.03 -0.03  0.98\n",
      "  -0.04 -0.02  0.98 -0.04 -0.05  0.99 -0.05 -0.06  0.98 -0.08  0.01  0.99\n",
      "  -0.06  0.01  0.98 -0.06 -0.01  0.97 -0.06  0.02  0.99 -0.03 -0.05  0.98\n",
      "  -0.05  0.02  0.99 -0.05 -0.02  1.   -0.05  0.01  1.   -0.07  0.01  0.99\n",
      "  -0.06 -0.03  0.98 -0.03 -0.05  0.98 -0.06 -0.01  0.98 -0.05  0.01  1.  ]\n",
      " [-0.04 -0.04  0.99 -0.06 -0.05  0.97 -0.04 -0.05  0.97 -0.05 -0.01  0.99\n",
      "  -0.04 -0.04  0.97 -0.05 -0.    0.99 -0.03 -0.05  0.97 -0.05 -0.04  0.98\n",
      "  -0.03 -0.05  0.98 -0.05 -0.05  0.97 -0.04 -0.05  0.98 -0.06 -0.05  0.98\n",
      "  -0.04 -0.04  0.98 -0.05 -0.05  0.98 -0.05 -0.05  0.98 -0.06 -0.04  0.98\n",
      "  -0.06 -0.06  0.98 -0.06 -0.02  0.99 -0.06 -0.03  0.98 -0.07  0.01  0.98]\n",
      " [-0.04  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.    0.99\n",
      "  -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97]\n",
      " [-0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.03  0.99\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98]\n",
      " [-0.07 -0.    0.99 -0.05  0.01  1.   -0.06 -0.    0.98 -0.05  0.    0.99\n",
      "  -0.05 -0.03  0.97 -0.05  0.    0.99 -0.04 -0.05  0.98 -0.05 -0.02  0.98\n",
      "  -0.06 -0.02  0.99 -0.05  0.01  0.99 -0.07  0.    0.99 -0.06  0.01  0.99\n",
      "  -0.06  0.01  0.98 -0.06  0.01  0.99 -0.05  0.    0.98 -0.04 -0.02  0.98\n",
      "  -0.05  0.    0.98 -0.04 -0.04  0.98 -0.05 -0.    0.99 -0.04 -0.05  0.99]\n",
      " [-0.04 -0.05  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.06  0.01  0.99\n",
      "  -0.06  0.    0.99 -0.05  0.    0.98 -0.04 -0.02  0.98 -0.04 -0.03  0.98\n",
      "  -0.04 -0.04  0.98 -0.06 -0.03  0.99 -0.05 -0.05  0.98 -0.07  0.01  0.98\n",
      "  -0.06 -0.01  0.98 -0.04 -0.04  0.97 -0.05 -0.    0.99 -0.04 -0.05  0.98\n",
      "  -0.05  0.01  0.99 -0.05 -0.03  0.99 -0.05  0.01  1.   -0.07  0.01  0.99]\n",
      " [-0.05 -0.04  0.98 -0.05 -0.04  0.99 -0.05 -0.05  0.98 -0.07 -0.01  0.99\n",
      "  -0.05 -0.04  0.97 -0.07  0.    0.98 -0.05 -0.04  0.97 -0.05 -0.03  0.97\n",
      "  -0.06 -0.03  0.98 -0.04 -0.04  0.98 -0.06 -0.04  0.97 -0.05 -0.03  0.99\n",
      "  -0.06 -0.04  0.97 -0.07  0.01  0.99 -0.05 -0.01  0.98 -0.06  0.    0.98\n",
      "  -0.05 -0.05  0.97 -0.04 -0.02  0.98 -0.05 -0.04  0.98 -0.04 -0.04  0.97]\n",
      " [-0.06  0.03  0.97 -0.09  0.    0.96 -0.05  0.    0.97 -0.09  0.02  0.98\n",
      "  -0.06  0.01  0.97 -0.06 -0.01  0.97 -0.04  0.    0.98 -0.06  0.03  0.98\n",
      "  -0.06  0.    0.98 -0.01 -0.03  0.97 -0.05  0.02  0.98 -0.07  0.    0.97\n",
      "  -0.02 -0.01  0.97 -0.05 -0.01  0.97 -0.05 -0.    0.97 -0.05 -0.02  0.97\n",
      "  -0.06  0.02  0.98 -0.06 -0.01  0.97 -0.05  0.03  0.98 -0.05  0.01  0.97]\n",
      " [-0.04  0.07  0.97 -0.01  0.14  0.95 -0.    0.1   1.03 -0.03  0.12  0.94\n",
      "   0.02  0.1   0.98 -0.04  0.01  0.99 -0.01  0.07  0.93 -0.01  0.11  0.91\n",
      "   0.01  0.09  0.98  0.04  0.12  0.98  0.    0.07  0.97 -0.02  0.04  0.97\n",
      "   0.    0.09  1.    0.05  0.1   1.    0.    0.1   0.97 -0.02  0.04  0.97\n",
      "  -0.02  0.07  1.   -0.03  0.05  0.99 -0.04  0.08  0.95 -0.04  0.08  0.99]\n",
      " [ 0.02  0.    0.98  0.02  0.01  0.98 -0.02 -0.05  0.92  0.    0.01  1.02\n",
      "   0.01 -0.    0.96 -0.04 -0.    1.   -0.05 -0.01  1.01  0.02 -0.01  0.97\n",
      "   0.02  0.    0.99  0.02  0.    0.98 -0.05 -0.    1.03 -0.04 -0.    0.99\n",
      "  -0.03 -0.    1.    0.07  0.01  0.95 -0.03 -0.01  1.01  0.02 -0.01  0.98\n",
      "   0.02  0.01  0.98  0.03  0.02  0.99 -0.03 -0.01  0.96 -0.   -0.01  0.96]\n",
      " [ 0.04 -0.02  0.91  0.02  0.02  1.02  0.06  0.02  1.    0.08  0.01  0.96\n",
      "   0.08  0.01  0.91  0.04  0.03  1.01  0.02  0.01  0.98  0.03  0.01  0.99\n",
      "   0.1   0.01  0.92  0.07  0.01  0.95  0.03  0.01  1.01 -0.05 -0.    1.\n",
      "   0.04 -0.04  0.87  0.02  0.    0.98  0.02  0.01  0.98  0.04  0.05  1.02\n",
      "   0.09  0.03  0.99  0.06  0.02  1.    0.07  0.01  0.95  0.07 -0.    0.93]\n",
      " [-0.07  0.01  0.98 -0.04 -0.02  0.98 -0.04 -0.03  0.98 -0.05  0.    0.98\n",
      "  -0.09  0.01  0.99 -0.04  0.    0.99 -0.07 -0.01  0.98 -0.03 -0.02  0.98\n",
      "  -0.02 -0.01  0.98 -0.05 -0.    0.98 -0.11  0.    0.98 -0.05 -0.02  0.98\n",
      "  -0.04  0.01  0.99 -0.09  0.03  0.99 -0.06  0.01  0.99 -0.05  0.    0.98\n",
      "  -0.03 -0.01  0.98 -0.07  0.01  0.98 -0.07 -0.02  0.98 -0.02  0.02  0.99]\n",
      " [-0.06 -0.01  0.98 -0.06 -0.    0.98 -0.06 -0.04  0.97 -0.05 -0.01  0.97\n",
      "  -0.05  0.    0.99 -0.04 -0.04  0.99 -0.05 -0.01  0.99 -0.07  0.01  0.99\n",
      "  -0.05  0.01  1.   -0.06  0.01  0.98 -0.05 -0.    0.99 -0.03 -0.05  0.97\n",
      "  -0.06 -0.05  0.97 -0.05 -0.02  0.99 -0.05 -0.03  0.98 -0.07  0.02  0.99\n",
      "  -0.05 -0.03  0.98 -0.07  0.01  0.99 -0.04 -0.02  0.99 -0.04 -0.02  0.97]\n",
      " [ 0.    0.12  0.95 -0.01  0.07  0.96  0.04  0.13  0.93  0.04  0.11  0.96\n",
      "  -0.01  0.02  1.01 -0.    0.1   0.98  0.04  0.07  1.   -0.02  0.07  0.99\n",
      "  -0.05  0.04  0.98 -0.01  0.1   0.99 -0.04  0.02  0.97 -0.04  0.06  1.\n",
      "  -0.06  0.1   0.95 -0.03  0.11  0.98 -0.03  0.05  0.99 -0.03  0.06  1.\n",
      "  -0.01  0.11  0.95 -0.01  0.13  0.98 -0.04  0.08  1.    0.01  0.11  0.95]\n",
      " [ 0.03  0.06  1.1   0.02 -0.    0.99  0.02  0.01  0.97  0.04  0.02  1.\n",
      "   0.    0.01  1.02  0.02 -0.    0.95 -0.02  0.    1.01 -0.01 -0.02  0.96\n",
      "   0.01  0.01  0.99  0.02  0.    0.98  0.02  0.    0.99  0.1   0.02  0.96\n",
      "  -0.04 -0.    1.    0.03  0.01  1.01  0.07  0.01  0.95  0.05  0.05  1.06\n",
      "   0.03  0.    0.99  0.02  0.01  0.98  0.02 -0.    0.97 -0.02  0.02  1.03]\n",
      " [-0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98 -0.04  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.99]\n",
      " [-0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.03  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.01  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98]\n",
      " [-0.06  0.    0.99 -0.06  0.04  0.99 -0.05 -0.02  0.98 -0.05  0.01  0.99\n",
      "  -0.01  0.01  0.99 -0.05 -0.02  0.97 -0.05  0.    0.98 -0.05 -0.03  0.98\n",
      "  -0.07 -0.    0.98 -0.04  0.02  0.99 -0.01 -0.01  0.99 -0.06 -0.    0.99\n",
      "  -0.08 -0.01  0.98 -0.04  0.01  0.99 -0.05  0.01  0.99 -0.07  0.02  0.99\n",
      "  -0.01 -0.01  0.99 -0.05  0.    0.98 -0.09 -0.04  0.96 -0.04 -0.01  0.98]\n",
      " [-0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.98\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98]\n",
      " [ 0.04  0.13  0.98  0.01  0.09  0.98  0.02  0.11  0.96 -0.03  0.06  1.\n",
      "   0.05  0.12  0.98  0.02  0.15  0.99 -0.04  0.08  1.   -0.    0.1   0.95\n",
      "   0.02  0.11  0.96  0.05  0.14  1.02 -0.04  0.06  1.   -0.02  0.04  1.\n",
      "   0.01  0.11  0.99  0.01  0.06  1.02  0.03  0.12  0.96 -0.05  0.11  0.95\n",
      "  -0.01  0.08  0.96 -0.08  0.07  0.98  0.01  0.09  0.98  0.04  0.15  0.98]\n",
      " [-0.05  0.02  1.   -0.06  0.    1.   -0.05  0.01  1.   -0.07  0.    0.98\n",
      "  -0.05  0.01  1.   -0.03 -0.05  0.98 -0.06 -0.05  0.97 -0.05 -0.03  0.99\n",
      "  -0.05 -0.01  0.99 -0.06 -0.    0.99 -0.05 -0.04  0.98 -0.06 -0.    0.98\n",
      "  -0.04 -0.03  0.98 -0.04 -0.02  0.98 -0.05 -0.05  0.98 -0.03 -0.04  0.98\n",
      "  -0.05 -0.06  0.97 -0.04 -0.05  0.98 -0.04 -0.06  0.98 -0.05 -0.05  0.98]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.98\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.98]\n",
      " [-0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.03  0.99\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98]\n",
      " [-0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.01  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98]\n",
      " [-0.06 -0.02  0.98 -0.06  0.03  0.99 -0.06  0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.04  0.01  0.98 -0.04 -0.01  0.99 -0.06  0.04  0.99\n",
      "  -0.06  0.    0.99 -0.    0.    0.98 -0.05 -0.02  0.98 -0.05 -0.01  0.98\n",
      "  -0.08  0.02  0.99 -0.1   0.01  0.98 -0.05  0.    0.98 -0.03  0.03  1.\n",
      "  -0.05 -0.02  0.98 -0.07  0.01  0.99 -0.03  0.02  0.99 -0.05  0.01  0.99]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.98\n",
      "  -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99]\n",
      " [-0.03  0.06  1.   -0.03  0.04  1.    0.01  0.1   1.02 -0.02  0.06  1.04\n",
      "   0.02  0.11  0.95 -0.01  0.15  0.95 -0.01  0.11  1.01 -0.08  0.1   0.99\n",
      "   0.03  0.11  0.98  0.06  0.1   1.01 -0.    0.11  1.01  0.02  0.11  1.04\n",
      "  -0.05  0.06  0.99  0.05  0.09  1.    0.    0.07  0.98  0.    0.08  0.96\n",
      "  -0.02  0.07  1.01  0.04  0.1   1.    0.03  0.11  1.   -0.02  0.09  1.  ]\n",
      " [-0.02  0.08  0.96 -0.02  0.06  1.   -0.03  0.04  0.96 -0.01  0.08  0.99\n",
      "  -0.04  0.04  0.98 -0.04  0.07  0.98 -0.08  0.03  0.94  0.01  0.1   0.96\n",
      "   0.04  0.16  0.98 -0.01  0.09  1.01  0.03  0.13  0.93 -0.01  0.09  0.99\n",
      "  -0.03  0.01  0.99 -0.01  0.1   0.99 -0.01  0.02  0.99 -0.04  0.07  0.98\n",
      "  -0.05  0.12  0.95  0.    0.08  0.96  0.04  0.13  0.99  0.03  0.11  0.97]\n",
      " [ 0.02  0.    1.    0.07  0.01  0.99  0.02 -0.01  0.94 -0.03 -0.03  0.98\n",
      "   0.03  0.04  0.97  0.02 -0.    0.98  0.02 -0.01  0.98  0.06 -0.04  0.98\n",
      "   0.03  0.    1.01 -0.02 -0.01  0.97  0.08  0.02  1.02  0.08  0.03  0.98\n",
      "   0.02 -0.02  0.99  0.02 -0.01  0.98  0.02 -0.01  0.98  0.06  0.01  1.\n",
      "   0.07  0.01  0.99 -0.02 -0.02  0.95  0.1   0.01  1.    0.01  0.02  0.99]\n",
      " [-0.05 -0.05  0.98 -0.06 -0.03  0.98 -0.05 -0.06  0.98 -0.07 -0.02  0.99\n",
      "  -0.06 -0.03  0.99 -0.07  0.    0.99 -0.06 -0.03  0.98 -0.07  0.01  0.98\n",
      "  -0.06 -0.05  0.97 -0.05 -0.02  0.97 -0.06 -0.05  0.97 -0.04 -0.05  0.98\n",
      "  -0.06 -0.01  0.99 -0.04 -0.04  0.98 -0.06 -0.05  0.97 -0.04 -0.04  0.98\n",
      "  -0.05 -0.01  0.99 -0.06 -0.    1.   -0.05  0.    0.99 -0.07  0.02  0.99]\n",
      " [ 0.05 -0.    0.96 -0.04 -0.01  0.98  0.02 -0.03  0.98  0.02  0.    0.99\n",
      "   0.02 -0.    0.99 -0.03 -0.02  0.97  0.04  0.01  1.02 -0.02 -0.01  0.97\n",
      "   0.09  0.01  1.01  0.06 -0.03  0.97  0.03  0.01  0.98  0.02  0.    0.99\n",
      "   0.02 -0.02  1.    0.04 -0.01  0.98 -0.04 -0.02  0.95  0.06  0.01  1.03\n",
      "  -0.04 -0.02  0.96  0.04 -0.01  0.97  0.02 -0.01  0.99  0.02 -0.    0.98]\n",
      " [-0.07  0.03  0.96 -0.01  0.08  0.94 -0.    0.01  0.96 -0.04  0.06  0.99\n",
      "  -0.07  0.04  0.96 -0.    0.09  0.97 -0.02  0.01  0.97 -0.04  0.06  0.98\n",
      "  -0.02  0.05  0.99 -0.01  0.1   0.99 -0.    0.09  0.96 -0.01  0.08  1.\n",
      "   0.03  0.08  1.    0.03  0.1   0.99  0.02  0.12  0.98  0.    0.11  0.95\n",
      "   0.02  0.11  0.96  0.02  0.14  0.98  0.02  0.11  0.96  0.03  0.14  0.97]\n",
      " [-0.05 -0.01  0.98 -0.08 -0.    0.98 -0.08 -0.03  0.97 -0.06  0.    0.99\n",
      "  -0.06  0.04  0.99 -0.05 -0.02  0.98 -0.05  0.01  0.99 -0.01  0.01  0.99\n",
      "  -0.05 -0.02  0.97 -0.05  0.    0.98 -0.05 -0.03  0.98 -0.07 -0.    0.98\n",
      "  -0.04  0.02  0.99 -0.01 -0.01  0.99 -0.06 -0.    0.99 -0.08 -0.01  0.98\n",
      "  -0.04  0.01  0.99 -0.05  0.01  0.99 -0.07  0.02  0.99 -0.01 -0.01  0.99]\n",
      " [-0.06  0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.98 -0.04  0.01  0.98\n",
      "  -0.04 -0.01  0.99 -0.06  0.04  0.99 -0.06  0.    0.99 -0.    0.    0.98\n",
      "  -0.05 -0.02  0.98 -0.05 -0.01  0.98 -0.08  0.02  0.99 -0.1   0.01  0.98\n",
      "  -0.05  0.    0.98 -0.03  0.03  1.   -0.05 -0.02  0.98 -0.07  0.01  0.99\n",
      "  -0.03  0.02  0.99 -0.05  0.01  0.99 -0.05  0.    0.98 -0.05 -0.03  0.98]\n",
      " [-0.02  0.04  0.97 -0.    0.09  1.    0.04  0.09  1.    0.03  0.09  0.99\n",
      "  -0.01  0.1   1.    0.03  0.13  0.99  0.    0.08  1.01 -0.02  0.03  0.98\n",
      "  -0.04  0.06  0.98 -0.04  0.05  0.96 -0.    0.1   1.03 -0.07  0.02  0.98\n",
      "  -0.01  0.09  0.96  0.01  0.16  0.96 -0.    0.1   1.03 -0.06  0.09  0.94\n",
      "   0.04  0.11  0.96  0.04  0.06  1.01 -0.01  0.11  1.   -0.04  0.03  0.97]\n",
      " [ 0.01  0.1   0.99 -0.04  0.04  0.98 -0.02  0.1   0.95 -0.06  0.06  0.92\n",
      "  -0.04  0.08  0.99 -0.01  0.1   0.98 -0.02  0.07  0.94 -0.    0.13  0.93\n",
      "  -0.05  0.06  0.99  0.02  0.07  1.01 -0.    0.07  0.95  0.02  0.02  0.97\n",
      "  -0.    0.09  0.99 -0.05  0.12  0.94 -0.01  0.07  0.95 -0.05  0.08  0.94\n",
      "  -0.04  0.06  0.99  0.05  0.13  0.98  0.01  0.08  0.98 -0.01  0.04  1.  ]\n",
      " [ 0.07  0.03  1.01  0.07  0.03  0.97  0.02  0.    0.99  0.02  0.    0.99\n",
      "   0.02  0.03  1.01 -0.04 -0.01  0.97  0.08  0.02  0.98  0.   -0.    0.96\n",
      "   0.1   0.02  0.96  0.03 -0.02  0.94  0.02  0.    0.98  0.02  0.01  0.98\n",
      "  -0.03  0.03  1.08 -0.02  0.01  1.01 -0.02  0.    1.01  0.08  0.02  0.95\n",
      "  -0.01  0.03  1.08  0.03  0.01  1.    0.02  0.    0.99  0.03  0.02  0.98]\n",
      " [-0.05 -0.02  0.98 -0.07  0.01  0.98 -0.03  0.    0.98 -0.03  0.03  0.99\n",
      "  -0.05 -0.    0.99 -0.03 -0.04  0.97 -0.05  0.02  0.99 -0.05 -0.01  0.98\n",
      "  -0.06  0.03  0.99 -0.08  0.    0.98 -0.05 -0.    0.99 -0.02 -0.03  0.98\n",
      "  -0.04 -0.01  0.98 -0.04  0.01  0.99 -0.06 -0.03  0.98 -0.05 -0.    0.98\n",
      "  -0.06 -0.    0.99 -0.03  0.01  0.99 -0.07  0.    0.99 -0.05 -0.02  0.98]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.06 -0.02  0.99 -0.07 -0.02  0.99 -0.06 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99]\n",
      " [-0.07 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.02  0.98]\n",
      " [-0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.99 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98]\n",
      " [ 0.03  0.09  1.01 -0.02  0.07  0.96 -0.03  0.04  0.99 -0.01  0.08  1.02\n",
      "  -0.05  0.02  0.97  0.02  0.11  0.95 -0.02  0.11  0.95 -0.    0.1   1.02\n",
      "  -0.03  0.12  0.94  0.01  0.1   0.98 -0.07  0.03  0.96 -0.01  0.08  0.95\n",
      "   0.04  0.14  0.99 -0.05  0.06  0.99 -0.04  0.05  0.99 -0.01  0.11  0.99\n",
      "  -0.02  0.11  0.93 -0.01  0.09  0.96 -0.04  0.09  0.95 -0.04  0.09  0.97]\n",
      " [-0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98]\n",
      " [-0.06 -0.03  0.99 -0.07  0.    0.99 -0.06 -0.03  0.98 -0.07  0.01  0.98\n",
      "  -0.06 -0.05  0.97 -0.05 -0.02  0.97 -0.06 -0.05  0.97 -0.04 -0.05  0.98\n",
      "  -0.06 -0.01  0.99 -0.04 -0.04  0.98 -0.06 -0.05  0.97 -0.04 -0.04  0.98\n",
      "  -0.05 -0.01  0.99 -0.06 -0.    1.   -0.05  0.    0.99 -0.07  0.02  0.99\n",
      "  -0.05 -0.05  0.97 -0.04 -0.02  0.97 -0.05 -0.05  0.98 -0.03 -0.06  0.98]\n",
      " [-0.03  0.07  0.99 -0.07  0.05  0.96  0.    0.08  0.96  0.03  0.14  0.96\n",
      "   0.03  0.12  0.96  0.02  0.06  1.01 -0.02  0.09  0.98  0.01  0.13  0.99\n",
      "  -0.01  0.09  0.96 -0.06  0.05  0.99 -0.04  0.06  0.96 -0.    0.07  0.96\n",
      "  -0.04  0.04  0.97 -0.03  0.09  0.95 -0.01  0.13  0.96  0.03  0.11  0.97\n",
      "   0.03  0.15  0.98  0.    0.09  0.94  0.07  0.13  0.95 -0.04  0.06  0.99]\n",
      " [-0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.04  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98\n",
      "  -0.04  0.04  0.99 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98]\n",
      " [-0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98]\n",
      " [-0.01  0.09  0.96 -0.06  0.05  0.99 -0.04  0.06  0.96 -0.    0.07  0.96\n",
      "  -0.04  0.04  0.97 -0.03  0.09  0.95 -0.01  0.13  0.96  0.03  0.11  0.97\n",
      "   0.03  0.15  0.98  0.    0.09  0.94  0.07  0.13  0.95 -0.04  0.06  0.99\n",
      "  -0.05  0.01  0.98 -0.01  0.07  0.93  0.07  0.1   0.98 -0.05  0.06  0.99\n",
      "  -0.07  0.04  0.97 -0.    0.07  0.94 -0.04  0.06  0.92  0.04  0.11  0.96]\n",
      " [-0.02 -0.03  0.98 -0.05 -0.    0.98 -0.09  0.01  0.99 -0.04 -0.01  0.98\n",
      "  -0.07  0.    0.98 -0.08 -0.02  0.98 -0.02 -0.01  0.98 -0.05 -0.    0.98\n",
      "  -0.09  0.02  0.98 -0.07 -0.02  0.98 -0.05 -0.01  0.98 -0.03  0.03  0.99\n",
      "  -0.06  0.01  0.99 -0.05  0.03  0.98 -0.07 -0.02  0.98 -0.04  0.02  0.99\n",
      "  -0.04 -0.01  0.98 -0.01  0.02  0.99 -0.05  0.    0.98 -0.04 -0.03  0.97]\n",
      " [-0.05 -0.05  0.97 -0.03 -0.06  0.98 -0.05 -0.05  0.98 -0.04 -0.05  0.98\n",
      "  -0.06 -0.04  0.98 -0.05 -0.04  0.98 -0.06 -0.03  0.98 -0.06 -0.03  0.99\n",
      "  -0.05 -0.04  0.98 -0.05 -0.05  0.98 -0.05 -0.03  0.99 -0.06 -0.04  0.98\n",
      "  -0.07  0.01  0.98 -0.05 -0.05  0.98 -0.07  0.01  0.98 -0.06 -0.01  0.98\n",
      "  -0.06 -0.01  0.97 -0.06 -0.02  0.98 -0.04 -0.06  0.97 -0.06 -0.05  0.97]\n",
      " [ 0.05  0.13  1.01 -0.01  0.07  0.96  0.03  0.13  0.98  0.02  0.11  0.99\n",
      "   0.    0.06  1.03 -0.03  0.07  0.99  0.01  0.15  0.96 -0.02  0.07  0.96\n",
      "   0.05  0.07  0.98  0.02  0.1   0.98 -0.07  0.06  0.95 -0.01  0.07  0.95\n",
      "  -0.05  0.08  0.94 -0.04  0.06  0.99  0.04  0.13  0.98  0.01  0.09  0.98\n",
      "  -0.03  0.02  0.99 -0.04  0.05  0.98 -0.    0.06  1.   -0.01  0.07  0.97]\n",
      " [-0.05 -0.    0.99 -0.06 -0.01  0.99 -0.05 -0.01  0.99 -0.06 -0.    0.98\n",
      "  -0.05 -0.05  0.98 -0.04 -0.05  0.98 -0.06 -0.03  0.98 -0.05 -0.04  0.98\n",
      "  -0.06 -0.02  0.98 -0.07  0.    0.99 -0.07  0.    0.99 -0.06  0.01  0.99\n",
      "  -0.06  0.01  0.99 -0.06 -0.03  0.98 -0.07  0.01  0.98 -0.06 -0.03  0.98\n",
      "  -0.06 -0.    0.98 -0.06  0.01  0.99 -0.05 -0.02  0.97 -0.06 -0.03  0.98]\n",
      " [-0.07  0.01  0.99 -0.05  0.    0.99 -0.06 -0.01  0.97 -0.05 -0.05  0.97\n",
      "  -0.03 -0.05  0.97 -0.05 -0.01  0.99 -0.03 -0.05  0.97 -0.05 -0.01  0.99\n",
      "  -0.03 -0.06  0.97 -0.05 -0.05  0.97 -0.04 -0.05  0.98 -0.06  0.01  0.99\n",
      "  -0.06 -0.03  0.99 -0.07 -0.    0.99 -0.08  0.01  0.99 -0.07  0.    0.99\n",
      "  -0.07  0.01  0.99 -0.08  0.01  0.98 -0.06  0.02  0.98 -0.05 -0.    0.98]\n",
      " [-0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.04  0.03  0.99\n",
      "  -0.03  0.03  0.99 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.04  0.01  0.99\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98]\n",
      " [-0.02  0.02  0.99 -0.06  0.    0.99 -0.05  0.01  0.98 -0.06 -0.02  0.98\n",
      "  -0.04  0.01  0.99 -0.03  0.    0.99 -0.07  0.03  0.99 -0.06  0.    0.99\n",
      "  -0.03  0.03  1.   -0.04  0.01  0.99 -0.04 -0.    0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.06  0.03  0.98 -0.09  0.03  0.98 -0.04  0.    0.99\n",
      "  -0.07 -0.01  0.98 -0.07  0.03  0.99 -0.05  0.01  0.99 -0.06  0.01  0.98]\n",
      " [-0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.   -0.02  0.94 -0.   -0.01  1.   -0.02 -0.03  0.98  0.03  0.03  0.97\n",
      "   0.02  0.    0.98  0.02 -0.    0.99  0.04  0.01  0.99  0.01 -0.    1.\n",
      "  -0.02 -0.01  0.98  0.04 -0.    0.98 -0.04 -0.01  0.99  0.02 -0.02  0.99\n",
      "   0.02 -0.01  0.98  0.03 -0.    1.   -0.04 -0.02  0.96 -0.02 -0.02  0.94\n",
      "   0.06  0.01  1.02 -0.04 -0.01  0.97  0.04 -0.04  0.97  0.02 -0.02  0.99]\n",
      " [-0.07  0.05  0.93 -0.04  0.06  0.99  0.05  0.16  0.98 -0.01  0.1   1.\n",
      "   0.01  0.17  1.    0.04  0.11  0.96  0.04  0.06  1.01 -0.01  0.1   1.\n",
      "  -0.02  0.04  1.02  0.02  0.11  0.96  0.05  0.08  1.01  0.02  0.08  0.98\n",
      "  -0.02  0.05  0.96 -0.03  0.06  0.96  0.04  0.11  0.99  0.02  0.08  0.99\n",
      "  -0.    0.08  0.96  0.03  0.13  0.99  0.    0.08  1.01  0.02  0.08  1.04]\n",
      " [ 0.02 -0.06  0.98  0.02  0.01  0.98  0.02 -0.    0.98  0.08  0.03  0.98\n",
      "   0.01 -0.01  0.96  0.01 -0.02  0.94 -0.01 -0.02  0.94  0.09  0.02  0.99\n",
      "   0.02  0.01  0.99  0.02  0.    0.99  0.02 -0.    0.97  0.07  0.02  1.\n",
      "   0.01 -0.    1.    0.02 -0.01  0.94  0.08  0.01  1.    0.06  0.04  0.98\n",
      "   0.02  0.01  0.97  0.02 -0.    0.99  0.03  0.03  0.99  0.05  0.01  1.  ]\n",
      " [-0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.06 -0.02  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.03  0.06  0.96  0.02  0.12  0.95 -0.01  0.13  0.94 -0.01  0.09  0.97\n",
      "   0.05  0.14  0.99 -0.01  0.07  0.94 -0.01  0.12  0.92 -0.04  0.06  0.98\n",
      "  -0.02  0.03  1.    0.    0.1   0.95 -0.06  0.03  0.96 -0.05  0.06  0.99\n",
      "   0.03  0.15  0.97 -0.01  0.08  0.97 -0.02  0.14  0.96 -0.04  0.07  0.98\n",
      "   0.06  0.12  0.99 -0.    0.07  0.98 -0.03  0.03  0.99  0.03  0.12  0.96]\n",
      " [-0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.06  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.06  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [ 0.08  0.01  1.   -0.01 -0.02  0.94  0.08  0.02  0.99  0.04  0.01  0.97\n",
      "   0.02 -0.    0.99  0.02 -0.01  0.98 -0.01  0.03  0.99  0.05  0.01  1.01\n",
      "   0.04  0.01  1.03  0.07  0.01  0.94 -0.04 -0.01  0.99  0.02  0.01  0.98\n",
      "   0.02 -0.01  0.97  0.03  0.    0.99  0.07 -0.    0.98  0.06  0.01  1.03\n",
      "  -0.01 -0.02  0.94 -0.04 -0.02  0.95 -0.01  0.01  0.99  0.02  0.01  0.97]\n",
      " [-0.04 -0.01  0.98 -0.04  0.01  0.99 -0.06 -0.03  0.98 -0.05 -0.    0.98\n",
      "  -0.06 -0.    0.99 -0.03  0.01  0.99 -0.07  0.    0.99 -0.05 -0.02  0.98\n",
      "  -0.08 -0.02  0.97 -0.05 -0.    0.99 -0.03  0.01  0.98 -0.04  0.    0.99\n",
      "  -0.05 -0.01  0.98 -0.08 -0.    0.98 -0.08 -0.03  0.97 -0.06  0.    0.99\n",
      "  -0.06  0.04  0.99 -0.05 -0.02  0.98 -0.05  0.01  0.99 -0.01  0.01  0.99]\n",
      " [ 0.09  0.01  1.01 -0.02 -0.02  0.94  0.01  0.    1.02 -0.04 -0.01  0.98\n",
      "   0.   -0.02  0.98  0.02 -0.    0.99  0.02 -0.    0.98 -0.04 -0.02  0.97\n",
      "  -0.02 -0.02  0.94  0.02 -0.01  0.94  0.07  0.01  1.02  0.06 -0.03  0.97\n",
      "   0.02  0.01  0.97  0.02 -0.    0.98  0.02 -0.02  1.    0.01 -0.01  0.97\n",
      "   0.02 -0.01  0.95  0.02  0.    1.02 -0.05 -0.02  0.96  0.06 -0.    0.97]\n",
      " [-0.05 -0.04  0.98 -0.07 -0.01  0.98 -0.05 -0.02  0.98 -0.09 -0.02  0.98\n",
      "  -0.05 -0.    0.98 -0.06 -0.    0.99 -0.05 -0.02  0.98 -0.04  0.01  0.99\n",
      "  -0.07  0.01  0.98 -0.08  0.03  0.99 -0.05 -0.    0.99 -0.04 -0.03  0.97\n",
      "  -0.07 -0.02  0.98 -0.07  0.01  0.99 -0.06  0.03  0.99 -0.09  0.01  0.98\n",
      "  -0.06  0.    0.98 -0.02  0.01  1.   -0.07 -0.01  0.98 -0.05 -0.01  0.98]\n",
      " [-0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98]\n",
      " [-0.03  0.01  0.99 -0.07  0.    0.99 -0.05 -0.02  0.98 -0.08 -0.02  0.97\n",
      "  -0.05 -0.    0.99 -0.03  0.01  0.98 -0.04  0.    0.99 -0.05 -0.01  0.98\n",
      "  -0.08 -0.    0.98 -0.08 -0.03  0.97 -0.06  0.    0.99 -0.06  0.04  0.99\n",
      "  -0.05 -0.02  0.98 -0.05  0.01  0.99 -0.01  0.01  0.99 -0.05 -0.02  0.97\n",
      "  -0.05  0.    0.98 -0.05 -0.03  0.98 -0.07 -0.    0.98 -0.04  0.02  0.99]\n",
      " [ 0.01  0.02  0.99  0.02  0.    0.98  0.02 -0.    0.99  0.03 -0.05  0.98\n",
      "   0.02 -0.01  0.96 -0.02 -0.01  1.    0.04  0.01  1.02  0.06  0.02  0.99\n",
      "   0.02 -0.01  0.99  0.02 -0.01  0.98  0.02 -0.    0.98 -0.01 -0.02  0.98\n",
      "   0.06  0.01  0.99  0.07  0.01  1.02 -0.   -0.    0.98  0.03 -0.04  0.97\n",
      "   0.02 -0.01  0.98  0.02 -0.01  0.97  0.02 -0.    1.    0.07 -0.    0.98]\n",
      " [-0.05 -0.    0.98 -0.04  0.01  0.98 -0.07  0.    0.98 -0.06 -0.02  0.98\n",
      "  -0.03  0.03  0.99 -0.05  0.    0.98 -0.07 -0.    0.99 -0.04  0.    0.99\n",
      "  -0.03 -0.    0.98 -0.07 -0.02  0.98 -0.03  0.03  1.   -0.05 -0.    0.98\n",
      "  -0.03 -0.04  0.97 -0.06 -0.02  0.98 -0.05  0.02  0.99 -0.06 -0.03  0.98\n",
      "  -0.06  0.03  1.   -0.05 -0.    0.98 -0.08  0.02  0.99 -0.07  0.01  0.98]\n",
      " [-0.06 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97]\n",
      " [-0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.03  0.98 -0.    0.03  0.99 -0.05  0.02  0.98]\n",
      " [-0.03  0.    1.   -0.04 -0.    1.01 -0.03 -0.01  0.99  0.02  0.02  0.98\n",
      "   0.03  0.01  0.98  0.03 -0.    0.91  0.09  0.01  0.95  0.02  0.01  1.02\n",
      "   0.08  0.01  0.96 -0.   -0.02  0.95  0.01 -0.05  0.89  0.02  0.    0.98\n",
      "   0.02  0.01  0.98  0.04 -0.02  0.91  0.02  0.02  1.02  0.06  0.02  1.\n",
      "   0.08  0.01  0.96  0.08  0.01  0.91  0.04  0.03  1.01  0.02  0.01  0.98]\n",
      " [-0.07  0.03  0.99 -0.04  0.01  0.99 -0.06  0.02  0.99 -0.1  -0.    0.98\n",
      "  -0.05  0.    0.99 -0.04  0.02  0.99 -0.07 -0.    0.98 -0.06 -0.02  0.98\n",
      "  -0.03 -0.01  0.98 -0.08 -0.03  0.97 -0.05  0.    0.98 -0.09  0.04  1.\n",
      "  -0.07 -0.01  0.98 -0.04  0.01  0.99 -0.09  0.02  0.99 -0.05 -0.03  0.97\n",
      "  -0.05 -0.    0.98 -0.03  0.01  0.98 -0.05  0.02  0.99 -0.06 -0.02  0.98]\n",
      " [-0.03  0.09  1.    0.    0.1   0.95  0.02  0.11  0.98  0.04  0.1   1.02\n",
      "   0.03  0.09  0.95 -0.05  0.03  0.97 -0.    0.09  1.02 -0.07  0.05  1.02\n",
      "   0.03  0.11  0.97 -0.01  0.13  0.95 -0.    0.11  1.02 -0.06  0.12  0.97\n",
      "  -0.    0.09  0.99  0.05  0.07  1.01 -0.    0.11  1.01 -0.02  0.05  1.03\n",
      "   0.01  0.1   0.96 -0.05  0.03  0.98 -0.02  0.09  0.98 -0.    0.13  0.95]\n",
      " [ 0.05  0.04  0.98  0.02 -0.01  0.99  0.02 -0.    0.99  0.02  0.    0.97\n",
      "   0.08  0.    0.99  0.04 -0.    0.96  0.02  0.    1.02 -0.03 -0.02  0.97\n",
      "   0.05  0.05  0.98  0.02  0.    0.98  0.02 -0.    0.98 -0.01 -0.02  1.\n",
      "  -0.03 -0.01  0.97  0.07  0.01  1.03  0.05 -0.    0.96 -0.04 -0.01  0.98\n",
      "   0.02 -0.03  0.98  0.02  0.    0.99  0.02 -0.    0.99 -0.03 -0.02  0.97]\n",
      " [-0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98]\n",
      " [ 0.06  0.01  0.95 -0.02  0.03  1.07  0.01 -0.03  0.95  0.02  0.01  0.99\n",
      "   0.02  0.01  0.98  0.07  0.02  0.98  0.02 -0.    0.95  0.07  0.02  0.99\n",
      "   0.1   0.02  0.97  0.07  0.02  0.95  0.03  0.02  0.98  0.02  0.    0.99\n",
      "   0.02  0.02  0.99  0.1   0.03  0.99  0.08  0.02  0.98  0.04  0.    0.95\n",
      "   0.08  0.03  1.    0.05  0.01  0.99  0.03  0.02  0.98  0.02  0.01  0.98]\n",
      " [-0.07  0.01  0.99 -0.05  0.    0.98 -0.06  0.    0.98 -0.04 -0.04  0.98\n",
      "  -0.05 -0.04  0.98 -0.05 -0.05  0.98 -0.07  0.    0.99 -0.06 -0.01  0.99\n",
      "  -0.07  0.01  0.98 -0.06  0.02  0.98 -0.04 -0.04  0.97 -0.06  0.01  0.99\n",
      "  -0.04 -0.05  0.98 -0.06 -0.01  0.99 -0.05 -0.02  1.   -0.05 -0.04  0.98\n",
      "  -0.07  0.01  0.99 -0.05 -0.01  0.99 -0.06 -0.01  0.97 -0.05 -0.05  0.97]\n",
      " [-0.04  0.04  0.98 -0.04  0.07  0.98 -0.08  0.03  0.94  0.01  0.1   0.96\n",
      "   0.04  0.16  0.98 -0.01  0.09  1.01  0.03  0.13  0.93 -0.01  0.09  0.99\n",
      "  -0.03  0.01  0.99 -0.01  0.1   0.99 -0.01  0.02  0.99 -0.04  0.07  0.98\n",
      "  -0.05  0.12  0.95  0.    0.08  0.96  0.04  0.13  0.99  0.03  0.11  0.97\n",
      "   0.05  0.09  1.   -0.    0.06  0.97 -0.02  0.1   1.    0.    0.1   0.95]\n",
      " [-0.03 -0.01  0.97  0.02  0.01  1.02 -0.01 -0.03  0.94  0.02  0.    0.96\n",
      "   0.02  0.01  0.98  0.02  0.01  0.99 -0.05  0.    1.02  0.09  0.02  0.98\n",
      "  -0.01  0.01  1.01  0.07  0.03  1.01  0.07  0.03  0.97  0.02  0.    0.99\n",
      "   0.02  0.    0.99  0.02  0.03  1.01 -0.04 -0.01  0.97  0.08  0.02  0.98\n",
      "   0.   -0.    0.96  0.1   0.02  0.96  0.03 -0.02  0.94  0.02  0.    0.98]\n",
      " [-0.02  0.07  1.   -0.03  0.05  0.99 -0.04  0.08  0.95 -0.04  0.08  0.99\n",
      "   0.01  0.12  0.96  0.03  0.11  0.98  0.06  0.13  1.03 -0.02  0.07  1.\n",
      "  -0.06  0.06  0.95  0.    0.11  1.01 -0.07  0.08  0.98 -0.04  0.07  0.98\n",
      "   0.05  0.15  0.99 -0.01  0.11  1.01  0.04  0.16  1.01  0.01  0.11  0.96\n",
      "  -0.03  0.03  0.99 -0.    0.11  0.99  0.02  0.14  0.99 -0.    0.09  0.96]\n",
      " [-0.05 -0.    0.98 -0.04 -0.03  0.98 -0.04 -0.01  0.98 -0.04 -0.04  0.98\n",
      "  -0.04 -0.05  0.98 -0.04 -0.05  0.98 -0.07 -0.02  0.99 -0.07  0.    0.99\n",
      "  -0.06  0.    0.99 -0.05  0.    0.98 -0.04 -0.02  0.98 -0.05 -0.04  0.99\n",
      "  -0.06 -0.03  0.98 -0.07  0.    0.99 -0.06  0.    0.99 -0.05 -0.02  0.97\n",
      "  -0.06  0.01  0.99 -0.04 -0.04  0.98 -0.06 -0.05  0.97 -0.05 -0.02  0.97]\n",
      " [-0.07  0.01  0.98 -0.06 -0.05  0.97 -0.04 -0.04  0.97 -0.05 -0.05  0.97\n",
      "  -0.04 -0.05  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.06  0.01  0.99\n",
      "  -0.06  0.    0.99 -0.05  0.    0.98 -0.04 -0.02  0.98 -0.04 -0.03  0.98\n",
      "  -0.04 -0.04  0.98 -0.06 -0.03  0.99 -0.05 -0.05  0.98 -0.07  0.01  0.98\n",
      "  -0.06 -0.01  0.98 -0.04 -0.04  0.97 -0.05 -0.    0.99 -0.04 -0.05  0.98]\n",
      " [-0.05  0.09  0.94 -0.    0.11  1.01 -0.05  0.13  0.97  0.02  0.1   0.96\n",
      "   0.06  0.12  1.   -0.01  0.1   1.    0.01  0.07  1.04  0.01  0.1   0.99\n",
      "  -0.05  0.05  0.98 -0.02  0.07  0.97  0.02  0.15  0.98 -0.02  0.08  0.96\n",
      "  -0.05  0.07  0.96 -0.03  0.1   0.98 -0.03  0.05  0.99 -0.03  0.05  0.99\n",
      "  -0.02  0.1   0.95 -0.02  0.09  0.93 -0.04  0.08  1.   -0.01  0.1   0.95]\n",
      " [ 0.   -0.    0.96  0.1   0.02  0.96  0.03 -0.02  0.94  0.02  0.    0.98\n",
      "   0.02  0.01  0.98 -0.03  0.03  1.08 -0.02  0.01  1.01 -0.02  0.    1.01\n",
      "   0.08  0.02  0.95 -0.01  0.03  1.08  0.03  0.01  1.    0.02  0.    0.99\n",
      "   0.03  0.02  0.98  0.1   0.01  0.95 -0.01  0.01  1.02  0.05  0.01  0.95\n",
      "  -0.05 -0.01  0.98  0.04  0.06  1.09  0.02  0.    0.98  0.02  0.01  0.98]\n",
      " [ 0.04  0.02  1.01 -0.02 -0.02  0.96 -0.   -0.02  0.95  0.02  0.    0.98\n",
      "   0.03  0.01  0.98  0.1   0.02  0.96  0.02  0.02  1.02  0.08  0.02  0.97\n",
      "   0.01 -0.01  0.95  0.06  0.05  1.05  0.03  0.01  1.    0.02  0.01  0.98\n",
      "   0.03  0.02  0.98  0.06 -0.    0.95 -0.03  0.    1.01  0.02  0.    0.95\n",
      "  -0.05 -0.01  0.98  0.02  0.05  1.07  0.02  0.01  0.98  0.02  0.01  0.98]\n",
      " [ 0.08  0.01  0.95  0.01 -0.04  0.9   0.02  0.03  0.98  0.02  0.01  0.98\n",
      "   0.02 -0.01  0.98  0.07  0.    0.94  0.05  0.    0.95 -0.01  0.01  1.01\n",
      "  -0.06 -0.01  1.01  0.02 -0.04  0.88  0.02  0.    0.98  0.02  0.    0.99\n",
      "   0.05 -0.    0.95  0.09  0.02  0.96  0.   -0.    0.96 -0.04 -0.    1.\n",
      "   0.09  0.02  0.95  0.02  0.01  0.98  0.02  0.    0.99  0.02  0.    0.98]\n",
      " [-0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.01  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98]\n",
      " [-0.05  0.01  1.   -0.05 -0.03  0.97 -0.05  0.    0.99 -0.04 -0.04  0.99\n",
      "  -0.05  0.01  0.99 -0.07  0.01  1.   -0.06 -0.05  0.97 -0.06 -0.    0.98\n",
      "  -0.05 -0.05  0.97 -0.04 -0.04  0.97 -0.05 -0.04  0.98 -0.05 -0.01  0.98\n",
      "  -0.05  0.01  0.99 -0.05 -0.    0.98 -0.04 -0.02  0.98 -0.05  0.    0.98\n",
      "  -0.04 -0.02  0.98 -0.04 -0.02  0.98 -0.05 -0.01  0.98 -0.04 -0.02  0.98]\n",
      " [-0.05  0.02  0.99 -0.05 -0.01  0.98 -0.06  0.03  0.99 -0.08  0.    0.98\n",
      "  -0.05 -0.    0.99 -0.02 -0.03  0.98 -0.04 -0.01  0.98 -0.04  0.01  0.99\n",
      "  -0.06 -0.03  0.98 -0.05 -0.    0.98 -0.06 -0.    0.99 -0.03  0.01  0.99\n",
      "  -0.07  0.    0.99 -0.05 -0.02  0.98 -0.08 -0.02  0.97 -0.05 -0.    0.99\n",
      "  -0.03  0.01  0.98 -0.04  0.    0.99 -0.05 -0.01  0.98 -0.08 -0.    0.98]\n",
      " [ 0.03  0.1   0.99  0.02  0.12  0.98  0.    0.11  0.95  0.02  0.11  0.96\n",
      "   0.02  0.14  0.98  0.02  0.11  0.96  0.03  0.14  0.97 -0.01  0.07  0.93\n",
      "   0.05  0.14  0.95 -0.01  0.07  1.    0.01  0.04  1.01 -0.    0.07  0.94\n",
      "   0.07  0.12  0.94 -0.02  0.07  0.99 -0.04  0.02  0.99 -0.    0.08  0.99\n",
      "   0.03  0.09  1.03 -0.04  0.06  0.99  0.03  0.06  1.01 -0.01  0.09  0.98]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.02  0.98\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97]\n",
      " [-0.02  0.09  0.95 -0.    0.1   0.94  0.04  0.14  0.94 -0.01  0.08  1.\n",
      "   0.02  0.09  1.01 -0.01  0.08  0.93  0.05  0.09  1.01 -0.05  0.06  0.99\n",
      "   0.05  0.16  0.98 -0.01  0.08  1.   -0.05  0.05  0.94  0.04  0.11  0.96\n",
      "   0.01  0.13  0.96  0.01  0.07  0.97 -0.03  0.03  1.01 -0.04  0.06  0.99\n",
      "  -0.01  0.06  1.   -0.03  0.07  0.97 -0.01  0.06  0.96  0.01  0.1   1.  ]\n",
      " [-0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.99\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98]\n",
      " [-0.05 -0.02  0.98 -0.07  0.01  0.98 -0.02  0.    0.98 -0.05  0.    0.99\n",
      "  -0.05  0.    0.98 -0.03 -0.02  0.98 -0.04  0.01  0.99 -0.07  0.    0.98\n",
      "  -0.03  0.02  0.99 -0.06  0.    0.98 -0.05 -0.02  0.98 -0.08  0.01  0.98\n",
      "  -0.04 -0.01  0.98 -0.04 -0.02  0.98 -0.08 -0.02  0.97 -0.06  0.    0.99\n",
      "  -0.05  0.05  1.   -0.04  0.01  0.99 -0.07 -0.01  0.98 -0.08 -0.01  0.98]\n",
      " [ 0.02 -0.    0.98  0.02  0.    0.99  0.04 -0.    0.96  0.09  0.03  0.99\n",
      "  -0.03  0.    1.    0.07  0.02  0.99 -0.02 -0.02  0.96 -0.   -0.03  0.93\n",
      "   0.02  0.01  0.98  0.02  0.01  0.98  0.12  0.02  0.97  0.06  0.01  0.95\n",
      "   0.07  0.01  0.95 -0.02  0.01  1.02  0.03  0.05  1.07  0.03  0.02  1.\n",
      "   0.02  0.01  0.98  0.03  0.02  0.99  0.   -0.01  0.97  0.09  0.02  0.97]\n",
      " [-0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98]\n",
      " [-0.01  0.12  1.    0.    0.1   0.95 -0.05  0.09  0.95 -0.04  0.08  0.96\n",
      "  -0.    0.07  0.96 -0.02  0.08  0.95  0.03  0.11  0.97  0.04  0.1   1.01\n",
      "   0.01  0.09  0.99 -0.02  0.05  1.    0.    0.11  1.01 -0.06  0.03  1.01\n",
      "   0.01  0.1   0.95  0.05  0.12  1.   -0.01  0.07  0.94  0.07  0.09  0.99\n",
      "  -0.05  0.06  0.99 -0.03  0.14  0.94 -0.01  0.07  0.97  0.03  0.16  1.01]\n",
      " [-0.02  0.1   0.91  0.02  0.1   0.98 -0.    0.15  0.96 -0.    0.08  0.97\n",
      "  -0.04  0.06  0.92  0.03  0.11  0.96 -0.07  0.07  0.96 -0.02  0.1   0.99\n",
      "  -0.01  0.12  1.    0.    0.1   0.95 -0.05  0.09  0.95 -0.04  0.08  0.96\n",
      "  -0.    0.07  0.96 -0.02  0.08  0.95  0.03  0.11  0.97  0.04  0.1   1.01\n",
      "   0.01  0.09  0.99 -0.02  0.05  1.    0.    0.11  1.01 -0.06  0.03  1.01]\n",
      " [ 0.    0.05  1.04  0.    0.08  0.99 -0.01  0.06  1.    0.    0.11  0.98\n",
      "   0.06  0.13  1.03 -0.02  0.07  0.99 -0.03  0.1   0.95 -0.01  0.09  1.02\n",
      "  -0.07  0.05  0.93 -0.04  0.06  0.99  0.05  0.16  0.98 -0.01  0.1   1.\n",
      "   0.01  0.17  1.    0.04  0.11  0.96  0.04  0.06  1.01 -0.01  0.1   1.\n",
      "  -0.02  0.04  1.02  0.02  0.11  0.96  0.05  0.08  1.01  0.02  0.08  0.98]\n",
      " [-0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.01  0.98\n",
      "  -0.07 -0.01  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.06 -0.02  0.98 -0.06 -0.03  0.98 -0.06 -0.02  0.98\n",
      "  -0.06 -0.02  0.98 -0.06 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97]\n",
      " [-0.01 -0.02  0.94 -0.04 -0.02  0.96  0.01  0.04  0.99  0.02 -0.01  0.98\n",
      "   0.02 -0.01  0.99  0.01 -0.04  0.99  0.05 -0.    0.98  0.01 -0.    1.\n",
      "   0.07  0.01  1.01  0.03  0.02  0.99  0.03  0.    0.97  0.02 -0.01  0.99\n",
      "   0.02 -0.    0.98 -0.05 -0.03  0.99  0.02 -0.01  0.96  0.06  0.    0.98\n",
      "  -0.02 -0.02  0.97  0.06 -0.02  0.97  0.02 -0.01  0.99  0.02 -0.    0.98]\n",
      " [-0.07 -0.03  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.02  0.98\n",
      "  -0.06 -0.02  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97]\n",
      " [-0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.02  0.98]\n",
      " [-0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.06 -0.    0.99 -0.06 -0.03  0.98 -0.05  0.02  0.99 -0.05 -0.02  0.98\n",
      "  -0.05  0.01  0.99 -0.06 -0.01  0.98 -0.05  0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.99 -0.05 -0.02  0.97 -0.05  0.01  0.99 -0.05 -0.    0.98\n",
      "  -0.05  0.02  0.98 -0.06 -0.01  0.98 -0.05  0.01  0.98 -0.05  0.    0.98]\n",
      " [ 0.08  0.01  1.01 -0.01 -0.01  0.99  0.09  0.01  1.    0.02 -0.05  0.98\n",
      "   0.02  0.01  0.98  0.02 -0.    0.97  0.03 -0.06  0.99  0.05  0.01  1.01\n",
      "  -0.01 -0.02  0.94 -0.01 -0.01  0.99 -0.01 -0.03  0.98  0.01 -0.02  1.\n",
      "   0.02 -0.    0.98  0.02 -0.    0.98 -0.02  0.04  0.99 -0.03 -0.02  0.94\n",
      "   0.03 -0.01  0.95  0.08  0.01  1.02 -0.01 -0.05  0.99  0.02  0.01  0.98]\n",
      " [ 0.03  0.13  0.99 -0.03  0.09  1.    0.02  0.09  1.   -0.04  0.06  0.99\n",
      "  -0.05  0.08  0.93  0.03  0.11  0.97  0.04  0.15  0.98 -0.    0.09  0.93\n",
      "   0.04  0.13  0.93 -0.04  0.06  1.   -0.05  0.01  0.98 -0.01  0.07  0.93\n",
      "  -0.    0.04  1.01  0.01  0.1   0.96  0.01  0.16  0.96 -0.    0.07  0.97\n",
      "   0.03  0.14  1.01 -0.05  0.06  0.99  0.03  0.07  1.01 -0.01  0.09  0.98]\n",
      " [-0.05  0.02  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.04  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98]\n",
      " [-0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.07 -0.01  0.98 -0.06  0.02  0.99 -0.07  0.03  0.99 -0.05  0.    0.99\n",
      "  -0.02  0.01  0.98 -0.06  0.02  0.99 -0.07 -0.    0.98 -0.05 -0.02  0.98\n",
      "  -0.09 -0.02  0.97 -0.05 -0.    0.98 -0.09  0.03  0.99 -0.05 -0.02  0.98\n",
      "  -0.04  0.01  0.99 -0.09  0.    0.98 -0.06  0.01  0.99 -0.05  0.    0.98\n",
      "  -0.08  0.02  0.99 -0.05 -0.01  0.98 -0.04  0.01  0.99 -0.08 -0.03  0.98]\n",
      " [-0.05 -0.    0.98 -0.05  0.    0.98 -0.05  0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [ 0.02 -0.01  0.98  0.06 -0.01  0.99  0.08  0.01  1.02  0.03  0.    1.02\n",
      "  -0.04 -0.02  0.95  0.08 -0.02  0.97  0.02  0.01  0.97  0.02 -0.    0.97\n",
      "   0.   -0.04  0.99 -0.05 -0.02  0.96  0.05 -0.    0.97 -0.02 -0.01  0.97\n",
      "   0.02  0.01  0.99  0.01  0.03  0.99  0.02 -0.    0.98  0.02 -0.    0.97\n",
      "  -0.01 -0.06  0.98  0.02  0.    1.    0.03  0.01  1.02  0.08  0.01  1.  ]\n",
      " [-0.04 -0.02  0.99  0.03  0.04  1.03  0.02  0.    0.99  0.02  0.    0.99\n",
      "  -0.05 -0.02  0.98 -0.02  0.    1.01 -0.02  0.    1.01  0.03 -0.    0.94\n",
      "  -0.03 -0.01  1.    0.02  0.02  0.98  0.02  0.01  0.98  0.03  0.01  1.\n",
      "   0.1   0.02  0.94 -0.    0.01  1.02  0.05  0.    0.95  0.08  0.03  1.\n",
      "  -0.02 -0.03  0.93  0.02  0.01  0.98  0.02  0.    0.99  0.06  0.02  0.97]\n",
      " [ 0.02  0.01  0.98  0.03  0.01  0.98  0.09  0.02  0.98  0.07  0.01  0.95\n",
      "   0.    0.01  1.01  0.04 -0.01  0.93  0.04  0.07  1.08  0.02 -0.    0.98\n",
      "   0.02  0.    0.99 -0.04 -0.04  0.94 -0.05 -0.    1.01  0.07  0.02  0.99\n",
      "   0.04  0.    0.95 -0.03 -0.02  0.98  0.02 -0.02  0.97  0.03  0.01  0.98\n",
      "   0.02  0.01  0.98  0.07 -0.    0.91  0.08  0.02  1.    0.01  0.01  1.01]\n",
      " [-0.07  0.01  0.98 -0.05 -0.05  0.97 -0.07  0.01  0.98 -0.06  0.01  0.98\n",
      "  -0.04 -0.03  0.97 -0.06 -0.    0.98 -0.04 -0.05  0.98 -0.06 -0.02  0.98\n",
      "  -0.05 -0.03  0.99 -0.06 -0.04  0.97 -0.07  0.01  0.99 -0.05  0.01  1.\n",
      "  -0.06 -0.01  0.98 -0.05 -0.01  0.99 -0.04 -0.03  0.97 -0.06 -0.01  0.99\n",
      "  -0.04 -0.04  0.99 -0.06 -0.02  0.98 -0.06 -0.01  0.99 -0.06  0.01  0.99]\n",
      " [ 0.02  0.02  1.02  0.06  0.02  1.    0.08  0.01  0.96  0.08  0.01  0.91\n",
      "   0.04  0.03  1.01  0.02  0.01  0.98  0.03  0.01  0.99  0.1   0.01  0.92\n",
      "   0.07  0.01  0.95  0.03  0.01  1.01 -0.05 -0.    1.    0.04 -0.04  0.87\n",
      "   0.02  0.    0.98  0.02  0.01  0.98  0.04  0.05  1.02  0.09  0.03  0.99\n",
      "   0.06  0.02  1.    0.07  0.01  0.95  0.07 -0.    0.93  0.02 -0.03  0.93]\n",
      " [-0.07  0.02  0.99 -0.01  0.01  0.99 -0.06  0.    0.98 -0.08 -0.01  0.98\n",
      "  -0.04  0.01  0.99 -0.07  0.01  0.98 -0.06  0.02  0.99 -0.03 -0.03  0.98\n",
      "  -0.05 -0.    0.98 -0.09  0.03  0.99 -0.07 -0.01  0.98 -0.06  0.01  0.99\n",
      "  -0.08  0.02  0.99 -0.05  0.01  0.98 -0.05 -0.    0.98 -0.03  0.01  0.98\n",
      "  -0.07 -0.    0.98 -0.06 -0.02  0.98 -0.01 -0.    0.98 -0.05 -0.    0.98]\n",
      " [-0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.06 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.01  0.99\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.02  0.98\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.03  0.08  1.01 -0.04  0.05  0.96 -0.02  0.07  0.98 -0.06  0.03  0.94\n",
      "  -0.05  0.07  0.99 -0.03  0.12  0.95 -0.    0.1   1.03 -0.06  0.03  1.01\n",
      "  -0.04  0.06  1.    0.02  0.16  0.96 -0.01  0.09  0.99 -0.06  0.11  0.97\n",
      "  -0.02  0.08  0.97  0.01  0.14  0.96 -0.    0.07  0.97 -0.    0.07  1.\n",
      "   0.01  0.1   0.95  0.03  0.12  0.97 -0.02  0.11  0.99 -0.03  0.05  1.  ]\n",
      " [-0.05  0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98 -0.06  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.06 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.99 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98]\n",
      " [-0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98]\n",
      " [-0.04  0.07  0.98 -0.    0.03  1.   -0.01  0.09  0.98  0.02  0.15  0.96\n",
      "  -0.04  0.06  0.99 -0.05  0.08  0.95 -0.01  0.12  0.99 -0.01  0.06  1.02\n",
      "  -0.03  0.06  0.96  0.04  0.11  0.97  0.04  0.16  0.98  0.02  0.1   0.96\n",
      "   0.03  0.13  0.99  0.    0.1   0.95  0.05  0.12  1.04  0.03  0.12  0.97\n",
      "  -0.06  0.06  0.95 -0.    0.11  1.   -0.06  0.09  0.96 -0.    0.09  0.97]\n",
      " [-0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.03  0.98\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.03  0.98 -0.06 -0.02  0.98]\n",
      " [ 0.02  0.11  0.95 -0.01  0.15  0.95 -0.01  0.11  1.01 -0.08  0.1   0.99\n",
      "   0.03  0.11  0.98  0.06  0.1   1.01 -0.    0.11  1.01  0.02  0.11  1.04\n",
      "  -0.05  0.06  0.99  0.05  0.09  1.    0.    0.07  0.98  0.    0.08  0.96\n",
      "  -0.02  0.07  1.01  0.04  0.1   1.    0.03  0.11  1.   -0.02  0.09  1.\n",
      "  -0.01  0.09  0.95  0.03  0.1   0.99  0.04  0.1   1.01 -0.01  0.07  1.  ]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.06 -0.02  0.98 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98]\n",
      " [ 0.03  0.01  0.98  0.1   0.02  0.96  0.02  0.02  1.02  0.08  0.02  0.97\n",
      "   0.01 -0.01  0.95  0.06  0.05  1.05  0.03  0.01  1.    0.02  0.01  0.98\n",
      "   0.03  0.02  0.98  0.06 -0.    0.95 -0.03  0.    1.01  0.02  0.    0.95\n",
      "  -0.05 -0.01  0.98  0.02  0.05  1.07  0.02  0.01  0.98  0.02  0.01  0.98\n",
      "  -0.02 -0.04  0.94  0.1   0.02  0.98 -0.04 -0.    1.    0.04  0.01  1.01]\n",
      " [-0.06  0.02  0.99 -0.04 -0.02  0.98 -0.05  0.02  0.99 -0.06  0.03  0.99\n",
      "  -0.01  0.01  0.99 -0.05  0.01  0.98 -0.02 -0.03  0.97 -0.07 -0.01  0.98\n",
      "  -0.04 -0.    0.98 -0.03  0.01  0.98 -0.05 -0.01  0.97 -0.05 -0.    0.98\n",
      "  -0.04  0.01  0.98 -0.07 -0.01  0.98 -0.04 -0.01  0.98 -0.03  0.03  0.99\n",
      "  -0.05  0.01  0.98 -0.06 -0.01  0.98 -0.06  0.03  0.98 -0.06  0.01  0.98]\n",
      " [-0.05 -0.03  0.97 -0.05 -0.    0.98 -0.03  0.01  0.98 -0.05  0.02  0.99\n",
      "  -0.06 -0.02  0.98 -0.01  0.    0.99 -0.06  0.    0.98 -0.07  0.01  0.99\n",
      "  -0.08  0.    0.99 -0.04  0.01  0.99 -0.06 -0.02  0.98 -0.02 -0.02  0.98\n",
      "  -0.06 -0.    0.98 -0.1  -0.02  0.98 -0.04 -0.01  0.98 -0.07 -0.    0.98\n",
      "  -0.03  0.    0.99 -0.06  0.03  1.   -0.06 -0.    0.98 -0.05 -0.04  0.97]\n",
      " [-0.03 -0.05  0.9   0.09  0.02  0.98  0.07  0.01  0.95 -0.04 -0.    0.99\n",
      "   0.03 -0.03  0.9   0.02  0.01  0.97  0.02  0.01  0.98  0.02  0.01  0.98\n",
      "  -0.01  0.02  1.01  0.08  0.01  0.95  0.02  0.01  1.01  0.1   0.02  0.97\n",
      "   0.04 -0.03  0.87  0.02  0.01  0.98  0.02  0.01  0.98  0.02  0.03  1.02\n",
      "   0.1   0.02  0.96 -0.03 -0.01  0.98  0.01  0.01  1.01  0.04  0.04  1.03]\n",
      " [-0.04 -0.    0.98 -0.02 -0.    0.99 -0.08 -0.02  0.97 -0.06  0.    0.98\n",
      "  -0.03 -0.02  0.98 -0.06  0.02  0.99 -0.07 -0.01  0.98 -0.02 -0.02  0.98\n",
      "  -0.06 -0.    0.99 -0.06  0.01  0.99 -0.08  0.    0.99 -0.05  0.02  0.99\n",
      "  -0.05 -0.02  0.98 -0.05  0.04  1.   -0.05  0.    0.99 -0.02 -0.03  0.97\n",
      "  -0.04  0.02  0.99 -0.05 -0.01  0.98 -0.07  0.03  0.99 -0.07 -0.03  0.97]\n",
      " [-0.09  0.02  0.99 -0.05 -0.03  0.97 -0.05 -0.    0.98 -0.03  0.01  0.98\n",
      "  -0.05  0.02  0.99 -0.06 -0.02  0.98 -0.01  0.    0.99 -0.06  0.    0.98\n",
      "  -0.07  0.01  0.99 -0.08  0.    0.99 -0.04  0.01  0.99 -0.06 -0.02  0.98\n",
      "  -0.02 -0.02  0.98 -0.06 -0.    0.98 -0.1  -0.02  0.98 -0.04 -0.01  0.98\n",
      "  -0.07 -0.    0.98 -0.03  0.    0.99 -0.06  0.03  1.   -0.06 -0.    0.98]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.03  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98]\n",
      " [-0.04  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98]\n",
      " [-0.01 -0.02  0.94  0.08  0.02  1.02  0.06 -0.02  0.97  0.02 -0.02  0.99\n",
      "   0.02  0.    0.99  0.02 -0.01  0.97 -0.05 -0.03  0.97  0.08  0.01  1.\n",
      "  -0.02 -0.01  0.97 -0.03 -0.01  0.98  0.    0.03  0.99  0.02  0.01  0.98\n",
      "   0.02 -0.    0.99  0.01 -0.06  0.97  0.07  0.01  1.01 -0.03 -0.02  0.96\n",
      "   0.01 -0.    1.01  0.05 -0.02  0.98  0.02 -0.01  1.    0.02 -0.01  0.99]\n",
      " [-0.04  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.03  0.98 -0.04  0.04  0.99 -0.04  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.99 -0.05  0.02  0.98]\n",
      " [ 0.07  0.01  0.99 -0.05 -0.01  0.98  0.02  0.03  0.99  0.02  0.    0.98\n",
      "   0.02 -0.    0.99  0.02 -0.06  1.   -0.01 -0.02  0.95  0.01 -0.01  0.94\n",
      "   0.04  0.01  1.01  0.08  0.03  0.98  0.02 -0.    0.97  0.02 -0.    0.99\n",
      "   0.03  0.01  0.97  0.1   0.01  1.   -0.02 -0.01  0.97  0.07  0.01  0.99\n",
      "   0.04  0.01  0.99 -0.01 -0.05  0.99  0.03 -0.    0.98  0.02 -0.    0.99]\n",
      " [-0.06 -0.01  0.97 -0.06 -0.02  0.98 -0.04 -0.06  0.97 -0.06 -0.05  0.97\n",
      "  -0.04 -0.05  0.97 -0.05 -0.02  0.98 -0.04 -0.03  0.99 -0.06 -0.02  0.98\n",
      "  -0.07 -0.    0.99 -0.06 -0.04  0.97 -0.06 -0.01  0.99 -0.06 -0.05  0.97\n",
      "  -0.06 -0.01  0.98 -0.05 -0.05  0.97 -0.03 -0.06  0.98 -0.05 -0.    0.99\n",
      "  -0.06 -0.03  0.99 -0.06  0.    0.99 -0.07  0.02  0.99 -0.05 -0.    0.99]\n",
      " [-0.06 -0.03  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.03  0.98\n",
      "  -0.07 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.08  0.04  0.99 -0.03  0.07  0.98  0.05  0.15  0.99 -0.    0.09  1.01\n",
      "   0.04  0.13  1.03 -0.03  0.07  0.99 -0.07  0.05  0.96  0.    0.08  0.96\n",
      "   0.03  0.14  0.96  0.03  0.12  0.96  0.02  0.06  1.01 -0.02  0.09  0.98\n",
      "   0.01  0.13  0.99 -0.01  0.09  0.96 -0.06  0.05  0.99 -0.04  0.06  0.96\n",
      "  -0.    0.07  0.96 -0.04  0.04  0.97 -0.03  0.09  0.95 -0.01  0.13  0.96]\n",
      " [-0.06  0.    1.   -0.05  0.01  1.   -0.07  0.    0.98 -0.05  0.01  1.\n",
      "  -0.03 -0.05  0.98 -0.06 -0.05  0.97 -0.05 -0.03  0.99 -0.05 -0.01  0.99\n",
      "  -0.06 -0.    0.99 -0.05 -0.04  0.98 -0.06 -0.    0.98 -0.04 -0.03  0.98\n",
      "  -0.04 -0.02  0.98 -0.05 -0.05  0.98 -0.03 -0.04  0.98 -0.05 -0.06  0.97\n",
      "  -0.04 -0.05  0.98 -0.04 -0.06  0.98 -0.05 -0.05  0.98 -0.04 -0.05  0.99]\n",
      " [-0.05 -0.04  0.98 -0.06 -0.02  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.    0.99 -0.06  0.01  0.99 -0.06 -0.    0.99 -0.06  0.01  0.98\n",
      "  -0.06 -0.    0.98 -0.06 -0.    0.98 -0.06  0.01  0.99 -0.04 -0.03  0.98\n",
      "  -0.06  0.01  0.99 -0.04 -0.05  0.98 -0.05  0.01  1.   -0.05 -0.03  0.99\n",
      "  -0.06 -0.02  0.98 -0.07 -0.    0.99 -0.05  0.01  1.   -0.06 -0.    0.98]\n",
      " [-0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98]\n",
      " [-0.03  0.    1.01  0.04 -0.02  0.9   0.02 -0.    0.97  0.02  0.01  0.98\n",
      "   0.02 -0.01  0.98  0.01  0.02  1.01  0.05  0.    0.95  0.06  0.02  0.99\n",
      "   0.1   0.02  0.97  0.07  0.    0.93  0.02 -0.    0.98  0.02  0.    0.99\n",
      "   0.04 -0.    0.96  0.09  0.03  0.99 -0.03  0.    1.    0.07  0.02  0.99\n",
      "  -0.02 -0.02  0.96 -0.   -0.03  0.93  0.02  0.01  0.98  0.02  0.01  0.98]\n",
      " [ 0.07  0.04  1.01  0.02  0.    0.99  0.02  0.01  0.98  0.01  0.03  1.03\n",
      "  -0.02 -0.01  0.96  0.08  0.01  0.95 -0.   -0.    0.96  0.04  0.04  1.03\n",
      "   0.01 -0.02  0.95  0.02  0.    0.99  0.02  0.    0.98 -0.01  0.05  1.11\n",
      "   0.06  0.02  1.01  0.07  0.02  0.99 -0.02 -0.01  0.96 -0.04 -0.    1.02\n",
      "   0.02  0.    0.98  0.02  0.01  0.98  0.02  0.01  0.98  0.09  0.02  0.99]\n",
      " [-0.07 -0.02  0.98 -0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.06 -0.03  0.98 -0.06 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97]\n",
      " [-0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98]\n",
      " [-0.07  0.    0.98 -0.07  0.01  0.99 -0.03 -0.    0.98 -0.03  0.02  0.99\n",
      "  -0.05 -0.    0.99 -0.07 -0.03  0.97 -0.07 -0.    0.98 -0.06 -0.02  0.98\n",
      "  -0.1  -0.    0.98 -0.05  0.    0.98 -0.06 -0.    0.98 -0.04  0.03  0.99\n",
      "  -0.04  0.01  0.99 -0.08 -0.    0.98 -0.08  0.02  0.98 -0.06 -0.    0.99\n",
      "  -0.04 -0.02  0.97 -0.06 -0.02  0.98 -0.07  0.01  0.99 -0.08  0.01  0.98]\n",
      " [ 0.08  0.02  1.    0.06  0.02  1.   -0.05 -0.01  0.98  0.06  0.05  1.05\n",
      "   0.03  0.01  0.99  0.02  0.01  0.99  0.01  0.    0.98  0.1   0.01  0.96\n",
      "   0.07  0.01  0.95 -0.03  0.01  1.01 -0.01 -0.02  0.95  0.06  0.06  1.07\n",
      "   0.03  0.01  0.98  0.03  0.01  0.97 -0.02  0.04  1.07 -0.05 -0.01  0.98\n",
      "   0.02  0.01  1.01  0.08  0.01  0.95 -0.03  0.02  1.06  0.01 -0.02  0.95]\n",
      " [ 0.06  0.07  1.08  0.02  0.    0.99  0.02  0.01  0.97 -0.03 -0.04  0.94\n",
      "   0.06  0.03  1.01 -0.03 -0.    0.98 -0.03  0.    1.01 -0.03 -0.02  0.91\n",
      "   0.04  0.03  1.01  0.02  0.01  0.98  0.02  0.01  0.99  0.04  0.03  1.03\n",
      "   0.    0.01  1.02 -0.02  0.    1.01 -0.   -0.01  0.95  0.06  0.06  1.06\n",
      "   0.03  0.02  0.99  0.03  0.01  0.98  0.02  0.02  0.99  0.09  0.03  1.  ]\n",
      " [-0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.06 -0.03  0.98 -0.07 -0.01  0.99 -0.07 -0.    0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97]\n",
      " [-0.    0.11  1.01  0.02  0.11  1.04 -0.05  0.06  0.99  0.05  0.09  1.\n",
      "   0.    0.07  0.98  0.    0.08  0.96 -0.02  0.07  1.01  0.04  0.1   1.\n",
      "   0.03  0.11  1.   -0.02  0.09  1.   -0.01  0.09  0.95  0.03  0.1   0.99\n",
      "   0.04  0.1   1.01 -0.01  0.07  1.   -0.01  0.05  1.    0.01  0.11  1.\n",
      "   0.    0.07  1.04  0.02  0.11  0.96 -0.05  0.07  0.95 -0.01  0.1   1.  ]\n",
      " [-0.07 -0.01  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.02  0.98\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.06 -0.03  0.98 -0.06 -0.02  0.98 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98]\n",
      " [-0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.    0.99 -0.07 -0.    0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.01  0.98 -0.07 -0.01  0.98]\n",
      " [-0.06  0.    0.99 -0.03 -0.01  0.98 -0.07  0.01  0.99 -0.05 -0.01  0.98\n",
      "  -0.04 -0.02  0.98 -0.09 -0.02  0.97 -0.05 -0.    0.99 -0.1   0.03  0.99\n",
      "  -0.03  0.01  0.99 -0.07  0.    0.98 -0.08 -0.01  0.98 -0.06  0.02  1.\n",
      "  -0.05 -0.    0.99 -0.08  0.01  0.98 -0.04  0.    0.98 -0.05  0.02  0.99\n",
      "  -0.06  0.01  0.98 -0.05  0.    0.98 -0.05 -0.01  0.98 -0.07 -0.    0.98]\n",
      " [-0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97]\n",
      " [-0.05  0.    0.98 -0.02  0.02  1.   -0.07 -0.02  0.98 -0.04  0.01  0.99\n",
      "  -0.01 -0.02  0.98 -0.06 -0.02  0.98 -0.05 -0.    0.98 -0.07 -0.02  0.98\n",
      "  -0.07  0.    0.98 -0.03  0.01  0.99 -0.04 -0.03  0.98 -0.06 -0.    0.99\n",
      "  -0.07  0.02  0.99 -0.04 -0.02  0.98 -0.04  0.01  0.99 -0.04  0.02  0.99\n",
      "  -0.06 -0.04  0.97 -0.05 -0.    0.98 -0.11  0.02  0.99 -0.06 -0.02  0.98]\n",
      " [-0.1  -0.03  0.97 -0.04 -0.01  0.98 -0.06 -0.01  0.98 -0.02 -0.02  0.98\n",
      "  -0.07  0.02  0.99 -0.06  0.03  0.98 -0.03 -0.01  0.98 -0.04  0.02  0.99\n",
      "  -0.06 -0.02  0.98 -0.01 -0.01  0.98 -0.06  0.    0.98 -0.06  0.01  0.99\n",
      "  -0.07 -0.02  0.98 -0.05  0.02  0.99 -0.04  0.01  0.99 -0.06  0.03  0.99\n",
      "  -0.05  0.    0.98 -0.03  0.03  0.99 -0.05  0.03  0.99 -0.06  0.01  0.99]\n",
      " [-0.06 -0.01  0.98 -0.05 -0.02  0.99 -0.06 -0.04  0.97 -0.07  0.01  1.\n",
      "  -0.06 -0.05  0.97 -0.05 -0.02  0.97 -0.05 -0.05  0.97 -0.03 -0.06  0.98\n",
      "  -0.05  0.01  0.99 -0.06 -0.01  0.99 -0.06  0.02  0.99 -0.08  0.02  0.99\n",
      "  -0.06  0.02  0.99 -0.05  0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.03 -0.04  0.98 -0.05  0.01  0.99 -0.03 -0.05  0.98 -0.04 -0.05  0.98]\n",
      " [-0.03  0.06  1.   -0.    0.12  0.96  0.01  0.1   0.98  0.01  0.1   0.96\n",
      "  -0.03  0.06  1.    0.05  0.12  0.99  0.02  0.14  0.99 -0.03  0.09  1.\n",
      "   0.    0.1   0.95  0.02  0.11  0.98  0.04  0.1   1.02  0.03  0.09  0.95\n",
      "  -0.05  0.03  0.97 -0.    0.09  1.02 -0.07  0.05  1.02  0.03  0.11  0.97\n",
      "  -0.01  0.13  0.95 -0.    0.11  1.02 -0.06  0.12  0.97 -0.    0.09  0.99]\n",
      " [-0.04  0.01  0.98 -0.04 -0.01  0.98 -0.03  0.02  0.99 -0.06  0.    0.98\n",
      "  -0.06  0.02  0.99 -0.04 -0.02  0.98 -0.05  0.02  0.99 -0.06  0.03  0.99\n",
      "  -0.01  0.01  0.99 -0.05  0.01  0.98 -0.02 -0.03  0.97 -0.07 -0.01  0.98\n",
      "  -0.04 -0.    0.98 -0.03  0.01  0.98 -0.05 -0.01  0.97 -0.05 -0.    0.98\n",
      "  -0.04  0.01  0.98 -0.07 -0.01  0.98 -0.04 -0.01  0.98 -0.03  0.03  0.99]\n",
      " [-0.05  0.02  0.98 -0.03  0.03  0.98 -0.04  0.01  0.98 -0.04  0.04  0.99\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98]\n",
      " [-0.04 -0.03  0.98 -0.05 -0.04  0.99 -0.05 -0.05  0.98 -0.07 -0.    0.99\n",
      "  -0.05 -0.05  0.98 -0.06  0.    0.98 -0.06 -0.02  0.98 -0.04 -0.04  0.97\n",
      "  -0.06 -0.03  0.97 -0.04 -0.04  0.99 -0.06  0.    0.99 -0.06 -0.01  1.\n",
      "  -0.05 -0.    0.99 -0.06 -0.    0.98 -0.05 -0.02  0.98 -0.04 -0.03  0.97\n",
      "  -0.06 -0.03  0.98 -0.04 -0.05  0.98 -0.05  0.01  1.   -0.06 -0.03  0.99]\n",
      " [-0.02  0.14  0.96 -0.04  0.07  0.98  0.06  0.12  0.99 -0.    0.07  0.98\n",
      "  -0.03  0.03  0.99  0.03  0.12  0.96  0.05  0.11  0.96  0.01  0.11  0.98\n",
      "  -0.02  0.05  0.96 -0.04  0.05  0.97  0.01  0.12  0.95 -0.02  0.11  0.97\n",
      "  -0.02  0.09  1.   -0.03  0.07  0.96  0.02  0.12  0.96  0.04  0.16  0.97\n",
      "   0.03  0.11  0.98 -0.01  0.03  1.   -0.    0.09  0.93  0.04  0.15  0.96]\n",
      " [-0.07 -0.01  0.99 -0.06 -0.04  0.98 -0.07  0.01  0.99 -0.05 -0.05  0.97\n",
      "  -0.04 -0.01  0.97 -0.05 -0.05  0.97 -0.04 -0.03  0.97 -0.05 -0.03  0.98\n",
      "  -0.04 -0.04  0.98 -0.06 -0.03  0.97 -0.07  0.01  0.99 -0.06 -0.05  0.97\n",
      "  -0.07  0.01  0.98 -0.05 -0.05  0.97 -0.05 -0.02  0.97 -0.04 -0.05  0.98\n",
      "  -0.03 -0.05  0.97 -0.06 -0.05  0.98 -0.04 -0.05  0.98 -0.05 -0.05  0.98]\n",
      " [-0.04 -0.01  0.99  0.02  0.01  0.98  0.02  0.    0.99  0.02 -0.01  0.98\n",
      "   0.04 -0.01  0.98 -0.01 -0.01  0.98  0.01 -0.02  0.94 -0.03 -0.02  0.96\n",
      "  -0.02 -0.04  0.99  0.02  0.    0.98  0.02 -0.    0.98  0.04  0.04  0.98\n",
      "  -0.04 -0.02  0.96  0.07  0.01  1.    0.01 -0.01  0.94  0.05  0.02  0.99\n",
      "   0.04  0.03  0.97  0.02  0.    0.99  0.02 -0.    0.99  0.09 -0.02  0.97]\n",
      " [-0.07 -0.02  0.99 -0.07  0.01  0.99 -0.06  0.01  0.98 -0.07  0.01  0.98\n",
      "  -0.06  0.01  0.98 -0.06 -0.    0.97 -0.05  0.02  0.99 -0.03 -0.04  0.98\n",
      "  -0.05  0.02  0.99 -0.03 -0.05  0.98 -0.05 -0.    0.99 -0.04 -0.04  0.99\n",
      "  -0.05 -0.03  0.99 -0.07  0.01  0.99 -0.05 -0.01  0.99 -0.07  0.    0.97\n",
      "  -0.06 -0.05  0.97 -0.04 -0.04  0.98 -0.06 -0.05  0.97 -0.07  0.01  0.99]\n",
      " [ 0.02 -0.    0.99  0.06 -0.01  0.97 -0.04 -0.02  0.95  0.07  0.01  1.02\n",
      "   0.05 -0.    0.96  0.06  0.03  0.99  0.02 -0.    0.99  0.02 -0.01  0.98\n",
      "   0.02 -0.01  0.98  0.06 -0.01  0.99  0.08  0.01  1.02  0.03  0.    1.02\n",
      "  -0.04 -0.02  0.95  0.08 -0.02  0.97  0.02  0.01  0.97  0.02 -0.    0.97\n",
      "   0.   -0.04  0.99 -0.05 -0.02  0.96  0.05 -0.    0.97 -0.02 -0.01  0.97]\n",
      " [-0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98]\n",
      " [ 0.05  0.15  0.99 -0.    0.09  1.01  0.04  0.13  1.03 -0.03  0.07  0.99\n",
      "  -0.07  0.05  0.96  0.    0.08  0.96  0.03  0.14  0.96  0.03  0.12  0.96\n",
      "   0.02  0.06  1.01 -0.02  0.09  0.98  0.01  0.13  0.99 -0.01  0.09  0.96\n",
      "  -0.06  0.05  0.99 -0.04  0.06  0.96 -0.    0.07  0.96 -0.04  0.04  0.97\n",
      "  -0.03  0.09  0.95 -0.01  0.13  0.96  0.03  0.11  0.97  0.03  0.15  0.98]\n",
      " [-0.01 -0.01  0.95  0.03  0.    0.95  0.07  0.03  1.01  0.06 -0.02  0.88\n",
      "   0.02 -0.    0.97  0.02  0.01  0.98  0.02 -0.01  0.98  0.01 -0.01  0.95\n",
      "  -0.01 -0.01  0.96  0.05  0.02  1.   -0.06 -0.01  0.99  0.07  0.04  1.01\n",
      "   0.02  0.    0.99  0.02  0.01  0.98  0.01  0.03  1.03 -0.02 -0.01  0.96\n",
      "   0.08  0.01  0.95 -0.   -0.    0.96  0.04  0.04  1.03  0.01 -0.02  0.95]\n",
      " [-0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.03  0.99\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98]\n",
      " [-0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98]\n",
      " [-0.01  0.09  0.97 -0.07  0.09  0.95 -0.01  0.07  0.95  0.03  0.13  0.97\n",
      "  -0.04  0.06  0.98  0.05  0.12  0.99  0.01  0.07  0.98 -0.03  0.12  0.98\n",
      "  -0.03  0.06  0.96  0.01  0.07  0.99  0.03  0.09  1.01 -0.02  0.07  0.96\n",
      "  -0.03  0.04  0.99 -0.01  0.08  1.02 -0.05  0.02  0.97  0.02  0.11  0.95\n",
      "  -0.02  0.11  0.95 -0.    0.1   1.02 -0.03  0.12  0.94  0.01  0.1   0.98]\n",
      " [ 0.04  0.1   1.04 -0.01  0.07  0.99 -0.03  0.04  0.99 -0.    0.11  1.02\n",
      "  -0.07  0.03  0.99 -0.05  0.06  0.99  0.05  0.15  0.99 -0.01  0.08  0.98\n",
      "   0.05  0.09  1.01 -0.03  0.07  0.99 -0.07  0.06  0.96 -0.    0.09  0.98\n",
      "  -0.04  0.05  0.91 -0.03  0.07  0.98  0.    0.13  0.96  0.    0.12  0.98\n",
      "  -0.02  0.05  0.97 -0.02  0.07  1.    0.01  0.06  1.   -0.04  0.05  0.95]\n",
      " [-0.03  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.99 -0.04  0.01  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98]\n",
      " [-0.06  0.03  0.99 -0.01  0.02  1.   -0.05 -0.    0.98 -0.04 -0.04  0.97\n",
      "  -0.06 -0.02  0.98 -0.05  0.02  0.99 -0.08 -0.02  0.98 -0.04  0.03  0.99\n",
      "  -0.06  0.    0.98 -0.08  0.02  0.99 -0.07  0.01  0.98 -0.04 -0.01  0.98\n",
      "  -0.06  0.03  0.99 -0.06  0.    0.99 -0.06 -0.01  0.98 -0.08 -0.    0.98\n",
      "  -0.04  0.    0.98 -0.06 -0.02  0.98 -0.02  0.03  0.99 -0.05  0.    0.98]\n",
      " [ 0.01  0.13  0.96  0.03  0.13  0.98 -0.03  0.05  0.99 -0.04  0.05  0.99\n",
      "  -0.01  0.08  0.99 -0.04  0.04  0.98 -0.01  0.09  1.02 -0.07  0.04  0.93\n",
      "  -0.05  0.06  0.99  0.05  0.14  1.   -0.01  0.07  0.97  0.05  0.11  1.\n",
      "   0.01  0.1   0.98 -0.05  0.01  0.98 -0.    0.09  0.97 -0.05  0.01  0.93\n",
      "  -0.05  0.06  0.99  0.06  0.1   1.   -0.01  0.09  1.    0.02  0.14  0.98]\n",
      " [-0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.06  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.04  0.99 -0.04  0.01  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.01  0.98]\n",
      " [ 0.1   0.02  1.   -0.02 -0.02  0.94  0.07  0.01  1.    0.05 -0.    0.98\n",
      "   0.01 -0.05  0.98  0.02 -0.01  0.98  0.02 -0.    0.98  0.04  0.03  0.97\n",
      "   0.03 -0.    0.99  0.02 -0.01  0.94 -0.02 -0.01  0.97 -0.02 -0.03  0.97\n",
      "   0.02  0.02  0.99  0.02 -0.01  0.99  0.02 -0.    0.99 -0.04 -0.05  0.99\n",
      "   0.05  0.    0.95 -0.02 -0.02  0.94  0.02  0.    1.01  0.01 -0.04  0.98]\n",
      " [-0.04 -0.    0.98 -0.04 -0.01  0.98 -0.06  0.03  0.99 -0.01  0.02  1.\n",
      "  -0.05 -0.    0.98 -0.04 -0.04  0.97 -0.06 -0.02  0.98 -0.05  0.02  0.99\n",
      "  -0.08 -0.02  0.98 -0.04  0.03  0.99 -0.06  0.    0.98 -0.08  0.02  0.99\n",
      "  -0.07  0.01  0.98 -0.04 -0.01  0.98 -0.06  0.03  0.99 -0.06  0.    0.99\n",
      "  -0.06 -0.01  0.98 -0.08 -0.    0.98 -0.04  0.    0.98 -0.06 -0.02  0.98]\n",
      " [-0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.03  0.98 -0.04  0.04  0.99 -0.04  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.99 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98]\n",
      " [-0.05 -0.    0.98 -0.06  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.06  0.    0.99 -0.06 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98 -0.06 -0.    0.98]\n",
      " [ 0.02 -0.01  0.99 -0.   -0.07  0.98  0.   -0.01  0.96  0.01 -0.    1.\n",
      "   0.06  0.01  1.03  0.08 -0.01  0.98  0.03  0.01  0.97  0.02  0.    0.99\n",
      "   0.02 -0.    0.98  0.05 -0.01  0.98  0.02  0.    1.01 -0.02 -0.02  0.94\n",
      "   0.   -0.01  0.96  0.07 -0.01  0.97  0.02 -0.01  0.98  0.02 -0.01  0.98\n",
      "   0.02 -0.02  0.98  0.09  0.01  1.01 -0.03 -0.02  0.96  0.05  0.01  1.03]\n",
      " [-0.07 -0.03  0.98 -0.06 -0.02  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.07 -0.02  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.06 -0.01  0.99 -0.06 -0.01  0.99]\n",
      " [ 0.02 -0.01  0.98  0.03 -0.01  0.98  0.07  0.01  0.99  0.07  0.01  1.02\n",
      "   0.01 -0.    0.99  0.02 -0.04  0.98  0.02 -0.01  0.98  0.02 -0.01  0.97\n",
      "   0.02 -0.02  1.    0.07  0.01  1.    0.   -0.    1.    0.06  0.    0.97\n",
      "  -0.03 -0.01  0.97  0.07  0.02  0.97  0.02 -0.01  0.99  0.02 -0.    0.98\n",
      "   0.05  0.06  0.98  0.03 -0.01  0.97 -0.   -0.02  0.94 -0.03 -0.02  0.96]\n",
      " [-0.06  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.06  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98]\n",
      " [-0.01  0.15  0.95  0.03  0.09  0.98  0.01  0.08  1.   -0.    0.12  1.\n",
      "   0.04  0.13  0.98  0.01  0.09  0.98  0.02  0.11  0.96 -0.03  0.06  1.\n",
      "   0.05  0.12  0.98  0.02  0.15  0.99 -0.04  0.08  1.   -0.    0.1   0.95\n",
      "   0.02  0.11  0.96  0.05  0.14  1.02 -0.04  0.06  1.   -0.02  0.04  1.\n",
      "   0.01  0.11  0.99  0.01  0.06  1.02  0.03  0.12  0.96 -0.05  0.11  0.95]\n",
      " [ 0.07  0.01  1.    0.01 -0.01  0.94  0.05  0.02  0.99  0.04  0.03  0.97\n",
      "   0.02  0.    0.99  0.02 -0.    0.99  0.09 -0.02  0.97  0.07  0.01  1.02\n",
      "   0.03  0.01  1.02  0.08  0.01  1.01 -0.01  0.02  0.99  0.02  0.01  0.97\n",
      "   0.02 -0.01  0.99  0.03  0.01  0.97 -0.04 -0.03  0.97  0.08  0.01  1.\n",
      "  -0.02 -0.01  0.96  0.09  0.01  0.99 -0.02 -0.02  1.    0.02 -0.    0.99]\n",
      " [-0.03  0.    1.01  0.06  0.02  1.    0.07  0.01  0.96 -0.02  0.02  1.06\n",
      "   0.01 -0.03  0.95  0.02  0.    0.99  0.02  0.01  0.98 -0.02 -0.02  0.95\n",
      "  -0.02  0.01  1.01 -0.01 -0.    0.97  0.09  0.02  0.98 -0.02 -0.04  0.92\n",
      "   0.02  0.    0.98  0.02  0.01  0.98  0.04  0.03  1.   -0.04 -0.02  0.96\n",
      "  -0.04 -0.    1.01  0.07  0.02  0.98  0.03  0.03  1.03  0.02  0.03  1.02]\n",
      " [-0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.06 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.06 -0.    0.98 -0.05 -0.    0.98 -0.06  0.    0.99 -0.06 -0.    0.98]\n",
      " [-0.07 -0.02  0.98 -0.07 -0.01  0.98 -0.07 -0.01  0.99 -0.07 -0.    0.99\n",
      "  -0.07 -0.    0.99 -0.07 -0.    0.99 -0.07 -0.    0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.01  0.98\n",
      "  -0.07 -0.01  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [ 0.05  0.12  1.   -0.01  0.07  0.94  0.07  0.09  0.99 -0.05  0.06  0.99\n",
      "  -0.03  0.14  0.94 -0.01  0.07  0.97  0.03  0.16  1.01 -0.03  0.07  1.\n",
      "   0.04  0.07  1.01 -0.01  0.09  0.99 -0.02  0.05  1.03 -0.02  0.07  1.01\n",
      "  -0.01  0.06  1.   -0.01  0.09  0.98  0.01  0.13  0.99 -0.03  0.07  0.96\n",
      "   0.01  0.08  1.01  0.01  0.05  0.98 -0.04  0.06  0.98 -0.    0.08  1.01]\n",
      " [-0.04  0.02  0.99 -0.05 -0.02  0.97 -0.05  0.    0.98 -0.07 -0.02  0.98\n",
      "  -0.07  0.    0.98 -0.06 -0.02  0.98 -0.02 -0.01  0.99 -0.06 -0.    0.99\n",
      "  -0.06  0.01  0.99 -0.03 -0.    0.98 -0.04  0.01  0.99 -0.04  0.03  0.99\n",
      "  -0.01 -0.01  0.99 -0.06 -0.    0.98 -0.11 -0.03  0.97 -0.06 -0.02  0.98\n",
      "  -0.06  0.01  0.99 -0.07 -0.02  0.97 -0.03  0.01  0.98 -0.05  0.    0.98]\n",
      " [-0.04  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.04  0.99 -0.04  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98]\n",
      " [-0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.99 -0.05  0.03  0.98\n",
      "  -0.03  0.04  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98]\n",
      " [-0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.03  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98]\n",
      " [ 0.05  0.16  0.98 -0.    0.09  1.01 -0.    0.17  0.99  0.03  0.11  0.96\n",
      "   0.05  0.09  1.01 -0.01  0.11  1.    0.04  0.15  1.02 -0.    0.09  0.99\n",
      "  -0.05  0.04  0.98 -0.    0.1   0.99 -0.04  0.06  0.92 -0.03  0.07  0.97\n",
      "   0.01  0.13  0.96 -0.    0.12  0.99 -0.02  0.05  0.96 -0.    0.09  1.\n",
      "   0.04  0.12  0.97  0.03  0.11  0.99  0.    0.11  0.99  0.03  0.14  0.98]\n",
      " [-0.06 -0.04  0.97 -0.05 -0.01  0.97 -0.05  0.    0.99 -0.04 -0.04  0.99\n",
      "  -0.05 -0.01  0.99 -0.07  0.01  0.99 -0.05  0.01  1.   -0.06  0.01  0.98\n",
      "  -0.05 -0.    0.99 -0.03 -0.05  0.97 -0.06 -0.05  0.97 -0.05 -0.02  0.99\n",
      "  -0.05 -0.03  0.98 -0.07  0.02  0.99 -0.05 -0.03  0.98 -0.07  0.01  0.99\n",
      "  -0.04 -0.02  0.99 -0.04 -0.02  0.97 -0.06 -0.05  0.97 -0.04 -0.05  0.98]\n",
      " [-0.03  0.03  0.98 -0.05  0.03  0.98 -0.04  0.04  0.99 -0.04  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.03  0.99\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98]\n",
      " [-0.07  0.09  0.95  0.    0.09  0.97 -0.03  0.04  0.98  0.01  0.1   0.99\n",
      "   0.05  0.12  0.98  0.03  0.1   0.99  0.02  0.12  0.98  0.03  0.13  0.97\n",
      "   0.02  0.08  1.    0.02  0.06  0.99  0.03  0.1   0.97 -0.    0.08  1.01\n",
      "  -0.01  0.08  1.01 -0.04  0.04  1.02 -0.02  0.08  0.96  0.02  0.16  0.96\n",
      "  -0.    0.1   1.02  0.    0.12  0.93  0.02  0.1   0.98  0.01  0.03  1.01]\n",
      " [ 0.04  0.02  0.99 -0.05 -0.01  0.96 -0.03  0.    1.    0.06  0.02  0.99\n",
      "   0.02  0.03  1.03  0.02  0.04  1.02  0.02  0.    0.99  0.02  0.01  0.98\n",
      "  -0.04 -0.06  0.89  0.05  0.    0.94 -0.04 -0.    0.99  0.04  0.02  1.01\n",
      "  -0.01 -0.03  0.95  0.02  0.    0.97  0.02  0.01  0.98  0.02  0.01  0.99\n",
      "   0.09  0.03  0.96 -0.02 -0.01  0.97  0.08  0.01  0.95  0.03 -0.01  0.94]\n",
      " [-0.05  0.06  0.99  0.05  0.09  1.    0.    0.07  0.98  0.    0.08  0.96\n",
      "  -0.02  0.07  1.01  0.04  0.1   1.    0.03  0.11  1.   -0.02  0.09  1.\n",
      "  -0.01  0.09  0.95  0.03  0.1   0.99  0.04  0.1   1.01 -0.01  0.07  1.\n",
      "  -0.01  0.05  1.    0.01  0.11  1.    0.    0.07  1.04  0.02  0.11  0.96\n",
      "  -0.05  0.07  0.95 -0.01  0.1   1.   -0.07  0.12  0.99 -0.01  0.08  0.99]\n",
      " [ 0.04  0.02  1.01  0.02  0.01  0.98  0.02  0.01  0.98 -0.05 -0.01  0.99\n",
      "   0.08  0.01  0.95  0.08  0.02  0.98 -0.03 -0.01  0.97 -0.03 -0.    1.02\n",
      "   0.02  0.    0.98  0.02  0.01  0.98  0.03  0.01  0.98  0.09  0.02  0.98\n",
      "   0.07  0.01  0.95  0.    0.01  1.01  0.04 -0.01  0.93  0.04  0.07  1.08\n",
      "   0.02 -0.    0.98  0.02  0.    0.99 -0.04 -0.04  0.94 -0.05 -0.    1.01]\n",
      " [-0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98]\n",
      " [ 0.08  0.01  1.02  0.03  0.    1.02 -0.04 -0.02  0.95  0.08 -0.02  0.97\n",
      "   0.02  0.01  0.97  0.02 -0.    0.97  0.   -0.04  0.99 -0.05 -0.02  0.96\n",
      "   0.05 -0.    0.97 -0.02 -0.01  0.97  0.02  0.01  0.99  0.01  0.03  0.99\n",
      "   0.02 -0.    0.98  0.02 -0.    0.97 -0.01 -0.06  0.98  0.02  0.    1.\n",
      "   0.03  0.01  1.02  0.08  0.01  1.    0.04  0.03  0.99  0.02  0.    0.97]\n",
      " [-0.07 -0.01  0.98 -0.05 -0.02  0.98 -0.09 -0.02  0.98 -0.05 -0.    0.98\n",
      "  -0.06 -0.    0.99 -0.05 -0.02  0.98 -0.04  0.01  0.99 -0.07  0.01  0.98\n",
      "  -0.08  0.03  0.99 -0.05 -0.    0.99 -0.04 -0.03  0.97 -0.07 -0.02  0.98\n",
      "  -0.07  0.01  0.99 -0.06  0.03  0.99 -0.09  0.01  0.98 -0.06  0.    0.98\n",
      "  -0.02  0.01  1.   -0.07 -0.01  0.98 -0.05 -0.01  0.98 -0.02 -0.02  0.98]\n",
      " [ 0.03 -0.01  0.99  0.02 -0.    0.99 -0.   -0.03  1.    0.02  0.    1.\n",
      "   0.07  0.01  0.99  0.02 -0.01  0.94 -0.03 -0.03  0.98  0.03  0.04  0.97\n",
      "   0.02 -0.    0.98  0.02 -0.01  0.98  0.06 -0.04  0.98  0.03  0.    1.01\n",
      "  -0.02 -0.01  0.97  0.08  0.02  1.02  0.08  0.03  0.98  0.02 -0.02  0.99\n",
      "   0.02 -0.01  0.98  0.02 -0.01  0.98  0.06  0.01  1.    0.07  0.01  0.99]\n",
      " [-0.08  0.01  0.99 -0.07  0.    0.99 -0.07  0.01  0.99 -0.08  0.01  0.98\n",
      "  -0.06  0.02  0.98 -0.05 -0.    0.98 -0.06  0.02  0.99 -0.04 -0.02  0.98\n",
      "  -0.05  0.01  0.99 -0.03 -0.04  0.97 -0.06  0.02  0.99 -0.03 -0.04  0.99\n",
      "  -0.05  0.01  0.99 -0.05 -0.03  1.   -0.05  0.01  1.   -0.07  0.    0.98\n",
      "  -0.05 -0.02  0.98 -0.04 -0.03  0.97 -0.05  0.01  1.   -0.03 -0.05  0.98]\n",
      " [-0.05  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98\n",
      "  -0.05  0.04  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.03  0.02  0.98]\n",
      " [-0.01 -0.02  0.94  0.07 -0.02  0.98  0.02 -0.01  0.99  0.02 -0.01  0.98\n",
      "   0.03  0.01  0.99  0.03 -0.02  0.98 -0.04 -0.02  0.95  0.07  0.01  0.99\n",
      "  -0.02 -0.02  0.96 -0.02  0.01  1.    0.02  0.    0.98  0.02 -0.    0.98\n",
      "   0.01 -0.02  1.   -0.05 -0.02  0.96  0.05  0.    0.97 -0.02 -0.02  0.96\n",
      "   0.09  0.01  0.99  0.03 -0.03  0.98  0.02 -0.01  0.99  0.02 -0.01  0.98]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.01  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97]\n",
      " [-0.05  0.02  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.03  0.98 -0.04  0.01  0.98\n",
      "  -0.04  0.04  0.99 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98]\n",
      " [-0.06 -0.03  0.98 -0.06 -0.    0.98 -0.06  0.01  0.99 -0.05 -0.02  0.97\n",
      "  -0.06 -0.03  0.98 -0.05 -0.02  0.97 -0.05 -0.04  0.97 -0.05 -0.03  0.97\n",
      "  -0.05  0.01  1.   -0.04 -0.05  0.98 -0.05 -0.02  0.99 -0.05 -0.03  0.99\n",
      "  -0.05  0.01  0.99 -0.05 -0.02  0.99 -0.05  0.01  0.99 -0.07  0.01  0.99\n",
      "  -0.05  0.01  0.99 -0.05 -0.    0.98 -0.04 -0.03  0.98 -0.04 -0.01  0.98]\n",
      " [-0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.04  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98 -0.04  0.02  0.98]\n",
      " [-0.08 -0.01  0.98 -0.08 -0.03  0.97 -0.05  0.    0.98 -0.09  0.03  0.98\n",
      "  -0.07  0.01  0.98 -0.07 -0.01  0.98 -0.06  0.03  0.99 -0.06  0.01  0.99\n",
      "  -0.05  0.01  0.98 -0.03 -0.02  0.98 -0.04 -0.    0.98 -0.06  0.02  0.99\n",
      "  -0.01  0.01  0.99 -0.05  0.    0.98 -0.08 -0.01  0.98 -0.07 -0.    0.98\n",
      "  -0.06  0.01  0.99 -0.07  0.02  0.99 -0.03  0.03  1.   -0.05  0.    0.98]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.06 -0.02  0.98]\n",
      " [ 0.02 -0.01  1.    0.04  0.02  0.99 -0.   -0.02  0.94  0.03  0.01  1.03\n",
      "   0.07  0.01  1.01  0.06  0.01  0.97  0.02 -0.01  0.98  0.02 -0.01  0.97\n",
      "  -0.01 -0.04  1.01  0.09  0.01  1.01 -0.02 -0.01  0.97  0.02  0.    1.02\n",
      "   0.01 -0.03  0.98  0.02  0.02  0.99  0.02 -0.01  0.98  0.02 -0.    0.97\n",
      "   0.06  0.02  1.    0.09  0.01  1.01  0.07  0.01  1.01 -0.04 -0.02  0.94]\n",
      " [-0.05  0.02  0.98 -0.04  0.04  0.99 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.02  0.98 -0.03  0.03  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.04  0.03  0.99 -0.03  0.03  0.99 -0.04  0.01  0.98]\n",
      " [-0.04 -0.05  0.98 -0.05 -0.04  0.98 -0.05 -0.04  0.98 -0.05 -0.05  0.98\n",
      "  -0.06 -0.02  0.98 -0.05 -0.05  0.98 -0.07 -0.    0.99 -0.06 -0.02  0.98\n",
      "  -0.06 -0.    0.98 -0.06 -0.03  0.97 -0.05 -0.05  0.97 -0.06 -0.04  0.97\n",
      "  -0.04 -0.05  0.98 -0.06 -0.03  0.98 -0.04 -0.04  0.98 -0.06 -0.05  0.97\n",
      "  -0.06 -0.01  0.99 -0.06 -0.03  0.98 -0.07  0.01  0.99 -0.05 -0.02  0.98]\n",
      " [-0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.04  0.04  0.98 -0.03  0.01  0.98 -0.05  0.03  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98]\n",
      " [-0.05 -0.    0.99 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.99\n",
      "  -0.06 -0.01  0.98 -0.05 -0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.05 -0.    0.98 -0.06 -0.    0.98 -0.06 -0.    0.98 -0.06 -0.    0.98\n",
      "  -0.06  0.    0.99 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.06 -0.    0.98]\n",
      " [-0.05 -0.02  0.97 -0.05 -0.04  0.97 -0.05 -0.03  0.97 -0.05  0.01  1.\n",
      "  -0.04 -0.05  0.98 -0.05 -0.02  0.99 -0.05 -0.03  0.99 -0.05  0.01  0.99\n",
      "  -0.05 -0.02  0.99 -0.05  0.01  0.99 -0.07  0.01  0.99 -0.05  0.01  0.99\n",
      "  -0.05 -0.    0.98 -0.04 -0.03  0.98 -0.04 -0.01  0.98 -0.04 -0.04  0.98\n",
      "  -0.04 -0.05  0.98 -0.04 -0.05  0.98 -0.07 -0.02  0.99 -0.07  0.    0.99]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98]\n",
      " [-0.05 -0.05  0.98 -0.05 -0.03  0.99 -0.06 -0.04  0.98 -0.07  0.01  0.98\n",
      "  -0.06 -0.04  0.98 -0.07  0.01  0.98 -0.06 -0.01  0.98 -0.06 -0.    0.97\n",
      "  -0.06  0.01  0.99 -0.04 -0.05  0.98 -0.06  0.    0.99 -0.04 -0.04  0.99\n",
      "  -0.06 -0.04  0.97 -0.06 -0.01  1.   -0.05 -0.05  0.98 -0.04 -0.04  0.97\n",
      "  -0.05 -0.04  0.97 -0.04 -0.05  0.99 -0.05 -0.01  0.99 -0.06 -0.02  0.99]\n",
      " [-0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98]\n",
      " [-0.01 -0.02  0.94  0.08  0.    0.99  0.04  0.05  0.97  0.02 -0.02  0.99\n",
      "   0.02 -0.    0.99 -0.01 -0.07  0.98  0.06  0.    0.98 -0.03 -0.02  0.94\n",
      "   0.   -0.    1.    0.07 -0.01  0.98  0.02 -0.01  0.98  0.02 -0.01  0.98\n",
      "   0.03  0.01  0.96  0.01 -0.02  0.98  0.01 -0.    1.   -0.01 -0.02  0.94\n",
      "  -0.05 -0.02  0.96 -0.03 -0.03  1.    0.02  0.01  0.97  0.02  0.    0.97]\n",
      " [-0.07 -0.03  0.98 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.01  0.99 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.02  0.98 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98]\n",
      " [-0.01  0.09  0.98  0.02  0.15  0.96 -0.04  0.06  0.99 -0.05  0.08  0.95\n",
      "  -0.01  0.12  0.99 -0.01  0.06  1.02 -0.03  0.06  0.96  0.04  0.11  0.97\n",
      "   0.04  0.16  0.98  0.02  0.1   0.96  0.03  0.13  0.99  0.    0.1   0.95\n",
      "   0.05  0.12  1.04  0.03  0.12  0.97 -0.06  0.06  0.95 -0.    0.11  1.\n",
      "  -0.06  0.09  0.96 -0.    0.09  0.97  0.05  0.13  0.98 -0.01  0.08  0.98]\n",
      " [-0.01 -0.02  0.94  0.02 -0.02  0.97  0.05  0.04  0.97  0.02  0.    0.98\n",
      "   0.02  0.    0.97 -0.05 -0.01  0.99  0.06  0.    0.98 -0.   -0.02  0.93\n",
      "   0.01 -0.    1.01 -0.04 -0.03  0.98  0.02 -0.03  0.99  0.02  0.    0.99\n",
      "   0.02 -0.01  0.98  0.1   0.02  1.   -0.02 -0.02  0.94  0.07  0.01  1.\n",
      "   0.05 -0.    0.98  0.01 -0.05  0.98  0.02 -0.01  0.98  0.02 -0.    0.98]\n",
      " [-0.01  0.01  0.99 -0.05  0.    0.99 -0.05 -0.05  0.96 -0.07 -0.02  0.98\n",
      "  -0.07  0.01  0.99 -0.03  0.01  0.99 -0.07  0.01  0.99 -0.05 -0.    0.98\n",
      "  -0.05 -0.04  0.98 -0.07 -0.01  0.98 -0.05 -0.02  0.98 -0.09 -0.02  0.98\n",
      "  -0.05 -0.    0.98 -0.06 -0.    0.99 -0.05 -0.02  0.98 -0.04  0.01  0.99\n",
      "  -0.07  0.01  0.98 -0.08  0.03  0.99 -0.05 -0.    0.99 -0.04 -0.03  0.97]\n",
      " [ 0.03 -0.01  0.98  0.07  0.01  0.99  0.07  0.01  1.02  0.01 -0.    0.99\n",
      "   0.02 -0.04  0.98  0.02 -0.01  0.98  0.02 -0.01  0.97  0.02 -0.02  1.\n",
      "   0.07  0.01  1.    0.   -0.    1.    0.06  0.    0.97 -0.03 -0.01  0.97\n",
      "   0.07  0.02  0.97  0.02 -0.01  0.99  0.02 -0.    0.98  0.05  0.06  0.98\n",
      "   0.03 -0.01  0.97 -0.   -0.02  0.94 -0.03 -0.02  0.96 -0.04 -0.03  0.98]\n",
      " [-0.05 -0.    0.98 -0.04  0.02  0.99 -0.07 -0.01  0.98 -0.05 -0.02  0.98\n",
      "  -0.01 -0.01  0.98 -0.06 -0.    0.99 -0.05  0.01  0.98 -0.03 -0.01  0.98\n",
      "  -0.04  0.    0.99 -0.08 -0.01  0.98 -0.02 -0.03  0.98 -0.05 -0.    0.98\n",
      "  -0.09  0.01  0.99 -0.04 -0.01  0.98 -0.07  0.    0.98 -0.08 -0.02  0.98\n",
      "  -0.02 -0.01  0.98 -0.05 -0.    0.98 -0.09  0.02  0.98 -0.07 -0.02  0.98]\n",
      " [ 0.02  0.    0.98  0.04  0.02  1.01 -0.04 -0.    1.    0.02 -0.01  0.98\n",
      "  -0.04 -0.01  0.97  0.04 -0.    0.96  0.07  0.01  1.01  0.09  0.02  1.01\n",
      "   0.06  0.04  0.97  0.02 -0.01  0.98  0.02 -0.    0.98  0.01 -0.03  0.99\n",
      "  -0.03 -0.02  0.95  0.08  0.01  1.   -0.01 -0.02  0.94  0.08  0.02  0.99\n",
      "   0.04  0.01  0.97  0.02 -0.    0.99  0.02 -0.01  0.98 -0.01  0.03  0.99]\n",
      " [-0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.04  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.02  0.98 -0.05  0.02  0.98 -0.04  0.04  0.99\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98]\n",
      " [ 0.07  0.01  1.02  0.01 -0.    0.99  0.02 -0.04  0.98  0.02 -0.01  0.98\n",
      "   0.02 -0.01  0.97  0.02 -0.02  1.    0.07  0.01  1.    0.   -0.    1.\n",
      "   0.06  0.    0.97 -0.03 -0.01  0.97  0.07  0.02  0.97  0.02 -0.01  0.99\n",
      "   0.02 -0.    0.98  0.05  0.06  0.98  0.03 -0.01  0.97 -0.   -0.02  0.94\n",
      "  -0.03 -0.02  0.96 -0.04 -0.03  0.98  0.02 -0.03  0.99  0.02 -0.01  0.99]\n",
      " [-0.05 -0.    0.98 -0.05  0.    0.99 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05  0.    0.98\n",
      "  -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98 -0.05 -0.    0.98\n",
      "  -0.05  0.01  0.98 -0.06 -0.01  0.98 -0.05 -0.01  0.98 -0.06  0.01  0.98\n",
      "  -0.05 -0.    0.98 -0.05  0.    0.98 -0.05  0.    0.98 -0.05 -0.    0.98]\n",
      " [ 0.08  0.02  0.99 -0.03 -0.02  0.94  0.06  0.    0.97  0.05 -0.    0.98\n",
      "   0.05 -0.03  0.97  0.03 -0.02  0.99  0.02 -0.    0.99  0.03  0.03  0.97\n",
      "   0.07  0.01  1.01 -0.03 -0.02  0.95  0.04  0.01  1.03  0.05 -0.02  0.98\n",
      "   0.02 -0.06  0.98  0.02  0.01  0.98  0.02 -0.    0.98  0.08  0.03  0.98\n",
      "   0.01 -0.01  0.96  0.01 -0.02  0.94 -0.01 -0.02  0.94  0.09  0.02  0.99]\n",
      " [ 0.02 -0.01  0.98  0.03  0.04  0.99 -0.03 -0.01  0.97  0.   -0.02  0.94\n",
      "  -0.   -0.    1.01  0.09  0.02  1.    0.03 -0.02  0.98  0.02  0.01  0.98\n",
      "   0.02 -0.    0.99 -0.04 -0.03  0.99  0.01 -0.    1.    0.02  0.    1.02\n",
      "   0.04 -0.    0.97  0.04 -0.03  0.98  0.03  0.01  0.98  0.02 -0.    0.99\n",
      "   0.02 -0.    0.97 -0.03 -0.02  0.96 -0.03 -0.02  0.96  0.07  0.    0.98]\n",
      " [-0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.01  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.06 -0.01  0.99]\n",
      " [-0.03 -0.05  0.97 -0.06 -0.05  0.97 -0.05 -0.02  0.99 -0.05 -0.03  0.98\n",
      "  -0.07  0.02  0.99 -0.05 -0.03  0.98 -0.07  0.01  0.99 -0.04 -0.02  0.99\n",
      "  -0.04 -0.02  0.97 -0.06 -0.05  0.97 -0.04 -0.05  0.98 -0.05 -0.05  0.98\n",
      "  -0.05 -0.04  0.98 -0.05 -0.05  0.98 -0.06 -0.02  0.99 -0.06 -0.02  0.99\n",
      "  -0.06 -0.02  0.99 -0.07  0.01  0.98 -0.06 -0.02  0.98 -0.07  0.01  0.98]\n",
      " [-0.06  0.    0.98 -0.06 -0.05  0.97 -0.07  0.01  0.99 -0.05  0.01  1.\n",
      "  -0.04 -0.05  0.98 -0.06 -0.03  0.98 -0.04 -0.04  0.99 -0.06 -0.05  0.97\n",
      "  -0.04 -0.05  0.97 -0.05 -0.01  0.99 -0.04 -0.04  0.97 -0.05 -0.    0.99\n",
      "  -0.03 -0.05  0.97 -0.05 -0.04  0.98 -0.03 -0.05  0.98 -0.05 -0.05  0.97\n",
      "  -0.04 -0.05  0.98 -0.06 -0.05  0.98 -0.04 -0.04  0.98 -0.05 -0.05  0.98]\n",
      " [-0.01  0.09  0.99  0.02  0.09  1.01 -0.02  0.09  0.97  0.01  0.07  1.\n",
      "  -0.01  0.08  0.98 -0.01  0.12  1.   -0.01  0.09  0.95 -0.03  0.05  0.99\n",
      "  -0.    0.04  0.99  0.01  0.08  0.95  0.02  0.12  1.   -0.    0.09  1.02\n",
      "  -0.02  0.03  0.99  0.01  0.1   0.95 -0.05  0.07  0.95 -0.01  0.09  1.02\n",
      "  -0.08  0.09  0.99 -0.03  0.07  0.99  0.06  0.14  0.99 -0.01  0.11  1.01]\n",
      " [-0.04  0.    0.99 -0.03 -0.    0.98 -0.07 -0.02  0.98 -0.03  0.03  1.\n",
      "  -0.05 -0.    0.98 -0.03 -0.04  0.97 -0.06 -0.02  0.98 -0.05  0.02  0.99\n",
      "  -0.06 -0.03  0.98 -0.06  0.03  1.   -0.05 -0.    0.98 -0.08  0.02  0.99\n",
      "  -0.07  0.01  0.98 -0.04 -0.01  0.98 -0.09  0.02  0.98 -0.05  0.    0.98\n",
      "  -0.04 -0.01  0.98 -0.05 -0.02  0.97 -0.04  0.    0.98 -0.03  0.    0.99]\n",
      " [-0.06 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.99 -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.03  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.02  0.98 -0.07 -0.01  0.98]\n",
      " [-0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.06 -0.02  0.98 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.03  0.98 -0.06 -0.02  0.98 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.97\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.06 -0.02  0.98\n",
      "  -0.06 -0.02  0.99 -0.06 -0.02  0.98 -0.06 -0.02  0.99 -0.06 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.07 -0.    0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [ 0.06 -0.01  0.99  0.08  0.01  1.02  0.03  0.    1.02 -0.04 -0.02  0.95\n",
      "   0.08 -0.02  0.97  0.02  0.01  0.97  0.02 -0.    0.97  0.   -0.04  0.99\n",
      "  -0.05 -0.02  0.96  0.05 -0.    0.97 -0.02 -0.01  0.97  0.02  0.01  0.99\n",
      "   0.01  0.03  0.99  0.02 -0.    0.98  0.02 -0.    0.97 -0.01 -0.06  0.98\n",
      "   0.02  0.    1.    0.03  0.01  1.02  0.08  0.01  1.    0.04  0.03  0.99]\n",
      " [-0.05 -0.    0.98 -0.06 -0.02  0.98 -0.05 -0.01  0.98 -0.05  0.02  0.99\n",
      "  -0.01 -0.    0.99 -0.06  0.    0.98 -0.06 -0.02  0.99 -0.06  0.03  0.99\n",
      "  -0.07 -0.01  0.98 -0.06 -0.02  0.98 -0.09 -0.01  0.97 -0.06  0.    0.98\n",
      "  -0.05  0.05  1.   -0.04  0.    0.98 -0.07  0.01  0.98 -0.08  0.01  0.98\n",
      "  -0.06  0.02  0.99 -0.05  0.    0.98 -0.08  0.02  0.99 -0.06  0.02  0.98]\n",
      " [-0.1  -0.01  0.98 -0.05 -0.01  0.98 -0.07  0.    0.98 -0.03 -0.02  0.98\n",
      "  -0.03  0.02  0.99 -0.05  0.    0.98 -0.11  0.03  0.98 -0.08 -0.    0.98\n",
      "  -0.06 -0.02  0.98 -0.04  0.03  0.99 -0.06  0.01  0.99 -0.05  0.    0.98\n",
      "  -0.07 -0.02  0.98 -0.04  0.01  0.99 -0.06  0.01  0.99 -0.02  0.02  1.\n",
      "  -0.05  0.    0.98 -0.03 -0.04  0.97 -0.06 -0.02  0.98 -0.07 -0.01  0.98]\n",
      " [-0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98\n",
      "  -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.03  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.04  0.04  0.98 -0.04  0.01  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.04  0.98\n",
      "  -0.03  0.02  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98]\n",
      " [ 0.1   0.02  0.97  0.07  0.    0.93  0.02 -0.    0.98  0.02  0.    0.99\n",
      "   0.04 -0.    0.96  0.09  0.03  0.99 -0.03  0.    1.    0.07  0.02  0.99\n",
      "  -0.02 -0.02  0.96 -0.   -0.03  0.93  0.02  0.01  0.98  0.02  0.01  0.98\n",
      "   0.12  0.02  0.97  0.06  0.01  0.95  0.07  0.01  0.95 -0.02  0.01  1.02\n",
      "   0.03  0.05  1.07  0.03  0.02  1.    0.02  0.01  0.98  0.03  0.02  0.99]\n",
      " [-0.07 -0.01  0.98 -0.04  0.01  0.98 -0.04 -0.01  0.99 -0.06  0.04  0.99\n",
      "  -0.06  0.    0.99 -0.    0.    0.98 -0.05 -0.02  0.98 -0.05 -0.01  0.98\n",
      "  -0.08  0.02  0.99 -0.1   0.01  0.98 -0.05  0.    0.98 -0.03  0.03  1.\n",
      "  -0.05 -0.02  0.98 -0.07  0.01  0.99 -0.03  0.02  0.99 -0.05  0.01  0.99\n",
      "  -0.05  0.    0.98 -0.05 -0.03  0.98 -0.05 -0.01  0.98 -0.05  0.02  0.99]\n",
      " [-0.06 -0.01  0.98 -0.04 -0.04  0.99 -0.06 -0.03  0.97 -0.06 -0.    1.\n",
      "  -0.05  0.01  1.   -0.07  0.01  0.98 -0.05  0.    0.99 -0.04 -0.04  0.97\n",
      "  -0.05 -0.01  0.99 -0.03 -0.05  0.98 -0.06 -0.01  0.98 -0.06 -0.01  0.99\n",
      "  -0.05  0.01  0.99 -0.07  0.01  0.99 -0.05  0.    0.99 -0.04  0.01  0.98\n",
      "  -0.04 -0.02  0.98 -0.04 -0.02  0.98 -0.04 -0.02  0.98 -0.04 -0.02  0.98]\n",
      " [-0.07  0.01  0.99 -0.05 -0.02  0.99 -0.06 -0.    0.98 -0.05  0.01  1.\n",
      "  -0.05 -0.03  0.97 -0.05  0.    0.99 -0.04 -0.04  0.99 -0.05  0.01  0.99\n",
      "  -0.07  0.01  1.   -0.06 -0.05  0.97 -0.06 -0.    0.98 -0.05 -0.05  0.97\n",
      "  -0.04 -0.04  0.97 -0.05 -0.04  0.98 -0.05 -0.01  0.98 -0.05  0.01  0.99\n",
      "  -0.05 -0.    0.98 -0.04 -0.02  0.98 -0.05  0.    0.98 -0.04 -0.02  0.98]\n",
      " [-0.06 -0.04  0.97 -0.04 -0.04  0.97 -0.06 -0.05  0.97 -0.05 -0.03  0.99\n",
      "  -0.06 -0.    0.99 -0.07  0.01  0.99 -0.05 -0.03  0.98 -0.07  0.01  0.98\n",
      "  -0.05 -0.05  0.97 -0.04 -0.03  0.97 -0.05 -0.05  0.97 -0.04 -0.05  0.98\n",
      "  -0.06 -0.01  0.99 -0.06 -0.04  0.99 -0.06 -0.02  0.98 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.    0.99 -0.07  0.01  0.99 -0.06 -0.    0.98]\n",
      " [-0.05  0.02  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.98\n",
      "  -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98 -0.05  0.03  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.03  0.98 -0.03  0.03  0.98 -0.05  0.02  0.98\n",
      "  -0.03  0.03  0.98 -0.05  0.02  0.98 -0.04  0.04  0.99 -0.05  0.02  0.98\n",
      "  -0.04  0.04  0.98 -0.04  0.01  0.98 -0.05  0.04  0.98 -0.03  0.02  0.98]\n",
      " [-0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.06 -0.02  0.98 -0.07 -0.01  0.99\n",
      "  -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.99 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98 -0.07 -0.03  0.98]\n",
      " [-0.07 -0.03  0.98 -0.06 -0.03  0.98 -0.06 -0.03  0.98 -0.06 -0.02  0.98\n",
      "  -0.06 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.02  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.06 -0.01  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [ 0.02 -0.    0.99  0.02  0.    0.99  0.01 -0.03  0.97 -0.03 -0.02  0.95\n",
      "  -0.01 -0.02  0.94  0.08  0.01  1.02  0.07 -0.01  0.97  0.03  0.01  0.98\n",
      "   0.02  0.    0.99  0.01 -0.03  1.    0.09  0.01  1.01  0.08  0.01  1.02\n",
      "  -0.01 -0.02  0.94  0.07 -0.    0.99  0.02  0.05  0.99  0.02 -0.01  0.99\n",
      "   0.02 -0.    0.98  0.02  0.05  0.99  0.03  0.    1.    0.07  0.01  0.99]\n",
      " [-0.06  0.    0.98 -0.09 -0.01  0.98 -0.05  0.02  0.99 -0.05 -0.02  0.98\n",
      "  -0.06 -0.03  0.98 -0.06 -0.    0.99 -0.07  0.03  1.   -0.07 -0.01  0.98\n",
      "  -0.07 -0.01  0.98 -0.04 -0.02  0.98 -0.1  -0.01  0.97 -0.06 -0.    0.99\n",
      "  -0.02  0.04  1.   -0.04  0.02  0.99 -0.05 -0.01  0.98 -0.02  0.    0.99\n",
      "  -0.08 -0.01  0.98 -0.06 -0.    0.98 -0.05 -0.02  0.98 -0.04  0.01  0.99]\n",
      " [-0.05 -0.01  0.98 -0.05 -0.05  0.97 -0.05 -0.02  0.97 -0.04 -0.04  0.98\n",
      "  -0.03 -0.05  0.97 -0.04 -0.04  0.98 -0.04 -0.04  0.98 -0.05 -0.05  0.98\n",
      "  -0.06 -0.03  0.99 -0.07  0.01  0.99 -0.05 -0.    0.98 -0.05 -0.01  0.98\n",
      "  -0.04 -0.02  0.98 -0.04 -0.05  0.98 -0.04 -0.04  0.98 -0.05 -0.04  0.99\n",
      "  -0.05 -0.04  0.98 -0.06 -0.01  0.99 -0.05 -0.05  0.98 -0.07  0.    0.99]\n",
      " [-0.05 -0.01  0.99 -0.06  0.    0.98 -0.05  0.01  1.   -0.05 -0.02  0.97\n",
      "  -0.05 -0.01  0.99 -0.05 -0.02  0.99 -0.06 -0.04  0.97 -0.06 -0.01  0.99\n",
      "  -0.05 -0.02  0.98 -0.07  0.01  0.99 -0.05 -0.03  0.98 -0.06 -0.01  0.98\n",
      "  -0.04 -0.04  0.98 -0.03 -0.05  0.98 -0.06 -0.04  0.97 -0.05 -0.04  0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.02  0.99 -0.07  0.01  0.98 -0.06  0.01  0.99]\n",
      " [-0.05 -0.    0.98 -0.09  0.03  0.99 -0.05 -0.02  0.98 -0.04  0.01  0.99\n",
      "  -0.09  0.    0.98 -0.06  0.01  0.99 -0.05  0.    0.98 -0.08  0.02  0.99\n",
      "  -0.05 -0.01  0.98 -0.04  0.01  0.99 -0.08 -0.03  0.98 -0.06 -0.    0.98\n",
      "  -0.06  0.02  0.98 -0.06 -0.01  0.97 -0.06 -0.01  0.98 -0.07  0.01  0.98\n",
      "  -0.03  0.03  0.99 -0.05  0.    0.98 -0.01 -0.01  1.   -0.05  0.02  0.98]\n",
      " [-0.07 -0.03  0.98 -0.07 -0.02  0.98 -0.06 -0.01  0.99 -0.07 -0.01  0.99\n",
      "  -0.07 -0.01  0.98 -0.07 -0.02  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.99 -0.07 -0.01  0.99 -0.07 -0.02  0.98 -0.07 -0.02  0.98\n",
      "  -0.07 -0.03  0.98 -0.07 -0.03  0.97 -0.07 -0.03  0.97 -0.07 -0.03  0.98\n",
      "  -0.06 -0.02  0.98 -0.07 -0.02  0.99 -0.06 -0.01  0.99 -0.07 -0.01  0.99]\n",
      " [-0.04  0.06  1.   -0.02  0.04  1.    0.01  0.11  0.99  0.01  0.06  1.02\n",
      "   0.03  0.12  0.96 -0.05  0.11  0.95 -0.01  0.08  0.96 -0.08  0.07  0.98\n",
      "   0.01  0.09  0.98  0.04  0.15  0.98 -0.01  0.09  0.99  0.02  0.09  1.01\n",
      "  -0.02  0.09  0.97  0.01  0.07  1.   -0.01  0.08  0.98 -0.01  0.12  1.\n",
      "  -0.01  0.09  0.95 -0.03  0.05  0.99 -0.    0.04  0.99  0.01  0.08  0.95]\n",
      " [-0.07 -0.01  0.98 -0.07 -0.    0.99 -0.07  0.01  0.99 -0.06 -0.    0.98\n",
      "  -0.06  0.01  0.98 -0.06 -0.01  0.98 -0.05 -0.02  0.97 -0.06  0.02  0.99\n",
      "  -0.04 -0.04  0.97 -0.06  0.01  0.99 -0.04 -0.05  0.98 -0.06 -0.01  0.99\n",
      "  -0.06 -0.02  0.99 -0.06 -0.04  0.97 -0.07  0.01  0.99 -0.05  0.01  0.99\n",
      "  -0.04 -0.03  0.97 -0.05 -0.01  0.99 -0.03 -0.05  0.97 -0.06  0.01  0.99]\n",
      " [ 0.07  0.03  1.01  0.06 -0.02  0.88  0.02 -0.    0.97  0.02  0.01  0.98\n",
      "   0.02 -0.01  0.98  0.01 -0.01  0.95 -0.01 -0.01  0.96  0.05  0.02  1.\n",
      "  -0.06 -0.01  0.99  0.07  0.04  1.01  0.02  0.    0.99  0.02  0.01  0.98\n",
      "   0.01  0.03  1.03 -0.02 -0.01  0.96  0.08  0.01  0.95 -0.   -0.    0.96\n",
      "   0.04  0.04  1.03  0.01 -0.02  0.95  0.02  0.    0.99  0.02  0.    0.98]\n",
      " [ 0.03  0.08  0.99  0.03  0.1   0.96  0.01  0.09  1.   -0.01  0.08  1.01\n",
      "  -0.03  0.04  1.03 -0.04  0.07  0.97 -0.01  0.14  0.95 -0.    0.1   1.03\n",
      "  -0.03  0.12  0.94  0.02  0.1   0.98 -0.04  0.01  0.99 -0.01  0.07  0.93\n",
      "  -0.01  0.11  0.91  0.01  0.09  0.98  0.04  0.12  0.98  0.    0.07  0.97\n",
      "  -0.02  0.04  0.97  0.    0.09  1.    0.05  0.1   1.    0.    0.1   0.97]\n",
      " [ 0.01 -0.03  1.    0.07 -0.01  0.99 -0.03 -0.02  0.94  0.04  0.01  1.03\n",
      "   0.05  0.02  0.99 -0.    0.01  1.    0.02  0.01  0.98  0.02 -0.    0.98\n",
      "   0.06 -0.01  0.97  0.02 -0.01  0.96  0.02  0.    1.02  0.07  0.01  1.02\n",
      "   0.09  0.03  0.99  0.03  0.02  0.98  0.02 -0.    0.98  0.02 -0.01  0.99\n",
      "  -0.02 -0.02  0.98  0.07  0.01  1.    0.06  0.01  1.02 -0.01 -0.01  0.98]]\n"
     ]
    }
   ],
   "source": [
    "#Altrimenti il testSet viene troppo lungo e impossibile da caricare\n",
    "if choosenIndex == 1:\n",
    "    half = X_test.shape[0] // 2\n",
    "    X_test = X_test[:half, :]\n",
    "    y_test = y_test[:half]\n",
    "print(X_test.shape)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Spotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 42\n",
    "scoring = 'f1_macro'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "\n",
    "#models.append(('XGB', XGBClassifier(random_state=seed)))\n",
    "models.append(('GNB', GaussianNB(var_smoothing=2e-9)))\n",
    "models.append(('LR',  LogisticRegression(random_state=seed)))\n",
    "models.append(('CART' , DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('SVC' , SVC(gamma=0.5, random_state=seed)))\n",
    "models.append(('RF', RandomForestClassifier(random_state=seed, n_estimators = 50)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB - 1,00 0,00\n",
      "LR - 0,98 0,00\n",
      "CART - 0,97 0,01\n",
      "SVC - 0,99 0,00\n",
      "RF - 1,00 0,00\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    # Dividere dati in n = num_folds\n",
    "    kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = np.array([])\n",
    "    for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "        X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "        X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "        model.fit(X_cross_train, y_cross_train)  \n",
    "        y_pred = model.predict(X_cross_test)\n",
    "        f1s = f1_score(y_cross_test, y_pred, average=\"weighted\")\n",
    "        cv_results = np.append(cv_results, [f1s])\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #msg = \"%s - %f - %f\" % (name, cv_results.mean(), cv_results.std())\n",
    "    msg = \"{} - {:.2f} {:.2f}\".format(name, cv_results.mean(), cv_results.std()).replace('.', ',')\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "fig.suptitle('Algorithms Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione dei modelli sul Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model GNB: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       241\n",
      "           1       1.00      1.00      1.00       282\n",
      "           2       1.00      1.00      1.00       213\n",
      "           3       1.00      1.00      1.00       291\n",
      "           4       1.00      1.00      1.00       282\n",
      "           5       1.00      1.00      1.00       357\n",
      "\n",
      "    accuracy                           1.00      1666\n",
      "   macro avg       1.00      1.00      1.00      1666\n",
      "weighted avg       1.00      1.00      1.00      1666\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-bec7e512edbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mclassification_report_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-bec7e512edbf>\u001b[0m in \u001b[0;36mclassification_report_csv\u001b[0;34m(report, model_name)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrow_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'      '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "def classification_report_csv(report, model_name):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    index = 0\n",
    "    row = lines[-4].split('    ')\n",
    "    accuracy = row[-2]\n",
    "    for line in lines[2:-5]:\n",
    "        row = {}\n",
    "        row_data = line.split('      ')\n",
    "        row['class'] = uniques[index]\n",
    "        row['precision'] = float(row_data[2]) \n",
    "        row['recall'] = float(row_data[3]) \n",
    "        row['f1_score'] = float(row_data[4])\n",
    "        row['accuracy'] = accuracy\n",
    "        report_data.append(row)\n",
    "        index += 1\n",
    "    dataframe = pd.DataFrame.from_dict(report_data)\n",
    "    dataframe.to_csv(tasks[choosenIndex]+ '/classificationReports'+ '/'+'classification_report' + model_name +  '.csv', index = False)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train,  y_train)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    print(f\"Model {name}: \")\n",
    "    report = classification_report(y_test, pred_test)\n",
    "    print(report)\n",
    "    classification_report_csv(report, name)\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNetwork():\n",
    "    n = 50\n",
    "    model = Sequential(name=\"Sequential-NN\")\n",
    "    model.add(layers.Dense(X.shape[1], activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(layers.Dense(np.unique(y).size * n, activation='relu'))\n",
    "    model.add(layers.Dense(np.unique(y).size, activation='softmax'))\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 519us/step - loss: 1.6089 - accuracy: 0.2176\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 557us/step - loss: 1.5890 - accuracy: 0.3020\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 531us/step - loss: 1.5633 - accuracy: 0.3590\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 511us/step - loss: 1.5227 - accuracy: 0.4124\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 495us/step - loss: 1.4641 - accuracy: 0.4679\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 495us/step - loss: 1.3927 - accuracy: 0.4672\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 532us/step - loss: 1.3187 - accuracy: 0.5100\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 516us/step - loss: 1.2494 - accuracy: 0.5345\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 509us/step - loss: 1.1915 - accuracy: 0.5637\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 502us/step - loss: 1.1448 - accuracy: 0.5819\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 529us/step - loss: 1.1041 - accuracy: 0.5929\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 503us/step - loss: 1.0691 - accuracy: 0.6075\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 512us/step - loss: 1.0392 - accuracy: 0.6204\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 499us/step - loss: 1.0083 - accuracy: 0.6321\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 487us/step - loss: 0.9790 - accuracy: 0.6439\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 517us/step - loss: 0.9527 - accuracy: 0.6489\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 526us/step - loss: 0.9271 - accuracy: 0.6656\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 533us/step - loss: 0.9021 - accuracy: 0.6734\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 530us/step - loss: 0.8771 - accuracy: 0.6774\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 529us/step - loss: 0.8516 - accuracy: 0.6976\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 513us/step - loss: 0.8290 - accuracy: 0.7044\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 487us/step - loss: 0.8057 - accuracy: 0.7194\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 492us/step - loss: 0.7862 - accuracy: 0.7258\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 486us/step - loss: 0.7655 - accuracy: 0.7382\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 503us/step - loss: 0.7454 - accuracy: 0.7468\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 485us/step - loss: 0.7251 - accuracy: 0.7564\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 496us/step - loss: 0.7068 - accuracy: 0.7642\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 497us/step - loss: 0.6901 - accuracy: 0.7653\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 500us/step - loss: 0.6722 - accuracy: 0.7810\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 489us/step - loss: 0.6550 - accuracy: 0.7867\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 498us/step - loss: 0.6391 - accuracy: 0.7934\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 515us/step - loss: 0.6249 - accuracy: 0.7981\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 517us/step - loss: 0.6098 - accuracy: 0.7974\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 498us/step - loss: 0.5925 - accuracy: 0.8077\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 554us/step - loss: 0.5776 - accuracy: 0.8113\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 565us/step - loss: 0.5642 - accuracy: 0.8198\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 506us/step - loss: 0.5500 - accuracy: 0.8216\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 506us/step - loss: 0.5366 - accuracy: 0.8294\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 503us/step - loss: 0.5249 - accuracy: 0.8330\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 499us/step - loss: 0.5110 - accuracy: 0.8337\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 532us/step - loss: 0.5000 - accuracy: 0.8369\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 540us/step - loss: 0.4873 - accuracy: 0.8437\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 516us/step - loss: 0.4778 - accuracy: 0.8483\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 498us/step - loss: 0.4655 - accuracy: 0.8522\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 501us/step - loss: 0.4548 - accuracy: 0.8551\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 528us/step - loss: 0.4446 - accuracy: 0.8572\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 512us/step - loss: 0.4344 - accuracy: 0.8547\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 555us/step - loss: 0.4268 - accuracy: 0.8650\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 514us/step - loss: 0.4160 - accuracy: 0.8657\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 488us/step - loss: 0.4067 - accuracy: 0.8714\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 497us/step - loss: 0.4014 - accuracy: 0.8689\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 501us/step - loss: 0.3885 - accuracy: 0.8736\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 516us/step - loss: 0.3807 - accuracy: 0.8778\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 518us/step - loss: 0.3747 - accuracy: 0.8807\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 517us/step - loss: 0.3668 - accuracy: 0.8803\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 501us/step - loss: 0.3589 - accuracy: 0.8832\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 502us/step - loss: 0.3526 - accuracy: 0.8907\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 561us/step - loss: 0.3456 - accuracy: 0.8857\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 535us/step - loss: 0.3380 - accuracy: 0.8878\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 517us/step - loss: 0.3341 - accuracy: 0.8892\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 504us/step - loss: 0.3258 - accuracy: 0.8935\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 568us/step - loss: 0.3189 - accuracy: 0.8917\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 613us/step - loss: 0.3150 - accuracy: 0.8971\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 533us/step - loss: 0.3099 - accuracy: 0.9031\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 567us/step - loss: 0.3036 - accuracy: 0.9049\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 546us/step - loss: 0.2999 - accuracy: 0.8999\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 564us/step - loss: 0.2919 - accuracy: 0.9078\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 535us/step - loss: 0.2896 - accuracy: 0.9060\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 544us/step - loss: 0.2827 - accuracy: 0.9138\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 564us/step - loss: 0.2756 - accuracy: 0.9142\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 522us/step - loss: 0.2728 - accuracy: 0.9085\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 521us/step - loss: 0.2690 - accuracy: 0.9163\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 509us/step - loss: 0.2620 - accuracy: 0.9149\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 531us/step - loss: 0.2597 - accuracy: 0.9149\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 517us/step - loss: 0.2541 - accuracy: 0.9149\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 501us/step - loss: 0.2513 - accuracy: 0.9170\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 523us/step - loss: 0.2468 - accuracy: 0.9224\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 547us/step - loss: 0.2431 - accuracy: 0.9241\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 556us/step - loss: 0.2382 - accuracy: 0.9241\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 507us/step - loss: 0.2355 - accuracy: 0.9266\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 513us/step - loss: 0.2305 - accuracy: 0.9302\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 505us/step - loss: 0.2258 - accuracy: 0.9327\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 501us/step - loss: 0.2250 - accuracy: 0.9288\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 532us/step - loss: 0.2203 - accuracy: 0.9334\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 547us/step - loss: 0.2156 - accuracy: 0.9295\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 530us/step - loss: 0.2134 - accuracy: 0.9309\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 494us/step - loss: 0.2098 - accuracy: 0.9341\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 485us/step - loss: 0.2064 - accuracy: 0.9345\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 511us/step - loss: 0.2034 - accuracy: 0.9391\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 514us/step - loss: 0.1981 - accuracy: 0.9391\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 510us/step - loss: 0.1958 - accuracy: 0.9391\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 495us/step - loss: 0.1931 - accuracy: 0.9420\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 544us/step - loss: 0.1911 - accuracy: 0.9437\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 558us/step - loss: 0.1887 - accuracy: 0.9473\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 540us/step - loss: 0.1848 - accuracy: 0.9427\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 568us/step - loss: 0.1811 - accuracy: 0.9473\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 576us/step - loss: 0.1799 - accuracy: 0.9466\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 600us/step - loss: 0.1768 - accuracy: 0.9480\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 617us/step - loss: 0.1747 - accuracy: 0.9501\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 614us/step - loss: 0.1720 - accuracy: 0.9466\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 766us/step - loss: 1.6077 - accuracy: 0.2183\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 602us/step - loss: 1.5729 - accuracy: 0.3376\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 566us/step - loss: 1.5239 - accuracy: 0.3711\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 616us/step - loss: 1.4604 - accuracy: 0.4021\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 544us/step - loss: 1.3883 - accuracy: 0.4505\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 542us/step - loss: 1.3178 - accuracy: 0.4662\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 535us/step - loss: 1.2540 - accuracy: 0.5110\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 536us/step - loss: 1.2013 - accuracy: 0.5321\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 482us/step - loss: 1.1594 - accuracy: 0.5442\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 499us/step - loss: 1.1220 - accuracy: 0.5580\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 552us/step - loss: 1.0913 - accuracy: 0.5759\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 572us/step - loss: 1.0628 - accuracy: 0.5687\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 577us/step - loss: 1.0382 - accuracy: 0.5894\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 572us/step - loss: 1.0158 - accuracy: 0.6129\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 580us/step - loss: 0.9903 - accuracy: 0.6243\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 582us/step - loss: 0.9687 - accuracy: 0.6375\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 571us/step - loss: 0.9477 - accuracy: 0.6389\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 567us/step - loss: 0.9286 - accuracy: 0.6414\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 567us/step - loss: 0.9094 - accuracy: 0.6627\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 539us/step - loss: 0.8911 - accuracy: 0.6731\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 566us/step - loss: 0.8707 - accuracy: 0.6766\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 590us/step - loss: 0.8530 - accuracy: 0.6923\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 520us/step - loss: 0.8357 - accuracy: 0.7087\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 540us/step - loss: 0.8196 - accuracy: 0.7137\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 556us/step - loss: 0.8039 - accuracy: 0.7222\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 520us/step - loss: 0.7884 - accuracy: 0.7219\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 525us/step - loss: 0.7728 - accuracy: 0.7343\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 531us/step - loss: 0.7585 - accuracy: 0.7368\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 498us/step - loss: 0.7465 - accuracy: 0.7468\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 524us/step - loss: 0.7321 - accuracy: 0.7550\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 504us/step - loss: 0.7189 - accuracy: 0.7546\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 517us/step - loss: 0.7065 - accuracy: 0.7593\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 499us/step - loss: 0.6966 - accuracy: 0.7682\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 494us/step - loss: 0.6842 - accuracy: 0.7724\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 660us/step - loss: 0.6720 - accuracy: 0.7756\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 575us/step - loss: 0.6611 - accuracy: 0.7838\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 539us/step - loss: 0.6511 - accuracy: 0.7835\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 777us/step - loss: 0.6403 - accuracy: 0.7949\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 593us/step - loss: 0.6320 - accuracy: 0.7863\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 585us/step - loss: 0.6202 - accuracy: 0.7988\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 618us/step - loss: 0.6103 - accuracy: 0.7963\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 596us/step - loss: 0.6032 - accuracy: 0.8048\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 605us/step - loss: 0.5933 - accuracy: 0.8027\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 617us/step - loss: 0.5830 - accuracy: 0.8073\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 600us/step - loss: 0.5739 - accuracy: 0.8141\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 598us/step - loss: 0.5647 - accuracy: 0.8180\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 590us/step - loss: 0.5580 - accuracy: 0.8234\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 564us/step - loss: 0.5454 - accuracy: 0.8152\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 568us/step - loss: 0.5382 - accuracy: 0.8291\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 534us/step - loss: 0.5294 - accuracy: 0.8298\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 643us/step - loss: 0.5231 - accuracy: 0.8344\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 563us/step - loss: 0.5152 - accuracy: 0.8308\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 566us/step - loss: 0.5062 - accuracy: 0.8365\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 611us/step - loss: 0.5000 - accuracy: 0.8390\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 621us/step - loss: 0.4920 - accuracy: 0.8394\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 530us/step - loss: 0.4848 - accuracy: 0.8390\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 527us/step - loss: 0.4760 - accuracy: 0.8483\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 527us/step - loss: 0.4695 - accuracy: 0.8465\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 633us/step - loss: 0.4617 - accuracy: 0.8494\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 666us/step - loss: 0.4555 - accuracy: 0.8494\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 588us/step - loss: 0.4452 - accuracy: 0.8590\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 545us/step - loss: 0.4397 - accuracy: 0.8600\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 558us/step - loss: 0.4332 - accuracy: 0.8611\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 525us/step - loss: 0.4236 - accuracy: 0.8643\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 520us/step - loss: 0.4177 - accuracy: 0.8654\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 505us/step - loss: 0.4108 - accuracy: 0.8679\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 513us/step - loss: 0.4045 - accuracy: 0.8768\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 499us/step - loss: 0.3983 - accuracy: 0.8750\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 492us/step - loss: 0.3898 - accuracy: 0.8793\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 498us/step - loss: 0.3823 - accuracy: 0.8800\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 513us/step - loss: 0.3787 - accuracy: 0.8807\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 548us/step - loss: 0.3721 - accuracy: 0.8889\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 552us/step - loss: 0.3621 - accuracy: 0.8871\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 518us/step - loss: 0.3587 - accuracy: 0.8853\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 506us/step - loss: 0.3510 - accuracy: 0.8896\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 497us/step - loss: 0.3463 - accuracy: 0.8903\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.3394 - accuracy: 0.8949\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 481us/step - loss: 0.3309 - accuracy: 0.9010\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 486us/step - loss: 0.3269 - accuracy: 0.8989\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 503us/step - loss: 0.3191 - accuracy: 0.9042\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 453us/step - loss: 0.3129 - accuracy: 0.9046\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 0.3072 - accuracy: 0.9049\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 499us/step - loss: 0.3026 - accuracy: 0.9088\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 522us/step - loss: 0.2947 - accuracy: 0.9095\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 497us/step - loss: 0.2907 - accuracy: 0.9142\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 470us/step - loss: 0.2856 - accuracy: 0.9174\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 488us/step - loss: 0.2801 - accuracy: 0.9124\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.2734 - accuracy: 0.9184\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 523us/step - loss: 0.2689 - accuracy: 0.9209\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 541us/step - loss: 0.2638 - accuracy: 0.9220\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 512us/step - loss: 0.2608 - accuracy: 0.9252\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 501us/step - loss: 0.2541 - accuracy: 0.9209\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 533us/step - loss: 0.2504 - accuracy: 0.9284\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 498us/step - loss: 0.2452 - accuracy: 0.9270\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 483us/step - loss: 0.2415 - accuracy: 0.9245\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 491us/step - loss: 0.2360 - accuracy: 0.9313\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 525us/step - loss: 0.2337 - accuracy: 0.9320\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 519us/step - loss: 0.2303 - accuracy: 0.9323\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 519us/step - loss: 0.2251 - accuracy: 0.9345\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 490us/step - loss: 0.2197 - accuracy: 0.9402\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 478us/step - loss: 1.6032 - accuracy: 0.2472\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 527us/step - loss: 1.5757 - accuracy: 0.3462\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 537us/step - loss: 1.5380 - accuracy: 0.4088\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 470us/step - loss: 1.4807 - accuracy: 0.4387\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 1.4052 - accuracy: 0.4573\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 525us/step - loss: 1.3253 - accuracy: 0.5242\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 506us/step - loss: 1.2499 - accuracy: 0.5627\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 548us/step - loss: 1.1892 - accuracy: 0.5890\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 561us/step - loss: 1.1373 - accuracy: 0.6086\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 604us/step - loss: 1.0936 - accuracy: 0.6332\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 598us/step - loss: 1.0559 - accuracy: 0.6610\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 613us/step - loss: 1.0202 - accuracy: 0.6770\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 578us/step - loss: 0.9880 - accuracy: 0.6802\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 539us/step - loss: 0.9584 - accuracy: 0.6941\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 487us/step - loss: 0.9289 - accuracy: 0.7058\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 519us/step - loss: 0.8990 - accuracy: 0.7169\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 0.8747 - accuracy: 0.7390\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 534us/step - loss: 0.8510 - accuracy: 0.7407\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 558us/step - loss: 0.8279 - accuracy: 0.7379\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 564us/step - loss: 0.8067 - accuracy: 0.7411\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 551us/step - loss: 0.7882 - accuracy: 0.7493\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 518us/step - loss: 0.7692 - accuracy: 0.7582\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 531us/step - loss: 0.7530 - accuracy: 0.7646\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 547us/step - loss: 0.7350 - accuracy: 0.7660\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 552us/step - loss: 0.7192 - accuracy: 0.7771\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 0.7055 - accuracy: 0.7817\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 493us/step - loss: 0.6924 - accuracy: 0.7788\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 534us/step - loss: 0.6786 - accuracy: 0.7849\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 553us/step - loss: 0.6672 - accuracy: 0.7877\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 481us/step - loss: 0.6547 - accuracy: 0.7899\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 453us/step - loss: 0.6428 - accuracy: 0.7934\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 513us/step - loss: 0.6323 - accuracy: 0.7895\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 455us/step - loss: 0.6200 - accuracy: 0.7949\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 457us/step - loss: 0.6094 - accuracy: 0.8027\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 575us/step - loss: 0.6011 - accuracy: 0.8002\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 524us/step - loss: 0.5912 - accuracy: 0.8066\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 456us/step - loss: 0.5816 - accuracy: 0.8073\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 0.5709 - accuracy: 0.8145\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 0.5622 - accuracy: 0.8198\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 546us/step - loss: 0.5511 - accuracy: 0.8244\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 535us/step - loss: 0.5437 - accuracy: 0.8248\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 507us/step - loss: 0.5343 - accuracy: 0.8283\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 521us/step - loss: 0.5230 - accuracy: 0.8319\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 528us/step - loss: 0.5166 - accuracy: 0.8383\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 533us/step - loss: 0.5083 - accuracy: 0.8408\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 0.4990 - accuracy: 0.8429\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.4894 - accuracy: 0.8533\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 456us/step - loss: 0.4803 - accuracy: 0.8486\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 0.4739 - accuracy: 0.8593\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 0.4659 - accuracy: 0.8558\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 479us/step - loss: 0.4590 - accuracy: 0.8590\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 0.4502 - accuracy: 0.8654\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 455us/step - loss: 0.4416 - accuracy: 0.8675\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 0.4347 - accuracy: 0.8661\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 485us/step - loss: 0.4296 - accuracy: 0.8700\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.4243 - accuracy: 0.8725\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 457us/step - loss: 0.4145 - accuracy: 0.8757\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.4096 - accuracy: 0.8761\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 481us/step - loss: 0.4032 - accuracy: 0.8807\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.3991 - accuracy: 0.8803\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 456us/step - loss: 0.3899 - accuracy: 0.8843\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 484us/step - loss: 0.3869 - accuracy: 0.8821\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 489us/step - loss: 0.3793 - accuracy: 0.8843\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 481us/step - loss: 0.3724 - accuracy: 0.8935\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 511us/step - loss: 0.3689 - accuracy: 0.8914\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 470us/step - loss: 0.3627 - accuracy: 0.8942\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 492us/step - loss: 0.3585 - accuracy: 0.8900\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.3521 - accuracy: 0.8978\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 456us/step - loss: 0.3505 - accuracy: 0.8935\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.3457 - accuracy: 0.8989\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 479us/step - loss: 0.3391 - accuracy: 0.9021\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 481us/step - loss: 0.3349 - accuracy: 0.9003\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 0.3306 - accuracy: 0.8989\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 512us/step - loss: 0.3250 - accuracy: 0.9035\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 502us/step - loss: 0.3210 - accuracy: 0.9063\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 0.3162 - accuracy: 0.9092\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 457us/step - loss: 0.3114 - accuracy: 0.9106\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 0.3085 - accuracy: 0.9110\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 0.3030 - accuracy: 0.9106\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 480us/step - loss: 0.3014 - accuracy: 0.9103\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 0.2971 - accuracy: 0.9106\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 470us/step - loss: 0.2921 - accuracy: 0.9106\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 458us/step - loss: 0.2877 - accuracy: 0.9177\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.2836 - accuracy: 0.9167\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 0.2805 - accuracy: 0.9170\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 495us/step - loss: 0.2792 - accuracy: 0.9156\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.2736 - accuracy: 0.9167\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 0.2713 - accuracy: 0.9181\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 0.2693 - accuracy: 0.9177\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 469us/step - loss: 0.2660 - accuracy: 0.9174\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 0.2617 - accuracy: 0.9256\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 474us/step - loss: 0.2595 - accuracy: 0.9199\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 479us/step - loss: 0.2563 - accuracy: 0.9252\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 484us/step - loss: 0.2533 - accuracy: 0.9241\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.2496 - accuracy: 0.9249\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 475us/step - loss: 0.2467 - accuracy: 0.9249\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 0.2457 - accuracy: 0.9231\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.2422 - accuracy: 0.9259\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 480us/step - loss: 0.2402 - accuracy: 0.9274\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 0.2362 - accuracy: 0.9284\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_51 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 482us/step - loss: 1.6037 - accuracy: 0.2632\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 1.5711 - accuracy: 0.3415\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 1.5268 - accuracy: 0.3764\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 1.4658 - accuracy: 0.4384\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 1.3888 - accuracy: 0.4630\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 530us/step - loss: 1.3149 - accuracy: 0.4957\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 527us/step - loss: 1.2449 - accuracy: 0.5288\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 465us/step - loss: 1.1873 - accuracy: 0.5705\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 1.1412 - accuracy: 0.5890\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 1.1011 - accuracy: 0.6154\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 1.0645 - accuracy: 0.6332\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 1.0343 - accuracy: 0.6481\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 482us/step - loss: 1.0024 - accuracy: 0.6617\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 496us/step - loss: 0.9748 - accuracy: 0.6745\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 560us/step - loss: 0.9456 - accuracy: 0.6848\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 558us/step - loss: 0.9158 - accuracy: 0.6952\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 541us/step - loss: 0.8853 - accuracy: 0.7090\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 512us/step - loss: 0.8555 - accuracy: 0.7215\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 507us/step - loss: 0.8306 - accuracy: 0.7343\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 496us/step - loss: 0.8036 - accuracy: 0.7393\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 0.7768 - accuracy: 0.7582\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.7508 - accuracy: 0.7632\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 516us/step - loss: 0.7271 - accuracy: 0.7678\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 488us/step - loss: 0.7025 - accuracy: 0.7753\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 0.6813 - accuracy: 0.7874\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 520us/step - loss: 0.6595 - accuracy: 0.7913\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 482us/step - loss: 0.6392 - accuracy: 0.7942\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 464us/step - loss: 0.6189 - accuracy: 0.8045\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 465us/step - loss: 0.6011 - accuracy: 0.8077\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 0.5826 - accuracy: 0.8159\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.5639 - accuracy: 0.8180\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 523us/step - loss: 0.5476 - accuracy: 0.8276\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 0.5312 - accuracy: 0.8312\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.5171 - accuracy: 0.8308\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 464us/step - loss: 0.5007 - accuracy: 0.8422\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 529us/step - loss: 0.4881 - accuracy: 0.8426\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 499us/step - loss: 0.4743 - accuracy: 0.8511\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 457us/step - loss: 0.4609 - accuracy: 0.8526\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 469us/step - loss: 0.4480 - accuracy: 0.8604\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 541us/step - loss: 0.4368 - accuracy: 0.8625\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 515us/step - loss: 0.4278 - accuracy: 0.8647\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.4167 - accuracy: 0.8697\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.4037 - accuracy: 0.8750\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 495us/step - loss: 0.3959 - accuracy: 0.8782\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 488us/step - loss: 0.3889 - accuracy: 0.8764\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 469us/step - loss: 0.3759 - accuracy: 0.8839\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 0.3666 - accuracy: 0.8868\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 454us/step - loss: 0.3588 - accuracy: 0.8896\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 0.3505 - accuracy: 0.8882\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.3413 - accuracy: 0.8910\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 496us/step - loss: 0.3347 - accuracy: 0.8910\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.3273 - accuracy: 0.8921\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.3205 - accuracy: 0.8971\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.3147 - accuracy: 0.9006\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 506us/step - loss: 0.3077 - accuracy: 0.9014\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 0.3003 - accuracy: 0.9031\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.2960 - accuracy: 0.9063\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 455us/step - loss: 0.2886 - accuracy: 0.9106\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 0.2825 - accuracy: 0.9110\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 482us/step - loss: 0.2782 - accuracy: 0.9113\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 479us/step - loss: 0.2714 - accuracy: 0.9149\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 486us/step - loss: 0.2687 - accuracy: 0.9131\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 568us/step - loss: 0.2624 - accuracy: 0.9177\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 0.2600 - accuracy: 0.9167\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.2543 - accuracy: 0.9192\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.2506 - accuracy: 0.9245\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 0.2468 - accuracy: 0.9209\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 484us/step - loss: 0.2417 - accuracy: 0.9238\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 0.2386 - accuracy: 0.9249\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 0.2353 - accuracy: 0.9270\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 485us/step - loss: 0.2300 - accuracy: 0.9277\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.2269 - accuracy: 0.9306\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 495us/step - loss: 0.2221 - accuracy: 0.9327\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 529us/step - loss: 0.2208 - accuracy: 0.9281\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 550us/step - loss: 0.2189 - accuracy: 0.9306\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 555us/step - loss: 0.2142 - accuracy: 0.9320\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 0.2112 - accuracy: 0.9338\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 507us/step - loss: 0.2071 - accuracy: 0.9355\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 553us/step - loss: 0.2044 - accuracy: 0.9373\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 506us/step - loss: 0.2024 - accuracy: 0.9387\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 492us/step - loss: 0.1979 - accuracy: 0.9409\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 505us/step - loss: 0.1966 - accuracy: 0.9416\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 508us/step - loss: 0.1925 - accuracy: 0.9423\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 497us/step - loss: 0.1912 - accuracy: 0.9430\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 506us/step - loss: 0.1881 - accuracy: 0.9412\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 491us/step - loss: 0.1853 - accuracy: 0.9405\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 497us/step - loss: 0.1823 - accuracy: 0.9448\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 481us/step - loss: 0.1797 - accuracy: 0.9494\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 490us/step - loss: 0.1783 - accuracy: 0.9441\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 500us/step - loss: 0.1757 - accuracy: 0.9437\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 590us/step - loss: 0.1749 - accuracy: 0.9473\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 493us/step - loss: 0.1728 - accuracy: 0.9462\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 504us/step - loss: 0.1713 - accuracy: 0.9455\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 497us/step - loss: 0.1682 - accuracy: 0.9501\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 475us/step - loss: 0.1654 - accuracy: 0.9473\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 520us/step - loss: 0.1651 - accuracy: 0.9494\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 520us/step - loss: 0.1622 - accuracy: 0.9480\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 508us/step - loss: 0.1587 - accuracy: 0.9526\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 505us/step - loss: 0.1576 - accuracy: 0.9533\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 518us/step - loss: 0.1553 - accuracy: 0.9537\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_54 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 1.6035 - accuracy: 0.2375\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 1.5753 - accuracy: 0.3412\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 483us/step - loss: 1.5364 - accuracy: 0.3704\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 499us/step - loss: 1.4811 - accuracy: 0.4063\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 1.4080 - accuracy: 0.4291\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 488us/step - loss: 1.3283 - accuracy: 0.4647\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 1.2600 - accuracy: 0.4918\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 1.2064 - accuracy: 0.5164\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 1.1618 - accuracy: 0.5434\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 483us/step - loss: 1.1264 - accuracy: 0.5641\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 489us/step - loss: 1.0907 - accuracy: 0.5776\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 456us/step - loss: 1.0634 - accuracy: 0.5894\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 1.0362 - accuracy: 0.6068\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 482us/step - loss: 1.0131 - accuracy: 0.6143\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 0.9885 - accuracy: 0.6254\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 0.9668 - accuracy: 0.6229\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 0.9473 - accuracy: 0.6474\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.9270 - accuracy: 0.6645\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 470us/step - loss: 0.9047 - accuracy: 0.6642\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 469us/step - loss: 0.8848 - accuracy: 0.6930\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.8648 - accuracy: 0.6827\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 481us/step - loss: 0.8469 - accuracy: 0.7172\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 483us/step - loss: 0.8267 - accuracy: 0.7151\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 455us/step - loss: 0.8093 - accuracy: 0.7236\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 514us/step - loss: 0.7912 - accuracy: 0.7333\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 505us/step - loss: 0.7734 - accuracy: 0.7372\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 458us/step - loss: 0.7574 - accuracy: 0.7518\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.7429 - accuracy: 0.7543\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 490us/step - loss: 0.7255 - accuracy: 0.7621\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 494us/step - loss: 0.7100 - accuracy: 0.7696\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 0.6951 - accuracy: 0.7753\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 0.6804 - accuracy: 0.7756\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 487us/step - loss: 0.6643 - accuracy: 0.7842\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 515us/step - loss: 0.6529 - accuracy: 0.7877\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 0.6386 - accuracy: 0.7959\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 481us/step - loss: 0.6239 - accuracy: 0.8020\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 0.6115 - accuracy: 0.8070\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 491us/step - loss: 0.5990 - accuracy: 0.8116\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 0.5883 - accuracy: 0.8180\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 464us/step - loss: 0.5771 - accuracy: 0.8162\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 0.5641 - accuracy: 0.8248\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.5530 - accuracy: 0.8291\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 495us/step - loss: 0.5410 - accuracy: 0.8298\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 480us/step - loss: 0.5296 - accuracy: 0.8348\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 464us/step - loss: 0.5195 - accuracy: 0.8348\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.5091 - accuracy: 0.8426\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 465us/step - loss: 0.4990 - accuracy: 0.8447\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 479us/step - loss: 0.4877 - accuracy: 0.8504\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 567us/step - loss: 0.4796 - accuracy: 0.8522\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 508us/step - loss: 0.4709 - accuracy: 0.8558\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 558us/step - loss: 0.4597 - accuracy: 0.8593\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 573us/step - loss: 0.4533 - accuracy: 0.8604\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 496us/step - loss: 0.4426 - accuracy: 0.8600\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 511us/step - loss: 0.4352 - accuracy: 0.8668\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 595us/step - loss: 0.4291 - accuracy: 0.8704\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 556us/step - loss: 0.4243 - accuracy: 0.8597\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 524us/step - loss: 0.4137 - accuracy: 0.8750\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 503us/step - loss: 0.4069 - accuracy: 0.8711\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 511us/step - loss: 0.4010 - accuracy: 0.8768\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 517us/step - loss: 0.3954 - accuracy: 0.8768\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 515us/step - loss: 0.3901 - accuracy: 0.8750\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 512us/step - loss: 0.3857 - accuracy: 0.8775\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 513us/step - loss: 0.3778 - accuracy: 0.8800\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 594us/step - loss: 0.3734 - accuracy: 0.8839\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 563us/step - loss: 0.3692 - accuracy: 0.8871\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 505us/step - loss: 0.3615 - accuracy: 0.8846\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 517us/step - loss: 0.3562 - accuracy: 0.8900\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 529us/step - loss: 0.3528 - accuracy: 0.8875\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 556us/step - loss: 0.3498 - accuracy: 0.8850\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 493us/step - loss: 0.3446 - accuracy: 0.8885\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 0.3395 - accuracy: 0.8928\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 498us/step - loss: 0.3347 - accuracy: 0.8921\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 0.3288 - accuracy: 0.8992\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 479us/step - loss: 0.3285 - accuracy: 0.8942\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 0.3238 - accuracy: 0.8992\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 526us/step - loss: 0.3233 - accuracy: 0.8939\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 527us/step - loss: 0.3144 - accuracy: 0.9028\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 0.3133 - accuracy: 0.9003\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 494us/step - loss: 0.3081 - accuracy: 0.9017\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 456us/step - loss: 0.3071 - accuracy: 0.9021\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.3027 - accuracy: 0.9074\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 517us/step - loss: 0.3007 - accuracy: 0.9063\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 0.2950 - accuracy: 0.9056\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 0.2931 - accuracy: 0.9071\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 464us/step - loss: 0.2903 - accuracy: 0.9060\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 0.2873 - accuracy: 0.9081\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 456us/step - loss: 0.2845 - accuracy: 0.9124\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 456us/step - loss: 0.2825 - accuracy: 0.9078\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 455us/step - loss: 0.2785 - accuracy: 0.9103\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 455us/step - loss: 0.2742 - accuracy: 0.9192\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 458us/step - loss: 0.2716 - accuracy: 0.9106\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 0.2694 - accuracy: 0.9224\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 0.2663 - accuracy: 0.9142\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.2651 - accuracy: 0.9149\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 475us/step - loss: 0.2622 - accuracy: 0.9177\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 482us/step - loss: 0.2596 - accuracy: 0.9192\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 495us/step - loss: 0.2590 - accuracy: 0.9181\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 481us/step - loss: 0.2524 - accuracy: 0.9224\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 458us/step - loss: 0.2558 - accuracy: 0.9177\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 482us/step - loss: 0.2490 - accuracy: 0.9206\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 541us/step - loss: 1.6057 - accuracy: 0.2340\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 515us/step - loss: 1.5816 - accuracy: 0.3134\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 551us/step - loss: 1.5482 - accuracy: 0.3586\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 492us/step - loss: 1.4991 - accuracy: 0.4060\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 475us/step - loss: 1.4308 - accuracy: 0.4491\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 494us/step - loss: 1.3529 - accuracy: 0.4950\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 487us/step - loss: 1.2779 - accuracy: 0.5281\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 500us/step - loss: 1.2135 - accuracy: 0.5467\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 525us/step - loss: 1.1613 - accuracy: 0.5698\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 505us/step - loss: 1.1179 - accuracy: 0.5801\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 494us/step - loss: 1.0819 - accuracy: 0.5976\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 516us/step - loss: 1.0485 - accuracy: 0.6022\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 1.0191 - accuracy: 0.6129\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 0.9923 - accuracy: 0.6222\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 470us/step - loss: 0.9654 - accuracy: 0.6296\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 470us/step - loss: 0.9381 - accuracy: 0.6524\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.9134 - accuracy: 0.6695\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 0.8908 - accuracy: 0.6667\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 0.8663 - accuracy: 0.6991\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 0.8420 - accuracy: 0.6994\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.8216 - accuracy: 0.7155\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 0.7993 - accuracy: 0.7315\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 464us/step - loss: 0.7802 - accuracy: 0.7358\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 478us/step - loss: 0.7581 - accuracy: 0.7461\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 506us/step - loss: 0.7391 - accuracy: 0.7610\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.7174 - accuracy: 0.7678\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 575us/step - loss: 0.7007 - accuracy: 0.7703\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 667us/step - loss: 0.6829 - accuracy: 0.7817\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 556us/step - loss: 0.6641 - accuracy: 0.7856\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 550us/step - loss: 0.6477 - accuracy: 0.7952\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 511us/step - loss: 0.6298 - accuracy: 0.7991\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 478us/step - loss: 0.6137 - accuracy: 0.8070\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 483us/step - loss: 0.5976 - accuracy: 0.8155\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 524us/step - loss: 0.5834 - accuracy: 0.8148\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 525us/step - loss: 0.5680 - accuracy: 0.8209\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 500us/step - loss: 0.5551 - accuracy: 0.8291\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 536us/step - loss: 0.5402 - accuracy: 0.8301\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 498us/step - loss: 0.5270 - accuracy: 0.8351\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 0.5159 - accuracy: 0.8383\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 0.5027 - accuracy: 0.8476\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 498us/step - loss: 0.4907 - accuracy: 0.8483\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 0.4798 - accuracy: 0.8490\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 0.4679 - accuracy: 0.8561\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 465us/step - loss: 0.4559 - accuracy: 0.8554\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 0.4454 - accuracy: 0.8657\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 0.4358 - accuracy: 0.8693\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 491us/step - loss: 0.4229 - accuracy: 0.8689\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 520us/step - loss: 0.4158 - accuracy: 0.8711\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 485us/step - loss: 0.4076 - accuracy: 0.8711\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 500us/step - loss: 0.3959 - accuracy: 0.8761\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 0.3883 - accuracy: 0.8832\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 586us/step - loss: 0.3783 - accuracy: 0.8821\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 510us/step - loss: 0.3712 - accuracy: 0.8832\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 493us/step - loss: 0.3618 - accuracy: 0.8871\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 501us/step - loss: 0.3564 - accuracy: 0.8803\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 0.3485 - accuracy: 0.8868\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 497us/step - loss: 0.3414 - accuracy: 0.8892\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 456us/step - loss: 0.3357 - accuracy: 0.8910\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 0.3276 - accuracy: 0.8960\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 475us/step - loss: 0.3209 - accuracy: 0.8942\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 515us/step - loss: 0.3133 - accuracy: 0.8964\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 487us/step - loss: 0.3091 - accuracy: 0.8971\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 515us/step - loss: 0.3046 - accuracy: 0.8978\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 490us/step - loss: 0.2978 - accuracy: 0.8992\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 464us/step - loss: 0.2918 - accuracy: 0.9046\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 508us/step - loss: 0.2878 - accuracy: 0.9046\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 481us/step - loss: 0.2816 - accuracy: 0.9010\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.2762 - accuracy: 0.9028\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 521us/step - loss: 0.2728 - accuracy: 0.9088\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 508us/step - loss: 0.2685 - accuracy: 0.9053\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 526us/step - loss: 0.2619 - accuracy: 0.9103\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 497us/step - loss: 0.2603 - accuracy: 0.9103\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 514us/step - loss: 0.2551 - accuracy: 0.9110\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 489us/step - loss: 0.2506 - accuracy: 0.9127\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 464us/step - loss: 0.2462 - accuracy: 0.9188\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 0.2427 - accuracy: 0.9192\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 469us/step - loss: 0.2400 - accuracy: 0.9202\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 470us/step - loss: 0.2331 - accuracy: 0.9188\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 458us/step - loss: 0.2284 - accuracy: 0.9227\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 469us/step - loss: 0.2263 - accuracy: 0.9281\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 0.2234 - accuracy: 0.9195\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 482us/step - loss: 0.2216 - accuracy: 0.9217\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 496us/step - loss: 0.2164 - accuracy: 0.9270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 484us/step - loss: 0.2135 - accuracy: 0.9241\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 483us/step - loss: 0.2104 - accuracy: 0.9298\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 458us/step - loss: 0.2078 - accuracy: 0.9266\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 485us/step - loss: 0.2029 - accuracy: 0.9323\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 0.2005 - accuracy: 0.9327\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.1970 - accuracy: 0.9380\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 0.1960 - accuracy: 0.9366\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 0.1923 - accuracy: 0.9323\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 480us/step - loss: 0.1899 - accuracy: 0.9363\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 492us/step - loss: 0.1869 - accuracy: 0.9341\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.1828 - accuracy: 0.9387\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 488us/step - loss: 0.1806 - accuracy: 0.9409\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 478us/step - loss: 0.1766 - accuracy: 0.9444\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 470us/step - loss: 0.1745 - accuracy: 0.9412\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 478us/step - loss: 0.1731 - accuracy: 0.9398\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 489us/step - loss: 0.1689 - accuracy: 0.9444\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 480us/step - loss: 0.1686 - accuracy: 0.9469\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_60 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 1.6059 - accuracy: 0.2365\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 481us/step - loss: 1.5745 - accuracy: 0.3376\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 1.5345 - accuracy: 0.3860\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 469us/step - loss: 1.4723 - accuracy: 0.4252\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 1.3993 - accuracy: 0.4576\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 1.3226 - accuracy: 0.4719\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 457us/step - loss: 1.2576 - accuracy: 0.5082\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 1.2054 - accuracy: 0.5331\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 481us/step - loss: 1.1636 - accuracy: 0.5509\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 1.1293 - accuracy: 0.5819\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 1.0999 - accuracy: 0.5833\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 484us/step - loss: 1.0730 - accuracy: 0.6097\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 456us/step - loss: 1.0488 - accuracy: 0.6001\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 484us/step - loss: 1.0260 - accuracy: 0.6150\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 1.0028 - accuracy: 0.6307\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 493us/step - loss: 0.9814 - accuracy: 0.6328\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 470us/step - loss: 0.9604 - accuracy: 0.6599\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 0.9376 - accuracy: 0.6642\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 519us/step - loss: 0.9185 - accuracy: 0.6692\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 516us/step - loss: 0.8958 - accuracy: 0.6806\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 0.8749 - accuracy: 0.7041\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 457us/step - loss: 0.8556 - accuracy: 0.7026\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 469us/step - loss: 0.8355 - accuracy: 0.7155\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 454us/step - loss: 0.8124 - accuracy: 0.7147\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 454us/step - loss: 0.7956 - accuracy: 0.7325\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 454us/step - loss: 0.7772 - accuracy: 0.7333\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 0.7566 - accuracy: 0.7368\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 453us/step - loss: 0.7390 - accuracy: 0.7504\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 454us/step - loss: 0.7239 - accuracy: 0.7543\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 456us/step - loss: 0.7069 - accuracy: 0.7561\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 465us/step - loss: 0.6925 - accuracy: 0.7610\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 455us/step - loss: 0.6767 - accuracy: 0.7682\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 457us/step - loss: 0.6622 - accuracy: 0.7664\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 455us/step - loss: 0.6494 - accuracy: 0.7717\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 479us/step - loss: 0.6345 - accuracy: 0.7781\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 480us/step - loss: 0.6227 - accuracy: 0.7817\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 465us/step - loss: 0.6095 - accuracy: 0.7874\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 457us/step - loss: 0.5968 - accuracy: 0.7888\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 475us/step - loss: 0.5872 - accuracy: 0.7892\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 457us/step - loss: 0.5747 - accuracy: 0.7974\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 491us/step - loss: 0.5639 - accuracy: 0.8009\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 530us/step - loss: 0.5529 - accuracy: 0.8038\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 482us/step - loss: 0.5432 - accuracy: 0.8098\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 453us/step - loss: 0.5332 - accuracy: 0.8148\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 501us/step - loss: 0.5222 - accuracy: 0.8184\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 521us/step - loss: 0.5132 - accuracy: 0.8202\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 493us/step - loss: 0.5051 - accuracy: 0.8223\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 0.4956 - accuracy: 0.8266\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 478us/step - loss: 0.4863 - accuracy: 0.8269\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 0.4770 - accuracy: 0.8312\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.4690 - accuracy: 0.8362\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 483us/step - loss: 0.4599 - accuracy: 0.8437\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 470us/step - loss: 0.4511 - accuracy: 0.8465\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 458us/step - loss: 0.4447 - accuracy: 0.8444\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 452us/step - loss: 0.4373 - accuracy: 0.8494\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 485us/step - loss: 0.4290 - accuracy: 0.8558\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 469us/step - loss: 0.4216 - accuracy: 0.8533\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 455us/step - loss: 0.4139 - accuracy: 0.8554\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.4076 - accuracy: 0.8579\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 457us/step - loss: 0.4013 - accuracy: 0.8625\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 465us/step - loss: 0.3946 - accuracy: 0.8643\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.3886 - accuracy: 0.8672\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 449us/step - loss: 0.3813 - accuracy: 0.8697\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 465us/step - loss: 0.3757 - accuracy: 0.8661\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 513us/step - loss: 0.3681 - accuracy: 0.8739\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 453us/step - loss: 0.3636 - accuracy: 0.8761\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 453us/step - loss: 0.3587 - accuracy: 0.8796\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.3534 - accuracy: 0.8828\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 457us/step - loss: 0.3457 - accuracy: 0.8875\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 0.3397 - accuracy: 0.8857\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 450us/step - loss: 0.3347 - accuracy: 0.8868\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 496us/step - loss: 0.3298 - accuracy: 0.8932\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.3267 - accuracy: 0.8917\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.3189 - accuracy: 0.8960\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 465us/step - loss: 0.3150 - accuracy: 0.8910\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 475us/step - loss: 0.3102 - accuracy: 0.9003\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.3053 - accuracy: 0.8971\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.3005 - accuracy: 0.8978\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.2951 - accuracy: 0.9031\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 470us/step - loss: 0.2912 - accuracy: 0.9021\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 490us/step - loss: 0.2863 - accuracy: 0.9063\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 454us/step - loss: 0.2829 - accuracy: 0.9103\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 0.2785 - accuracy: 0.9106\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 451us/step - loss: 0.2744 - accuracy: 0.9124\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.2693 - accuracy: 0.9174\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 455us/step - loss: 0.2663 - accuracy: 0.9145\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 0.2616 - accuracy: 0.9167\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 496us/step - loss: 0.2590 - accuracy: 0.9209\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.2541 - accuracy: 0.9224\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.2505 - accuracy: 0.9188\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 487us/step - loss: 0.2471 - accuracy: 0.9209\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 0.2429 - accuracy: 0.9256\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 492us/step - loss: 0.2393 - accuracy: 0.9281\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 482us/step - loss: 0.2354 - accuracy: 0.9295\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 0.2330 - accuracy: 0.9284\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 479us/step - loss: 0.2285 - accuracy: 0.9291\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.2274 - accuracy: 0.9320\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 485us/step - loss: 0.2233 - accuracy: 0.9323\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 475us/step - loss: 0.2185 - accuracy: 0.9320\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.2162 - accuracy: 0.9341\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_63 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 1.6094 - accuracy: 0.2525\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 1.5645 - accuracy: 0.3451\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 464us/step - loss: 1.5089 - accuracy: 0.3704\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 492us/step - loss: 1.4302 - accuracy: 0.4199\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 1.3468 - accuracy: 0.4758\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 458us/step - loss: 1.2707 - accuracy: 0.5153\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 1.2087 - accuracy: 0.5395\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 1.1592 - accuracy: 0.5541\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 1.1209 - accuracy: 0.5798\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 1.0840 - accuracy: 0.6011\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 496us/step - loss: 1.0514 - accuracy: 0.6264\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 530us/step - loss: 1.0231 - accuracy: 0.6421\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 635us/step - loss: 0.9911 - accuracy: 0.6339\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 518us/step - loss: 0.9613 - accuracy: 0.6624\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 527us/step - loss: 0.9331 - accuracy: 0.6827\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 546us/step - loss: 0.9050 - accuracy: 0.6866\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 642us/step - loss: 0.8778 - accuracy: 0.7087\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 516us/step - loss: 0.8498 - accuracy: 0.7123\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 500us/step - loss: 0.8215 - accuracy: 0.7407\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 503us/step - loss: 0.7930 - accuracy: 0.7379\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 483us/step - loss: 0.7687 - accuracy: 0.7650\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.7436 - accuracy: 0.7714\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 495us/step - loss: 0.7173 - accuracy: 0.8002\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 0.6931 - accuracy: 0.7860\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 455us/step - loss: 0.6713 - accuracy: 0.8038\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 499us/step - loss: 0.6482 - accuracy: 0.8170\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 484us/step - loss: 0.6276 - accuracy: 0.8198\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 626us/step - loss: 0.6048 - accuracy: 0.8319\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 507us/step - loss: 0.5858 - accuracy: 0.8362\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 497us/step - loss: 0.5664 - accuracy: 0.8426\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 521us/step - loss: 0.5477 - accuracy: 0.8458\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 488us/step - loss: 0.5309 - accuracy: 0.8536\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 503us/step - loss: 0.5137 - accuracy: 0.8558\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 475us/step - loss: 0.4989 - accuracy: 0.8579\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 491us/step - loss: 0.4837 - accuracy: 0.8718\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 513us/step - loss: 0.4682 - accuracy: 0.8707\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 497us/step - loss: 0.4548 - accuracy: 0.8746\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 457us/step - loss: 0.4405 - accuracy: 0.8786\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 0.4282 - accuracy: 0.8786\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 500us/step - loss: 0.4162 - accuracy: 0.8821\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 454us/step - loss: 0.4040 - accuracy: 0.8850\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 495us/step - loss: 0.3933 - accuracy: 0.8889\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 589us/step - loss: 0.3839 - accuracy: 0.8882\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 516us/step - loss: 0.3718 - accuracy: 0.8942\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 0.3637 - accuracy: 0.8996\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 502us/step - loss: 0.3558 - accuracy: 0.8935\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 464us/step - loss: 0.3450 - accuracy: 0.8992\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 495us/step - loss: 0.3363 - accuracy: 0.9006\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 483us/step - loss: 0.3287 - accuracy: 0.9046\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 491us/step - loss: 0.3180 - accuracy: 0.9056\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 485us/step - loss: 0.3119 - accuracy: 0.9071\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.3042 - accuracy: 0.9120\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.2946 - accuracy: 0.9124\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 488us/step - loss: 0.2913 - accuracy: 0.9127\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 486us/step - loss: 0.2820 - accuracy: 0.9181\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 465us/step - loss: 0.2784 - accuracy: 0.9170\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 491us/step - loss: 0.2714 - accuracy: 0.9188\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 0.2629 - accuracy: 0.9206\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 478us/step - loss: 0.2574 - accuracy: 0.9209\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 0.2514 - accuracy: 0.9249\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 489us/step - loss: 0.2476 - accuracy: 0.9263\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.2398 - accuracy: 0.9291\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 494us/step - loss: 0.2347 - accuracy: 0.9298\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 458us/step - loss: 0.2304 - accuracy: 0.9302\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 454us/step - loss: 0.2256 - accuracy: 0.9313\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.2191 - accuracy: 0.9355\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 455us/step - loss: 0.2150 - accuracy: 0.9359\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.2098 - accuracy: 0.9380\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.2051 - accuracy: 0.9402\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 0.1999 - accuracy: 0.9384\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 480us/step - loss: 0.1975 - accuracy: 0.9395\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 0.1932 - accuracy: 0.9448\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 0.1869 - accuracy: 0.9462\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 457us/step - loss: 0.1852 - accuracy: 0.9441\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 453us/step - loss: 0.1789 - accuracy: 0.9480\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 470us/step - loss: 0.1758 - accuracy: 0.9487\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 458us/step - loss: 0.1719 - accuracy: 0.9512\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 493us/step - loss: 0.1678 - accuracy: 0.9519\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 489us/step - loss: 0.1641 - accuracy: 0.9544\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 0.1615 - accuracy: 0.9551\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.1586 - accuracy: 0.9569\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.1553 - accuracy: 0.9562\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 479us/step - loss: 0.1516 - accuracy: 0.9576\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 576us/step - loss: 0.1474 - accuracy: 0.9601\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 503us/step - loss: 0.1456 - accuracy: 0.9583\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 563us/step - loss: 0.1429 - accuracy: 0.9623\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 553us/step - loss: 0.1394 - accuracy: 0.9623\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 486us/step - loss: 0.1389 - accuracy: 0.9598\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 502us/step - loss: 0.1344 - accuracy: 0.9633\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 607us/step - loss: 0.1303 - accuracy: 0.9637\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 532us/step - loss: 0.1288 - accuracy: 0.9647\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 0.1266 - accuracy: 0.9647\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 492us/step - loss: 0.1222 - accuracy: 0.9683\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 507us/step - loss: 0.1232 - accuracy: 0.9687\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 505us/step - loss: 0.1208 - accuracy: 0.9701\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 580us/step - loss: 0.1173 - accuracy: 0.9683\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 575us/step - loss: 0.1130 - accuracy: 0.9719\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 518us/step - loss: 0.1117 - accuracy: 0.9701\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 552us/step - loss: 0.1113 - accuracy: 0.9701\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 565us/step - loss: 0.1075 - accuracy: 0.9708\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_66 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 471us/step - loss: 1.5995 - accuracy: 0.2464\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 491us/step - loss: 1.5685 - accuracy: 0.3725\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 1.5252 - accuracy: 0.4099\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 503us/step - loss: 1.4619 - accuracy: 0.4366\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 507us/step - loss: 1.3845 - accuracy: 0.4726\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 506us/step - loss: 1.3065 - accuracy: 0.5071\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 518us/step - loss: 1.2383 - accuracy: 0.5477\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 499us/step - loss: 1.1846 - accuracy: 0.5734\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 1.1412 - accuracy: 0.5969\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 1.1065 - accuracy: 0.5890\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 464us/step - loss: 1.0770 - accuracy: 0.6093\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 1.0460 - accuracy: 0.6232\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 1.0201 - accuracy: 0.6300\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 457us/step - loss: 0.9935 - accuracy: 0.6524\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 483us/step - loss: 0.9691 - accuracy: 0.6567\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 486us/step - loss: 0.9444 - accuracy: 0.6784\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 496us/step - loss: 0.9182 - accuracy: 0.6834\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 494us/step - loss: 0.8979 - accuracy: 0.6984\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 491us/step - loss: 0.8721 - accuracy: 0.6976\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 485us/step - loss: 0.8496 - accuracy: 0.7222\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 480us/step - loss: 0.8255 - accuracy: 0.7283\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 464us/step - loss: 0.8045 - accuracy: 0.7304\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 0.7826 - accuracy: 0.7486\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 584us/step - loss: 0.7597 - accuracy: 0.7571\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 555us/step - loss: 0.7403 - accuracy: 0.7667\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 500us/step - loss: 0.7206 - accuracy: 0.7717\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 475us/step - loss: 0.7017 - accuracy: 0.7721\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 495us/step - loss: 0.6822 - accuracy: 0.7799\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 520us/step - loss: 0.6666 - accuracy: 0.7874\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.6501 - accuracy: 0.7949\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 469us/step - loss: 0.6361 - accuracy: 0.7991\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 516us/step - loss: 0.6182 - accuracy: 0.8020\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 485us/step - loss: 0.6062 - accuracy: 0.8070\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.5880 - accuracy: 0.8155\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 464us/step - loss: 0.5762 - accuracy: 0.8159\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 0.5600 - accuracy: 0.8234\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 525us/step - loss: 0.5485 - accuracy: 0.8244\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 542us/step - loss: 0.5344 - accuracy: 0.8294\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 567us/step - loss: 0.5219 - accuracy: 0.8312\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 478us/step - loss: 0.5129 - accuracy: 0.8351\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 0.4985 - accuracy: 0.8429\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 0.4885 - accuracy: 0.8422\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 469us/step - loss: 0.4772 - accuracy: 0.8540\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 496us/step - loss: 0.4652 - accuracy: 0.8540\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 490us/step - loss: 0.4577 - accuracy: 0.8554\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 479us/step - loss: 0.4454 - accuracy: 0.8643\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 0.4351 - accuracy: 0.8654\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 0.4260 - accuracy: 0.8675\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 0.4178 - accuracy: 0.8714\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 487us/step - loss: 0.4080 - accuracy: 0.8704\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 0.3989 - accuracy: 0.8739\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 0.3896 - accuracy: 0.8839\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 0.3826 - accuracy: 0.8778\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 496us/step - loss: 0.3755 - accuracy: 0.8796\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 0.3663 - accuracy: 0.8892\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.3601 - accuracy: 0.8871\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 0.3532 - accuracy: 0.8914\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.3452 - accuracy: 0.8949\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 0.3391 - accuracy: 0.8942\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 491us/step - loss: 0.3313 - accuracy: 0.8978\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.3252 - accuracy: 0.9021\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 0.3204 - accuracy: 0.9035\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 0.3135 - accuracy: 0.9049\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.3056 - accuracy: 0.9074\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.3015 - accuracy: 0.9056\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.2959 - accuracy: 0.9071\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.2890 - accuracy: 0.9135\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 482us/step - loss: 0.2841 - accuracy: 0.9110\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.2801 - accuracy: 0.9142\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.2744 - accuracy: 0.9156\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 464us/step - loss: 0.2682 - accuracy: 0.9177\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 0.2638 - accuracy: 0.9192\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 484us/step - loss: 0.2585 - accuracy: 0.9209\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 482us/step - loss: 0.2527 - accuracy: 0.9206\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 0.2495 - accuracy: 0.9227\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 0.2449 - accuracy: 0.9227\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 0.2416 - accuracy: 0.9238\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 456us/step - loss: 0.2365 - accuracy: 0.9281\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.2324 - accuracy: 0.9266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 457us/step - loss: 0.2285 - accuracy: 0.9313\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.2226 - accuracy: 0.9363\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.2197 - accuracy: 0.9330\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 484us/step - loss: 0.2146 - accuracy: 0.9373\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 481us/step - loss: 0.2116 - accuracy: 0.9359\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 0.2082 - accuracy: 0.9377\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 517us/step - loss: 0.2041 - accuracy: 0.9352\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 0.2007 - accuracy: 0.9377\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.1985 - accuracy: 0.9420\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.1950 - accuracy: 0.9423\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.1899 - accuracy: 0.9434\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 485us/step - loss: 0.1856 - accuracy: 0.9444\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 0.1851 - accuracy: 0.9452\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.1799 - accuracy: 0.9444\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 475us/step - loss: 0.1777 - accuracy: 0.9498\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.1748 - accuracy: 0.9466\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 584us/step - loss: 0.1716 - accuracy: 0.9516\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 526us/step - loss: 0.1708 - accuracy: 0.9473\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 592us/step - loss: 0.1668 - accuracy: 0.9551\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 506us/step - loss: 0.1633 - accuracy: 0.9530\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 568us/step - loss: 0.1627 - accuracy: 0.9498\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_69 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 524us/step - loss: 1.6075 - accuracy: 0.2361\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 590us/step - loss: 1.5852 - accuracy: 0.3155\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 527us/step - loss: 1.5524 - accuracy: 0.3750\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 473us/step - loss: 1.5029 - accuracy: 0.4220\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 512us/step - loss: 1.4385 - accuracy: 0.4533\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 517us/step - loss: 1.3662 - accuracy: 0.4975\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 585us/step - loss: 1.2950 - accuracy: 0.5175\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 595us/step - loss: 1.2322 - accuracy: 0.5420\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 524us/step - loss: 1.1821 - accuracy: 0.5527\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 511us/step - loss: 1.1405 - accuracy: 0.5783\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 569us/step - loss: 1.1037 - accuracy: 0.5783\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 507us/step - loss: 1.0727 - accuracy: 0.5812\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 519us/step - loss: 1.0417 - accuracy: 0.6065\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 478us/step - loss: 1.0141 - accuracy: 0.6125\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 480us/step - loss: 0.9895 - accuracy: 0.6218\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 512us/step - loss: 0.9613 - accuracy: 0.6417\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 515us/step - loss: 0.9386 - accuracy: 0.6471\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.9143 - accuracy: 0.6528\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.8914 - accuracy: 0.6681\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 494us/step - loss: 0.8702 - accuracy: 0.6756\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.8509 - accuracy: 0.6816\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 470us/step - loss: 0.8313 - accuracy: 0.7009\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 0.8125 - accuracy: 0.7001\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 475us/step - loss: 0.7953 - accuracy: 0.7151\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 455us/step - loss: 0.7773 - accuracy: 0.7244\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.7584 - accuracy: 0.7361\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 480us/step - loss: 0.7420 - accuracy: 0.7361\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 482us/step - loss: 0.7285 - accuracy: 0.7550\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 532us/step - loss: 0.7130 - accuracy: 0.7532\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 490us/step - loss: 0.6989 - accuracy: 0.7632\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 488us/step - loss: 0.6827 - accuracy: 0.7678\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 481us/step - loss: 0.6701 - accuracy: 0.7710\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 0.6572 - accuracy: 0.7714\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 515us/step - loss: 0.6452 - accuracy: 0.7803\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 533us/step - loss: 0.6304 - accuracy: 0.7970\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 527us/step - loss: 0.6170 - accuracy: 0.7924\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 506us/step - loss: 0.6047 - accuracy: 0.7977\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 499us/step - loss: 0.5942 - accuracy: 0.7999\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 475us/step - loss: 0.5808 - accuracy: 0.8120\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.5690 - accuracy: 0.8088\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 594us/step - loss: 0.5581 - accuracy: 0.8194\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 501us/step - loss: 0.5454 - accuracy: 0.8312\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 523us/step - loss: 0.5353 - accuracy: 0.8291\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 498us/step - loss: 0.5237 - accuracy: 0.8308\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 489us/step - loss: 0.5133 - accuracy: 0.8365\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 485us/step - loss: 0.5025 - accuracy: 0.8380\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 488us/step - loss: 0.4930 - accuracy: 0.8451\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 484us/step - loss: 0.4828 - accuracy: 0.8469\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.4724 - accuracy: 0.8472\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 0.4625 - accuracy: 0.8561\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 482us/step - loss: 0.4537 - accuracy: 0.8533\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 526us/step - loss: 0.4437 - accuracy: 0.8657\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.4338 - accuracy: 0.8647\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 485us/step - loss: 0.4273 - accuracy: 0.8686\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 487us/step - loss: 0.4163 - accuracy: 0.8714\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 464us/step - loss: 0.4077 - accuracy: 0.8732\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 474us/step - loss: 0.3988 - accuracy: 0.8782\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 499us/step - loss: 0.3916 - accuracy: 0.8818\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 532us/step - loss: 0.3836 - accuracy: 0.8807\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 595us/step - loss: 0.3762 - accuracy: 0.8832\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 511us/step - loss: 0.3686 - accuracy: 0.8853\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 0.3601 - accuracy: 0.8903\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 0.3543 - accuracy: 0.8914\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 0.3481 - accuracy: 0.8953\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 470us/step - loss: 0.3396 - accuracy: 0.8957\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 476us/step - loss: 0.3327 - accuracy: 0.8996\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 485us/step - loss: 0.3241 - accuracy: 0.9021\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 478us/step - loss: 0.3215 - accuracy: 0.9049\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 465us/step - loss: 0.3149 - accuracy: 0.9060\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 472us/step - loss: 0.3090 - accuracy: 0.9113\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 0.2993 - accuracy: 0.9113\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 468us/step - loss: 0.2945 - accuracy: 0.9149\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.2899 - accuracy: 0.9170\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 485us/step - loss: 0.2828 - accuracy: 0.9184\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 477us/step - loss: 0.2790 - accuracy: 0.9202\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 458us/step - loss: 0.2730 - accuracy: 0.9206\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 0.2691 - accuracy: 0.9192\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 0.2629 - accuracy: 0.9224\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 466us/step - loss: 0.2572 - accuracy: 0.9259\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.2523 - accuracy: 0.9327\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 460us/step - loss: 0.2463 - accuracy: 0.9302\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.2414 - accuracy: 0.9288\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.2386 - accuracy: 0.9298\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.2324 - accuracy: 0.9341\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 463us/step - loss: 0.2278 - accuracy: 0.9352\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 458us/step - loss: 0.2239 - accuracy: 0.9391\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 457us/step - loss: 0.2213 - accuracy: 0.9380\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 0.2163 - accuracy: 0.9355\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 0.2120 - accuracy: 0.9398\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 458us/step - loss: 0.2075 - accuracy: 0.9412\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 467us/step - loss: 0.2056 - accuracy: 0.9409\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.1991 - accuracy: 0.9448\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 0.1955 - accuracy: 0.9452\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.1934 - accuracy: 0.9473\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 462us/step - loss: 0.1884 - accuracy: 0.9498\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 461us/step - loss: 0.1861 - accuracy: 0.9526\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 459us/step - loss: 0.1802 - accuracy: 0.9501\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 471us/step - loss: 0.1781 - accuracy: 0.9573\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 535us/step - loss: 0.1745 - accuracy: 0.9544\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 511us/step - loss: 0.1703 - accuracy: 0.9573\n",
      "Average score of Cross Validation: 0.9205096177872363\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 10\n",
    "num_folds = 10\n",
    "\n",
    "kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "cv_results = np.array([])\n",
    "for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "    X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "    X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "    model = getNetwork()\n",
    "    model.fit(X_cross_train, y_cross_train, epochs=EPOCHS, batch_size=BATCH_SIZE)  \n",
    "    y_pred = model.predict(X_cross_test)\n",
    "    predictions_categorical = np.argmax(y_pred, axis=1)\n",
    "    f1s = f1_score(y_cross_test, predictions_categorical, average=\"weighted\")\n",
    "    cv_results = np.append(cv_results, [f1s])\n",
    "\n",
    "print(f'Average score of Cross Validation: {cv_results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 300)               18300     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 1806      \n",
      "=================================================================\n",
      "Total params: 23,766\n",
      "Trainable params: 23,766\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "500/500 [==============================] - 0s 861us/step - loss: 1.6702 - accuracy: 0.3916 - val_loss: 1.4755 - val_accuracy: 0.5672\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 0s 761us/step - loss: 1.1903 - accuracy: 0.6270 - val_loss: 0.9119 - val_accuracy: 0.7095\n",
      "Epoch 3/100\n",
      "385/500 [======================>.......] - ETA: 0s - loss: 0.7805 - accuracy: 0.8392"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-7f5f08a24029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \"\"\"\n\u001b[1;32m   1867\u001b[0m     \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1868\u001b[0;31m     \u001b[0mexecuting_eagerly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m     \u001b[0;31m# Copy saveable status of function's graph to current FuncGraph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 10\n",
    "model = getNetwork()\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.25)\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "report = classification_report(y_test, pred)\n",
    "classification_report_csv(report, \"NN\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Models in C code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpudc11l0m/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpudc11l0m/assets\n"
     ]
    }
   ],
   "source": [
    "# Neural network with TinyMLGen\n",
    "with open(tasks[choosenIndex] + '/exportedModels/NNmodel.h', 'w') as f:\n",
    "    f.write(tiny.port(model, optimize=False))\n",
    "\n",
    "# Classifiers with MicroMLGen\n",
    "for name, model in models:\n",
    "    prepath = tasks[choosenIndex] + '/exportedModels/'\n",
    "    path = prepath + name + '.h'\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(port(model, optimize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione Inferance Rate medio (Intensit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = read_csv(\"InfTimeReport.csv\")\n",
    "g = sbs.barplot(x=csv['Algoritmo'], y=csv['InfTime'])\n",
    "g.set_yscale(\"log\")\n",
    "plt.ylabel(\"Inference Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoria occupata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = read_csv(\"MemOccupationReport.csv\")\n",
    "g = sbs.barplot(x=csv['Algoritmo'], y=csv['MemOccupata'])\n",
    "g.set_yscale('log')\n",
    "plt.ylabel(\"MemOccupata in Byte\")\n",
    "plt.show()\n",
    "# SVC in overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
