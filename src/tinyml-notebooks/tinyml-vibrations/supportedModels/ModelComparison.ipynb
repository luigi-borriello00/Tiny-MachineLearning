{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Comparison for TinyML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "import pickle\n",
    "from pandas import read_csv\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,  classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold,StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dense, Input, concatenate, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from micromlgen import port\n",
    "import tinymlgen as tiny\n",
    "\n",
    "import warnings\n",
    "import seaborn as sbs\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['../data/X-intensity.pkl', '../data/X-all.pkl', '../data/X-10-25.pkl', '../data/X-1-2.pkl', '../data/X-25_50-50_25.pkl']\n",
    "labels = ['../data/y-intensity.pkl', '../data/y-all.pkl', '../data/y-10-25.pkl', '../data/y-1-2.pkl', '../data/y-25_50-50_25.pkl']\n",
    "choosenIndex = 0\n",
    "tasks = ['intensity', 'all','10-25','1-2', '25-50']\n",
    "with open(data[choosenIndex], 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open(labels[choosenIndex], 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if choosenIndex == 1:\n",
    "    X = X[165:-13]\n",
    "    y = y[165:-13]\n",
    "if choosenIndex == 2:\n",
    "    X = X[146:-13]\n",
    "    y = y[146:-13]\n",
    "if choosenIndex == 3:\n",
    "    X = X[101:-13]\n",
    "    y = y[101:-13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900, 60)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)\n",
    "uniques = np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(780, 60)\n",
      "[[-0.15  0.06  0.91 -0.04 -0.06  0.9   0.   -0.07  0.91  0.15 -0.15  1.03\n",
      "   0.17 -0.16  1.06  0.11 -0.12  1.05 -0.11  0.07  1.   -0.13  0.05  0.96\n",
      "  -0.12  0.05  0.94 -0.03  0.04  0.93  0.01  0.06  0.97 -0.    0.08  1.04\n",
      "  -0.03  0.07  1.06 -0.09  0.08  1.02 -0.13  0.04  0.95 -0.1  -0.    0.93\n",
      "  -0.04 -0.03  0.92  0.16 -0.16  1.01  0.17 -0.15  1.03 -0.08  0.08  1.02]\n",
      " [-0.13  0.05  0.91  0.17 -0.2   0.97  0.18 -0.16  1.03  0.16 -0.13  1.05\n",
      "  -0.08  0.08  1.03 -0.11  0.06  1.01 -0.1   0.04  0.9  -0.07  0.03  0.89\n",
      "  -0.02  0.02  0.91 -0.02  0.13  1.01 -0.01  0.12  1.03 -0.01  0.11  1.05\n",
      "  -0.15  0.12  1.02 -0.16  0.08  1.    0.03 -0.21  0.89  0.08 -0.19  0.9\n",
      "   0.15 -0.21  0.93  0.06  0.02  1.05  0.    0.07  1.05 -0.04  0.06  1.04]\n",
      " [-0.12  0.    0.97 -0.09 -0.05  0.94 -0.04 -0.07  0.95  0.16 -0.14  1.\n",
      "   0.16 -0.15  1.03 -0.11  0.09  1.02 -0.12  0.07  1.   -0.13  0.05  0.97\n",
      "  -0.03  0.02  0.93  0.01  0.07  0.95 -0.    0.08  1.   -0.04  0.08  1.05\n",
      "  -0.09  0.07  1.03 -0.14  0.04  0.95 -0.11  0.    0.91 -0.02 -0.02  0.92\n",
      "   0.19 -0.18  1.01  0.16 -0.14  1.02  0.16 -0.16  1.05 -0.11  0.09  1.02]\n",
      " [-0.08 -0.1   0.99  0.02 -0.17  1.07  0.07 -0.26  0.93 -0.06  0.15  1.01\n",
      "   0.02  0.06  1.05 -0.13  0.2   0.94 -0.07  0.19  1.05  0.    0.05  1.09\n",
      "  -0.12  0.17  0.95 -0.05  0.13  1.03 -0.01  0.11  1.07 -0.12  0.11  0.98\n",
      "  -0.04  0.06  1.05 -0.08 -0.18  0.92 -0.09 -0.15  0.97 -0.   -0.21  1.07\n",
      "  -0.08  0.08  0.92 -0.1   0.13  0.94 -0.02  0.13  1.03 -0.1   0.12  0.93]\n",
      " [-0.05 -0.28  0.91  0.04  0.02  1.05  0.01 -0.02  0.96 -0.1   0.11  0.91\n",
      "   0.02 -0.04  1.02 -0.1   0.08  0.95 -0.12  0.13  0.97 -0.04  0.01  0.93\n",
      "  -0.12  0.08  0.9  -0.01 -0.03  0.96 -0.12  0.15  0.92 -0.08  0.15  1.\n",
      "  -0.05 -0.26  0.91 -0.08 -0.23  0.95  0.04 -0.27  1.04 -0.11  0.05  0.91\n",
      "  -0.01  0.04  1.03  0.06 -0.05  0.93 -0.09  0.16  0.99  0.02  0.03  1.06]\n",
      " [ 0.02 -0.34  0.96 -0.06 -0.29  0.88  0.07 -0.09  1.04  0.02 -0.11  0.94\n",
      "  -0.05 -0.05  0.89  0.05 -0.01  1.03  0.01 -0.02  0.94 -0.02  0.13  1.03\n",
      "   0.03  0.02  1.02 -0.   -0.05  0.95 -0.02  0.12  1.04  0.01  0.07  1.06\n",
      "  -0.01  0.06  0.95 -0.   -0.16  1.03  0.04 -0.21  1.06 -0.07 -0.03  1.02\n",
      "   0.02 -0.1   1.06  0.07 -0.17  1.09 -0.09  0.16  0.99 -0.    0.09  1.03]\n",
      " [-0.05  0.15  1.05  0.04 -0.03  1.05 -0.09  0.16  0.98  0.01  0.08  1.06\n",
      "  -0.01 -0.02  0.96 -0.02  0.07  1.04  0.02 -0.06  0.99 -0.08 -0.    0.92\n",
      "   0.06 -0.31  1.06 -0.05 -0.28  0.91  0.04  0.02  1.05  0.01 -0.02  0.96\n",
      "  -0.1   0.11  0.91  0.02 -0.04  1.02 -0.1   0.08  0.95 -0.12  0.13  0.97\n",
      "  -0.04  0.01  0.93 -0.12  0.08  0.9  -0.01 -0.03  0.96 -0.12  0.15  0.92]\n",
      " [-0.05  0.12  1.03  0.05  0.02  1.04 -0.02  0.11  1.07 -0.15  0.13  0.88\n",
      "   0.02  0.1   1.07 -0.15  0.15  0.95 -0.14  0.11  0.84 -0.06  0.04  1.05\n",
      "  -0.16  0.1   0.89  0.06 -0.18  1.1  -0.1  -0.06  0.92  0.01 -0.13  0.9\n",
      "  -0.11  0.14  0.95 -0.11  0.11  0.84  0.08  0.05  1.01 -0.15  0.15  0.86\n",
      "   0.03  0.08  0.97  0.03  0.05  0.89 -0.03  0.1   0.94  0.01  0.13  1.1 ]\n",
      " [ 0.   -0.04  1.06 -0.08  0.09  0.92 -0.1   0.16  0.95 -0.03  0.14  1.02\n",
      "  -0.11  0.1   0.92 -0.13  0.21  0.94 -0.05  0.01  0.92 -0.12  0.06  0.92\n",
      "  -0.13  0.15  0.94 -0.04 -0.05  0.94 -0.11 -0.01  0.91 -0.13  0.05  0.93\n",
      "  -0.   -0.31  0.95 -0.08 -0.21  0.9   0.05 -0.05  1.    0.   -0.04  0.92\n",
      "  -0.07  0.04  0.89  0.04 -0.04  0.99 -0.03 -0.02  0.93 -0.1   0.09  0.91]\n",
      " [ 0.   -0.01  0.97 -0.12  0.08  0.91 -0.06  0.16  1.   -0.09  0.01  0.95\n",
      "  -0.13  0.19  0.96  0.    0.06  1.06 -0.12  0.12  0.9  -0.04  0.14  1.02\n",
      "  -0.07 -0.1   0.92 -0.09 -0.04  0.96  0.01 -0.11  1.02 -0.1  -0.13  0.91\n",
      "  -0.   -0.13  1.04  0.06 -0.23  1.05 -0.08  0.16  0.97  0.02  0.05  1.04\n",
      "   0.01 -0.04  0.91 -0.05  0.15  1.05  0.04 -0.03  1.05 -0.09  0.16  0.98]\n",
      " [-0.1   0.12  0.92 -0.04  0.    0.92 -0.11  0.13  0.93 -0.12  0.22  0.98\n",
      "  -0.09 -0.    0.86 -0.13  0.13  0.94 -0.03 -0.03  0.92 -0.11  0.05  0.93\n",
      "  -0.12  0.12  0.99 -0.06 -0.26  0.9  -0.1  -0.16  0.94 -0.03 -0.19  1.06\n",
      "  -0.08  0.07  0.92 -0.09  0.16  0.98 -0.    0.06  0.88 -0.13  0.19  0.94\n",
      "  -0.05  0.18  1.04 -0.13  0.07  0.94 -0.1   0.19  0.97 -0.04  0.13  1.03]\n",
      " [-0.    0.08  1.07  0.    0.06  0.99 -0.05  0.05  0.92  0.04 -0.07  1.08\n",
      "   0.01 -0.15  0.96 -0.08 -0.11  0.88  0.07 -0.31  1.03  0.   -0.29  0.94\n",
      "   0.04  0.05  1.1   0.06 -0.03  0.97 -0.02 -0.    0.9   0.02  0.03  1.09\n",
      "   0.03 -0.05  0.98 -0.03 -0.01  0.93  0.01  0.11  1.07  0.    0.04  0.98\n",
      "  -0.03  0.13  1.06  0.01  0.03  1.08  0.03 -0.05  1.01 -0.02 -0.22  1.07]\n",
      " [-0.06  0.02  0.87 -0.1   0.11  0.91 -0.09 -0.05  0.96 -0.06  0.02  0.89\n",
      "  -0.1   0.15  0.94 -0.   -0.02  0.94 -0.07 -0.03  0.88 -0.12  0.07  0.93\n",
      "   0.   -0.01  0.97 -0.05  0.01  0.91 -0.11  0.06  0.92  0.06 -0.35  1.01\n",
      "   0.01 -0.34  0.94  0.04  0.01  1.06  0.08 -0.04  1.04  0.04 -0.09  0.94\n",
      "  -0.01  0.09  1.03  0.04  0.03  1.08  0.05 -0.03  0.99 -0.06  0.18  1.03]\n",
      " [-0.    0.06  0.99 -0.12  0.04  0.87  0.02 -0.13  0.89 -0.05 -0.1   0.94\n",
      "   0.02 -0.04  1.07 -0.   -0.05  1.01  0.12 -0.06  1.06 -0.    0.11  1.08\n",
      "   0.06  0.04  1.02 -0.04  0.    0.88  0.01 -0.04  0.94 -0.12 -0.01  0.9\n",
      "  -0.13  0.11  1.01 -0.13  0.07  0.95 -0.04  0.09  1.07 -0.09  0.07  1.04\n",
      "   0.08  0.02  1.11  0.05 -0.07  0.97  0.18 -0.2   0.99  0.07 -0.21  0.87]\n",
      " [ 0.04 -0.26  1.   -0.02  0.12  1.03  0.04  0.01  1.02 -0.07  0.06  0.93\n",
      "   0.03  0.05  1.06 -0.06  0.02  0.92  0.    0.08  1.06 -0.01 -0.02  0.95\n",
      "  -0.12  0.09  0.91  0.01  0.02  0.98 -0.1   0.1   0.91 -0.08  0.16  1.\n",
      "  -0.05 -0.25  0.93 -0.06 -0.21  0.96  0.04 -0.24  0.92 -0.11  0.08  0.93\n",
      "   0.01  0.06  1.05 -0.13  0.1   0.9  -0.04  0.13  1.04  0.03  0.01  1.04]\n",
      " [-0.1   0.2   1.01 -0.03  0.14  1.03 -0.12  0.13  0.94 -0.09  0.18  1.01\n",
      "  -0.09 -0.04  0.89 -0.11 -0.02  0.92 -0.08  0.01  1.02 -0.06 -0.22  0.9\n",
      "  -0.09 -0.14  0.94 -0.06 -0.12  1.03 -0.07  0.04  0.88 -0.1   0.12  0.91\n",
      "  -0.01 -0.03  0.95 -0.08  0.06  0.89 -0.12  0.17  0.95 -0.02 -0.01  0.92\n",
      "  -0.09  0.    0.87 -0.12  0.11  0.95 -0.01 -0.04  0.93 -0.07  0.    0.9 ]\n",
      " [ 0.01 -0.01  0.94 -0.13  0.02  0.92 -0.15  0.15  1.01 -0.14  0.06  0.95\n",
      "  -0.05  0.09  1.08  0.03  0.07  1.09  0.03  0.06  1.1   0.04  0.01  1.01\n",
      "   0.18 -0.19  1.04  0.11 -0.21  0.9   0.01 -0.19  0.87 -0.04 -0.03  0.86\n",
      "  -0.12  0.06  0.92 -0.06  0.11  1.04 -0.16  0.14  0.98 -0.03  0.12  1.08\n",
      "   0.06  0.09  1.07 -0.    0.09  1.09  0.01  0.01  0.97  0.02  0.05  1.05]\n",
      " [-0.02  0.02  0.89 -0.11  0.14  0.92  0.01  0.09  0.9  -0.1   0.22  1.02\n",
      "   0.02  0.04  1.07 -0.04  0.14  1.04  0.    0.07  0.99 -0.09  0.08  0.88\n",
      "  -0.   -0.13  0.92 -0.11  0.    0.93 -0.07 -0.01  1.03 -0.09 -0.11  0.96\n",
      "   0.01 -0.2   1.08 -0.09  0.17  0.96  0.04  0.05  1.06  0.04 -0.02  0.94\n",
      "   0.03  0.03  1.09 -0.01 -0.02  0.95 -0.11  0.16  0.93 -0.06 -0.02  0.9 ]\n",
      " [-0.07  0.13  1.09  0.06  0.04  1.04 -0.    0.09  1.09 -0.   -0.02  0.93\n",
      "  -0.12 -0.02  0.88 -0.1   0.05  0.87 -0.17  0.09  0.94 -0.06 -0.19  0.92\n",
      "   0.01 -0.11  1.05  0.15 -0.16  1.09  0.07 -0.01  1.07  0.1  -0.05  0.99\n",
      "   0.   -0.06  0.86  0.   -0.    0.92 -0.12  0.04  0.87 -0.16  0.12  0.89\n",
      "  -0.13  0.02  0.93 -0.1   0.1   1.08 -0.15  0.09  0.97 -0.01  0.08  1.1 ]\n",
      " [-0.11  0.08  0.93 -0.09  0.16  1.   -0.07  0.04  0.93 -0.15  0.16  0.96\n",
      "  -0.03  0.11  1.07 -0.12  0.13  0.89 -0.04  0.1   1.04  0.04  0.05  1.03\n",
      "  -0.06 -0.14  0.98  0.04 -0.17  1.03 -0.09 -0.04  0.95  0.02 -0.12  1.06\n",
      "   0.04 -0.17  1.01 -0.03  0.06  1.02  0.05 -0.01  1.05 -0.06  0.03  0.93\n",
      "   0.02  0.01  1.06 -0.03 -0.07  0.95 -0.14  0.07  1.05 -0.01 -0.03  0.97]\n",
      " [-0.12  0.11  1.05 -0.14  0.07  1.   -0.13  0.04  0.94 -0.07  0.12  0.89\n",
      "  -0.04  0.14  0.95 -0.05  0.1   1.08 -0.11  0.12  1.05 -0.15  0.1   1.04\n",
      "   0.04 -0.23  0.89  0.09 -0.2   0.89  0.15 -0.21  0.93  0.03  0.06  1.06\n",
      "  -0.03  0.1   1.05 -0.14  0.07  0.98 -0.13  0.04  0.94 -0.1   0.03  0.9\n",
      "  -0.04  0.14  0.95 -0.01  0.11  1.01 -0.02  0.13  1.03 -0.14  0.11  1.04]\n",
      " [ 0.04 -0.15  0.93  0.07 -0.04  1.08 -0.09  0.04  0.87 -0.14  0.23  0.91\n",
      "   0.01  0.05  0.94  0.    0.13  1.05  0.07  0.02  1.1  -0.15  0.21  0.9\n",
      "  -0.03  0.16  1.04 -0.15  0.14  0.84  0.04  0.12  1.1   0.01 -0.06  1.\n",
      "  -0.01  0.02  1.09 -0.09 -0.    0.87 -0.09  0.01  0.89  0.11 -0.2   1.01\n",
      "  -0.01  0.09  0.86  0.07  0.1   1.1  -0.14  0.18  0.85 -0.11  0.21  0.98]\n",
      " [ 0.07 -0.02  1.08 -0.06  0.14  1.06  0.06  0.04  1.06 -0.01  0.02  1.09\n",
      "   0.04 -0.01  0.99 -0.07 -0.02  0.9  -0.05  0.01  0.91 -0.17  0.05  0.93\n",
      "  -0.06  0.1   1.07 -0.09  0.08  1.04  0.07  0.    1.08  0.06 -0.07  0.96\n",
      "   0.16 -0.18  0.97  0.05 -0.19  0.86 -0.03 -0.    0.9  -0.16  0.07  0.91\n",
      "  -0.08  0.12  1.04 -0.16  0.14  1.02 -0.02  0.11  1.08  0.04 -0.    0.98]\n",
      " [ 0.02 -0.01  1.06 -0.12  0.05  0.93 -0.03  0.03  0.93 -0.1   0.17  0.95\n",
      "   0.03  0.05  1.03 -0.01 -0.13  1.04 -0.03 -0.2   0.91 -0.08 -0.13  0.97\n",
      "  -0.09 -0.12  0.92  0.05 -0.16  1.06 -0.08  0.15  0.98  0.04 -0.02  1.02\n",
      "  -0.12  0.08  0.89 -0.07 -0.    0.91 -0.1   0.13  1.02  0.03 -0.04  1.02\n",
      "   0.    0.07  1.07 -0.07  0.02  0.93 -0.   -0.04  0.97 -0.13  0.05  0.9 ]\n",
      " [ 0.03 -0.21  0.99 -0.1  -0.07  0.88 -0.01 -0.27  0.92 -0.07 -0.12  0.97\n",
      "   0.08 -0.24  0.93 -0.02  0.11  1.03  0.04 -0.01  0.99 -0.02  0.08  1.06\n",
      "  -0.02 -0.03  0.97 -0.14  0.12  0.92 -0.06 -0.02  0.91 -0.1   0.15  0.96\n",
      "   0.02  0.02  1.07 -0.05  0.13  1.01  0.03 -0.06  1.01  0.02 -0.25  1.04\n",
      "   0.03 -0.32  0.97 -0.09 -0.25  0.91 -0.02 -0.01  0.92 -0.09  0.12  0.98]\n",
      " [-0.01 -0.11  0.96 -0.1   0.01  0.92  0.    0.04  1.05  0.09  0.02  1.05\n",
      "   0.04  0.08  1.08  0.01  0.    0.94  0.02 -0.    0.98 -0.09 -0.04  0.9\n",
      "  -0.14  0.07  0.98 -0.16  0.09  0.94 -0.02  0.1   1.07  0.04  0.05  1.04\n",
      "   0.12 -0.06  1.05  0.06 -0.14  0.92 -0.04 -0.11  0.92 -0.01 -0.12  0.87\n",
      "  -0.03 -0.04  0.99 -0.16  0.12  0.96 -0.    0.12  1.07  0.04  0.03  1.01]\n",
      " [-0.09  0.03  0.99  0.01 -0.15  0.97  0.04 -0.07  1.1   0.15 -0.13  1.12\n",
      "  -0.1   0.09  0.88  0.11 -0.14  0.99 -0.05  0.06  0.85  0.08  0.11  1.11\n",
      "   0.04 -0.03  1.03 -0.1   0.21  0.98 -0.05  0.03  0.92 -0.16  0.16  0.84\n",
      "   0.02  0.12  1.06 -0.02 -0.02  0.97 -0.02  0.07  1.09 -0.12  0.03  0.9\n",
      "  -0.1  -0.03  0.86  0.12 -0.25  1.04 -0.02  0.    1.01  0.05  0.12  1.09]\n",
      " [ 0.13 -0.14  0.97  0.16 -0.15  1.03 -0.02 -0.03  1.03 -0.06 -0.04  0.99\n",
      "  -0.08 -0.06  0.89  0.02  0.05  0.93  0.06  0.06  1.    0.03  0.09  1.06\n",
      "  -0.09  0.1   1.04 -0.14  0.07  1.   -0.1   0.08  0.87 -0.06  0.1   0.91\n",
      "   0.01  0.07  1.    0.01 -0.01  1.07 -0.01 -0.04  1.04 -0.05 -0.1   0.99\n",
      "   0.06 -0.16  0.88  0.1  -0.14  0.91  0.02  0.1   1.06 -0.05  0.1   1.05]\n",
      " [-0.06 -0.11  0.93 -0.1   0.01  0.93 -0.03 -0.28  0.95 -0.08 -0.23  0.91\n",
      "  -0.02 -0.13  1.02 -0.08  0.05  0.95 -0.1   0.17  0.97 -0.07 -0.02  0.92\n",
      "  -0.16  0.14  0.92 -0.01  0.1   1.04 -0.12  0.06  0.92 -0.07  0.14  1.\n",
      "   0.03  0.04  1.06 -0.09  0.17  0.97  0.02  0.01  1.06 -0.09 -0.2   0.93\n",
      "   0.02 -0.28  1.04  0.04 -0.27  1.01 -0.02  0.04  1.03  0.05 -0.    1.05]\n",
      " [ 0.01 -0.09  0.93  0.07 -0.15  1.03  0.12 -0.12  1.05  0.06 -0.14  1.04\n",
      "  -0.12  0.03  0.94 -0.09  0.02  0.91 -0.03  0.02  0.91  0.02  0.13  1.05\n",
      "  -0.06  0.14  1.08 -0.08  0.11  0.96 -0.15  0.03  0.92 -0.13  0.04  0.88\n",
      "   0.04  0.03  1.02  0.06  0.03  1.08  0.01  0.05  1.09  0.   -0.2   0.98\n",
      "  -0.01 -0.19  0.93  0.05 -0.23  0.88  0.06  0.04  1.    0.04  0.08  1.03]\n",
      " [ 0.01 -0.03  0.88 -0.09  0.09  0.92 -0.1   0.14  0.95 -0.03  0.02  0.92\n",
      "  -0.1   0.12  0.92 -0.13  0.19  0.94 -0.03 -0.03  0.92 -0.1   0.01  0.89\n",
      "  -0.13  0.14  0.91 -0.03  0.04  0.93 -0.1   0.09  0.9   0.06 -0.31  1.05\n",
      "   0.02 -0.33  0.96 -0.04 -0.29  0.89  0.06 -0.07  1.1   0.07 -0.12  0.99\n",
      "   0.01 -0.12  0.93  0.01  0.06  1.07  0.05  0.    1.05 -0.12  0.21  0.98]\n",
      " [ 0.02 -0.21  0.87  0.01 -0.12  1.   -0.14  0.09  0.95  0.02  0.06  1.07\n",
      "  -0.07  0.15  1.06  0.07  0.02  1.05 -0.04  0.01  0.9  -0.01 -0.06  0.93\n",
      "  -0.14  0.01  0.91 -0.13  0.08  1.04 -0.17  0.13  0.98  0.02  0.07  1.08\n",
      "   0.02  0.05  1.06  0.14 -0.13  1.01  0.05 -0.17  0.87  0.06 -0.17  0.88\n",
      "  -0.08 -0.04  0.9   0.04 -0.03  1.04 -0.13  0.15  1.02  0.03  0.09  1.07]\n",
      " [-0.12  0.12  0.85  0.07  0.04  1.1  -0.16  0.09  0.96 -0.08  0.21  1.02\n",
      "  -0.1   0.05  0.87 -0.18  0.16  0.89  0.05 -0.03  1.08 -0.15  0.16  0.97\n",
      "   0.08 -0.13  1.11 -0.06 -0.15  0.88  0.14 -0.18  1.15  0.06 -0.    1.\n",
      "  -0.05  0.24  0.99  0.06  0.06  1.1  -0.15  0.17  0.87  0.09 -0.02  1.03\n",
      "  -0.06  0.06  0.92 -0.03  0.22  1.06 -0.15  0.17  0.86 -0.17  0.21  0.93]\n",
      " [ 0.04 -0.12  0.92  0.09 -0.13  0.95  0.14 -0.13  1.    0.07 -0.07  1.04\n",
      "   0.03 -0.08  1.04 -0.13  0.05  0.96 -0.11  0.04  0.92 -0.07  0.03  0.89\n",
      "  -0.01  0.12  1.   -0.02  0.14  1.04 -0.02  0.12  1.05 -0.13  0.08  1.02\n",
      "  -0.16  0.07  0.97 -0.03 -0.07  0.91  0.03 -0.1   0.92  0.08 -0.1   0.95\n",
      "   0.14 -0.15  1.05  0.06 -0.09  1.04  0.01 -0.09  1.02 -0.12  0.04  0.92]\n",
      " [-0.04  0.13  1.02 -0.06 -0.09  0.91 -0.08 -0.02  0.98  0.04 -0.1   1.02\n",
      "  -0.06 -0.14  0.95  0.02 -0.21  1.05  0.03 -0.26  0.98 -0.01  0.08  1.04\n",
      "   0.04 -0.01  1.02 -0.07  0.14  1.02  0.02 -0.    1.04 -0.03 -0.01  0.95\n",
      "  -0.02  0.07  1.05  0.03  0.    1.   -0.08  0.04  0.95  0.04  0.01  1.05\n",
      "  -0.03 -0.03  0.93 -0.11  0.03  1.04  0.04 -0.3   1.01 -0.07 -0.23  0.93]\n",
      " [ 0.    0.07  0.98 -0.01  0.08  1.05 -0.05  0.07  1.05 -0.15  0.06  0.97\n",
      "  -0.14  0.03  0.94 -0.11  0.02  0.92  0.16 -0.21  0.96  0.17 -0.17  1.01\n",
      "   0.18 -0.19  1.03 -0.06  0.08  1.03 -0.11  0.09  1.01 -0.12  0.06  0.92\n",
      "  -0.09  0.02  0.9  -0.03  0.03  0.92 -0.02  0.11  1.02  0.    0.1   1.03\n",
      "  -0.01  0.08  1.05 -0.16  0.1   1.   -0.16  0.07  0.96 -0.14  0.05  0.93]\n",
      " [-0.13  0.11  1.04 -0.13  0.06  1.   -0.12  0.02  0.94 -0.06  0.12  0.91\n",
      "  -0.    0.11  0.99  0.    0.12  1.02 -0.16  0.1   1.03 -0.15  0.07  0.97\n",
      "   0.09 -0.2   0.9   0.16 -0.2   0.96  0.17 -0.16  1.02 -0.07  0.08  1.03\n",
      "  -0.11  0.06  1.   -0.12  0.04  0.95  0.    0.04  0.94  0.03  0.07  0.99\n",
      "   0.04  0.1   1.05 -0.09  0.09  1.03 -0.14  0.04  0.99 -0.09  0.07  0.89]\n",
      " [-0.03  0.19  1.05  0.08 -0.08  1.15 -0.13  0.03  0.92  0.03 -0.02  1.06\n",
      "  -0.04 -0.1   0.88  0.13 -0.07  1.11  0.07  0.03  1.02 -0.1   0.25  0.95\n",
      "  -0.01  0.04  0.92 -0.12  0.12  0.85  0.07  0.04  1.1  -0.16  0.09  0.96\n",
      "  -0.08  0.21  1.02 -0.1   0.05  0.87 -0.18  0.16  0.89  0.05 -0.03  1.08\n",
      "  -0.15  0.16  0.97  0.08 -0.13  1.11 -0.06 -0.15  0.88  0.14 -0.18  1.15]\n",
      " [ 0.07 -0.06  1.   -0.05 -0.01  0.88  0.   -0.02  0.94 -0.1   0.13  0.93\n",
      "  -0.07  0.19  1.03 -0.13  0.15  0.92 -0.03  0.16  1.03  0.01 -0.02  0.96\n",
      "   0.01  0.07  1.1  -0.04  0.04  0.93 -0.14  0.17  1.01 -0.07 -0.24  0.88\n",
      "  -0.08 -0.16  1.   -0.1   0.03  0.94  0.02  0.01  1.05  0.06 -0.14  0.98\n",
      "   0.05  0.04  1.08 -0.02 -0.    0.93 -0.11  0.18  0.93 -0.1   0.02  0.88]\n",
      " [-0.18  0.09  0.98 -0.17  0.07  0.94  0.11 -0.2   0.92  0.14 -0.17  0.97\n",
      "   0.04  0.02  1.06 -0.    0.03  1.05 -0.06  0.05  1.03 -0.13  0.05  0.93\n",
      "  -0.09  0.03  0.9  -0.05  0.03  0.9  -0.02  0.13  1.02  0.01  0.12  1.04\n",
      "  -0.03  0.1   1.03 -0.16  0.08  0.99 -0.18  0.07  0.95  0.05 -0.14  0.92\n",
      "   0.13 -0.14  0.97  0.16 -0.15  1.03 -0.02 -0.03  1.03 -0.06 -0.04  0.99]\n",
      " [ 0.01 -0.11  0.91  0.19 -0.18  1.04  0.22 -0.21  1.05 -0.11  0.12  1.03\n",
      "  -0.15  0.09  1.   -0.15  0.06  0.95 -0.06 -0.02  0.92 -0.03 -0.    0.94\n",
      "   0.01  0.    0.97  0.01  0.08  1.07  0.02  0.05  1.07 -0.04  0.06  1.06\n",
      "  -0.08  0.01  0.98 -0.12 -0.01  0.92 -0.06 -0.06  0.9   0.16 -0.18  0.97\n",
      "   0.22 -0.25  1.03 -0.02  0.09  1.05 -0.07  0.09  1.04 -0.09  0.08  1.02]\n",
      " [-0.16  0.07  0.91 -0.08  0.12  1.04 -0.16  0.14  1.02 -0.02  0.11  1.08\n",
      "   0.04 -0.    0.98  0.02  0.06  1.04 -0.05 -0.03  0.9  -0.01 -0.    0.93\n",
      "  -0.15  0.07  0.92 -0.1   0.12  1.04  0.01 -0.13  0.99  0.14 -0.13  1.06\n",
      "   0.18 -0.21  0.97  0.08  0.02  1.03 -0.01 -0.02  0.9  -0.13  0.    0.96\n",
      "  -0.12  0.02  0.89 -0.17  0.12  0.97 -0.14  0.02  0.93 -0.09  0.1   1.06]\n",
      " [ 0.04 -0.18  0.88 -0.03 -0.11  0.93 -0.02 -0.1   0.89 -0.04 -0.02  0.98\n",
      "   0.07 -0.02  1.08 -0.06  0.14  1.06  0.06  0.04  1.06 -0.01  0.02  1.09\n",
      "   0.04 -0.01  0.99 -0.07 -0.02  0.9  -0.05  0.01  0.91 -0.17  0.05  0.93\n",
      "  -0.06  0.1   1.07 -0.09  0.08  1.04  0.07  0.    1.08  0.06 -0.07  0.96\n",
      "   0.16 -0.18  0.97  0.05 -0.19  0.86 -0.03 -0.    0.9  -0.16  0.07  0.91]\n",
      " [ 0.01  0.06  1.05 -0.03  0.07  0.93  0.02 -0.2   1.06  0.04 -0.29  0.99\n",
      "  -0.02 -0.24  0.92  0.06 -0.17  1.1   0.05 -0.22  0.97 -0.04 -0.14  0.9\n",
      "   0.05  0.    1.04 -0.   -0.02  0.92 -0.08  0.06  1.06  0.01 -0.05  0.97\n",
      "  -0.05 -0.04  0.91  0.01  0.06  1.06 -0.02  0.06  0.93 -0.09  0.06  0.89\n",
      "   0.03 -0.17  0.99 -0.04 -0.17  0.9  -0.1  -0.08  0.92  0.03 -0.31  0.95]\n",
      " [-0.05  0.18  1.04 -0.13  0.07  0.94 -0.1   0.19  0.97 -0.04  0.13  1.03\n",
      "  -0.14  0.18  0.94 -0.05  0.14  1.06  0.02  0.04  1.09 -0.06 -0.2   1.02\n",
      "   0.03 -0.31  1.05 -0.1   0.08  0.96 -0.    0.03  1.06  0.06 -0.05  1.08\n",
      "  -0.08  0.16  1.02  0.01  0.04  1.07  0.04 -0.01  1.01 -0.05  0.17  1.05\n",
      "   0.    0.06  1.07 -0.09  0.19  1.02 -0.01  0.06  1.07  0.01  0.07  1.01]\n",
      " [ 0.07  0.03  1.1  -0.04  0.15  1.01 -0.12  0.14  0.84 -0.01  0.1   1.06\n",
      "  -0.14  0.17  0.87 -0.03  0.08  0.9  -0.15  0.18  0.92 -0.1   0.07  0.86\n",
      "   0.04  0.1   0.99 -0.16  0.16  0.85  0.    0.08  0.99 -0.08 -0.08  0.9\n",
      "   0.04 -0.15  0.97  0.13 -0.22  1.16 -0.02  0.01  0.88  0.11 -0.01  1.09\n",
      "  -0.03  0.09  1.04  0.06  0.05  1.02 -0.01  0.11  1.05  0.01  0.03  0.98]\n",
      " [-0.01  0.17  1.09  0.16 -0.23  1.14 -0.09 -0.07  0.96  0.06 -0.25  0.99\n",
      "  -0.08  0.04  0.85  0.09  0.09  1.09 -0.14  0.2   0.85 -0.05  0.22  1.01\n",
      "  -0.07  0.06  0.86 -0.15  0.15  0.85  0.02  0.03  1.03 -0.14  0.23  0.97\n",
      "   0.    0.19  1.07 -0.18  0.17  0.85  0.03  0.1   1.08  0.03 -0.22  1.01\n",
      "   0.05 -0.09  1.12  0.13 -0.06  1.11 -0.12  0.09  0.85  0.09 -0.09  1.  ]\n",
      " [ 0.   -0.16  1.03  0.04 -0.14  1.05 -0.03 -0.2   0.92  0.03  0.09  1.06\n",
      "  -0.03 -0.02  0.92 -0.15  0.1   0.9  -0.02  0.03  0.96 -0.16  0.04  0.9\n",
      "  -0.01  0.02  0.97 -0.11  0.01  0.94 -0.06  0.16  1.01 -0.08 -0.02  0.92\n",
      "  -0.08  0.06  0.98  0.02  0.    1.04 -0.08 -0.18  0.95  0.02 -0.22  1.05\n",
      "  -0.12  0.15  0.88 -0.01  0.08  1.04  0.05  0.02  1.01 -0.06  0.12  1.01]\n",
      " [-0.09  0.18  0.98 -0.1   0.12  0.83 -0.07  0.19  1.02 -0.14  0.15  0.86\n",
      "   0.03  0.    0.98 -0.17  0.18  0.87 -0.04  0.11  0.94  0.01  0.13  0.89\n",
      "  -0.05  0.02  0.93  0.07  0.01  1.15  0.   -0.16  0.92  0.15 -0.21  1.15\n",
      "  -0.01 -0.06  1.04  0.09  0.04  1.07 -0.05  0.16  1.03 -0.12  0.15  0.85\n",
      "  -0.03  0.15  1.05 -0.14  0.17  0.86 -0.05  0.15  1.07 -0.16  0.18  0.87]\n",
      " [-0.11  0.09  0.89 -0.01  0.07  1.04 -0.02 -0.01  0.94  0.    0.    1.05\n",
      "  -0.1   0.05  0.95 -0.02  0.03  0.95 -0.14  0.13  0.91  0.02  0.1   1.06\n",
      "  -0.05 -0.09  1.02  0.04 -0.22  1.   -0.09 -0.21  0.89 -0.   -0.22  0.92\n",
      "  -0.08 -0.06  0.97 -0.06  0.06  0.93 -0.03  0.12  1.03  0.03 -0.02  1.02\n",
      "  -0.02  0.06  1.06 -0.02 -0.04  0.98 -0.15  0.14  0.92 -0.07  0.01  0.92]\n",
      " [-0.06 -0.22  0.97  0.05 -0.31  1.04  0.01 -0.03  1.05 -0.01 -0.09  0.92\n",
      "  -0.08  0.09  0.97 -0.12  0.07  0.92 -0.02  0.08  1.05 -0.03 -0.02  0.97\n",
      "   0.02 -0.01  1.06 -0.12  0.05  0.93 -0.03  0.03  0.93 -0.1   0.17  0.95\n",
      "   0.03  0.05  1.03 -0.01 -0.13  1.04 -0.03 -0.2   0.91 -0.08 -0.13  0.97\n",
      "  -0.09 -0.12  0.92  0.05 -0.16  1.06 -0.08  0.15  0.98  0.04 -0.02  1.02]\n",
      " [-0.07 -0.27  0.92  0.01 -0.09  0.94 -0.1   0.11  0.94  0.06 -0.05  1.05\n",
      "  -0.04  0.13  1.05  0.01 -0.03  1.   -0.13  0.13  0.9  -0.05 -0.03  0.92\n",
      "  -0.12  0.14  0.99  0.02  0.01  0.94 -0.04  0.12  1.05 -0.    0.04  0.96\n",
      "   0.03 -0.21  1.02 -0.03 -0.26  0.9  -0.07 -0.21  0.97 -0.07 -0.06  0.94\n",
      "  -0.01 -0.03  1.05  0.04 -0.14  0.99  0.01  0.04  1.05 -0.03  0.    0.93]\n",
      " [ 0.09 -0.12  1.16  0.11 -0.17  1.02 -0.04  0.09  1.01  0.07  0.07  1.1\n",
      "  -0.13  0.2   0.86  0.08  0.03  1.03 -0.04  0.06  0.91 -0.    0.14  1.06\n",
      "  -0.14  0.18  0.86 -0.13  0.28  0.97 -0.08  0.1   0.88 -0.15  0.12  0.9\n",
      "   0.04 -0.04  1.05 -0.07  0.06  1.05  0.12 -0.14  1.14 -0.1  -0.01  0.89\n",
      "   0.1  -0.18  1.01 -0.09  0.1   0.84  0.1   0.03  1.09 -0.01  0.06  0.94]\n",
      " [-0.04  0.02  0.93 -0.02  0.05  0.94  0.    0.07  0.98 -0.01  0.08  1.05\n",
      "  -0.05  0.07  1.05 -0.15  0.06  0.97 -0.14  0.03  0.94 -0.11  0.02  0.92\n",
      "   0.16 -0.21  0.96  0.17 -0.17  1.01  0.18 -0.19  1.03 -0.06  0.08  1.03\n",
      "  -0.11  0.09  1.01 -0.12  0.06  0.92 -0.09  0.02  0.9  -0.03  0.03  0.92\n",
      "  -0.02  0.11  1.02  0.    0.1   1.03 -0.01  0.08  1.05 -0.16  0.1   1.  ]\n",
      " [ 0.01  0.08  1.09  0.07 -0.13  1.06  0.17 -0.17  1.07  0.02  0.07  1.07\n",
      "   0.07  0.04  1.04 -0.   -0.01  0.89  0.04  0.01  0.97 -0.08 -0.01  0.89\n",
      "  -0.15  0.08  0.95 -0.13 -0.01  0.89 -0.12  0.07  1.02 -0.16  0.1   0.93\n",
      "  -0.07  0.12  1.06  0.04  0.07  1.1   0.12 -0.13  1.07  0.17 -0.18  1.01\n",
      "   0.07 -0.22  0.88  0.06 -0.08  0.9  -0.07 -0.05  0.89 -0.1   0.    0.88]\n",
      " [-0.1   0.15  0.95 -0.05  0.03  0.9  -0.11  0.15  0.93 -0.1   0.22  0.93\n",
      "  -0.09 -0.03  0.86 -0.13  0.13  0.92 -0.04  0.04  0.93 -0.12  0.11  0.91\n",
      "  -0.14  0.16  0.95 -0.01 -0.3   0.93 -0.08 -0.2   0.91 -0.1  -0.19  0.96\n",
      "  -0.01 -0.13  0.92 -0.08  0.01  0.93  0.03 -0.02  0.97 -0.03  0.01  0.91\n",
      "  -0.1   0.13  0.93  0.01 -0.05  0.96 -0.06 -0.03  0.89 -0.11  0.11  0.93]\n",
      " [-0.09  0.07  1.03 -0.14  0.04  0.95 -0.11  0.    0.91 -0.02 -0.02  0.92\n",
      "   0.19 -0.18  1.01  0.16 -0.14  1.02  0.16 -0.16  1.05 -0.11  0.09  1.02\n",
      "  -0.12  0.06  0.99 -0.08  0.01  0.91 -0.03  0.02  0.93 -0.    0.06  0.95\n",
      "  -0.01  0.12  1.02 -0.01  0.08  1.04 -0.05  0.07  1.05 -0.16  0.07  0.97\n",
      "  -0.13  0.05  0.94 -0.09 -0.2   0.94  0.18 -0.2   0.99  0.18 -0.16  1.03]\n",
      " [-0.08  0.12  0.91 -0.    0.15  1.07  0.11 -0.2   1.15 -0.08 -0.07  0.97\n",
      "   0.03  0.1   1.04 -0.09  0.04  0.85  0.09  0.04  1.12  0.04  0.04  0.98\n",
      "  -0.03  0.17  1.02 -0.06  0.07  0.86 -0.15  0.17  0.87  0.04  0.    1.01\n",
      "  -0.11  0.21  0.87  0.    0.13  1.07 -0.18  0.19  0.86 -0.09  0.03  0.99\n",
      "   0.01 -0.15  0.97  0.04 -0.07  1.1   0.15 -0.13  1.12 -0.1   0.09  0.88]\n",
      " [-0.01 -0.19  0.93  0.05 -0.23  0.88  0.06  0.04  1.    0.04  0.08  1.03\n",
      "  -0.13  0.11  1.04 -0.13  0.06  1.   -0.12  0.02  0.94 -0.06  0.12  0.91\n",
      "  -0.    0.11  0.99  0.    0.12  1.02 -0.16  0.1   1.03 -0.15  0.07  0.97\n",
      "   0.09 -0.2   0.9   0.16 -0.2   0.96  0.17 -0.16  1.02 -0.07  0.08  1.03\n",
      "  -0.11  0.06  1.   -0.12  0.04  0.95  0.    0.04  0.94  0.03  0.07  0.99]\n",
      " [-0.06 -0.15  0.88  0.14 -0.18  1.15  0.06 -0.    1.   -0.05  0.24  0.99\n",
      "   0.06  0.06  1.1  -0.15  0.17  0.87  0.09 -0.02  1.03 -0.06  0.06  0.92\n",
      "  -0.03  0.22  1.06 -0.15  0.17  0.86 -0.17  0.21  0.93 -0.05  0.1   0.92\n",
      "  -0.08 -0.08  0.89  0.13 -0.22  1.14 -0.08 -0.02  0.97  0.03  0.11  1.05\n",
      "  -0.09  0.02  0.85  0.11  0.02  1.08  0.02  0.04  0.95  0.    0.18  1.03]\n",
      " [-0.02  0.11  1.03  0.04 -0.01  0.99 -0.02  0.08  1.06 -0.02 -0.03  0.97\n",
      "  -0.14  0.12  0.92 -0.06 -0.02  0.91 -0.1   0.15  0.96  0.02  0.02  1.07\n",
      "  -0.05  0.13  1.01  0.03 -0.06  1.01  0.02 -0.25  1.04  0.03 -0.32  0.97\n",
      "  -0.09 -0.25  0.91 -0.02 -0.01  0.92 -0.09  0.12  0.98 -0.09  0.08  0.94\n",
      "  -0.05  0.11  1.06  0.02 -0.03  1.01 -0.02  0.04  1.06 -0.03 -0.05  0.93]\n",
      " [ 0.02  0.06  1.07  0.06  0.05  1.    0.05  0.05  1.08  0.02 -0.02  0.93\n",
      "  -0.12  0.02  0.88 -0.08 -0.04  0.88 -0.14  0.04  0.94 -0.16  0.09  0.92\n",
      "  -0.12  0.13  1.03  0.03  0.07  1.11  0.1  -0.11  1.07  0.17 -0.2   1.01\n",
      "   0.07 -0.2   0.87  0.04 -0.05  0.9  -0.08 -0.01  0.88 -0.06  0.04  0.88\n",
      "  -0.15  0.14  1.02  0.01  0.1   1.08 -0.04  0.1   1.12  0.02  0.02  1.02]\n",
      " [-0.03 -0.17  0.92  0.04 -0.2   0.89  0.13 -0.07  1.01  0.09  0.    1.04\n",
      "   0.03  0.01  1.06 -0.15  0.06  0.98 -0.12  0.05  0.94 -0.07  0.09  0.95\n",
      "  -0.02  0.12  1.03 -0.02  0.12  1.05 -0.14  0.1   1.01 -0.17  0.06  0.96\n",
      "  -0.15  0.08  0.91  0.11 -0.12  0.96  0.14 -0.12  1.03  0.13 -0.08  1.06\n",
      "  -0.03 -0.06  1.   -0.05 -0.08  0.94 -0.03  0.03  0.9   0.04  0.04  0.95]\n",
      " [-0.11  0.12  0.91 -0.02  0.1   1.05 -0.03  0.03  0.93  0.03 -0.09  1.04\n",
      "  -0.11 -0.05  0.9  -0.01 -0.05  1.04 -0.09 -0.17  0.95  0.07 -0.26  1.07\n",
      "  -0.03  0.1   1.04  0.02 -0.04  0.97 -0.1   0.13  0.9  -0.07  0.02  0.94\n",
      "  -0.09  0.14  1.    0.03 -0.03  1.04 -0.04  0.09  1.04 -0.03 -0.05  0.94\n",
      "   0.03  0.05  1.05 -0.07  0.04  0.92 -0.08  0.12  1.02 -0.09 -0.25  0.9 ]\n",
      " [-0.01  0.02  0.95 -0.11  0.03  1.05  0.03  0.04  1.   -0.08  0.09  0.92\n",
      "   0.05 -0.28  1.05 -0.04 -0.27  0.91 -0.09 -0.21  0.94 -0.01 -0.05  0.94\n",
      "  -0.11  0.06  0.9  -0.03  0.1   1.04 -0.11  0.08  0.93 -0.09  0.16  1.\n",
      "  -0.07  0.04  0.93 -0.15  0.16  0.96 -0.03  0.11  1.07 -0.12  0.13  0.89\n",
      "  -0.04  0.1   1.04  0.04  0.05  1.03 -0.06 -0.14  0.98  0.04 -0.17  1.03]\n",
      " [-0.08  0.07  0.89  0.02  0.01  0.99 -0.03 -0.05  0.92 -0.11  0.01  0.88\n",
      "   0.01  0.03  1.01 -0.04  0.06  0.93 -0.11  0.09  0.91  0.05 -0.34  1.02\n",
      "  -0.01 -0.31  0.93  0.05 -0.05  1.09  0.06 -0.14  1.01  0.02 -0.12  0.94\n",
      "   0.03  0.04  1.09  0.03 -0.03  1.   -0.02 -0.    0.93  0.    0.07  1.07\n",
      "   0.01 -0.06  0.96 -0.02  0.11  1.04  0.01  0.07  1.05 -0.02  0.04  0.92]\n",
      " [-0.06 -0.    0.92  0.01  0.06  1.04 -0.   -0.04  0.97 -0.09  0.03  1.04\n",
      "   0.06 -0.3   1.03 -0.05 -0.24  0.91  0.05  0.01  1.06 -0.   -0.01  0.94\n",
      "  -0.09  0.08  0.9   0.01  0.    1.01 -0.11  0.07  0.94 -0.1   0.16  0.97\n",
      "  -0.05 -0.02  0.93 -0.14  0.08  0.9   0.    0.04  0.97 -0.1   0.1   0.94\n",
      "  -0.07  0.18  0.98 -0.05 -0.27  0.91 -0.07 -0.2   0.93  0.   -0.22  1.03]\n",
      " [ 0.01  0.1   1.09 -0.03  0.08  1.11  0.01  0.07  1.03 -0.06 -0.02  0.89\n",
      "   0.    0.01  0.95 -0.12  0.03  0.9  -0.16  0.09  0.98 -0.01 -0.17  0.92\n",
      "   0.05 -0.13  1.05 -0.12  0.1   0.98  0.02  0.06  1.07  0.06  0.05  1.\n",
      "   0.05  0.05  1.08  0.02 -0.02  0.93 -0.12  0.02  0.88 -0.08 -0.04  0.88\n",
      "  -0.14  0.04  0.94 -0.16  0.09  0.92 -0.12  0.13  1.03  0.03  0.07  1.11]\n",
      " [-0.08  0.04  0.95  0.04  0.01  1.05 -0.03 -0.03  0.93 -0.11  0.03  1.04\n",
      "   0.04 -0.3   1.01 -0.07 -0.23  0.93  0.06 -0.01  1.05 -0.02 -0.01  0.93\n",
      "  -0.1   0.07  0.91  0.03 -0.01  1.02 -0.09  0.05  0.93 -0.13  0.15  0.96\n",
      "  -0.02 -0.04  0.95 -0.12  0.07  0.91  0.03  0.02  1.   -0.07  0.07  0.92\n",
      "  -0.13  0.14  0.91 -0.   -0.28  0.94 -0.08 -0.25  0.89 -0.03 -0.19  1.01]\n",
      " [ 0.02  0.03  1.03 -0.14  0.23  0.97  0.    0.19  1.07 -0.18  0.17  0.85\n",
      "   0.03  0.1   1.08  0.03 -0.22  1.01  0.05 -0.09  1.12  0.13 -0.06  1.11\n",
      "  -0.12  0.09  0.85  0.09 -0.09  1.   -0.05  0.04  0.88  0.07  0.1   1.1\n",
      "  -0.13  0.15  0.84 -0.14  0.25  0.97 -0.03  0.01  0.93 -0.13  0.18  0.84\n",
      "   0.    0.14  1.05 -0.17  0.24  0.87  0.    0.07  1.1  -0.11 -0.01  0.9 ]\n",
      " [-0.06 -0.05  1.    0.02  0.09  1.07 -0.09  0.09  0.84  0.07  0.07  1.12\n",
      "   0.07  0.03  1.02 -0.12  0.21  0.96 -0.01  0.07  0.93 -0.12  0.09  0.85\n",
      "   0.01  0.14  1.1   0.02  0.07  1.03 -0.14  0.21  0.97 -0.06  0.09  0.92\n",
      "  -0.08 -0.1   0.88  0.14 -0.19  1.16 -0.1  -0.05  0.9  -0.06  0.13  0.99\n",
      "   0.   -0.05  0.91 -0.12  0.13  0.84  0.09  0.06  1.09 -0.15  0.19  0.86]\n",
      " [-0.1   0.    0.88 -0.15  0.09  0.93 -0.1   0.14  1.06 -0.14  0.08  1.02\n",
      "  -0.04  0.12  1.11  0.03  0.04  1.04  0.04  0.06  1.1  -0.01  0.04  0.95\n",
      "  -0.13  0.04  0.88  0.02 -0.17  0.88 -0.06 -0.09  0.94 -0.05 -0.1   0.9\n",
      "   0.01 -0.06  1.02  0.11 -0.03  1.07 -0.02  0.12  1.08  0.07  0.05  1.04\n",
      "  -0.    0.01  0.91  0.04 -0.    1.   -0.04 -0.05  0.9  -0.02  0.03  0.95]\n",
      " [-0.15  0.16  0.92 -0.09  0.1   0.87  0.04  0.04  1.   -0.15  0.13  0.84\n",
      "  -0.01  0.15  0.99 -0.14  0.05  0.87  0.02 -0.03  0.99  0.08 -0.09  1.12\n",
      "   0.04 -0.1   0.94  0.13 -0.15  1.15 -0.05 -0.04  1.    0.09  0.06  1.06\n",
      "  -0.05  0.12  1.03  0.05  0.02  1.04 -0.02  0.11  1.07 -0.15  0.13  0.88\n",
      "   0.02  0.1   1.07 -0.15  0.15  0.95 -0.14  0.11  0.84 -0.06  0.04  1.05]\n",
      " [-0.14  0.11  0.84 -0.06  0.04  1.05 -0.16  0.1   0.89  0.06 -0.18  1.1\n",
      "  -0.1  -0.06  0.92  0.01 -0.13  0.9  -0.11  0.14  0.95 -0.11  0.11  0.84\n",
      "   0.08  0.05  1.01 -0.15  0.15  0.86  0.03  0.08  0.97  0.03  0.05  0.89\n",
      "  -0.03  0.1   0.94  0.01  0.13  1.1  -0.07  0.12  0.87  0.04  0.05  1.1\n",
      "  -0.08  0.08  1.03  0.13 -0.2   1.09  0.05 -0.13  1.09 -0.06 -0.11  0.91]\n",
      " [-0.07  0.14  1.02  0.02 -0.    1.04 -0.03 -0.01  0.95 -0.02  0.07  1.05\n",
      "   0.03  0.    1.   -0.08  0.04  0.95  0.04  0.01  1.05 -0.03 -0.03  0.93\n",
      "  -0.11  0.03  1.04  0.04 -0.3   1.01 -0.07 -0.23  0.93  0.06 -0.01  1.05\n",
      "  -0.02 -0.01  0.93 -0.1   0.07  0.91  0.03 -0.01  1.02 -0.09  0.05  0.93\n",
      "  -0.13  0.15  0.96 -0.02 -0.04  0.95 -0.12  0.07  0.91  0.03  0.02  1.  ]\n",
      " [ 0.07 -0.19  0.87 -0.    0.    0.93 -0.14  0.04  0.89 -0.15  0.14  1.\n",
      "  -0.16  0.09  0.97 -0.07  0.14  1.08  0.04  0.03  1.03  0.01  0.08  1.09\n",
      "  -0.02 -0.01  0.93 -0.14  0.    0.97 -0.12  0.03  0.9  -0.15  0.13  0.99\n",
      "  -0.02 -0.12  0.96  0.12 -0.15  1.06  0.16 -0.2   0.96  0.08 -0.03  0.99\n",
      "  -0.05 -0.05  0.87 -0.13  0.08  0.95 -0.19  0.09  0.94 -0.06  0.12  1.07]\n",
      " [ 0.04 -0.17  1.03 -0.09 -0.04  0.95  0.02 -0.12  1.06  0.04 -0.17  1.01\n",
      "  -0.03  0.06  1.02  0.05 -0.01  1.05 -0.06  0.03  0.93  0.02  0.01  1.06\n",
      "  -0.03 -0.07  0.95 -0.14  0.07  1.05 -0.01 -0.03  0.97 -0.08  0.1   0.95\n",
      "   0.04 -0.12  1.01 -0.06 -0.11  0.93 -0.1   0.01  0.93 -0.03 -0.28  0.95\n",
      "  -0.08 -0.23  0.91 -0.02 -0.13  1.02 -0.08  0.05  0.95 -0.1   0.17  0.97]\n",
      " [-0.16  0.07  0.94 -0.05  0.1   1.09 -0.04  0.08  1.07  0.04  0.05  1.06\n",
      "  -0.04  0.03  0.91  0.06 -0.08  0.93 -0.06 -0.07  0.9  -0.07  0.    0.86\n",
      "  -0.02 -0.1   0.94  0.13 -0.11  1.05 -0.06  0.13  1.05  0.07  0.03  1.06\n",
      "  -0.01  0.01  0.93  0.02 -0.01  0.96 -0.11 -0.02  0.9  -0.17  0.13  1.\n",
      "  -0.14  0.04  0.96 -0.03  0.09  1.09 -0.05  0.1   1.06  0.06  0.05  1.06]\n",
      " [ 0.04  0.    1.01 -0.06 -0.04  0.89 -0.14  0.03  0.95 -0.14  0.05  0.9\n",
      "  -0.13  0.1   1.   -0.12  0.04  0.96  0.01  0.03  1.07  0.12 -0.08  1.05\n",
      "   0.15 -0.12  1.06  0.14 -0.15  0.95  0.03 -0.18  0.85 -0.02  0.    0.91\n",
      "  -0.14  0.06  0.89 -0.14 -0.03  0.89 -0.14  0.06  0.94 -0.08  0.13  1.08\n",
      "  -0.13  0.1   1.   -0.01  0.08  1.09  0.01  0.06  1.04  0.08 -0.    1.09]\n",
      " [ 0.02  0.07  1.05  0.06  0.01  1.04 -0.1   0.23  1.   -0.03  0.14  1.04\n",
      "   0.02  0.06  1.08 -0.12  0.17  0.95 -0.04  0.17  1.04 -0.11 -0.    0.93\n",
      "  -0.13  0.04  0.96 -0.06  0.04  1.05 -0.08 -0.21  0.91 -0.1  -0.12  0.94\n",
      "  -0.06 -0.09  1.03 -0.06  0.02  0.87 -0.1   0.11  0.91 -0.09 -0.05  0.96\n",
      "  -0.06  0.02  0.89 -0.1   0.15  0.94 -0.   -0.02  0.94 -0.07 -0.03  0.88]\n",
      " [ 0.01  0.    0.97  0.01  0.08  1.07  0.02  0.05  1.07 -0.04  0.06  1.06\n",
      "  -0.08  0.01  0.98 -0.12 -0.01  0.92 -0.06 -0.06  0.9   0.16 -0.18  0.97\n",
      "   0.22 -0.25  1.03 -0.02  0.09  1.05 -0.07  0.09  1.04 -0.09  0.08  1.02\n",
      "  -0.15  0.03  0.93 -0.12 -0.01  0.92 -0.12 -0.    0.91 -0.02  0.04  0.93\n",
      "  -0.    0.06  0.96  0.05  0.06  1.09  0.02  0.07  1.09  0.    0.07  1.08]\n",
      " [ 0.06 -0.31  1.05  0.02 -0.33  0.96 -0.04 -0.29  0.89  0.06 -0.07  1.1\n",
      "   0.07 -0.12  0.99  0.01 -0.12  0.93  0.01  0.06  1.07  0.05  0.    1.05\n",
      "  -0.12  0.21  0.98 -0.05  0.18  1.04 -0.01  0.11  1.05 -0.13  0.16  0.94\n",
      "  -0.08  0.19  1.02 -0.01  0.11  1.05 -0.11 -0.1   0.92 -0.09 -0.11  1.01\n",
      "  -0.01 -0.17  0.89 -0.09 -0.06  0.93 -0.08  0.02  0.99 -0.05  0.01  0.89]\n",
      " [-0.13  0.16  0.98  0.    0.03  1.04 -0.05 -0.19  1.05  0.07 -0.32  1.09\n",
      "  -0.02 -0.32  0.94  0.07 -0.06  1.   -0.05 -0.01  0.88  0.   -0.02  0.94\n",
      "  -0.1   0.13  0.93 -0.07  0.19  1.03 -0.13  0.15  0.92 -0.03  0.16  1.03\n",
      "   0.01 -0.02  0.96  0.01  0.07  1.1  -0.04  0.04  0.93 -0.14  0.17  1.01\n",
      "  -0.07 -0.24  0.88 -0.08 -0.16  1.   -0.1   0.03  0.94  0.02  0.01  1.05]\n",
      " [-0.09  0.15  1.05  0.02  0.07  1.08 -0.03  0.12  1.09  0.03  0.02  1.\n",
      "  -0.07 -0.06  0.9  -0.07  0.02  0.92 -0.16  0.07  0.93 -0.06 -0.05  0.9\n",
      "  -0.04  0.02  1.04  0.12 -0.04  1.07  0.15 -0.1   1.05  0.14 -0.15  0.95\n",
      "  -0.02 -0.16  0.87 -0.09 -0.01  0.88 -0.14  0.11  0.95 -0.14  0.05  0.94\n",
      "  -0.07  0.14  1.08  0.03  0.06  1.07  0.01  0.08  1.11 -0.03  0.02  0.95]\n",
      " [-0.12  0.23  0.97 -0.05  0.03  0.92 -0.03  0.2   0.86  0.02  0.09  1.07\n",
      "  -0.18  0.22  0.91  0.03 -0.07  1.11 -0.07 -0.15  0.9   0.11 -0.17  1.17\n",
      "   0.1  -0.09  1.01 -0.04  0.14  1.   -0.01 -0.03  0.9  -0.13  0.17  0.84\n",
      "   0.09  0.03  1.05 -0.03  0.01  0.93 -0.01  0.18  1.06 -0.14  0.14  0.87\n",
      "  -0.16  0.24  0.91 -0.07  0.15  0.92 -0.03  0.19  1.05  0.08 -0.08  1.15]\n",
      " [-0.07 -0.02  0.89 -0.13  0.08  0.96  0.01 -0.16  0.92  0.08 -0.11  1.05\n",
      "   0.19 -0.19  1.02  0.06  0.06  1.08 -0.    0.01  0.93 -0.13  0.04  0.89\n",
      "  -0.1   0.    0.89 -0.18  0.14  1.   -0.15  0.05  0.98 -0.04  0.11  1.1\n",
      "  -0.    0.05  1.02  0.05  0.05  1.05 -0.04  0.02  0.92 -0.15  0.09  0.94\n",
      "   0.01 -0.19  0.88  0.03 -0.12  1.02 -0.08  0.07  1.    0.06  0.05  1.07]\n",
      " [ 0.   -0.16  0.96 -0.07 -0.06  0.9  -0.11 -0.03  0.92  0.01 -0.31  0.94\n",
      "  -0.07 -0.2   0.93 -0.1  -0.11  0.97 -0.03 -0.02  0.9  -0.09  0.11  0.92\n",
      "   0.01 -0.03  0.96 -0.07  0.05  0.88 -0.13  0.19  0.94 -0.03 -0.04  0.92\n",
      "  -0.11  0.03  0.91 -0.12  0.15  0.94 -0.07  0.04  0.91 -0.13  0.11  0.93\n",
      "   0.03 -0.34  0.94 -0.08 -0.23  0.9  -0.1  -0.15  0.94 -0.   -0.07  0.91]\n",
      " [ 0.02  0.03  0.98  0.02  0.1   1.1  -0.07  0.15  0.9   0.02  0.13  1.11\n",
      "  -0.07  0.05  0.88  0.07 -0.03  1.09 -0.03 -0.01  1.08  0.12 -0.2   1.06\n",
      "   0.07 -0.11  1.1  -0.09 -0.06  0.9   0.06  0.06  1.1  -0.11  0.13  0.94\n",
      "  -0.09  0.09  0.85 -0.08  0.08  0.86 -0.01  0.04  0.99 -0.07 -0.01  0.86\n",
      "  -0.01  0.09  0.91 -0.06 -0.02  0.93 -0.11 -0.02  0.96 -0.12  0.2   0.91]\n",
      " [-0.06 -0.02  0.91 -0.1   0.15  0.96  0.02  0.02  1.07 -0.05  0.13  1.01\n",
      "   0.03 -0.06  1.01  0.02 -0.25  1.04  0.03 -0.32  0.97 -0.09 -0.25  0.91\n",
      "  -0.02 -0.01  0.92 -0.09  0.12  0.98 -0.09  0.08  0.94 -0.05  0.11  1.06\n",
      "   0.02 -0.03  1.01 -0.02  0.04  1.06 -0.03 -0.05  0.93 -0.13  0.13  0.95\n",
      "  -0.09  0.08  0.92 -0.07  0.16  1.02  0.01 -0.24  0.9   0.01 -0.24  1.05]\n",
      " [ 0.03 -0.21  1.05  0.02 -0.25  0.95 -0.06 -0.2   0.88  0.07 -0.21  1.04\n",
      "   0.03 -0.24  0.95 -0.05 -0.13  0.89  0.06 -0.03  1.03  0.01 -0.01  0.93\n",
      "  -0.07  0.06  1.07  0.03 -0.04  1.   -0.02 -0.04  0.95 -0.    0.08  1.07\n",
      "   0.    0.06  0.99 -0.05  0.05  0.92  0.04 -0.07  1.08  0.01 -0.15  0.96\n",
      "  -0.08 -0.11  0.88  0.07 -0.31  1.03  0.   -0.29  0.94  0.04  0.05  1.1 ]\n",
      " [ 0.02  0.06  1.05 -0.14  0.14  0.87  0.01  0.11  1.1  -0.13  0.15  0.94\n",
      "   0.03  0.1   1.1  -0.09  0.12  1.01 -0.16  0.15  0.86  0.01 -0.08  1.1\n",
      "  -0.11 -0.04  0.89 -0.01 -0.08  0.95 -0.1  -0.    0.91 -0.03 -0.06  0.88\n",
      "  -0.09  0.13  0.96 -0.1   0.12  0.83  0.06  0.07  1.01 -0.13  0.14  0.86\n",
      "   0.02  0.03  0.98  0.02  0.1   1.1  -0.07  0.15  0.9   0.02  0.13  1.11]\n",
      " [ 0.04  0.03  1.01 -0.06 -0.01  0.89 -0.15  0.08  0.97 -0.14 -0.    0.92\n",
      "  -0.09  0.09  1.07  0.    0.09  1.02  0.03  0.08  1.08 -0.    0.04  0.96\n",
      "   0.12 -0.13  0.97  0.02 -0.19  0.87 -0.03 -0.11  0.95 -0.06 -0.05  0.92\n",
      "   0.04 -0.01  1.04  0.12 -0.03  1.05  0.05  0.08  1.07 -0.    0.    0.95\n",
      "   0.01 -0.02  1.   -0.08 -0.05  0.9  -0.13  0.05  0.96 -0.16  0.04  0.92]\n",
      " [ 0.02 -0.15  1.02  0.01 -0.16  0.98  0.   -0.16  1.03  0.04 -0.14  1.05\n",
      "  -0.03 -0.2   0.92  0.03  0.09  1.06 -0.03 -0.02  0.92 -0.15  0.1   0.9\n",
      "  -0.02  0.03  0.96 -0.16  0.04  0.9  -0.01  0.02  0.97 -0.11  0.01  0.94\n",
      "  -0.06  0.16  1.01 -0.08 -0.02  0.92 -0.08  0.06  0.98  0.02  0.    1.04\n",
      "  -0.08 -0.18  0.95  0.02 -0.22  1.05 -0.12  0.15  0.88 -0.01  0.08  1.04]\n",
      " [-0.14  0.19  0.85 -0.12  0.22  0.98 -0.02  0.01  0.94 -0.15  0.14  0.84\n",
      "   0.03  0.12  1.1  -0.17  0.18  0.87 -0.08  0.08  1.03 -0.04 -0.04  0.93\n",
      "   0.04 -0.02  1.11  0.14 -0.17  1.12 -0.07  0.02  0.97  0.09  0.07  1.11\n",
      "  -0.09  0.18  0.98 -0.1   0.12  0.83 -0.07  0.19  1.02 -0.14  0.15  0.86\n",
      "   0.03  0.    0.98 -0.17  0.18  0.87 -0.04  0.11  0.94  0.01  0.13  0.89]\n",
      " [ 0.    0.07  1.05 -0.04  0.06  1.04 -0.14  0.06  0.96 -0.11  0.04  0.91\n",
      "  -0.08  0.03  0.92 -0.02  0.12  0.96 -0.01  0.13  1.02 -0.09  0.09  1.04\n",
      "  -0.14  0.11  1.03 -0.17  0.1   0.99  0.03 -0.2   0.9   0.07 -0.18  0.9\n",
      "   0.14 -0.2   0.95  0.08  0.01  1.05  0.    0.03  1.04 -0.14  0.07  0.98\n",
      "  -0.13  0.04  0.95 -0.09  0.03  0.91 -0.03  0.13  0.99 -0.01  0.13  1.03]\n",
      " [-0.06 -0.11  0.91  0.05  0.03  1.07 -0.13  0.1   0.88  0.06  0.05  1.1\n",
      "  -0.15  0.15  0.91 -0.07  0.1   0.86 -0.12  0.16  0.96 -0.14  0.09  0.86\n",
      "   0.03  0.08  1.02 -0.16  0.16  0.84 -0.01  0.11  0.97 -0.1  -0.06  0.9\n",
      "   0.02 -0.15  0.98  0.12 -0.15  1.15 -0.   -0.    0.9   0.12 -0.04  1.1\n",
      "  -0.02  0.04  1.03  0.07  0.07  1.02  0.02  0.06  1.05 -0.14  0.14  0.87]\n",
      " [-0.04  0.01  0.92 -0.03  0.07  0.96  0.01  0.07  1.06 -0.06  0.07  1.04\n",
      "  -0.13  0.07  1.01 -0.06 -0.05  0.91  0.01 -0.09  0.93  0.07 -0.15  1.03\n",
      "   0.12 -0.12  1.05  0.06 -0.14  1.04 -0.12  0.03  0.94 -0.09  0.02  0.91\n",
      "  -0.03  0.02  0.91  0.02  0.13  1.05 -0.06  0.14  1.08 -0.08  0.11  0.96\n",
      "  -0.15  0.03  0.92 -0.13  0.04  0.88  0.04  0.03  1.02  0.06  0.03  1.08]\n",
      " [-0.18  0.19  0.84  0.02  0.11  1.01 -0.1   0.25  0.97  0.04  0.03  1.11\n",
      "  -0.16  0.1   0.9   0.05 -0.09  1.07  0.   -0.18  0.92  0.14 -0.1   1.1\n",
      "   0.07  0.05  1.03 -0.11  0.23  0.95 -0.    0.04  0.92 -0.13  0.11  0.85\n",
      "   0.07  0.04  1.09 -0.15  0.22  0.88 -0.05  0.2   1.04 -0.12  0.01  0.85\n",
      "   0.01  0.19  0.86  0.01 -0.    1.03 -0.11  0.24  1.02  0.11 -0.16  1.15]\n",
      " [-0.09 -0.02  0.9  -0.17  0.12  0.99 -0.03  0.13  1.08 -0.02  0.09  1.1\n",
      "   0.03  0.05  1.03  0.06  0.06  1.08 -0.02 -0.01  0.93 -0.16  0.05  0.92\n",
      "  -0.   -0.2   0.88  0.   -0.15  0.99  0.14 -0.12  1.07  0.02  0.08  1.08\n",
      "   0.05  0.04  1.02  0.06  0.06  1.08 -0.   -0.02  0.93 -0.14  0.04  0.9\n",
      "  -0.12 -0.05  0.88 -0.13  0.08  1.   -0.03  0.11  1.11 -0.02  0.09  1.09]\n",
      " [-0.02  0.01  0.91  0.03  0.04  0.96  0.06  0.04  1.03 -0.07  0.12  1.07\n",
      "  -0.11  0.09  1.04 -0.14  0.08  0.99 -0.11  0.05  0.88 -0.09  0.08  0.89\n",
      "   0.05  0.03  1.06  0.04  0.05  1.08 -0.    0.07  1.07  0.02 -0.21  0.98\n",
      "  -0.02 -0.19  0.94  0.03 -0.22  0.91  0.02  0.03  0.94  0.05  0.04  1.\n",
      "   0.06  0.12  1.08 -0.1   0.12  1.04 -0.12  0.07  1.01 -0.11  0.04  0.89]\n",
      " [ 0.01  0.06  1.08 -0.1  -0.05  0.99 -0.01 -0.12  1.05 -0.09 -0.07  0.94\n",
      "  -0.08  0.    0.98  0.   -0.04  1.06 -0.08  0.09  0.92 -0.1   0.16  0.95\n",
      "  -0.03  0.14  1.02 -0.11  0.1   0.92 -0.13  0.21  0.94 -0.05  0.01  0.92\n",
      "  -0.12  0.06  0.92 -0.13  0.15  0.94 -0.04 -0.05  0.94 -0.11 -0.01  0.91\n",
      "  -0.13  0.05  0.93 -0.   -0.31  0.95 -0.08 -0.21  0.9   0.05 -0.05  1.  ]\n",
      " [ 0.05  0.    1.04 -0.   -0.02  0.92 -0.08  0.06  1.06  0.01 -0.05  0.97\n",
      "  -0.05 -0.04  0.91  0.01  0.06  1.06 -0.02  0.06  0.93 -0.09  0.06  0.89\n",
      "   0.03 -0.17  0.99 -0.04 -0.17  0.9  -0.1  -0.08  0.92  0.03 -0.31  0.95\n",
      "  -0.08 -0.17  0.91  0.04 -0.05  0.95 -0.04  0.02  0.87 -0.1   0.12  0.92\n",
      "  -0.04  0.    0.92 -0.11  0.13  0.93 -0.12  0.22  0.98 -0.09 -0.    0.86]\n",
      " [ 0.03  0.04  1.03 -0.07  0.03  0.9   0.09 -0.18  0.91 -0.01 -0.17  0.88\n",
      "  -0.04 -0.11  0.87 -0.07 -0.03  0.93  0.03  0.04  1.06 -0.11  0.15  1.04\n",
      "   0.04  0.08  1.08  0.    0.02  0.97  0.02  0.02  1.03 -0.06 -0.05  0.92\n",
      "  -0.05  0.03  0.94 -0.16  0.04  0.9  -0.13  0.11  1.   -0.1   0.02  0.97\n",
      "   0.04  0.    1.07  0.11 -0.08  1.01  0.17 -0.13  1.05  0.1  -0.19  0.9 ]\n",
      " [-0.09  0.1   1.04 -0.14  0.07  1.   -0.1   0.08  0.87 -0.06  0.1   0.91\n",
      "   0.01  0.07  1.    0.01 -0.01  1.07 -0.01 -0.04  1.04 -0.05 -0.1   0.99\n",
      "   0.06 -0.16  0.88  0.1  -0.14  0.91  0.02  0.1   1.06 -0.05  0.1   1.05\n",
      "  -0.11  0.1   1.03 -0.1   0.02  0.9  -0.04  0.01  0.92 -0.03  0.07  0.96\n",
      "   0.01  0.07  1.06 -0.06  0.07  1.04 -0.13  0.07  1.01 -0.06 -0.05  0.91]\n",
      " [ 0.    0.17  1.08 -0.16  0.16  0.85  0.04  0.01  1.03 -0.11  0.11  0.86\n",
      "  -0.    0.18  1.06  0.06 -0.11  1.07 -0.05  0.04  1.06 -0.04 -0.13  0.95\n",
      "  -0.09 -0.04  0.86  0.12 -0.2   1.02 -0.01  0.05  1.01  0.08  0.13  1.1\n",
      "  -0.14  0.18  0.85 -0.08  0.24  1.   -0.1   0.05  0.88  0.05  0.12  1.1\n",
      "  -0.04  0.09  0.98 -0.07  0.22  1.02 -0.13  0.08  0.85 -0.14  0.16  0.91]\n",
      " [ 0.06 -0.14  1.01  0.02 -0.12  0.94  0.03  0.04  1.09  0.03 -0.03  1.\n",
      "  -0.02 -0.    0.93  0.    0.07  1.07  0.01 -0.06  0.96 -0.02  0.11  1.04\n",
      "   0.01  0.07  1.05 -0.02  0.04  0.92  0.02 -0.15  1.04  0.03 -0.26  0.99\n",
      "  -0.03 -0.22  0.92  0.07 -0.21  1.1   0.05 -0.28  0.98 -0.04  0.08  1.03\n",
      "   0.05 -0.    1.04  0.01 -0.03  0.93 -0.01  0.09  1.04  0.02 -0.02  1.02]\n",
      " [-0.01  0.08  1.04  0.04 -0.01  1.02 -0.07  0.14  1.02  0.02 -0.    1.04\n",
      "  -0.03 -0.01  0.95 -0.02  0.07  1.05  0.03  0.    1.   -0.08  0.04  0.95\n",
      "   0.04  0.01  1.05 -0.03 -0.03  0.93 -0.11  0.03  1.04  0.04 -0.3   1.01\n",
      "  -0.07 -0.23  0.93  0.06 -0.01  1.05 -0.02 -0.01  0.93 -0.1   0.07  0.91\n",
      "   0.03 -0.01  1.02 -0.09  0.05  0.93 -0.13  0.15  0.96 -0.02 -0.04  0.95]\n",
      " [-0.12  0.15  0.92 -0.08  0.15  1.   -0.05 -0.26  0.91 -0.08 -0.23  0.95\n",
      "   0.04 -0.27  1.04 -0.11  0.05  0.91 -0.01  0.04  1.03  0.06 -0.05  0.93\n",
      "  -0.09  0.16  0.99  0.02  0.03  1.06 -0.16  0.15  0.96 -0.02  0.07  1.05\n",
      "   0.03 -0.01  1.01 -0.04  0.13  1.03  0.02  0.08  1.05 -0.06  0.07  0.91\n",
      "   0.03 -0.17  1.03  0.01 -0.2   0.96  0.02 -0.12  1.05  0.04 -0.2   1.  ]\n",
      " [-0.04  0.02  0.87 -0.1   0.12  0.92 -0.04  0.    0.92 -0.11  0.13  0.93\n",
      "  -0.12  0.22  0.98 -0.09 -0.    0.86 -0.13  0.13  0.94 -0.03 -0.03  0.92\n",
      "  -0.11  0.05  0.93 -0.12  0.12  0.99 -0.06 -0.26  0.9  -0.1  -0.16  0.94\n",
      "  -0.03 -0.19  1.06 -0.08  0.07  0.92 -0.09  0.16  0.98 -0.    0.06  0.88\n",
      "  -0.13  0.19  0.94 -0.05  0.18  1.04 -0.13  0.07  0.94 -0.1   0.19  0.97]\n",
      " [ 0.07 -0.26  0.93 -0.06  0.15  1.01  0.02  0.06  1.05 -0.13  0.2   0.94\n",
      "  -0.07  0.19  1.05  0.    0.05  1.09 -0.12  0.17  0.95 -0.05  0.13  1.03\n",
      "  -0.01  0.11  1.07 -0.12  0.11  0.98 -0.04  0.06  1.05 -0.08 -0.18  0.92\n",
      "  -0.09 -0.15  0.97 -0.   -0.21  1.07 -0.08  0.08  0.92 -0.1   0.13  0.94\n",
      "  -0.02  0.13  1.03 -0.1   0.12  0.93 -0.13  0.2   0.95 -0.05 -0.03  0.9 ]\n",
      " [ 0.    0.08  1.06 -0.01 -0.02  0.95 -0.12  0.09  0.91  0.01  0.02  0.98\n",
      "  -0.1   0.1   0.91 -0.08  0.16  1.   -0.05 -0.25  0.93 -0.06 -0.21  0.96\n",
      "   0.04 -0.24  0.92 -0.11  0.08  0.93  0.01  0.06  1.05 -0.13  0.1   0.9\n",
      "  -0.04  0.13  1.04  0.03  0.01  1.04 -0.09  0.17  1.   -0.    0.06  1.06\n",
      "  -0.02 -0.02  0.93 -0.01  0.13  1.07  0.    0.02  0.96 -0.12  0.14  0.92]\n",
      " [ 0.02  0.07  0.99 -0.04  0.09  1.06 -0.1   0.07  1.02 -0.15  0.08  0.89\n",
      "  -0.08  0.07  0.89 -0.02  0.07  0.95  0.11 -0.11  1.07  0.08 -0.17  1.06\n",
      "   0.   -0.15  1.02 -0.06 -0.04  0.86  0.   -0.02  0.9   0.03  0.1   1.06\n",
      "  -0.02  0.12  1.07 -0.08  0.1   1.05 -0.13  0.02  0.93 -0.11  0.02  0.88\n",
      "  -0.08  0.04  0.89  0.04  0.05  1.05  0.01  0.06  1.07 -0.04  0.09  1.07]\n",
      " [ 0.03 -0.12  1.08  0.07 -0.21  1.03  0.01  0.08  1.04  0.04  0.02  1.09\n",
      "   0.02 -0.03  0.94 -0.06  0.12  1.05  0.02 -0.04  1.01 -0.04 -0.03  0.93\n",
      "   0.    0.08  1.07 -0.01  0.04  0.94 -0.09  0.06  0.88  0.03 -0.15  1.\n",
      "  -0.02 -0.15  0.93 -0.1  -0.1   0.91  0.04 -0.31  0.96 -0.06 -0.22  0.88\n",
      "   0.05 -0.03  1.01 -0.   -0.03  0.91 -0.08  0.07  0.91  0.01 -0.04  0.97]\n",
      " [-0.04 -0.11  0.87 -0.07 -0.03  0.93  0.03  0.04  1.06 -0.11  0.15  1.04\n",
      "   0.04  0.08  1.08  0.    0.02  0.97  0.02  0.02  1.03 -0.06 -0.05  0.92\n",
      "  -0.05  0.03  0.94 -0.16  0.04  0.9  -0.13  0.11  1.   -0.1   0.02  0.97\n",
      "   0.04  0.    1.07  0.11 -0.08  1.01  0.17 -0.13  1.05  0.1  -0.19  0.9\n",
      "  -0.04  0.03  0.96 -0.11  0.01  0.88 -0.15  0.13  0.96 -0.15  0.06  0.94]\n",
      " [-0.13  0.12  0.9  -0.02  0.1   1.04 -0.12  0.09  0.93 -0.06  0.15  1.01\n",
      "   0.02  0.01  1.05 -0.13  0.14  0.95 -0.03  0.07  1.07  0.01  0.    0.99\n",
      "  -0.02  0.11  1.04  0.04  0.01  1.02 -0.05 -0.25  1.    0.04 -0.24  1.04\n",
      "   0.   -0.36  0.93 -0.02  0.07  1.04  0.05 -0.03  1.01 -0.06 -0.    0.93\n",
      "   0.02  0.05  1.05 -0.03 -0.01  0.97 -0.06  0.1   1.04  0.04 -0.03  1.03]\n",
      " [-0.08 -0.08  0.93  0.05  0.04  1.05 -0.04 -0.    0.92 -0.12  0.12  0.9\n",
      "   0.   -0.05  0.98 -0.13  0.09  0.9   0.01  0.02  1.01 -0.06  0.05  0.93\n",
      "  -0.12  0.13  0.93 -0.02 -0.09  0.91 -0.11 -0.03  0.89 -0.01 -0.02  1.03\n",
      "  -0.09 -0.19  0.93 -0.05 -0.18  1.    0.05 -0.23  0.92 -0.13  0.12  0.9\n",
      "  -0.02  0.1   1.04 -0.12  0.09  0.93 -0.06  0.15  1.01  0.02  0.01  1.05]\n",
      " [-0.03 -0.06  0.88 -0.09  0.13  0.96 -0.1   0.12  0.83  0.06  0.07  1.01\n",
      "  -0.13  0.14  0.86  0.02  0.03  0.98  0.02  0.1   1.1  -0.07  0.15  0.9\n",
      "   0.02  0.13  1.11 -0.07  0.05  0.88  0.07 -0.03  1.09 -0.03 -0.01  1.08\n",
      "   0.12 -0.2   1.06  0.07 -0.11  1.1  -0.09 -0.06  0.9   0.06  0.06  1.1\n",
      "  -0.11  0.13  0.94 -0.09  0.09  0.85 -0.08  0.08  0.86 -0.01  0.04  0.99]\n",
      " [ 0.03 -0.27  1.06  0.05 -0.31  1.02  0.   -0.32  0.94  0.03 -0.07  1.07\n",
      "   0.07 -0.12  1.06  0.04 -0.15  0.96 -0.01  0.08  1.04  0.04  0.01  1.09\n",
      "  -0.12  0.21  0.99 -0.04  0.14  1.04  0.01  0.06  1.07 -0.11  0.17  0.97\n",
      "  -0.02  0.12  1.04  0.01  0.07  1.08 -0.09 -0.08  0.99 -0.   -0.14  1.04\n",
      "   0.04 -0.09  0.94 -0.07 -0.05  1.    0.01 -0.09  1.06 -0.09  0.11  0.93]\n",
      " [-0.    0.09  1.04 -0.   -0.04  0.99 -0.1   0.09  1.03  0.04 -0.04  1.04\n",
      "  -0.06 -0.01  0.94  0.02  0.09  1.06 -0.02  0.03  0.95 -0.11  0.11  0.9\n",
      "   0.03 -0.29  1.   -0.06 -0.23  0.93 -0.08 -0.18  0.97 -0.04 -0.1   0.92\n",
      "  -0.12  0.02  0.91  0.   -0.01  0.97 -0.12  0.08  0.91 -0.06  0.16  1.\n",
      "  -0.09  0.01  0.95 -0.13  0.19  0.96  0.    0.06  1.06 -0.12  0.12  0.9 ]\n",
      " [-0.03  0.02  0.91  0.02  0.13  1.05 -0.06  0.14  1.08 -0.08  0.11  0.96\n",
      "  -0.15  0.03  0.92 -0.13  0.04  0.88  0.04  0.03  1.02  0.06  0.03  1.08\n",
      "   0.01  0.05  1.09  0.   -0.2   0.98 -0.01 -0.19  0.93  0.05 -0.23  0.88\n",
      "   0.06  0.04  1.    0.04  0.08  1.03 -0.13  0.11  1.04 -0.13  0.06  1.\n",
      "  -0.12  0.02  0.94 -0.06  0.12  0.91 -0.    0.11  0.99  0.    0.12  1.02]\n",
      " [-0.08 -0.18  0.93  0.03 -0.06  0.93 -0.05 -0.    0.88 -0.1  -0.02  1.03\n",
      "  -0.01 -0.02  0.94 -0.08  0.07  0.89  0.02  0.01  0.99 -0.03 -0.05  0.92\n",
      "  -0.11  0.01  0.88  0.01  0.03  1.01 -0.04  0.06  0.93 -0.11  0.09  0.91\n",
      "   0.05 -0.34  1.02 -0.01 -0.31  0.93  0.05 -0.05  1.09  0.06 -0.14  1.01\n",
      "   0.02 -0.12  0.94  0.03  0.04  1.09  0.03 -0.03  1.   -0.02 -0.    0.93]\n",
      " [ 0.06 -0.07  1.1   0.07 -0.12  0.99  0.01 -0.12  0.93  0.01  0.06  1.07\n",
      "   0.05  0.    1.05 -0.12  0.21  0.98 -0.05  0.18  1.04 -0.01  0.11  1.05\n",
      "  -0.13  0.16  0.94 -0.08  0.19  1.02 -0.01  0.11  1.05 -0.11 -0.1   0.92\n",
      "  -0.09 -0.11  1.01 -0.01 -0.17  0.89 -0.09 -0.06  0.93 -0.08  0.02  0.99\n",
      "  -0.05  0.01  0.89 -0.1   0.13  0.93 -0.1   0.16  0.97 -0.05 -0.    0.9 ]\n",
      " [-0.1  -0.17  0.93  0.   -0.23  1.06 -0.09  0.08  0.98  0.05 -0.03  1.07\n",
      "  -0.06  0.15  1.03  0.05  0.02  1.09 -0.01  0.02  0.92  0.02 -0.02  0.99\n",
      "  -0.08  0.01  0.87 -0.13  0.21  0.96 -0.11  0.11  0.95 -0.09  0.21  1.03\n",
      "  -0.11 -0.06  0.92 -0.01 -0.11  1.05  0.03 -0.24  0.99  0.06 -0.19  1.1\n",
      "   0.03 -0.25  0.95 -0.09 -0.06  0.94 -0.02  0.02  0.89 -0.11  0.14  0.92]\n",
      " [ 0.02  0.    1.05 -0.11  0.2   1.02 -0.12  0.08  1.15 -0.09 -0.06  0.9\n",
      "   0.09 -0.25  1.02 -0.07  0.04  0.85  0.09  0.08  1.09 -0.13  0.22  0.89\n",
      "   0.02  0.14  1.06 -0.12  0.14  0.85  0.08 -0.02  1.05 -0.05  0.06  0.91\n",
      "  -0.02  0.17  1.07  0.01  0.07  1.03 -0.14  0.22  0.98 -0.1   0.13  0.89\n",
      "  -0.09 -0.03  0.9   0.1  -0.26  1.07 -0.02 -0.05  1.06  0.09 -0.01  1.11]\n",
      " [-0.07  0.15  0.92 -0.03  0.19  1.05  0.08 -0.08  1.15 -0.13  0.03  0.92\n",
      "   0.03 -0.02  1.06 -0.04 -0.1   0.88  0.13 -0.07  1.11  0.07  0.03  1.02\n",
      "  -0.1   0.25  0.95 -0.01  0.04  0.92 -0.12  0.12  0.85  0.07  0.04  1.1\n",
      "  -0.16  0.09  0.96 -0.08  0.21  1.02 -0.1   0.05  0.87 -0.18  0.16  0.89\n",
      "   0.05 -0.03  1.08 -0.15  0.16  0.97  0.08 -0.13  1.11 -0.06 -0.15  0.88]\n",
      " [-0.06  0.12  1.07 -0.08  0.11  1.07  0.01  0.09  1.09 -0.02 -0.05  0.93\n",
      "   0.    0.04  0.96 -0.13  0.04  0.9  -0.14  0.12  1.   -0.06 -0.04  0.97\n",
      "   0.08 -0.05  1.07  0.07 -0.08  1.05  0.15 -0.11  1.04  0.07 -0.15  0.9\n",
      "   0.    0.01  0.94 -0.14  0.03  0.89 -0.17  0.13  0.97 -0.14  0.04  0.95\n",
      "  -0.1   0.13  1.07 -0.1   0.1   1.03  0.02  0.08  1.09 -0.01  0.05  0.98]\n",
      " [-0.05  0.2   1.04 -0.12  0.01  0.85  0.01  0.19  0.86  0.01 -0.    1.03\n",
      "  -0.11  0.24  1.02  0.11 -0.16  1.15 -0.1  -0.09  0.89  0.14 -0.24  1.09\n",
      "  -0.    0.01  0.92  0.02  0.17  1.05 -0.11  0.11  0.85 -0.14  0.19  0.87\n",
      "   0.04  0.    0.98 -0.11  0.07  0.86  0.02  0.14  1.08 -0.16  0.18  0.87\n",
      "  -0.14  0.21  0.98 -0.08  0.12  0.91 -0.    0.15  1.07  0.11 -0.2   1.15]\n",
      " [-0.08  0.01  0.87 -0.13  0.21  0.96 -0.11  0.11  0.95 -0.09  0.21  1.03\n",
      "  -0.11 -0.06  0.92 -0.01 -0.11  1.05  0.03 -0.24  0.99  0.06 -0.19  1.1\n",
      "   0.03 -0.25  0.95 -0.09 -0.06  0.94 -0.02  0.02  0.89 -0.11  0.14  0.92\n",
      "   0.01  0.09  0.9  -0.1   0.22  1.02  0.02  0.04  1.07 -0.04  0.14  1.04\n",
      "   0.    0.07  0.99 -0.09  0.08  0.88 -0.   -0.13  0.92 -0.11  0.    0.93]\n",
      " [-0.13  0.2   0.94 -0.07  0.19  1.05  0.    0.05  1.09 -0.12  0.17  0.95\n",
      "  -0.05  0.13  1.03 -0.01  0.11  1.07 -0.12  0.11  0.98 -0.04  0.06  1.05\n",
      "  -0.08 -0.18  0.92 -0.09 -0.15  0.97 -0.   -0.21  1.07 -0.08  0.08  0.92\n",
      "  -0.1   0.13  0.94 -0.02  0.13  1.03 -0.1   0.12  0.93 -0.13  0.2   0.95\n",
      "  -0.05 -0.03  0.9  -0.11  0.05  0.91 -0.13  0.16  0.93 -0.05  0.05  0.92]\n",
      " [ 0.05  0.03  1.06  0.04  0.05  1.08 -0.    0.07  1.07  0.02 -0.21  0.98\n",
      "  -0.02 -0.19  0.94  0.03 -0.22  0.91  0.02  0.03  0.94  0.05  0.04  1.\n",
      "   0.06  0.12  1.08 -0.1   0.12  1.04 -0.12  0.07  1.01 -0.11  0.04  0.89\n",
      "  -0.08  0.07  0.89 -0.05  0.11  0.94 -0.01  0.07  1.09 -0.08  0.11  1.05\n",
      "  -0.1   0.09  1.03  0.04 -0.24  0.89  0.09 -0.22  0.88  0.07  0.03  1.04]\n",
      " [-0.13  0.05  0.91 -0.    0.02  0.99 -0.11  0.06  0.94 -0.1   0.2   0.98\n",
      "  -0.06 -0.04  0.91 -0.15  0.22  0.92 -0.04  0.12  1.03 -0.11  0.11  0.94\n",
      "  -0.09  0.16  0.98 -0.05 -0.16  0.9  -0.1  -0.12  0.94  0.   -0.1   1.04\n",
      "  -0.1  -0.05  0.9  -0.01 -0.07  1.03  0.04 -0.13  1.03 -0.03  0.14  1.05\n",
      "   0.04  0.04  1.01 -0.09  0.1   1.02  0.04 -0.01  1.04 -0.1   0.07  0.95]\n",
      " [-0.12  0.16  0.93 -0.05  0.2   1.05  0.04 -0.02  0.95 -0.02  0.09  1.05\n",
      "  -0.01  0.06  0.94  0.03 -0.04  1.07 -0.05 -0.07  0.91 -0.13  0.06  0.94\n",
      "  -0.08 -0.2   0.91 -0.06 -0.13  1.04  0.07 -0.28  1.09  0.    0.11  1.03\n",
      "   0.05 -0.03  0.97  0.02  0.05  1.09 -0.01 -0.03  0.95 -0.11  0.15  0.93\n",
      "  -0.06 -0.02  0.88 -0.13  0.14  0.92 -0.03  0.14  1.03 -0.12  0.18  1.01]\n",
      " [-0.03  0.13  1.08 -0.02  0.09  1.1   0.03  0.05  1.03  0.06  0.06  1.08\n",
      "  -0.02 -0.01  0.93 -0.16  0.05  0.92 -0.   -0.2   0.88  0.   -0.15  0.99\n",
      "   0.14 -0.12  1.07  0.02  0.08  1.08  0.05  0.04  1.02  0.06  0.06  1.08\n",
      "  -0.   -0.02  0.93 -0.14  0.04  0.9  -0.12 -0.05  0.88 -0.13  0.08  1.\n",
      "  -0.03  0.11  1.11 -0.02  0.09  1.09  0.03  0.04  1.04 -0.06 -0.13  1.06]\n",
      " [ 0.01  0.04  1.05 -0.05  0.02  1.02  0.03 -0.21  0.99 -0.1  -0.07  0.88\n",
      "  -0.01 -0.27  0.92 -0.07 -0.12  0.97  0.08 -0.24  0.93 -0.02  0.11  1.03\n",
      "   0.04 -0.01  0.99 -0.02  0.08  1.06 -0.02 -0.03  0.97 -0.14  0.12  0.92\n",
      "  -0.06 -0.02  0.91 -0.1   0.15  0.96  0.02  0.02  1.07 -0.05  0.13  1.01\n",
      "   0.03 -0.06  1.01  0.02 -0.25  1.04  0.03 -0.32  0.97 -0.09 -0.25  0.91]\n",
      " [-0.   -0.02  0.92 -0.08  0.06  1.06  0.01 -0.05  0.97 -0.05 -0.04  0.91\n",
      "   0.01  0.06  1.06 -0.02  0.06  0.93 -0.09  0.06  0.89  0.03 -0.17  0.99\n",
      "  -0.04 -0.17  0.9  -0.1  -0.08  0.92  0.03 -0.31  0.95 -0.08 -0.17  0.91\n",
      "   0.04 -0.05  0.95 -0.04  0.02  0.87 -0.1   0.12  0.92 -0.04  0.    0.92\n",
      "  -0.11  0.13  0.93 -0.12  0.22  0.98 -0.09 -0.    0.86 -0.13  0.13  0.94]\n",
      " [ 0.04 -0.27  1.04 -0.11  0.05  0.91 -0.01  0.04  1.03  0.06 -0.05  0.93\n",
      "  -0.09  0.16  0.99  0.02  0.03  1.06 -0.16  0.15  0.96 -0.02  0.07  1.05\n",
      "   0.03 -0.01  1.01 -0.04  0.13  1.03  0.02  0.08  1.05 -0.06  0.07  0.91\n",
      "   0.03 -0.17  1.03  0.01 -0.2   0.96  0.02 -0.12  1.05  0.04 -0.2   1.\n",
      "  -0.08 -0.08  0.93  0.05  0.04  1.05 -0.04 -0.    0.92 -0.12  0.12  0.9 ]\n",
      " [ 0.04  0.05  1.05  0.01  0.06  1.07 -0.04  0.09  1.07 -0.04 -0.15  0.96\n",
      "  -0.01 -0.17  0.92  0.08 -0.09  0.93  0.1  -0.06  0.99  0.11 -0.06  1.05\n",
      "  -0.1   0.1   1.04 -0.13  0.08  1.01 -0.14  0.05  0.96 -0.05  0.04  0.9\n",
      "  -0.03  0.1   0.95  0.02  0.05  1.07 -0.05  0.07  1.05 -0.09  0.08  1.03\n",
      "  -0.06 -0.1   0.93 -0.02 -0.11  0.89  0.06 -0.15  0.91  0.15 -0.13  1.04]\n",
      " [ 0.04  0.02  1.05 -0.08  0.13  1.03  0.02 -0.04  1.02 -0.15  0.11  0.9\n",
      "  -0.07  0.03  0.92 -0.09  0.12  1.01  0.01  0.02  1.    0.01 -0.01  1.03\n",
      "  -0.06 -0.05  0.91  0.05 -0.3   1.02 -0.1  -0.18  0.9   0.05 -0.27  1.06\n",
      "  -0.04  0.13  1.01  0.04 -0.03  0.99 -0.11  0.12  0.89 -0.08  0.05  0.93\n",
      "  -0.06  0.11  1.02  0.02  0.08  0.89 -0.01  0.04  1.06 -0.07 -0.01  0.93]\n",
      " [ 0.01  0.08  1.07 -0.01  0.04  0.97 -0.07  0.08  0.93  0.03 -0.2   1.07\n",
      "   0.04 -0.27  0.99 -0.02 -0.26  0.93  0.04 -0.15  1.08  0.07 -0.2   1.02\n",
      "  -0.05  0.15  1.02  0.02  0.07  1.05  0.06  0.01  1.04 -0.1   0.23  1.\n",
      "  -0.03  0.14  1.04  0.02  0.06  1.08 -0.12  0.17  0.95 -0.04  0.17  1.04\n",
      "  -0.11 -0.    0.93 -0.13  0.04  0.96 -0.06  0.04  1.05 -0.08 -0.21  0.91]\n",
      " [-0.11  0.23  1.01 -0.03  0.12  1.04  0.03  0.01  0.91 -0.07  0.17  1.02\n",
      "  -0.    0.1   1.06 -0.12  0.08  0.97 -0.02  0.04  1.06  0.03 -0.07  1.08\n",
      "  -0.06 -0.11  1.02  0.04 -0.25  1.06  0.08 -0.31  1.05 -0.04  0.14  1.02\n",
      "   0.03  0.06  1.07 -0.12  0.21  0.95 -0.07  0.18  1.04 -0.    0.08  1.07\n",
      "  -0.12  0.16  0.91 -0.08  0.16  1.   -0.02  0.12  1.03 -0.14  0.15  0.93]\n",
      " [ 0.09 -0.19  0.92  0.   -0.18  0.94 -0.02 -0.09  0.88 -0.1  -0.01  0.95\n",
      "  -0.14  0.07  0.94 -0.09  0.15  1.05  0.02  0.07  1.08 -0.03  0.12  1.09\n",
      "   0.03  0.02  1.   -0.07 -0.06  0.9  -0.07  0.02  0.92 -0.16  0.07  0.93\n",
      "  -0.06 -0.05  0.9  -0.04  0.02  1.04  0.12 -0.04  1.07  0.15 -0.1   1.05\n",
      "   0.14 -0.15  0.95 -0.02 -0.16  0.87 -0.09 -0.01  0.88 -0.14  0.11  0.95]\n",
      " [-0.11  0.08  1.   -0.15  0.08  0.96 -0.15 -0.18  0.89  0.05 -0.18  0.9\n",
      "   0.1  -0.2   0.93  0.15 -0.12  1.05  0.11 -0.06  1.07  0.08 -0.05  1.06\n",
      "  -0.13  0.08  0.99 -0.14  0.06  0.96 -0.14  0.06  0.94 -0.06  0.    0.91\n",
      "  -0.05  0.04  0.92  0.04  0.04  1.05  0.04  0.04  1.07  0.02  0.06  1.07\n",
      "  -0.03 -0.08  1.02 -0.04 -0.11  0.97 -0.03 -0.14  0.94  0.07 -0.17  0.9 ]\n",
      " [ 0.04  0.05  1.1   0.06 -0.03  0.97 -0.02 -0.    0.9   0.02  0.03  1.09\n",
      "   0.03 -0.05  0.98 -0.03 -0.01  0.93  0.01  0.11  1.07  0.    0.04  0.98\n",
      "  -0.03  0.13  1.06  0.01  0.03  1.08  0.03 -0.05  1.01 -0.02 -0.22  1.07\n",
      "   0.05 -0.3   1.09  0.07 -0.36  1.02 -0.03  0.13  1.04  0.04  0.05  1.08\n",
      "   0.06 -0.03  0.94 -0.06  0.15  1.05  0.01  0.06  1.08 -0.13  0.18  0.92]\n",
      " [-0.03  0.13  1.06  0.01  0.03  1.08  0.03 -0.05  1.01 -0.02 -0.22  1.07\n",
      "   0.05 -0.3   1.09  0.07 -0.36  1.02 -0.03  0.13  1.04  0.04  0.05  1.08\n",
      "   0.06 -0.03  0.94 -0.06  0.15  1.05  0.01  0.06  1.08 -0.13  0.18  0.92\n",
      "  -0.07  0.17  1.02 -0.02  0.13  1.04 -0.14  0.18  0.95 -0.08  0.17  1.05\n",
      "   0.    0.08  1.07 -0.1  -0.2   0.95 -0.03 -0.19  1.05 -0.08 -0.    0.92]\n",
      " [ 0.03 -0.05  1.01 -0.02 -0.22  1.07  0.05 -0.3   1.09  0.07 -0.36  1.02\n",
      "  -0.03  0.13  1.04  0.04  0.05  1.08  0.06 -0.03  0.94 -0.06  0.15  1.05\n",
      "   0.01  0.06  1.08 -0.13  0.18  0.92 -0.07  0.17  1.02 -0.02  0.13  1.04\n",
      "  -0.14  0.18  0.95 -0.08  0.17  1.05  0.    0.08  1.07 -0.1  -0.2   0.95\n",
      "  -0.03 -0.19  1.05 -0.08 -0.    0.92 -0.11  0.1   0.94 -0.03  0.1   1.05]\n",
      " [-0.03  0.04  0.93  0.01  0.06  0.97 -0.    0.08  1.04 -0.03  0.07  1.06\n",
      "  -0.09  0.08  1.02 -0.13  0.04  0.95 -0.1  -0.    0.93 -0.04 -0.03  0.92\n",
      "   0.16 -0.16  1.01  0.17 -0.15  1.03 -0.08  0.08  1.02 -0.1   0.08  1.\n",
      "  -0.13  0.05  0.96 -0.04  0.01  0.92 -0.01  0.03  0.95  0.    0.06  0.98\n",
      "   0.    0.09  1.05 -0.04  0.07  1.04 -0.08  0.08  0.98 -0.15  0.06  0.94]\n",
      " [-0.01 -0.22  0.91 -0.11  0.11  0.84  0.09  0.02  1.02 -0.1   0.21  0.96\n",
      "   0.04  0.14  1.09 -0.13  0.16  0.85  0.09  0.02  1.05 -0.03 -0.01  0.93\n",
      "  -0.03  0.2   1.06  0.04  0.09  1.09 -0.18  0.23  0.88 -0.02  0.1   0.97\n",
      "  -0.04 -0.18  0.92  0.11 -0.17  1.18 -0.08 -0.11  0.9  -0.1   0.17  0.95\n",
      "   0.01 -0.05  0.92  0.02  0.1   0.85  0.1   0.03  1.09 -0.14  0.21  0.88]\n",
      " [ 0.03  0.    0.98 -0.17  0.18  0.87 -0.04  0.11  0.94  0.01  0.13  0.89\n",
      "  -0.05  0.02  0.93  0.07  0.01  1.15  0.   -0.16  0.92  0.15 -0.21  1.15\n",
      "  -0.01 -0.06  1.04  0.09  0.04  1.07 -0.05  0.16  1.03 -0.12  0.15  0.85\n",
      "  -0.03  0.15  1.05 -0.14  0.17  0.86 -0.05  0.15  1.07 -0.16  0.18  0.87\n",
      "  -0.06  0.06  0.93 -0.19  0.19  0.87 -0.05  0.09  0.9   0.08  0.03  1.12]\n",
      " [-0.18  0.1   0.95 -0.01  0.11  1.08  0.02  0.04  1.05  0.15 -0.11  1.06\n",
      "   0.09 -0.19  0.92  0.   -0.18  0.94 -0.02 -0.09  0.88 -0.1  -0.01  0.95\n",
      "  -0.14  0.07  0.94 -0.09  0.15  1.05  0.02  0.07  1.08 -0.03  0.12  1.09\n",
      "   0.03  0.02  1.   -0.07 -0.06  0.9  -0.07  0.02  0.92 -0.16  0.07  0.93\n",
      "  -0.06 -0.05  0.9  -0.04  0.02  1.04  0.12 -0.04  1.07  0.15 -0.1   1.05]\n",
      " [ 0.04 -0.05  0.95 -0.04  0.02  0.87 -0.1   0.12  0.92 -0.04  0.    0.92\n",
      "  -0.11  0.13  0.93 -0.12  0.22  0.98 -0.09 -0.    0.86 -0.13  0.13  0.94\n",
      "  -0.03 -0.03  0.92 -0.11  0.05  0.93 -0.12  0.12  0.99 -0.06 -0.26  0.9\n",
      "  -0.1  -0.16  0.94 -0.03 -0.19  1.06 -0.08  0.07  0.92 -0.09  0.16  0.98\n",
      "  -0.    0.06  0.88 -0.13  0.19  0.94 -0.05  0.18  1.04 -0.13  0.07  0.94]\n",
      " [ 0.06 -0.09  1.1   0.11  0.07  1.09 -0.12  0.2   0.86  0.06  0.01  0.98\n",
      "  -0.08  0.08  0.86  0.07  0.09  1.11 -0.14  0.18  0.86 -0.12  0.23  0.97\n",
      "  -0.05  0.03  0.92 -0.03  0.2   0.86  0.02  0.09  1.07 -0.18  0.22  0.91\n",
      "   0.03 -0.07  1.11 -0.07 -0.15  0.9   0.11 -0.17  1.17  0.1  -0.09  1.01\n",
      "  -0.04  0.14  1.   -0.01 -0.03  0.9  -0.13  0.17  0.84  0.09  0.03  1.05]\n",
      " [-0.01 -0.06  0.96 -0.01  0.11  1.04  0.01  0.04  1.   -0.04  0.02  1.05\n",
      "   0.02 -0.01  1.07  0.02 -0.1   0.98  0.   -0.23  1.06  0.06 -0.3   1.1\n",
      "   0.05 -0.36  0.98  0.01  0.08  1.03  0.06  0.01  1.05  0.04 -0.04  0.93\n",
      "  -0.01  0.08  1.04  0.05 -0.01  1.03 -0.08  0.18  1.01 -0.02  0.12  1.03\n",
      "   0.02  0.02  1.   -0.06  0.19  1.06  0.    0.08  1.09  0.01  0.02  1.  ]\n",
      " [-0.16  0.05  0.92 -0.   -0.2   0.88  0.   -0.15  0.99  0.14 -0.12  1.07\n",
      "   0.02  0.08  1.08  0.05  0.04  1.02  0.06  0.06  1.08 -0.   -0.02  0.93\n",
      "  -0.14  0.04  0.9  -0.12 -0.05  0.88 -0.13  0.08  1.   -0.03  0.11  1.11\n",
      "  -0.02  0.09  1.09  0.03  0.04  1.04 -0.06 -0.13  1.06  0.1  -0.2   0.9\n",
      "   0.01 -0.2   0.92 -0.06 -0.07  0.88 -0.04  0.04  1.02  0.07  0.01  1.07]\n",
      " [-0.05 -0.29  0.9  -0.11 -0.23  1.06 -0.01 -0.13  0.95 -0.1  -0.04  0.93\n",
      "   0.01 -0.01  0.99 -0.1   0.05  0.94 -0.1   0.14  0.98 -0.05 -0.01  0.92\n",
      "  -0.15  0.14  0.94 -0.03  0.08  1.05 -0.11  0.07  0.91 -0.04  0.13  1.02\n",
      "  -0.06 -0.09  0.91 -0.08 -0.02  0.98  0.04 -0.1   1.02 -0.06 -0.14  0.95\n",
      "   0.02 -0.21  1.05  0.03 -0.26  0.98 -0.01  0.08  1.04  0.04 -0.01  1.02]\n",
      " [ 0.02  0.02  1.09  0.04  0.03  1.02 -0.03 -0.04  0.9  -0.01  0.01  0.96\n",
      "  -0.1  -0.01  0.87 -0.15  0.05  0.94 -0.15  0.05  0.91 -0.14  0.12  1.01\n",
      "   0.01  0.08  1.09  0.07 -0.13  1.06  0.17 -0.17  1.07  0.02  0.07  1.07\n",
      "   0.07  0.04  1.04 -0.   -0.01  0.89  0.04  0.01  0.97 -0.08 -0.01  0.89\n",
      "  -0.15  0.08  0.95 -0.13 -0.01  0.89 -0.12  0.07  1.02 -0.16  0.1   0.93]\n",
      " [ 0.13 -0.07  1.01  0.09  0.    1.04  0.03  0.01  1.06 -0.15  0.06  0.98\n",
      "  -0.12  0.05  0.94 -0.07  0.09  0.95 -0.02  0.12  1.03 -0.02  0.12  1.05\n",
      "  -0.14  0.1   1.01 -0.17  0.06  0.96 -0.15  0.08  0.91  0.11 -0.12  0.96\n",
      "   0.14 -0.12  1.03  0.13 -0.08  1.06 -0.03 -0.06  1.   -0.05 -0.08  0.94\n",
      "  -0.03  0.03  0.9   0.04  0.04  0.95  0.05  0.07  1.02 -0.07  0.11  1.06]\n",
      " [-0.04  0.03  0.96 -0.11  0.01  0.88 -0.15  0.13  0.96 -0.15  0.06  0.94\n",
      "  -0.1   0.13  1.07  0.03  0.07  1.08 -0.    0.09  1.1  -0.02  0.05  0.99\n",
      "  -0.12 -0.    1.01 -0.07 -0.02  0.89 -0.13  0.08  0.96  0.01 -0.16  0.92\n",
      "   0.08 -0.11  1.05  0.19 -0.19  1.02  0.06  0.06  1.08 -0.    0.01  0.93\n",
      "  -0.13  0.04  0.89 -0.1   0.    0.89 -0.18  0.14  1.   -0.15  0.05  0.98]\n",
      " [-0.11 -0.03  0.9  -0.15  0.07  1.02 -0.18  0.1   0.95 -0.01  0.11  1.08\n",
      "   0.02  0.04  1.05  0.15 -0.11  1.06  0.09 -0.19  0.92  0.   -0.18  0.94\n",
      "  -0.02 -0.09  0.88 -0.1  -0.01  0.95 -0.14  0.07  0.94 -0.09  0.15  1.05\n",
      "   0.02  0.07  1.08 -0.03  0.12  1.09  0.03  0.02  1.   -0.07 -0.06  0.9\n",
      "  -0.07  0.02  0.92 -0.16  0.07  0.93 -0.06 -0.05  0.9  -0.04  0.02  1.04]\n",
      " [-0.03 -0.08  0.9  -0.08  0.04  0.92 -0.1   0.11  0.94 -0.02 -0.    0.93\n",
      "  -0.09  0.07  0.89 -0.11  0.18  1.   -0.01 -0.06  0.95 -0.08 -0.03  0.87\n",
      "   0.01  0.08  1.07 -0.01  0.04  0.97 -0.07  0.08  0.93  0.03 -0.2   1.07\n",
      "   0.04 -0.27  0.99 -0.02 -0.26  0.93  0.04 -0.15  1.08  0.07 -0.2   1.02\n",
      "  -0.05  0.15  1.02  0.02  0.07  1.05  0.06  0.01  1.04 -0.1   0.23  1.  ]\n",
      " [ 0.07  0.03  1.06 -0.01  0.01  0.93  0.02 -0.01  0.96 -0.11 -0.02  0.9\n",
      "  -0.17  0.13  1.   -0.14  0.04  0.96 -0.03  0.09  1.09 -0.05  0.1   1.06\n",
      "   0.06  0.05  1.06 -0.01 -0.02  0.93  0.12 -0.2   0.93  0.02 -0.21  0.87\n",
      "   0.01 -0.12  1.   -0.14  0.09  0.95  0.02  0.06  1.07 -0.07  0.15  1.06\n",
      "   0.07  0.02  1.05 -0.04  0.01  0.9  -0.01 -0.06  0.93 -0.14  0.01  0.91]\n",
      " [-0.05 -0.08  0.94 -0.03  0.03  0.9   0.04  0.04  0.95  0.05  0.07  1.02\n",
      "  -0.07  0.11  1.06 -0.11  0.09  1.04 -0.14  0.06  1.   -0.13  0.08  0.87\n",
      "  -0.05  0.1   0.91  0.07 -0.    1.07  0.03 -0.01  1.08 -0.03 -0.02  1.05\n",
      "  -0.   -0.17  0.9   0.01 -0.17  0.87  0.07 -0.11  0.91  0.04  0.11  1.04\n",
      "  -0.02  0.11  1.06 -0.15  0.06  0.99 -0.12  0.02  0.94 -0.08 -0.    0.91]\n",
      " [-0.   -0.19  0.89  0.02 -0.2   0.86  0.03 -0.17  0.86  0.06  0.05  1.03\n",
      "   0.06  0.07  1.06  0.02  0.1   1.06 -0.11  0.09  1.03 -0.13  0.08  1.\n",
      "  -0.11  0.02  0.86 -0.09  0.05  0.86 -0.07  0.08  0.89  0.07  0.01  1.08\n",
      "   0.06  0.01  1.1   0.03  0.02  1.1  -0.   -0.16  0.97  0.   -0.2   0.94\n",
      "  -0.04  0.    0.89 -0.03  0.02  0.91  0.02  0.02  0.95 -0.02  0.11  1.08]\n",
      " [-0.08  0.21  1.02 -0.1   0.05  0.87 -0.18  0.16  0.89  0.05 -0.03  1.08\n",
      "  -0.15  0.16  0.97  0.08 -0.13  1.11 -0.06 -0.15  0.88  0.14 -0.18  1.15\n",
      "   0.06 -0.    1.   -0.05  0.24  0.99  0.06  0.06  1.1  -0.15  0.17  0.87\n",
      "   0.09 -0.02  1.03 -0.06  0.06  0.92 -0.03  0.22  1.06 -0.15  0.17  0.86\n",
      "  -0.17  0.21  0.93 -0.05  0.1   0.92 -0.08 -0.08  0.89  0.13 -0.22  1.14]\n",
      " [ 0.04 -0.02  1.02 -0.12  0.08  0.89 -0.07 -0.    0.91 -0.1   0.13  1.02\n",
      "   0.03 -0.04  1.02  0.    0.07  1.07 -0.07  0.02  0.93 -0.   -0.04  0.97\n",
      "  -0.13  0.05  0.9   0.02 -0.04  1.05 -0.   -0.24  1.04  0.01 -0.3   0.97\n",
      "  -0.09 -0.2   0.94 -0.08  0.07  0.93  0.    0.08  1.05  0.02 -0.04  0.94\n",
      "   0.04  0.01  1.05 -0.12  0.08  0.94 -0.02 -0.05  0.93 -0.14  0.14  0.96]\n",
      " [ 0.1  -0.09  1.01 -0.04  0.14  1.   -0.01 -0.03  0.9  -0.13  0.17  0.84\n",
      "   0.09  0.03  1.05 -0.03  0.01  0.93 -0.01  0.18  1.06 -0.14  0.14  0.87\n",
      "  -0.16  0.24  0.91 -0.07  0.15  0.92 -0.03  0.19  1.05  0.08 -0.08  1.15\n",
      "  -0.13  0.03  0.92  0.03 -0.02  1.06 -0.04 -0.1   0.88  0.13 -0.07  1.11\n",
      "   0.07  0.03  1.02 -0.1   0.25  0.95 -0.01  0.04  0.92 -0.12  0.12  0.85]\n",
      " [-0.13  0.15  0.86  0.1  -0.01  1.06  0.02  0.05  0.93 -0.02  0.12  1.05\n",
      "  -0.09  0.09  0.85 -0.14  0.16  0.86  0.02  0.04  1.03 -0.13  0.19  0.97\n",
      "  -0.01  0.15  1.07 -0.17  0.18  0.86 -0.08 -0.01  0.98  0.01 -0.21  0.99\n",
      "   0.06 -0.11  1.11  0.12 -0.07  1.12 -0.12  0.1   0.86  0.09 -0.07  1.01\n",
      "  -0.04  0.07  0.88  0.04  0.1   1.07  0.04  0.04  1.11 -0.16  0.18  0.87]\n",
      " [-0.09 -0.03  0.89 -0.07 -0.    0.9  -0.15  0.06  0.94 -0.06  0.09  1.06\n",
      "  -0.1   0.08  1.03  0.06  0.02  1.07  0.07 -0.06  0.96  0.18 -0.19  1.\n",
      "   0.07 -0.19  0.87 -0.    0.    0.93 -0.14  0.04  0.89 -0.15  0.14  1.\n",
      "  -0.16  0.09  0.97 -0.07  0.14  1.08  0.04  0.03  1.03  0.01  0.08  1.09\n",
      "  -0.02 -0.01  0.93 -0.14  0.    0.97 -0.12  0.03  0.9  -0.15  0.13  0.99]\n",
      " [-0.11  0.    0.91 -0.02 -0.02  0.92  0.19 -0.18  1.01  0.16 -0.14  1.02\n",
      "   0.16 -0.16  1.05 -0.11  0.09  1.02 -0.12  0.06  0.99 -0.08  0.01  0.91\n",
      "  -0.03  0.02  0.93 -0.    0.06  0.95 -0.01  0.12  1.02 -0.01  0.08  1.04\n",
      "  -0.05  0.07  1.05 -0.16  0.07  0.97 -0.13  0.05  0.94 -0.09 -0.2   0.94\n",
      "   0.18 -0.2   0.99  0.18 -0.16  1.03 -0.07  0.1   1.03 -0.1   0.07  1.02]\n",
      " [ 0.02 -0.12  0.94  0.03  0.04  1.09  0.03 -0.03  1.   -0.02 -0.    0.93\n",
      "   0.    0.07  1.07  0.01 -0.06  0.96 -0.02  0.11  1.04  0.01  0.07  1.05\n",
      "  -0.02  0.04  0.92  0.02 -0.15  1.04  0.03 -0.26  0.99 -0.03 -0.22  0.92\n",
      "   0.07 -0.21  1.1   0.05 -0.28  0.98 -0.04  0.08  1.03  0.05 -0.    1.04\n",
      "   0.01 -0.03  0.93 -0.01  0.09  1.04  0.02 -0.02  1.02 -0.01 -0.06  0.96]\n",
      " [ 0.04  0.03  1.11 -0.16  0.1   0.9   0.05 -0.09  1.07  0.   -0.18  0.92\n",
      "   0.14 -0.1   1.1   0.07  0.05  1.03 -0.11  0.23  0.95 -0.    0.04  0.92\n",
      "  -0.13  0.11  0.85  0.07  0.04  1.09 -0.15  0.22  0.88 -0.05  0.2   1.04\n",
      "  -0.12  0.01  0.85  0.01  0.19  0.86  0.01 -0.    1.03 -0.11  0.24  1.02\n",
      "   0.11 -0.16  1.15 -0.1  -0.09  0.89  0.14 -0.24  1.09 -0.    0.01  0.92]\n",
      " [-0.06  0.07  0.99  0.03 -0.02  1.04 -0.07 -0.19  0.95  0.03 -0.22  1.05\n",
      "   0.04 -0.26  1.   -0.02  0.12  1.03  0.04  0.01  1.02 -0.07  0.06  0.93\n",
      "   0.03  0.05  1.06 -0.06  0.02  0.92  0.    0.08  1.06 -0.01 -0.02  0.95\n",
      "  -0.12  0.09  0.91  0.01  0.02  0.98 -0.1   0.1   0.91 -0.08  0.16  1.\n",
      "  -0.05 -0.25  0.93 -0.06 -0.21  0.96  0.04 -0.24  0.92 -0.11  0.08  0.93]\n",
      " [-0.03  0.2   0.86  0.02  0.09  1.07 -0.18  0.22  0.91  0.03 -0.07  1.11\n",
      "  -0.07 -0.15  0.9   0.11 -0.17  1.17  0.1  -0.09  1.01 -0.04  0.14  1.\n",
      "  -0.01 -0.03  0.9  -0.13  0.17  0.84  0.09  0.03  1.05 -0.03  0.01  0.93\n",
      "  -0.01  0.18  1.06 -0.14  0.14  0.87 -0.16  0.24  0.91 -0.07  0.15  0.92\n",
      "  -0.03  0.19  1.05  0.08 -0.08  1.15 -0.13  0.03  0.92  0.03 -0.02  1.06]\n",
      " [-0.19  0.19  0.87 -0.05  0.09  0.9   0.08  0.03  1.12 -0.03 -0.12  0.89\n",
      "   0.13 -0.25  1.12 -0.11  0.06  0.84  0.09 -0.02  1.02  0.04  0.06  1.06\n",
      "   0.02  0.07  0.94  0.05  0.05  1.11 -0.12  0.2   0.95  0.05  0.09  1.1\n",
      "  -0.11  0.18  1.01 -0.15  0.13  1.05 -0.02  0.14  1.06 -0.18  0.18  0.86\n",
      "   0.09 -0.16  1.12 -0.1  -0.05  0.97 -0.06 -0.1   0.88 -0.04  0.04  1.01]\n",
      " [-0.01  0.08  1.04  0.05 -0.01  1.03 -0.08  0.18  1.01 -0.02  0.12  1.03\n",
      "   0.02  0.02  1.   -0.06  0.19  1.06  0.    0.08  1.09  0.01  0.02  1.\n",
      "  -0.01 -0.24  1.06  0.05 -0.29  1.09  0.05 -0.38  0.99  0.    0.06  1.05\n",
      "   0.06 -0.02  1.09  0.06  0.18  0.98 -0.01  0.09  1.03  0.04  0.04  1.08\n",
      "  -0.12  0.25  0.97 -0.03  0.18  1.02  0.01  0.07  1.05 -0.12  0.19  0.98]\n",
      " [-0.09 -0.25  0.9   0.04 -0.26  1.04 -0.02 -0.32  0.91  0.07 -0.05  1.03\n",
      "  -0.09  0.05  0.92 -0.   -0.02  0.97 -0.13  0.14  0.95  0.03  0.02  1.05\n",
      "  -0.1   0.15  1.02  0.01 -0.06  1.02 -0.14  0.08  0.9  -0.06  0.03  0.91\n",
      "  -0.1   0.18  0.97  0.03  0.    1.02  0.01 -0.17  1.04 -0.01 -0.32  0.93\n",
      "   0.08 -0.14  1.06 -0.08 -0.06  0.93  0.   -0.02  1.04 -0.13  0.1   0.91]\n",
      " [ 0.04  0.15  1.09 -0.16  0.18  0.89 -0.04  0.23  1.03 -0.16  0.15  0.84\n",
      "   0.01  0.12  1.09  0.06 -0.23  1.02  0.04 -0.1   1.12 -0.05 -0.03  1.07\n",
      "  -0.1   0.12  0.92  0.01 -0.06  0.89 -0.14  0.17  0.85  0.07  0.04  1.03\n",
      "  -0.08  0.24  0.97  0.    0.17  1.08 -0.16  0.16  0.85  0.04  0.01  1.03\n",
      "  -0.11  0.11  0.86 -0.    0.18  1.06  0.06 -0.11  1.07 -0.05  0.04  1.06]\n",
      " [-0.07  0.19  1.02 -0.14  0.15  0.86  0.03  0.    0.98 -0.17  0.18  0.87\n",
      "  -0.04  0.11  0.94  0.01  0.13  0.89 -0.05  0.02  0.93  0.07  0.01  1.15\n",
      "   0.   -0.16  0.92  0.15 -0.21  1.15 -0.01 -0.06  1.04  0.09  0.04  1.07\n",
      "  -0.05  0.16  1.03 -0.12  0.15  0.85 -0.03  0.15  1.05 -0.14  0.17  0.86\n",
      "  -0.05  0.15  1.07 -0.16  0.18  0.87 -0.06  0.06  0.93 -0.19  0.19  0.87]\n",
      " [ 0.05 -0.11  1.05 -0.06  0.12  1.02  0.02 -0.01  1.01 -0.11  0.02  1.06\n",
      "  -0.05  0.01  0.92 -0.12  0.11  1.   -0.11  0.12  0.91 -0.02  0.1   1.05\n",
      "  -0.03  0.03  0.93  0.03 -0.09  1.04 -0.11 -0.05  0.9  -0.01 -0.05  1.04\n",
      "  -0.09 -0.17  0.95  0.07 -0.26  1.07 -0.03  0.1   1.04  0.02 -0.04  0.97\n",
      "  -0.1   0.13  0.9  -0.07  0.02  0.94 -0.09  0.14  1.    0.03 -0.03  1.04]\n",
      " [-0.04 -0.02  1.01  0.02 -0.19  0.92 -0.12  0.13  0.83  0.09  0.05  1.07\n",
      "   0.01  0.05  0.96 -0.05  0.16  1.04 -0.09  0.1   0.86 -0.16  0.17  0.88\n",
      "   0.01  0.11  1.03 -0.14  0.21  0.95 -0.02  0.14  1.08 -0.13  0.11  0.86\n",
      "   0.05  0.03  0.89  0.14 -0.25  1.08 -0.06 -0.05  1.    0.02  0.09  1.07\n",
      "  -0.09  0.09  0.84  0.07  0.07  1.12  0.07  0.03  1.02 -0.12  0.21  0.96]\n",
      " [ 0.05  0.04  1.06  0.07 -0.18  1.02  0.01 -0.17  1.   -0.07 -0.02  0.87\n",
      "  -0.03 -0.    0.89  0.02  0.02  0.94 -0.02  0.11  1.09 -0.06  0.11  1.08\n",
      "  -0.1   0.11  1.04 -0.13  0.02  0.88 -0.1   0.02  0.87  0.03  0.06  1.04\n",
      "   0.04  0.05  1.07 -0.02  0.1   1.06  0.   -0.18  0.97  0.   -0.2   0.93\n",
      "   0.03 -0.22  0.88  0.06 -0.01  0.98  0.1  -0.02  1.04 -0.08  0.1   1.05]\n",
      " [-0.09  0.09  0.92 -0.1   0.14  0.95 -0.03  0.02  0.92 -0.1   0.12  0.92\n",
      "  -0.13  0.19  0.94 -0.03 -0.03  0.92 -0.1   0.01  0.89 -0.13  0.14  0.91\n",
      "  -0.03  0.04  0.93 -0.1   0.09  0.9   0.06 -0.31  1.05  0.02 -0.33  0.96\n",
      "  -0.04 -0.29  0.89  0.06 -0.07  1.1   0.07 -0.12  0.99  0.01 -0.12  0.93\n",
      "   0.01  0.06  1.07  0.05  0.    1.05 -0.12  0.21  0.98 -0.05  0.18  1.04]\n",
      " [-0.07  0.14  1.08  0.04  0.03  1.03  0.01  0.08  1.09 -0.02 -0.01  0.93\n",
      "  -0.14  0.    0.97 -0.12  0.03  0.9  -0.15  0.13  0.99 -0.02 -0.12  0.96\n",
      "   0.12 -0.15  1.06  0.16 -0.2   0.96  0.08 -0.03  0.99 -0.05 -0.05  0.87\n",
      "  -0.13  0.08  0.95 -0.19  0.09  0.94 -0.06  0.12  1.07 -0.08  0.11  1.07\n",
      "   0.01  0.09  1.09 -0.02 -0.05  0.93  0.    0.04  0.96 -0.13  0.04  0.9 ]\n",
      " [-0.02  0.1   1.   -0.02  0.11  1.05 -0.08  0.08  1.03 -0.14  0.09  0.99\n",
      "  -0.16  0.06  0.94  0.03 -0.13  0.91  0.08 -0.13  0.93  0.15 -0.16  1.01\n",
      "   0.08 -0.11  1.06  0.   -0.05  1.02 -0.12  0.04  0.92 -0.1   0.03  0.89\n",
      "  -0.04  0.03  0.9  -0.01  0.09  1.05 -0.02  0.12  1.07 -0.04  0.1   1.08\n",
      "  -0.15  0.06  0.94 -0.15  0.05  0.91  0.01 -0.07  0.94  0.09 -0.09  0.98]\n",
      " [ 0.02  0.08  0.89 -0.01  0.04  1.06 -0.07 -0.01  0.93  0.01  0.    0.99\n",
      "  -0.12  0.12  0.89  0.    0.05  1.04 -0.02 -0.2   1.01  0.04 -0.32  1.01\n",
      "  -0.08 -0.25  0.9  -0.05 -0.06  0.92 -0.06  0.11  1.   -0.12  0.09  0.91\n",
      "  -0.    0.08  1.04 -0.03 -0.03  0.96  0.02  0.02  1.05 -0.08  0.01  0.95\n",
      "  -0.08  0.13  1.01 -0.12  0.13  0.91 -0.01  0.1   1.04 -0.07 -0.11  0.97]\n",
      " [-0.12  0.07  1.02 -0.16  0.1   0.93 -0.07  0.12  1.06  0.04  0.07  1.1\n",
      "   0.12 -0.13  1.07  0.17 -0.18  1.01  0.07 -0.22  0.88  0.06 -0.08  0.9\n",
      "  -0.07 -0.05  0.89 -0.1   0.    0.88 -0.15  0.09  0.93 -0.1   0.14  1.06\n",
      "  -0.14  0.08  1.02 -0.04  0.12  1.11  0.03  0.04  1.04  0.04  0.06  1.1\n",
      "  -0.01  0.04  0.95 -0.13  0.04  0.88  0.02 -0.17  0.88 -0.06 -0.09  0.94]\n",
      " [-0.02  0.12  1.01 -0.01  0.12  1.05 -0.14  0.13  1.02 -0.16  0.09  0.97\n",
      "  -0.16  0.08  0.93  0.13 -0.18  0.94  0.18 -0.2   1.01  0.16 -0.17  1.05\n",
      "  -0.06  0.05  1.02 -0.08  0.03  0.99 -0.11  0.03  0.88 -0.03  0.03  0.9\n",
      "  -0.    0.04  0.93 -0.01  0.11  1.05 -0.02  0.11  1.07 -0.02  0.09  1.08\n",
      "  -0.17  0.1   0.97 -0.17  0.09  0.94 -0.16  0.09  0.9   0.11 -0.17  0.93]\n",
      " [-0.16  0.04  0.92 -0.1   0.11  1.03  0.04  0.09  1.09  0.05  0.02  1.07\n",
      "   0.1  -0.08  1.01  0.18 -0.15  1.04  0.11 -0.17  0.9  -0.01 -0.15  0.87\n",
      "  -0.08  0.    0.89 -0.15  0.09  0.95 -0.03  0.12  1.07 -0.11  0.13  1.07\n",
      "  -0.01  0.1   1.09  0.02  0.07  1.09  0.01  0.07  1.03 -0.06 -0.01  0.91\n",
      "   0.01 -0.01  0.93 -0.12  0.02  0.91 -0.11  0.09  1.02 -0.01 -0.15  0.93]\n",
      " [-0.12 -0.    1.01 -0.07 -0.02  0.89 -0.13  0.08  0.96  0.01 -0.16  0.92\n",
      "   0.08 -0.11  1.05  0.19 -0.19  1.02  0.06  0.06  1.08 -0.    0.01  0.93\n",
      "  -0.13  0.04  0.89 -0.1   0.    0.89 -0.18  0.14  1.   -0.15  0.05  0.98\n",
      "  -0.04  0.11  1.1  -0.    0.05  1.02  0.05  0.05  1.05 -0.04  0.02  0.92\n",
      "  -0.15  0.09  0.94  0.01 -0.19  0.88  0.03 -0.12  1.02 -0.08  0.07  1.  ]\n",
      " [-0.06  0.09  1.06 -0.1   0.08  1.03  0.06  0.02  1.07  0.07 -0.06  0.96\n",
      "   0.18 -0.19  1.    0.07 -0.19  0.87 -0.    0.    0.93 -0.14  0.04  0.89\n",
      "  -0.15  0.14  1.   -0.16  0.09  0.97 -0.07  0.14  1.08  0.04  0.03  1.03\n",
      "   0.01  0.08  1.09 -0.02 -0.01  0.93 -0.14  0.    0.97 -0.12  0.03  0.9\n",
      "  -0.15  0.13  0.99 -0.02 -0.12  0.96  0.12 -0.15  1.06  0.16 -0.2   0.96]\n",
      " [ 0.02 -0.12  1.06  0.04 -0.17  1.01 -0.03  0.06  1.02  0.05 -0.01  1.05\n",
      "  -0.06  0.03  0.93  0.02  0.01  1.06 -0.03 -0.07  0.95 -0.14  0.07  1.05\n",
      "  -0.01 -0.03  0.97 -0.08  0.1   0.95  0.04 -0.12  1.01 -0.06 -0.11  0.93\n",
      "  -0.1   0.01  0.93 -0.03 -0.28  0.95 -0.08 -0.23  0.91 -0.02 -0.13  1.02\n",
      "  -0.08  0.05  0.95 -0.1   0.17  0.97 -0.07 -0.02  0.92 -0.16  0.14  0.92]\n",
      " [ 0.07 -0.15  0.9   0.    0.01  0.94 -0.14  0.03  0.89 -0.17  0.13  0.97\n",
      "  -0.14  0.04  0.95 -0.1   0.13  1.07 -0.1   0.1   1.03  0.02  0.08  1.09\n",
      "  -0.01  0.05  0.98  0.06 -0.01  1.02 -0.03 -0.05  0.9  -0.12  0.03  0.94\n",
      "  -0.01 -0.17  0.89  0.03 -0.11  1.03  0.18 -0.14  0.97  0.03  0.1   1.07\n",
      "   0.04  0.05  0.99  0.05  0.04  1.02 -0.05 -0.02  0.9  -0.16  0.06  0.94]\n",
      " [-0.01 -0.3   0.93 -0.08 -0.2   0.91 -0.1  -0.19  0.96 -0.01 -0.13  0.92\n",
      "  -0.08  0.01  0.93  0.03 -0.02  0.97 -0.03  0.01  0.91 -0.1   0.13  0.93\n",
      "   0.01 -0.05  0.96 -0.06 -0.03  0.89 -0.11  0.11  0.93 -0.01  0.07  0.95\n",
      "  -0.07  0.07  0.9   0.03 -0.21  1.05  0.02 -0.25  0.95 -0.06 -0.2   0.88\n",
      "   0.07 -0.21  1.04  0.03 -0.24  0.95 -0.05 -0.13  0.89  0.06 -0.03  1.03]\n",
      " [-0.12  0.16  0.9  -0.02  0.13  1.05  0.02  0.06  0.99 -0.03 -0.18  1.02\n",
      "   0.05 -0.23  1.06 -0.09  0.06  0.96  0.03 -0.01  1.07  0.01 -0.05  0.98\n",
      "  -0.02  0.11  1.04  0.02  0.03  1.02 -0.09  0.09  0.93  0.02  0.04  1.05\n",
      "  -0.06 -0.    0.92  0.01  0.11  1.07 -0.01  0.06  0.96 -0.11  0.13  0.91\n",
      "   0.02 -0.19  0.99 -0.07 -0.17  0.9  -0.07 -0.11  0.97 -0.05 -0.13  0.92]\n",
      " [-0.03  0.16  1.03  0.01 -0.02  0.96  0.01  0.07  1.1  -0.04  0.04  0.93\n",
      "  -0.14  0.17  1.01 -0.07 -0.24  0.88 -0.08 -0.16  1.   -0.1   0.03  0.94\n",
      "   0.02  0.01  1.05  0.06 -0.14  0.98  0.05  0.04  1.08 -0.02 -0.    0.93\n",
      "  -0.11  0.18  0.93 -0.1   0.02  0.88 -0.1   0.2   0.98 -0.13  0.15  0.96\n",
      "  -0.03  0.11  1.04  0.    0.02  0.97  0.04 -0.22  1.04 -0.03 -0.23  0.91]\n",
      " [-0.08 -0.24  0.88 -0.1  -0.16  0.93 -0.08 -0.18  1.   -0.03 -0.08  0.9\n",
      "  -0.08  0.04  0.92 -0.1   0.11  0.94 -0.02 -0.    0.93 -0.09  0.07  0.89\n",
      "  -0.11  0.18  1.   -0.01 -0.06  0.95 -0.08 -0.03  0.87  0.01  0.08  1.07\n",
      "  -0.01  0.04  0.97 -0.07  0.08  0.93  0.03 -0.2   1.07  0.04 -0.27  0.99\n",
      "  -0.02 -0.26  0.93  0.04 -0.15  1.08  0.07 -0.2   1.02 -0.05  0.15  1.02]\n",
      " [ 0.04 -0.09  0.94 -0.07 -0.05  1.    0.01 -0.09  1.06 -0.09  0.11  0.93\n",
      "  -0.08  0.15  0.99  0.    0.08  1.03 -0.12  0.17  0.94 -0.1   0.2   1.01\n",
      "  -0.03  0.14  1.03 -0.12  0.13  0.94 -0.09  0.18  1.01 -0.09 -0.04  0.89\n",
      "  -0.11 -0.02  0.92 -0.08  0.01  1.02 -0.06 -0.22  0.9  -0.09 -0.14  0.94\n",
      "  -0.06 -0.12  1.03 -0.07  0.04  0.88 -0.1   0.12  0.91 -0.01 -0.03  0.95]\n",
      " [-0.04  0.13  1.09  0.03  0.    1.   -0.07 -0.04  0.89 -0.06 -0.02  0.9\n",
      "  -0.16  0.04  0.92 -0.06  0.09  1.06 -0.1   0.1   1.04  0.07  0.03  1.09\n",
      "   0.06 -0.11  1.07  0.16 -0.21  0.95  0.02 -0.19  0.86 -0.06  0.    0.87\n",
      "  -0.14  0.09  0.95 -0.02  0.11  1.07 -0.1   0.15  1.07  0.05  0.05  1.06\n",
      "  -0.02 -0.04  0.92 -0.03  0.    0.93 -0.14 -0.    0.9  -0.14  0.06  0.91]\n",
      " [-0.13  0.23  0.97 -0.06 -0.01  0.91 -0.17  0.2   0.86  0.01  0.09  1.03\n",
      "  -0.12 -0.24  0.95  0.11 -0.14  1.16 -0.09 -0.11  0.92 -0.04  0.11  1.\n",
      "  -0.06 -0.04  0.86  0.11  0.04  1.1   0.02  0.04  0.99 -0.02  0.21  1.02\n",
      "  -0.1   0.09  0.85 -0.15  0.2   0.9  -0.01 -0.04  1.   -0.14  0.17  0.84\n",
      "   0.02  0.09  1.09 -0.16  0.24  0.87  0.   -0.01  1.09 -0.1  -0.06  0.9 ]\n",
      " [-0.03  0.06  0.92 -0.02  0.14  1.06  0.02  0.13  1.09 -0.17  0.19  0.87\n",
      "  -0.02  0.12  0.97 -0.12  0.08  0.88  0.03  0.1   1.12  0.13 -0.23  1.09\n",
      "  -0.05 -0.05  1.    0.04 -0.21  0.96 -0.11  0.09  0.83  0.09  0.07  1.1\n",
      "  -0.15  0.21  0.87 -0.07  0.19  1.02 -0.09  0.08  0.86 -0.17  0.18  0.87\n",
      "   0.02  0.06  1.02 -0.13  0.22  0.97  0.02  0.13  1.07 -0.18  0.16  0.87]\n",
      " [-0.13  0.04  0.91 -0.12  0.04  0.89  0.16 -0.19  0.97  0.2  -0.2   1.03\n",
      "   0.18 -0.16  1.06 -0.02  0.02  1.02 -0.05  0.01  1.   -0.11  0.03  0.9\n",
      "  -0.08  0.01  0.89 -0.03 -0.    0.92  0.    0.09  1.04 -0.01  0.09  1.05\n",
      "  -0.01  0.09  1.07 -0.11  0.08  1.   -0.15  0.08  0.96 -0.15 -0.18  0.89\n",
      "   0.05 -0.18  0.9   0.1  -0.2   0.93  0.15 -0.12  1.05  0.11 -0.06  1.07]\n",
      " [ 0.04  0.1   1.07  0.04  0.04  1.11 -0.16  0.18  0.87  0.04 -0.01  1.03\n",
      "  -0.08  0.12  0.89 -0.02  0.17  1.05 -0.15  0.14  0.83 -0.11  0.01  0.89\n",
      "   0.08 -0.14  1.08 -0.1   0.01  0.95  0.05 -0.04  1.08 -0.05 -0.08  0.87\n",
      "  -0.12  0.16  0.83  0.08  0.04  1.03 -0.12  0.2   0.93 -0.04  0.18  1.05\n",
      "  -0.09  0.07  0.87  0.01  0.1   1.09  0.01  0.11  1.05 -0.16  0.2   0.89]\n",
      " [-0.11  0.13  1.08 -0.02  0.09  1.09  0.03 -0.02  0.97  0.02  0.07  1.05\n",
      "  -0.05  0.02  0.9   0.04 -0.06  0.97 -0.06 -0.06  0.88 -0.1   0.01  0.96\n",
      "  -0.01 -0.15  0.89  0.01 -0.11  0.99  0.13 -0.1   1.07 -0.04  0.12  1.06\n",
      "   0.07  0.06  1.08  0.02  0.02  1.09  0.04  0.03  1.02 -0.03 -0.04  0.9\n",
      "  -0.01  0.01  0.96 -0.1  -0.01  0.87 -0.15  0.05  0.94 -0.15  0.05  0.91]\n",
      " [-0.17 -0.03  0.9  -0.16  0.07  0.94 -0.05  0.1   1.09 -0.04  0.08  1.07\n",
      "   0.04  0.05  1.06 -0.04  0.03  0.91  0.06 -0.08  0.93 -0.06 -0.07  0.9\n",
      "  -0.07  0.    0.86 -0.02 -0.1   0.94  0.13 -0.11  1.05 -0.06  0.13  1.05\n",
      "   0.07  0.03  1.06 -0.01  0.01  0.93  0.02 -0.01  0.96 -0.11 -0.02  0.9\n",
      "  -0.17  0.13  1.   -0.14  0.04  0.96 -0.03  0.09  1.09 -0.05  0.1   1.06]\n",
      " [-0.14  0.06  0.89 -0.14 -0.03  0.89 -0.14  0.06  0.94 -0.08  0.13  1.08\n",
      "  -0.13  0.1   1.   -0.01  0.08  1.09  0.01  0.06  1.04  0.08 -0.    1.09\n",
      "   0.05 -0.04  0.97 -0.08 -0.06  0.99  0.08 -0.22  0.87 -0.03 -0.13  0.89\n",
      "  -0.09  0.01  0.88 -0.13  0.09  0.94 -0.09  0.12  1.04 -0.15  0.11  0.98\n",
      "  -0.07  0.12  1.08  0.07  0.06  1.08 -0.02  0.11  1.09  0.02  0.06  1.06]\n",
      " [ 0.04 -0.11  1.09 -0.02 -0.14  1.1  -0.13  0.15  0.86  0.1  -0.01  1.06\n",
      "   0.02  0.05  0.93 -0.02  0.12  1.05 -0.09  0.09  0.85 -0.14  0.16  0.86\n",
      "   0.02  0.04  1.03 -0.13  0.19  0.97 -0.01  0.15  1.07 -0.17  0.18  0.86\n",
      "  -0.08 -0.01  0.98  0.01 -0.21  0.99  0.06 -0.11  1.11  0.12 -0.07  1.12\n",
      "  -0.12  0.1   0.86  0.09 -0.07  1.01 -0.04  0.07  0.88  0.04  0.1   1.07]\n",
      " [-0.02  0.08  1.05 -0.03 -0.02  0.97  0.02 -0.01  1.06 -0.12  0.05  0.93\n",
      "  -0.03  0.03  0.93 -0.1   0.17  0.95  0.03  0.05  1.03 -0.01 -0.13  1.04\n",
      "  -0.03 -0.2   0.91 -0.08 -0.13  0.97 -0.09 -0.12  0.92  0.05 -0.16  1.06\n",
      "  -0.08  0.15  0.98  0.04 -0.02  1.02 -0.12  0.08  0.89 -0.07 -0.    0.91\n",
      "  -0.1   0.13  1.02  0.03 -0.04  1.02  0.    0.07  1.07 -0.07  0.02  0.93]\n",
      " [-0.05  0.04  0.92 -0.16  0.09  0.91 -0.09  0.12  1.05 -0.05 -0.03  1.01\n",
      "   0.12 -0.1   1.09  0.09 -0.06  1.06  0.13 -0.09  1.01  0.05 -0.13  0.87\n",
      "   0.02  0.02  0.92 -0.11  0.04  0.88 -0.04 -0.04  0.9  -0.13  0.04  0.95\n",
      "  -0.11  0.12  1.06 -0.16  0.07  0.97 -0.01  0.09  1.08  0.04  0.06  1.07\n",
      "   0.08 -0.01  1.09  0.07 -0.08  0.97  0.17 -0.16  0.99  0.07 -0.2   0.87]\n",
      " [-0.04  0.06  0.9   0.03  0.13  1.08  0.03  0.05  1.04 -0.13  0.23  0.97\n",
      "  -0.06 -0.01  0.91 -0.17  0.2   0.86  0.01  0.09  1.03 -0.12 -0.24  0.95\n",
      "   0.11 -0.14  1.16 -0.09 -0.11  0.92 -0.04  0.11  1.   -0.06 -0.04  0.86\n",
      "   0.11  0.04  1.1   0.02  0.04  0.99 -0.02  0.21  1.02 -0.1   0.09  0.85\n",
      "  -0.15  0.2   0.9  -0.01 -0.04  1.   -0.14  0.17  0.84  0.02  0.09  1.09]\n",
      " [ 0.06  0.07  1.06  0.02  0.1   1.06 -0.11  0.09  1.03 -0.13  0.08  1.\n",
      "  -0.11  0.02  0.86 -0.09  0.05  0.86 -0.07  0.08  0.89  0.07  0.01  1.08\n",
      "   0.06  0.01  1.1   0.03  0.02  1.1  -0.   -0.16  0.97  0.   -0.2   0.94\n",
      "  -0.04  0.    0.89 -0.03  0.02  0.91  0.02  0.02  0.95 -0.02  0.11  1.08\n",
      "  -0.04  0.11  1.09 -0.07  0.11  1.07 -0.14  0.03  0.95 -0.14  0.02  0.92]\n",
      " [-0.13  0.16  0.94 -0.08  0.19  1.02 -0.01  0.11  1.05 -0.11 -0.1   0.92\n",
      "  -0.09 -0.11  1.01 -0.01 -0.17  0.89 -0.09 -0.06  0.93 -0.08  0.02  0.99\n",
      "  -0.05  0.01  0.89 -0.1   0.13  0.93 -0.1   0.16  0.97 -0.05 -0.    0.9\n",
      "  -0.11  0.12  0.93 -0.13  0.22  0.96 -0.06  0.03  0.89 -0.1   0.04  0.94\n",
      "   0.   -0.16  0.96 -0.07 -0.06  0.9  -0.11 -0.03  0.92  0.01 -0.31  0.94]\n",
      " [-0.08 -0.18  1.   -0.03 -0.08  0.9  -0.08  0.04  0.92 -0.1   0.11  0.94\n",
      "  -0.02 -0.    0.93 -0.09  0.07  0.89 -0.11  0.18  1.   -0.01 -0.06  0.95\n",
      "  -0.08 -0.03  0.87  0.01  0.08  1.07 -0.01  0.04  0.97 -0.07  0.08  0.93\n",
      "   0.03 -0.2   1.07  0.04 -0.27  0.99 -0.02 -0.26  0.93  0.04 -0.15  1.08\n",
      "   0.07 -0.2   1.02 -0.05  0.15  1.02  0.02  0.07  1.05  0.06  0.01  1.04]\n",
      " [ 0.11 -0.14  0.99 -0.05  0.06  0.85  0.08  0.11  1.11  0.04 -0.03  1.03\n",
      "  -0.1   0.21  0.98 -0.05  0.03  0.92 -0.16  0.16  0.84  0.02  0.12  1.06\n",
      "  -0.02 -0.02  0.97 -0.02  0.07  1.09 -0.12  0.03  0.9  -0.1  -0.03  0.86\n",
      "   0.12 -0.25  1.04 -0.02  0.    1.01  0.05  0.12  1.09 -0.13  0.16  0.83\n",
      "   0.1   0.06  0.94  0.    0.03  0.93 -0.02  0.17  1.06  0.05  0.13  1.09]\n",
      " [-0.02 -0.1   0.89 -0.04 -0.02  0.98  0.07 -0.02  1.08 -0.06  0.14  1.06\n",
      "   0.06  0.04  1.06 -0.01  0.02  1.09  0.04 -0.01  0.99 -0.07 -0.02  0.9\n",
      "  -0.05  0.01  0.91 -0.17  0.05  0.93 -0.06  0.1   1.07 -0.09  0.08  1.04\n",
      "   0.07  0.    1.08  0.06 -0.07  0.96  0.16 -0.18  0.97  0.05 -0.19  0.86\n",
      "  -0.03 -0.    0.9  -0.16  0.07  0.91 -0.08  0.12  1.04 -0.16  0.14  1.02]\n",
      " [ 0.03 -0.06  1.01  0.02 -0.25  1.04  0.03 -0.32  0.97 -0.09 -0.25  0.91\n",
      "  -0.02 -0.01  0.92 -0.09  0.12  0.98 -0.09  0.08  0.94 -0.05  0.11  1.06\n",
      "   0.02 -0.03  1.01 -0.02  0.04  1.06 -0.03 -0.05  0.93 -0.13  0.13  0.95\n",
      "  -0.09  0.08  0.92 -0.07  0.16  1.02  0.01 -0.24  0.9   0.01 -0.24  1.05\n",
      "  -0.02 -0.33  0.93  0.07 -0.1   1.06 -0.05 -0.05  0.93 -0.04  0.05  1.03]\n",
      " [-0.07  0.04  0.86 -0.16  0.21  0.86  0.06  0.03  1.   -0.11  0.02  0.88\n",
      "   0.04  0.15  1.09 -0.16  0.18  0.89 -0.04  0.23  1.03 -0.16  0.15  0.84\n",
      "   0.01  0.12  1.09  0.06 -0.23  1.02  0.04 -0.1   1.12 -0.05 -0.03  1.07\n",
      "  -0.1   0.12  0.92  0.01 -0.06  0.89 -0.14  0.17  0.85  0.07  0.04  1.03\n",
      "  -0.08  0.24  0.97  0.    0.17  1.08 -0.16  0.16  0.85  0.04  0.01  1.03]\n",
      " [-0.04 -0.03  0.96  0.02  0.02  1.04 -0.11  0.07  0.93 -0.04 -0.04  0.91\n",
      "  -0.09  0.08  0.99  0.02 -0.08  1.    0.06 -0.29  1.06 -0.06 -0.25  0.92\n",
      "  -0.   -0.23  1.04 -0.11  0.13  0.92  0.07 -0.01  1.04 -0.02  0.08  1.06\n",
      "  -0.03 -0.02  0.95 -0.13  0.15  0.97 -0.11  0.06  0.94 -0.06  0.09  1.05\n",
      "  -0.04 -0.04  0.93  0.04  0.04  1.04 -0.1   0.09  0.92 -0.   -0.32  0.91]\n",
      " [ 0.18 -0.2   0.99  0.07 -0.21  0.87 -0.02 -0.13  0.9  -0.09  0.03  0.86\n",
      "  -0.15  0.11  0.96  0.01  0.06  0.94 -0.09  0.14  1.08  0.01  0.1   1.09\n",
      "  -0.03  0.08  1.11  0.01  0.07  1.03 -0.06 -0.02  0.89  0.    0.01  0.95\n",
      "  -0.12  0.03  0.9  -0.16  0.09  0.98 -0.01 -0.17  0.92  0.05 -0.13  1.05\n",
      "  -0.12  0.1   0.98  0.02  0.06  1.07  0.06  0.05  1.    0.05  0.05  1.08]\n",
      " [ 0.03 -0.07  0.93 -0.05  0.01  0.87  0.04 -0.02  1.03 -0.01  0.02  0.95\n",
      "  -0.09  0.06  1.05  0.01 -0.    0.99 -0.04 -0.04  0.92  0.01  0.06  1.1\n",
      "   0.01  0.02  0.98 -0.06  0.06  0.91  0.07 -0.31  1.08  0.03 -0.34  0.96\n",
      "  -0.05 -0.28  0.87  0.07 -0.11  1.03  0.02 -0.11  0.94  0.03  0.05  1.09\n",
      "   0.04 -0.02  1.   -0.02  0.01  0.93  0.01  0.06  1.07  0.   -0.06  0.95]\n",
      " [ 0.02 -0.    0.94  0.03  0.    0.99 -0.09 -0.03  0.89 -0.07 -0.    0.9\n",
      "  -0.15  0.06  0.94 -0.06  0.09  1.06 -0.1   0.08  1.03  0.06  0.02  1.07\n",
      "   0.07 -0.06  0.96  0.18 -0.19  1.    0.07 -0.19  0.87 -0.    0.    0.93\n",
      "  -0.14  0.04  0.89 -0.15  0.14  1.   -0.16  0.09  0.97 -0.07  0.14  1.08\n",
      "   0.04  0.03  1.03  0.01  0.08  1.09 -0.02 -0.01  0.93 -0.14  0.    0.97]\n",
      " [-0.13  0.14  1.03  0.05  0.06  1.11  0.13 -0.14  1.08  0.17 -0.24  0.95\n",
      "   0.01 -0.19  0.86 -0.06 -0.    0.85 -0.13  0.08  0.95 -0.16  0.09  0.94\n",
      "  -0.07  0.13  1.09  0.06  0.04  1.04 -0.    0.09  1.09 -0.   -0.02  0.93\n",
      "  -0.12 -0.02  0.88 -0.1   0.05  0.87 -0.17  0.09  0.94 -0.06 -0.19  0.92\n",
      "   0.01 -0.11  1.05  0.15 -0.16  1.09  0.07 -0.01  1.07  0.1  -0.05  0.99]\n",
      " [-0.07  0.12  0.89 -0.04  0.14  0.95 -0.05  0.1   1.08 -0.11  0.12  1.05\n",
      "  -0.15  0.1   1.04  0.04 -0.23  0.89  0.09 -0.2   0.89  0.15 -0.21  0.93\n",
      "   0.03  0.06  1.06 -0.03  0.1   1.05 -0.14  0.07  0.98 -0.13  0.04  0.94\n",
      "  -0.1   0.03  0.9  -0.04  0.14  0.95 -0.01  0.11  1.01 -0.02  0.13  1.03\n",
      "  -0.14  0.11  1.04 -0.18  0.1   1.   -0.17 -0.19  0.88  0.09 -0.22  0.9 ]\n",
      " [ 0.1  -0.05  0.99  0.   -0.06  0.86  0.   -0.    0.92 -0.12  0.04  0.87\n",
      "  -0.16  0.12  0.89 -0.13  0.02  0.93 -0.1   0.1   1.08 -0.15  0.09  0.97\n",
      "  -0.01  0.08  1.1   0.03  0.06  1.05  0.13 -0.08  1.09  0.1  -0.15  0.95\n",
      "  -0.   -0.18  0.88  0.04 -0.17  0.85 -0.05 -0.07  0.91 -0.13  0.05  0.88\n",
      "  -0.15  0.12  0.98 -0.02  0.1   1.09 -0.11  0.13  1.08 -0.02  0.09  1.09]\n",
      " [ 0.1  -0.13  0.94  0.04  0.04  1.01 -0.04 -0.    0.89  0.01 -0.02  0.94\n",
      "  -0.11  0.    0.9  -0.17  0.1   0.99 -0.16  0.04  0.94 -0.06  0.09  1.06\n",
      "   0.03  0.06  1.09  0.02  0.06  1.08  0.07 -0.01  1.01 -0.01 -0.16  1.04\n",
      "   0.11 -0.2   0.89 -0.01 -0.16  0.88 -0.11  0.02  0.87 -0.16  0.12  0.96\n",
      "  -0.    0.1   1.08 -0.08  0.14  1.06  0.04  0.05  1.06 -0.01 -0.03  1.11]\n",
      " [ 0.05  0.04  1.03 -0.02 -0.02  0.9  -0.01 -0.04  0.95 -0.11 -0.03  0.9\n",
      "  -0.15  0.07  1.02 -0.18  0.1   0.95 -0.01  0.11  1.08  0.02  0.04  1.05\n",
      "   0.15 -0.11  1.06  0.09 -0.19  0.92  0.   -0.18  0.94 -0.02 -0.09  0.88\n",
      "  -0.1  -0.01  0.95 -0.14  0.07  0.94 -0.09  0.15  1.05  0.02  0.07  1.08\n",
      "  -0.03  0.12  1.09  0.03  0.02  1.   -0.07 -0.06  0.9  -0.07  0.02  0.92]\n",
      " [-0.08  0.09  0.97 -0.12  0.07  0.92 -0.02  0.08  1.05 -0.03 -0.02  0.97\n",
      "   0.02 -0.01  1.06 -0.12  0.05  0.93 -0.03  0.03  0.93 -0.1   0.17  0.95\n",
      "   0.03  0.05  1.03 -0.01 -0.13  1.04 -0.03 -0.2   0.91 -0.08 -0.13  0.97\n",
      "  -0.09 -0.12  0.92  0.05 -0.16  1.06 -0.08  0.15  0.98  0.04 -0.02  1.02\n",
      "  -0.12  0.08  0.89 -0.07 -0.    0.91 -0.1   0.13  1.02  0.03 -0.04  1.02]\n",
      " [-0.06  0.03  0.89 -0.02  0.02  0.92  0.03  0.04  1.06 -0.03  0.12  1.08\n",
      "  -0.06  0.09  1.06 -0.16  0.04  0.94 -0.14  0.04  0.89 -0.1   0.05  0.88\n",
      "   0.07  0.01  1.03  0.09 -0.03  1.08  0.05  0.01  1.09 -0.   -0.15  0.98\n",
      "  -0.01 -0.19  0.93 -0.02  0.01  0.91  0.03  0.04  0.96  0.06  0.04  1.03\n",
      "  -0.07  0.12  1.07 -0.11  0.09  1.04 -0.14  0.08  0.99 -0.11  0.05  0.88]\n",
      " [-0.04 -0.01  0.99  0.05  0.16  1.07 -0.12  0.13  0.84 -0.11  0.26  0.96\n",
      "  -0.04  0.06  0.9   0.03  0.13  1.08  0.03  0.05  1.04 -0.13  0.23  0.97\n",
      "  -0.06 -0.01  0.91 -0.17  0.2   0.86  0.01  0.09  1.03 -0.12 -0.24  0.95\n",
      "   0.11 -0.14  1.16 -0.09 -0.11  0.92 -0.04  0.11  1.   -0.06 -0.04  0.86\n",
      "   0.11  0.04  1.1   0.02  0.04  0.99 -0.02  0.21  1.02 -0.1   0.09  0.85]\n",
      " [ 0.07 -0.13  1.06  0.17 -0.17  1.07  0.02  0.07  1.07  0.07  0.04  1.04\n",
      "  -0.   -0.01  0.89  0.04  0.01  0.97 -0.08 -0.01  0.89 -0.15  0.08  0.95\n",
      "  -0.13 -0.01  0.89 -0.12  0.07  1.02 -0.16  0.1   0.93 -0.07  0.12  1.06\n",
      "   0.04  0.07  1.1   0.12 -0.13  1.07  0.17 -0.18  1.01  0.07 -0.22  0.88\n",
      "   0.06 -0.08  0.9  -0.07 -0.05  0.89 -0.1   0.    0.88 -0.15  0.09  0.93]\n",
      " [ 0.02 -0.19  0.87 -0.03 -0.11  0.95 -0.06 -0.05  0.92  0.04 -0.01  1.04\n",
      "   0.12 -0.03  1.05  0.05  0.08  1.07 -0.    0.    0.95  0.01 -0.02  1.\n",
      "  -0.08 -0.05  0.9  -0.13  0.05  0.96 -0.16  0.04  0.92 -0.1   0.11  1.03\n",
      "   0.04  0.09  1.09  0.05  0.02  1.07  0.1  -0.08  1.01  0.18 -0.15  1.04\n",
      "   0.11 -0.17  0.9  -0.01 -0.15  0.87 -0.08  0.    0.89 -0.15  0.09  0.95]\n",
      " [ 0.05  0.01  1.09 -0.   -0.15  0.98 -0.01 -0.19  0.93 -0.02  0.01  0.91\n",
      "   0.03  0.04  0.96  0.06  0.04  1.03 -0.07  0.12  1.07 -0.11  0.09  1.04\n",
      "  -0.14  0.08  0.99 -0.11  0.05  0.88 -0.09  0.08  0.89  0.05  0.03  1.06\n",
      "   0.04  0.05  1.08 -0.    0.07  1.07  0.02 -0.21  0.98 -0.02 -0.19  0.94\n",
      "   0.03 -0.22  0.91  0.02  0.03  0.94  0.05  0.04  1.    0.06  0.12  1.08]\n",
      " [-0.13  0.08  0.95 -0.19  0.09  0.94 -0.06  0.12  1.07 -0.08  0.11  1.07\n",
      "   0.01  0.09  1.09 -0.02 -0.05  0.93  0.    0.04  0.96 -0.13  0.04  0.9\n",
      "  -0.14  0.12  1.   -0.06 -0.04  0.97  0.08 -0.05  1.07  0.07 -0.08  1.05\n",
      "   0.15 -0.11  1.04  0.07 -0.15  0.9   0.    0.01  0.94 -0.14  0.03  0.89\n",
      "  -0.17  0.13  0.97 -0.14  0.04  0.95 -0.1   0.13  1.07 -0.1   0.1   1.03]\n",
      " [ 0.01  0.04  0.96  0.05  0.06  1.12 -0.06  0.09  0.89  0.02  0.13  1.08\n",
      "  -0.08  0.16  1.03  0.03  0.03  1.05 -0.    0.05  1.09 -0.16  0.12  0.89\n",
      "   0.11 -0.19  1.14 -0.06 -0.07  0.99  0.07  0.03  1.1  -0.04  0.15  1.01\n",
      "  -0.12  0.14  0.84 -0.01  0.1   1.06 -0.14  0.17  0.87 -0.03  0.08  0.9\n",
      "  -0.15  0.18  0.92 -0.1   0.07  0.86  0.04  0.1   0.99 -0.16  0.16  0.85]\n",
      " [-0.03 -0.04  0.9  -0.01  0.01  0.96 -0.1  -0.01  0.87 -0.15  0.05  0.94\n",
      "  -0.15  0.05  0.91 -0.14  0.12  1.01  0.01  0.08  1.09  0.07 -0.13  1.06\n",
      "   0.17 -0.17  1.07  0.02  0.07  1.07  0.07  0.04  1.04 -0.   -0.01  0.89\n",
      "   0.04  0.01  0.97 -0.08 -0.01  0.89 -0.15  0.08  0.95 -0.13 -0.01  0.89\n",
      "  -0.12  0.07  1.02 -0.16  0.1   0.93 -0.07  0.12  1.06  0.04  0.07  1.1 ]\n",
      " [-0.12  0.04  0.87  0.02 -0.13  0.89 -0.05 -0.1   0.94  0.02 -0.04  1.07\n",
      "  -0.   -0.05  1.01  0.12 -0.06  1.06 -0.    0.11  1.08  0.06  0.04  1.02\n",
      "  -0.04  0.    0.88  0.01 -0.04  0.94 -0.12 -0.01  0.9  -0.13  0.11  1.01\n",
      "  -0.13  0.07  0.95 -0.04  0.09  1.07 -0.09  0.07  1.04  0.08  0.02  1.11\n",
      "   0.05 -0.07  0.97  0.18 -0.2   0.99  0.07 -0.21  0.87 -0.02 -0.13  0.9 ]\n",
      " [-0.01  0.1   1.07 -0.09  0.09  1.01 -0.12  0.08  0.99 -0.13  0.04  0.96\n",
      "  -0.01 -0.14  0.91  0.04 -0.15  0.91  0.17 -0.15  1.03  0.17 -0.14  1.04\n",
      "   0.12 -0.06  1.05 -0.16  0.09  1.   -0.16  0.06  0.95 -0.14  0.03  0.92\n",
      "  -0.02  0.02  0.96  0.01  0.04  0.99  0.01  0.07  1.07 -0.02  0.06  1.06\n",
      "  -0.07  0.08  1.02 -0.09 -0.04  0.93 -0.03 -0.11  0.91  0.01 -0.11  0.91]\n",
      " [-0.05  0.01  1.   -0.11  0.03  0.9  -0.08  0.01  0.89 -0.03 -0.    0.92\n",
      "   0.    0.09  1.04 -0.01  0.09  1.05 -0.01  0.09  1.07 -0.11  0.08  1.\n",
      "  -0.15  0.08  0.96 -0.15 -0.18  0.89  0.05 -0.18  0.9   0.1  -0.2   0.93\n",
      "   0.15 -0.12  1.05  0.11 -0.06  1.07  0.08 -0.05  1.06 -0.13  0.08  0.99\n",
      "  -0.14  0.06  0.96 -0.14  0.06  0.94 -0.06  0.    0.91 -0.05  0.04  0.92]\n",
      " [ 0.01  0.07  1.05 -0.12  0.19  0.98 -0.02  0.17  1.04  0.01  0.06  1.08\n",
      "  -0.1  -0.05  0.99 -0.01 -0.12  1.05 -0.09 -0.07  0.94 -0.08  0.    0.98\n",
      "   0.   -0.04  1.06 -0.08  0.09  0.92 -0.1   0.16  0.95 -0.03  0.14  1.02\n",
      "  -0.11  0.1   0.92 -0.13  0.21  0.94 -0.05  0.01  0.92 -0.12  0.06  0.92\n",
      "  -0.13  0.15  0.94 -0.04 -0.05  0.94 -0.11 -0.01  0.91 -0.13  0.05  0.93]\n",
      " [-0.    0.05  0.94 -0.14  0.11  0.87  0.06  0.04  1.06 -0.14  0.24  0.95\n",
      "   0.01  0.18  1.05 -0.19  0.21  0.84 -0.02  0.13  1.   -0.05 -0.13  0.94\n",
      "   0.08 -0.09  1.16 -0.1  -0.21  1.04 -0.03  0.04  0.98  0.01 -0.16  0.91\n",
      "  -0.11  0.15  0.85  0.08  0.06  1.05 -0.11  0.22  0.94 -0.01  0.18  1.06\n",
      "  -0.14  0.14  0.86  0.05  0.01  1.06 -0.06  0.07  0.92 -0.03  0.2   1.05]\n",
      " [-0.14  0.06  0.96 -0.11  0.04  0.91 -0.08  0.03  0.92 -0.02  0.12  0.96\n",
      "  -0.01  0.13  1.02 -0.09  0.09  1.04 -0.14  0.11  1.03 -0.17  0.1   0.99\n",
      "   0.03 -0.2   0.9   0.07 -0.18  0.9   0.14 -0.2   0.95  0.08  0.01  1.05\n",
      "   0.    0.03  1.04 -0.14  0.07  0.98 -0.13  0.04  0.95 -0.09  0.03  0.91\n",
      "  -0.03  0.13  0.99 -0.01  0.13  1.03 -0.03  0.12  1.05 -0.15  0.08  1.  ]\n",
      " [-0.11  0.11  0.9   0.03 -0.29  1.   -0.06 -0.23  0.93 -0.08 -0.18  0.97\n",
      "  -0.04 -0.1   0.92 -0.12  0.02  0.91  0.   -0.01  0.97 -0.12  0.08  0.91\n",
      "  -0.06  0.16  1.   -0.09  0.01  0.95 -0.13  0.19  0.96  0.    0.06  1.06\n",
      "  -0.12  0.12  0.9  -0.04  0.14  1.02 -0.07 -0.1   0.92 -0.09 -0.04  0.96\n",
      "   0.01 -0.11  1.02 -0.1  -0.13  0.91 -0.   -0.13  1.04  0.06 -0.23  1.05]\n",
      " [ 0.01 -0.03  1.05 -0.01 -0.09  0.92 -0.08  0.09  0.97 -0.12  0.07  0.92\n",
      "  -0.02  0.08  1.05 -0.03 -0.02  0.97  0.02 -0.01  1.06 -0.12  0.05  0.93\n",
      "  -0.03  0.03  0.93 -0.1   0.17  0.95  0.03  0.05  1.03 -0.01 -0.13  1.04\n",
      "  -0.03 -0.2   0.91 -0.08 -0.13  0.97 -0.09 -0.12  0.92  0.05 -0.16  1.06\n",
      "  -0.08  0.15  0.98  0.04 -0.02  1.02 -0.12  0.08  0.89 -0.07 -0.    0.91]\n",
      " [ 0.04  0.05  1.08 -0.    0.07  1.07  0.02 -0.21  0.98 -0.02 -0.19  0.94\n",
      "   0.03 -0.22  0.91  0.02  0.03  0.94  0.05  0.04  1.    0.06  0.12  1.08\n",
      "  -0.1   0.12  1.04 -0.12  0.07  1.01 -0.11  0.04  0.89 -0.08  0.07  0.89\n",
      "  -0.05  0.11  0.94 -0.01  0.07  1.09 -0.08  0.11  1.05 -0.1   0.09  1.03\n",
      "   0.04 -0.24  0.89  0.09 -0.22  0.88  0.07  0.03  1.04  0.03  0.06  1.07]\n",
      " [-0.15  0.1   0.9  -0.02  0.03  0.96 -0.16  0.04  0.9  -0.01  0.02  0.97\n",
      "  -0.11  0.01  0.94 -0.06  0.16  1.01 -0.08 -0.02  0.92 -0.08  0.06  0.98\n",
      "   0.02  0.    1.04 -0.08 -0.18  0.95  0.02 -0.22  1.05 -0.12  0.15  0.88\n",
      "  -0.01  0.08  1.04  0.05  0.02  1.01 -0.06  0.12  1.01  0.03 -0.    1.05\n",
      "  -0.05  0.01  0.93 -0.    0.09  1.06 -0.01  0.02  0.95 -0.11  0.03  1.05]\n",
      " [ 0.04 -0.31  0.96 -0.06 -0.22  0.88  0.05 -0.03  1.01 -0.   -0.03  0.91\n",
      "  -0.08  0.07  0.91  0.01 -0.04  0.97 -0.06  0.02  0.89 -0.12  0.16  0.94\n",
      "  -0.02  0.    0.91 -0.1   0.01  0.89  0.02 -0.04  0.99 -0.04 -0.02  0.92\n",
      "  -0.11  0.04  0.92  0.05 -0.36  0.96 -0.04 -0.29  0.89 -0.08 -0.18  0.93\n",
      "   0.03 -0.06  0.93 -0.05 -0.    0.88 -0.1  -0.02  1.03 -0.01 -0.02  0.94]\n",
      " [-0.16  0.16  0.85  0.    0.08  0.99 -0.08 -0.08  0.9   0.04 -0.15  0.97\n",
      "   0.13 -0.22  1.16 -0.02  0.01  0.88  0.11 -0.01  1.09 -0.03  0.09  1.04\n",
      "   0.06  0.05  1.02 -0.01  0.11  1.05  0.01  0.03  0.98 -0.    0.13  1.1\n",
      "  -0.15  0.18  0.91  0.03  0.08  1.11 -0.11  0.17  0.99 -0.17  0.18  0.84\n",
      "   0.04 -0.11  1.11 -0.09 -0.06  0.91  0.1  -0.07  1.12 -0.06  0.01  0.99]\n",
      " [-0.11  0.11  1.06 -0.14  0.04  0.88 -0.13  0.06  0.85 -0.09  0.1   0.87\n",
      "   0.05  0.05  1.08  0.01  0.06  1.1  -0.06  0.08  0.99 -0.02 -0.17  0.94\n",
      "   0.03 -0.22  0.88  0.03  0.04  0.96  0.06  0.06  1.02  0.04  0.09  1.05\n",
      "  -0.12  0.11  1.05 -0.14  0.07  1.   -0.13  0.04  0.94 -0.07  0.12  0.89\n",
      "  -0.04  0.14  0.95 -0.05  0.1   1.08 -0.11  0.12  1.05 -0.15  0.1   1.04]\n",
      " [-0.1  -0.09  0.94 -0.09 -0.15  0.93 -0.   -0.14  1.04  0.04 -0.22  0.99\n",
      "   0.04  0.02  1.05 -0.04  0.    0.92 -0.07  0.15  1.   -0.15  0.11  0.9\n",
      "  -0.02  0.05  1.06 -0.04 -0.03  0.96  0.02  0.02  1.04 -0.11  0.07  0.93\n",
      "  -0.04 -0.04  0.91 -0.09  0.08  0.99  0.02 -0.08  1.    0.06 -0.29  1.06\n",
      "  -0.06 -0.25  0.92 -0.   -0.23  1.04 -0.11  0.13  0.92  0.07 -0.01  1.04]\n",
      " [ 0.14 -0.24  1.09 -0.    0.01  0.92  0.02  0.17  1.05 -0.11  0.11  0.85\n",
      "  -0.14  0.19  0.87  0.04  0.    0.98 -0.11  0.07  0.86  0.02  0.14  1.08\n",
      "  -0.16  0.18  0.87 -0.14  0.21  0.98 -0.08  0.12  0.91 -0.    0.15  1.07\n",
      "   0.11 -0.2   1.15 -0.08 -0.07  0.97  0.03  0.1   1.04 -0.09  0.04  0.85\n",
      "   0.09  0.04  1.12  0.04  0.04  0.98 -0.03  0.17  1.02 -0.06  0.07  0.86]\n",
      " [ 0.04  0.    1.    0.15 -0.2   1.07  0.11 -0.18  1.05  0.07 -0.19  1.03\n",
      "  -0.11  0.02  0.9  -0.07  0.01  0.87 -0.04  0.06  1.04  0.01  0.09  1.07\n",
      "  -0.02  0.11  1.09 -0.13  0.06  0.98 -0.14  0.03  0.93 -0.14  0.02  0.89\n",
      "  -0.    0.02  0.95  0.03  0.04  1.01  0.05  0.04  1.06  0.07 -0.18  1.02\n",
      "   0.01 -0.17  1.   -0.07 -0.02  0.87 -0.03 -0.    0.89  0.02  0.02  0.94]\n",
      " [-0.01 -0.14  0.91  0.03 -0.14  0.9   0.09 -0.19  0.93  0.13 -0.1   1.03\n",
      "   0.08 -0.05  1.05 -0.12  0.08  1.   -0.14  0.06  0.97 -0.12  0.05  0.93\n",
      "  -0.03  0.07  0.93 -0.02  0.09  0.98 -0.03  0.13  1.02 -0.04  0.07  1.04\n",
      "  -0.09  0.1   1.01 -0.14 -0.04  0.94 -0.02 -0.11  0.91  0.01 -0.11  0.91\n",
      "   0.16 -0.16  1.02  0.15 -0.13  1.04  0.1  -0.1   1.06 -0.12  0.09  1.01]\n",
      " [-0.06  0.15  1.05  0.01  0.06  1.08 -0.13  0.18  0.92 -0.07  0.17  1.02\n",
      "  -0.02  0.13  1.04 -0.14  0.18  0.95 -0.08  0.17  1.05  0.    0.08  1.07\n",
      "  -0.1  -0.2   0.95 -0.03 -0.19  1.05 -0.08 -0.    0.92 -0.11  0.1   0.94\n",
      "  -0.03  0.1   1.05 -0.09  0.11  0.92 -0.12  0.18  0.94 -0.05  0.14  1.04\n",
      "  -0.11  0.07  0.91 -0.13  0.18  0.94 -0.07  0.07  0.92 -0.12  0.12  0.94]\n",
      " [-0.09 -0.04  0.96  0.01 -0.11  1.02 -0.1  -0.13  0.91 -0.   -0.13  1.04\n",
      "   0.06 -0.23  1.05 -0.08  0.16  0.97  0.02  0.05  1.04  0.01 -0.04  0.91\n",
      "  -0.05  0.15  1.05  0.04 -0.03  1.05 -0.09  0.16  0.98  0.01  0.08  1.06\n",
      "  -0.01 -0.02  0.96 -0.02  0.07  1.04  0.02 -0.06  0.99 -0.08 -0.    0.92\n",
      "   0.06 -0.31  1.06 -0.05 -0.28  0.91  0.04  0.02  1.05  0.01 -0.02  0.96]\n",
      " [ 0.11 -0.11  1.07  0.08 -0.17  1.06  0.   -0.15  1.02 -0.06 -0.04  0.86\n",
      "   0.   -0.02  0.9   0.03  0.1   1.06 -0.02  0.12  1.07 -0.08  0.1   1.05\n",
      "  -0.13  0.02  0.93 -0.11  0.02  0.88 -0.08  0.04  0.89  0.04  0.05  1.05\n",
      "   0.01  0.06  1.07 -0.04  0.09  1.07 -0.04 -0.15  0.96 -0.01 -0.17  0.92\n",
      "   0.08 -0.09  0.93  0.1  -0.06  0.99  0.11 -0.06  1.05 -0.1   0.1   1.04]\n",
      " [ 0.07  0.03  1.04  0.03  0.06  1.07 -0.03  0.09  1.05 -0.13  0.06  0.97\n",
      "  -0.12  0.05  0.91 -0.07  0.02  0.9  -0.02  0.12  1.01 -0.01  0.12  1.05\n",
      "  -0.14  0.13  1.02 -0.16  0.09  0.97 -0.16  0.08  0.93  0.13 -0.18  0.94\n",
      "   0.18 -0.2   1.01  0.16 -0.17  1.05 -0.06  0.05  1.02 -0.08  0.03  0.99\n",
      "  -0.11  0.03  0.88 -0.03  0.03  0.9  -0.    0.04  0.93 -0.01  0.11  1.05]\n",
      " [-0.12  0.13  1.02 -0.18  0.14  0.98 -0.02  0.11  1.08  0.04  0.01  1.\n",
      "   0.03  0.04  1.05 -0.04 -0.04  0.91 -0.13  0.01  0.93 -0.15  0.07  0.91\n",
      "  -0.12  0.15  1.04  0.05  0.05  1.08  0.15 -0.13  1.07  0.13 -0.19  0.94\n",
      "   0.02 -0.04  0.98 -0.05 -0.06  0.86 -0.12  0.07  0.95 -0.18  0.1   0.93\n",
      "  -0.08  0.15  1.05  0.06  0.06  1.07 -0.03  0.12  1.09  0.01 -0.02  0.98]\n",
      " [ 0.16 -0.19  0.97  0.2  -0.2   1.03  0.18 -0.16  1.06 -0.02  0.02  1.02\n",
      "  -0.05  0.01  1.   -0.11  0.03  0.9  -0.08  0.01  0.89 -0.03 -0.    0.92\n",
      "   0.    0.09  1.04 -0.01  0.09  1.05 -0.01  0.09  1.07 -0.11  0.08  1.\n",
      "  -0.15  0.08  0.96 -0.15 -0.18  0.89  0.05 -0.18  0.9   0.1  -0.2   0.93\n",
      "   0.15 -0.12  1.05  0.11 -0.06  1.07  0.08 -0.05  1.06 -0.13  0.08  0.99]\n",
      " [ 0.07 -0.21  0.87 -0.02 -0.13  0.9  -0.09  0.03  0.86 -0.15  0.11  0.96\n",
      "   0.01  0.06  0.94 -0.09  0.14  1.08  0.01  0.1   1.09 -0.03  0.08  1.11\n",
      "   0.01  0.07  1.03 -0.06 -0.02  0.89  0.    0.01  0.95 -0.12  0.03  0.9\n",
      "  -0.16  0.09  0.98 -0.01 -0.17  0.92  0.05 -0.13  1.05 -0.12  0.1   0.98\n",
      "   0.02  0.06  1.07  0.06  0.05  1.    0.05  0.05  1.08  0.02 -0.02  0.93]\n",
      " [-0.15  0.19  0.86 -0.09  0.2   1.03 -0.07  0.05  0.88 -0.    0.15  1.09\n",
      "   0.02  0.11  1.06 -0.16  0.2   0.87 -0.04  0.12  1.1  -0.07 -0.1   0.91\n",
      "   0.09 -0.11  1.16  0.12 -0.18  1.06 -0.07  0.09  0.94  0.04 -0.14  0.95\n",
      "  -0.09  0.11  0.83  0.08  0.07  1.12 -0.14  0.19  0.85 -0.12  0.22  0.98\n",
      "  -0.02  0.01  0.94 -0.15  0.14  0.84  0.03  0.12  1.1  -0.17  0.18  0.87]\n",
      " [ 0.02  0.04  1.09 -0.06 -0.2   1.02  0.03 -0.31  1.05 -0.1   0.08  0.96\n",
      "  -0.    0.03  1.06  0.06 -0.05  1.08 -0.08  0.16  1.02  0.01  0.04  1.07\n",
      "   0.04 -0.01  1.01 -0.05  0.17  1.05  0.    0.06  1.07 -0.09  0.19  1.02\n",
      "  -0.01  0.06  1.07  0.01  0.07  1.01 -0.02 -0.16  1.05  0.05 -0.25  1.06\n",
      "   0.02 -0.29  0.95  0.03 -0.12  1.08  0.07 -0.21  1.03  0.01  0.08  1.04]\n",
      " [-0.15  0.14  1.02  0.01  0.1   1.08 -0.04  0.1   1.12  0.02  0.02  1.02\n",
      "  -0.05 -0.06  0.89 -0.05  0.04  0.92 -0.16  0.09  0.91 -0.09  0.12  1.05\n",
      "  -0.05 -0.03  1.01  0.12 -0.1   1.09  0.09 -0.06  1.06  0.13 -0.09  1.01\n",
      "   0.05 -0.13  0.87  0.02  0.02  0.92 -0.11  0.04  0.88 -0.04 -0.04  0.9\n",
      "  -0.13  0.04  0.95 -0.11  0.12  1.06 -0.16  0.07  0.97 -0.01  0.09  1.08]\n",
      " [ 0.04  0.11  1.04 -0.02  0.11  1.06 -0.15  0.06  0.99 -0.12  0.02  0.94\n",
      "  -0.08 -0.    0.91  0.01  0.1   0.99  0.02  0.07  1.03 -0.02  0.06  1.05\n",
      "  -0.12  0.    0.97 -0.09 -0.05  0.94 -0.04 -0.07  0.95  0.16 -0.14  1.\n",
      "   0.16 -0.15  1.03 -0.11  0.09  1.02 -0.12  0.07  1.   -0.13  0.05  0.97\n",
      "  -0.03  0.02  0.93  0.01  0.07  0.95 -0.    0.08  1.   -0.04  0.08  1.05]\n",
      " [-0.11  0.11  0.94 -0.09  0.16  0.98 -0.05 -0.16  0.9  -0.1  -0.12  0.94\n",
      "   0.   -0.1   1.04 -0.1  -0.05  0.9  -0.01 -0.07  1.03  0.04 -0.13  1.03\n",
      "  -0.03  0.14  1.05  0.04  0.04  1.01 -0.09  0.1   1.02  0.04 -0.01  1.04\n",
      "  -0.1   0.07  0.95  0.03  0.06  1.01 -0.08  0.05  0.95 -0.08  0.19  0.98\n",
      "  -0.07 -0.08  0.92 -0.09  0.06  0.97  0.04 -0.08  1.03 -0.04 -0.12  0.98]\n",
      " [-0.14  0.16  0.86  0.02  0.04  1.03 -0.13  0.19  0.97 -0.01  0.15  1.07\n",
      "  -0.17  0.18  0.86 -0.08 -0.01  0.98  0.01 -0.21  0.99  0.06 -0.11  1.11\n",
      "   0.12 -0.07  1.12 -0.12  0.1   0.86  0.09 -0.07  1.01 -0.04  0.07  0.88\n",
      "   0.04  0.1   1.07  0.04  0.04  1.11 -0.16  0.18  0.87  0.04 -0.01  1.03\n",
      "  -0.08  0.12  0.89 -0.02  0.17  1.05 -0.15  0.14  0.83 -0.11  0.01  0.89]\n",
      " [ 0.08  0.03  1.12 -0.03 -0.12  0.89  0.13 -0.25  1.12 -0.11  0.06  0.84\n",
      "   0.09 -0.02  1.02  0.04  0.06  1.06  0.02  0.07  0.94  0.05  0.05  1.11\n",
      "  -0.12  0.2   0.95  0.05  0.09  1.1  -0.11  0.18  1.01 -0.15  0.13  1.05\n",
      "  -0.02  0.14  1.06 -0.18  0.18  0.86  0.09 -0.16  1.12 -0.1  -0.05  0.97\n",
      "  -0.06 -0.1   0.88 -0.04  0.04  1.01 -0.11  0.07  0.85  0.07 -0.06  0.98]\n",
      " [-0.14  0.11  0.9  -0.06  0.01  0.91 -0.11  0.18  0.94  0.01  0.04  1.05\n",
      "  -0.05  0.02  1.02  0.03 -0.21  0.99 -0.1  -0.07  0.88 -0.01 -0.27  0.92\n",
      "  -0.07 -0.12  0.97  0.08 -0.24  0.93 -0.02  0.11  1.03  0.04 -0.01  0.99\n",
      "  -0.02  0.08  1.06 -0.02 -0.03  0.97 -0.14  0.12  0.92 -0.06 -0.02  0.91\n",
      "  -0.1   0.15  0.96  0.02  0.02  1.07 -0.05  0.13  1.01  0.03 -0.06  1.01]\n",
      " [ 0.   -0.04  1.   -0.05  0.1   1.03  0.03  0.01  1.03 -0.06  0.01  0.93\n",
      "   0.02  0.01  1.04 -0.   -0.06  0.96 -0.1   0.01  0.9   0.06 -0.3   1.03\n",
      "  -0.05 -0.28  0.91  0.06  0.01  1.06 -0.04 -0.01  0.91 -0.07  0.13  0.99\n",
      "  -0.13  0.1   0.92 -0.01  0.07  1.05 -0.04 -0.01  0.94  0.02 -0.    1.05\n",
      "  -0.1   0.04  0.94 -0.06  0.1   0.94 -0.12  0.14  0.92  0.02  0.07  1.06]\n",
      " [-0.05 -0.13  0.89  0.06 -0.03  1.03  0.01 -0.01  0.93 -0.07  0.06  1.07\n",
      "   0.03 -0.04  1.   -0.02 -0.04  0.95 -0.    0.08  1.07  0.    0.06  0.99\n",
      "  -0.05  0.05  0.92  0.04 -0.07  1.08  0.01 -0.15  0.96 -0.08 -0.11  0.88\n",
      "   0.07 -0.31  1.03  0.   -0.29  0.94  0.04  0.05  1.1   0.06 -0.03  0.97\n",
      "  -0.02 -0.    0.9   0.02  0.03  1.09  0.03 -0.05  0.98 -0.03 -0.01  0.93]\n",
      " [-0.1   0.05  0.88  0.05  0.12  1.1  -0.04  0.09  0.98 -0.07  0.22  1.02\n",
      "  -0.13  0.08  0.85 -0.14  0.16  0.91 -0.01 -0.03  1.   -0.01 -0.18  0.88\n",
      "   0.15 -0.2   1.12 -0.07 -0.04  0.94  0.03  0.17  1.05 -0.11  0.13  0.84\n",
      "   0.08  0.06  1.05 -0.03  0.03  0.92  0.02  0.15  1.08 -0.14  0.15  0.87\n",
      "  -0.13  0.22  0.97 -0.06  0.01  0.91 -0.02  0.21  1.06  0.02  0.08  1.05]\n",
      " [-0.05 -0.03  0.9  -0.11  0.05  0.91 -0.13  0.16  0.93 -0.05  0.05  0.92\n",
      "  -0.11  0.1   0.92 -0.13  0.16  0.95 -0.02 -0.3   0.91 -0.07 -0.23  0.92\n",
      "  -0.1  -0.1   0.98  0.   -0.07  0.92 -0.08  0.02  0.91  0.02 -0.03  1.01\n",
      "  -0.02  0.01  0.94 -0.1   0.09  0.9   0.01  0.    0.98 -0.03 -0.05  0.93\n",
      "  -0.1   0.03  0.88  0.02  0.04  1.01 -0.03  0.05  0.93  0.03 -0.27  1.06]\n",
      " [ 0.07 -0.19  0.86 -0.05 -0.09  0.91 -0.14  0.06  0.9  -0.12  0.13  1.03\n",
      "   0.04  0.08  1.08 -0.04  0.13  1.09  0.03  0.    1.   -0.07 -0.04  0.89\n",
      "  -0.06 -0.02  0.9  -0.16  0.04  0.92 -0.06  0.09  1.06 -0.1   0.1   1.04\n",
      "   0.07  0.03  1.09  0.06 -0.11  1.07  0.16 -0.21  0.95  0.02 -0.19  0.86\n",
      "  -0.06  0.    0.87 -0.14  0.09  0.95 -0.02  0.11  1.07 -0.1   0.15  1.07]\n",
      " [-0.15  0.04  0.99 -0.17  0.05  0.94 -0.15  0.05  0.89  0.06 -0.03  0.97\n",
      "   0.1  -0.04  1.04  0.08 -0.03  1.04  0.01 -0.14  1.02 -0.03 -0.14  0.97\n",
      "  -0.05  0.03  0.89  0.01  0.05  0.91  0.05  0.04  0.97 -0.06  0.13  1.08\n",
      "  -0.08  0.11  1.06 -0.12  0.1   1.04 -0.16  0.04  0.9  -0.13  0.06  0.86\n",
      "   0.05  0.03  1.01  0.07 -0.    1.07  0.04  0.04  1.08 -0.01 -0.13  1.01]\n",
      " [ 0.06 -0.3   1.11  0.02 -0.28  0.95 -0.09 -0.12  0.94 -0.04  0.01  0.87\n",
      "  -0.1   0.14  0.94  0.02  0.04  1.06 -0.09  0.21  1.02  0.02  0.01  1.09\n",
      "  -0.05  0.13  1.04  0.    0.03  0.99 -0.09  0.01  0.87 -0.01  0.    0.93\n",
      "  -0.13  0.1   0.92 -0.04  0.1   1.06 -0.09 -0.18  0.96  0.04 -0.28  1.07\n",
      "  -0.04  0.1   1.04  0.07 -0.02  1.06 -0.03 -0.02  0.89  0.02 -0.03  0.97]\n",
      " [ 0.16 -0.16  1.04 -0.1   0.09  1.02 -0.12  0.06  0.99 -0.13  0.05  0.96\n",
      "  -0.04  0.02  0.93 -0.02  0.05  0.94  0.    0.07  0.98 -0.01  0.08  1.05\n",
      "  -0.05  0.07  1.05 -0.15  0.06  0.97 -0.14  0.03  0.94 -0.11  0.02  0.92\n",
      "   0.16 -0.21  0.96  0.17 -0.17  1.01  0.18 -0.19  1.03 -0.06  0.08  1.03\n",
      "  -0.11  0.09  1.01 -0.12  0.06  0.92 -0.09  0.02  0.9  -0.03  0.03  0.92]\n",
      " [-0.08  0.07  1.    0.06  0.05  1.07  0.05  0.    0.95  0.04  0.03  1.01\n",
      "  -0.06 -0.01  0.89 -0.15  0.08  0.97 -0.14 -0.    0.92 -0.09  0.09  1.07\n",
      "   0.    0.09  1.02  0.03  0.08  1.08 -0.    0.04  0.96  0.12 -0.13  0.97\n",
      "   0.02 -0.19  0.87 -0.03 -0.11  0.95 -0.06 -0.05  0.92  0.04 -0.01  1.04\n",
      "   0.12 -0.03  1.05  0.05  0.08  1.07 -0.    0.    0.95  0.01 -0.02  1.  ]\n",
      " [-0.17  0.1   0.99  0.03 -0.2   0.9   0.07 -0.18  0.9   0.14 -0.2   0.95\n",
      "   0.08  0.01  1.05  0.    0.03  1.04 -0.14  0.07  0.98 -0.13  0.04  0.95\n",
      "  -0.09  0.03  0.91 -0.03  0.13  0.99 -0.01  0.13  1.03 -0.03  0.12  1.05\n",
      "  -0.15  0.08  1.   -0.17  0.08  0.97  0.03 -0.16  0.9   0.09 -0.17  0.93\n",
      "   0.13 -0.16  0.97  0.07 -0.04  1.04  0.01 -0.03  1.04 -0.05  0.03  1.02]\n",
      " [ 0.04  0.07  1.1   0.12 -0.13  1.07  0.17 -0.18  1.01  0.07 -0.22  0.88\n",
      "   0.06 -0.08  0.9  -0.07 -0.05  0.89 -0.1   0.    0.88 -0.15  0.09  0.93\n",
      "  -0.1   0.14  1.06 -0.14  0.08  1.02 -0.04  0.12  1.11  0.03  0.04  1.04\n",
      "   0.04  0.06  1.1  -0.01  0.04  0.95 -0.13  0.04  0.88  0.02 -0.17  0.88\n",
      "  -0.06 -0.09  0.94 -0.05 -0.1   0.9   0.01 -0.06  1.02  0.11 -0.03  1.07]\n",
      " [-0.06  0.02  0.91 -0.02  0.02  0.95  0.17 -0.12  1.07  0.15 -0.09  1.07\n",
      "   0.16 -0.14  1.07 -0.03 -0.09  0.96 -0.1  -0.03  0.94 -0.09  0.03  0.9\n",
      "  -0.06 -0.01  0.91 -0.03  0.01  0.93  0.    0.09  1.05  0.    0.09  1.06\n",
      "  -0.01  0.1   1.07 -0.09  0.09  1.01 -0.12  0.08  0.99 -0.13  0.04  0.96\n",
      "  -0.01 -0.14  0.91  0.04 -0.15  0.91  0.17 -0.15  1.03  0.17 -0.14  1.04]\n",
      " [-0.08 -0.1   0.88  0.14 -0.19  1.16 -0.1  -0.05  0.9  -0.06  0.13  0.99\n",
      "   0.   -0.05  0.91 -0.12  0.13  0.84  0.09  0.06  1.09 -0.15  0.19  0.86\n",
      "  -0.09  0.2   1.03 -0.07  0.05  0.88 -0.    0.15  1.09  0.02  0.11  1.06\n",
      "  -0.16  0.2   0.87 -0.04  0.12  1.1  -0.07 -0.1   0.91  0.09 -0.11  1.16\n",
      "   0.12 -0.18  1.06 -0.07  0.09  0.94  0.04 -0.14  0.95 -0.09  0.11  0.83]\n",
      " [-0.1   0.15  0.93 -0.03 -0.04  0.93 -0.1   0.07  0.89 -0.13  0.16  0.92\n",
      "  -0.03  0.08  0.92 -0.08  0.04  0.88 -0.12  0.13  0.96  0.02 -0.18  0.97\n",
      "  -0.04 -0.15  0.92  0.06 -0.25  1.1   0.06 -0.31  1.01  0.01 -0.27  0.94\n",
      "   0.03  0.06  1.07  0.06 -0.01  1.03  0.03 -0.03  0.94 -0.02  0.11  1.03\n",
      "   0.03  0.03  1.07 -0.11  0.19  0.96 -0.05  0.15  1.02 -0.    0.11  1.05]\n",
      " [-0.18  0.07  0.95  0.05 -0.14  0.92  0.13 -0.14  0.97  0.16 -0.15  1.03\n",
      "  -0.02 -0.03  1.03 -0.06 -0.04  0.99 -0.08 -0.06  0.89  0.02  0.05  0.93\n",
      "   0.06  0.06  1.    0.03  0.09  1.06 -0.09  0.1   1.04 -0.14  0.07  1.\n",
      "  -0.1   0.08  0.87 -0.06  0.1   0.91  0.01  0.07  1.    0.01 -0.01  1.07\n",
      "  -0.01 -0.04  1.04 -0.05 -0.1   0.99  0.06 -0.16  0.88  0.1  -0.14  0.91]\n",
      " [-0.03  0.1   1.04  0.02 -0.04  0.97 -0.1   0.13  0.9  -0.07  0.02  0.94\n",
      "  -0.09  0.14  1.    0.03 -0.03  1.04 -0.04  0.09  1.04 -0.03 -0.05  0.94\n",
      "   0.03  0.05  1.05 -0.07  0.04  0.92 -0.08  0.12  1.02 -0.09 -0.25  0.9\n",
      "   0.04 -0.26  1.04 -0.02 -0.32  0.91  0.07 -0.05  1.03 -0.09  0.05  0.92\n",
      "  -0.   -0.02  0.97 -0.13  0.14  0.95  0.03  0.02  1.05 -0.1   0.15  1.02]\n",
      " [-0.07  0.01  0.87 -0.04  0.06  1.04  0.01  0.09  1.07 -0.02  0.11  1.09\n",
      "  -0.13  0.06  0.98 -0.14  0.03  0.93 -0.14  0.02  0.89 -0.    0.02  0.95\n",
      "   0.03  0.04  1.01  0.05  0.04  1.06  0.07 -0.18  1.02  0.01 -0.17  1.\n",
      "  -0.07 -0.02  0.87 -0.03 -0.    0.89  0.02  0.02  0.94 -0.02  0.11  1.09\n",
      "  -0.06  0.11  1.08 -0.1   0.11  1.04 -0.13  0.02  0.88 -0.1   0.02  0.87]\n",
      " [ 0.02  0.04  0.99 -0.02  0.21  1.02 -0.1   0.09  0.85 -0.15  0.2   0.9\n",
      "  -0.01 -0.04  1.   -0.14  0.17  0.84  0.02  0.09  1.09 -0.16  0.24  0.87\n",
      "   0.   -0.01  1.09 -0.1  -0.06  0.9   0.1  -0.1   1.17  0.08 -0.23  0.98\n",
      "   0.05  0.    1.05  0.09  0.05  1.08 -0.11  0.23  0.9   0.03  0.02  0.93\n",
      "  -0.11  0.13  0.85  0.07  0.03  1.05 -0.14  0.27  0.93 -0.    0.2   1.05]\n",
      " [ 0.04  0.05  1.06 -0.04  0.03  0.91  0.06 -0.08  0.93 -0.06 -0.07  0.9\n",
      "  -0.07  0.    0.86 -0.02 -0.1   0.94  0.13 -0.11  1.05 -0.06  0.13  1.05\n",
      "   0.07  0.03  1.06 -0.01  0.01  0.93  0.02 -0.01  0.96 -0.11 -0.02  0.9\n",
      "  -0.17  0.13  1.   -0.14  0.04  0.96 -0.03  0.09  1.09 -0.05  0.1   1.06\n",
      "   0.06  0.05  1.06 -0.01 -0.02  0.93  0.12 -0.2   0.93  0.02 -0.21  0.87]\n",
      " [-0.03 -0.01  0.97 -0.06  0.1   1.04  0.04 -0.03  1.03 -0.07  0.    0.94\n",
      "   0.03  0.07  1.07  0.    0.03  0.96 -0.11  0.08  0.94  0.06 -0.23  1.03\n",
      "  -0.05 -0.29  0.9  -0.11 -0.23  1.06 -0.01 -0.13  0.95 -0.1  -0.04  0.93\n",
      "   0.01 -0.01  0.99 -0.1   0.05  0.94 -0.1   0.14  0.98 -0.05 -0.01  0.92\n",
      "  -0.15  0.14  0.94 -0.03  0.08  1.05 -0.11  0.07  0.91 -0.04  0.13  1.02]\n",
      " [ 0.03  0.    0.99 -0.09 -0.03  0.89 -0.07 -0.    0.9  -0.15  0.06  0.94\n",
      "  -0.06  0.09  1.06 -0.1   0.08  1.03  0.06  0.02  1.07  0.07 -0.06  0.96\n",
      "   0.18 -0.19  1.    0.07 -0.19  0.87 -0.    0.    0.93 -0.14  0.04  0.89\n",
      "  -0.15  0.14  1.   -0.16  0.09  0.97 -0.07  0.14  1.08  0.04  0.03  1.03\n",
      "   0.01  0.08  1.09 -0.02 -0.01  0.93 -0.14  0.    0.97 -0.12  0.03  0.9 ]\n",
      " [-0.07 -0.23  0.93  0.06 -0.01  1.05 -0.02 -0.01  0.93 -0.1   0.07  0.91\n",
      "   0.03 -0.01  1.02 -0.09  0.05  0.93 -0.13  0.15  0.96 -0.02 -0.04  0.95\n",
      "  -0.12  0.07  0.91  0.03  0.02  1.   -0.07  0.07  0.92 -0.13  0.14  0.91\n",
      "  -0.   -0.28  0.94 -0.08 -0.25  0.89 -0.03 -0.19  1.01 -0.06 -0.06  0.94\n",
      "  -0.09  0.03  0.96 -0.06  0.01  0.93 -0.13  0.1   0.89 -0.04  0.12  1.04]\n",
      " [ 0.01  0.02  1.    0.01 -0.01  1.03 -0.06 -0.05  0.91  0.05 -0.3   1.02\n",
      "  -0.1  -0.18  0.9   0.05 -0.27  1.06 -0.04  0.13  1.01  0.04 -0.03  0.99\n",
      "  -0.11  0.12  0.89 -0.08  0.05  0.93 -0.06  0.11  1.02  0.02  0.08  0.89\n",
      "  -0.01  0.04  1.06 -0.07 -0.01  0.93  0.01  0.    0.99 -0.12  0.12  0.89\n",
      "   0.    0.05  1.04 -0.02 -0.2   1.01  0.04 -0.32  1.01 -0.08 -0.25  0.9 ]\n",
      " [ 0.    0.05  1.04 -0.02 -0.2   1.01  0.04 -0.32  1.01 -0.08 -0.25  0.9\n",
      "  -0.05 -0.06  0.92 -0.06  0.11  1.   -0.12  0.09  0.91 -0.    0.08  1.04\n",
      "  -0.03 -0.03  0.96  0.02  0.02  1.05 -0.08  0.01  0.95 -0.08  0.13  1.01\n",
      "  -0.12  0.13  0.91 -0.01  0.1   1.04 -0.07 -0.11  0.97  0.04 -0.21  1.04\n",
      "  -0.06 -0.22  0.92  0.03 -0.21  0.98 -0.11 -0.06  0.91  0.05 -0.11  1.05]\n",
      " [-0.02 -0.05  0.93  0.    0.04  0.96 -0.13  0.04  0.9  -0.14  0.12  1.\n",
      "  -0.06 -0.04  0.97  0.08 -0.05  1.07  0.07 -0.08  1.05  0.15 -0.11  1.04\n",
      "   0.07 -0.15  0.9   0.    0.01  0.94 -0.14  0.03  0.89 -0.17  0.13  0.97\n",
      "  -0.14  0.04  0.95 -0.1   0.13  1.07 -0.1   0.1   1.03  0.02  0.08  1.09\n",
      "  -0.01  0.05  0.98  0.06 -0.01  1.02 -0.03 -0.05  0.9  -0.12  0.03  0.94]\n",
      " [-0.01  0.1   1.1   0.01  0.08  1.1  -0.    0.06  0.99 -0.12  0.04  0.87\n",
      "   0.02 -0.13  0.89 -0.05 -0.1   0.94  0.02 -0.04  1.07 -0.   -0.05  1.01\n",
      "   0.12 -0.06  1.06 -0.    0.11  1.08  0.06  0.04  1.02 -0.04  0.    0.88\n",
      "   0.01 -0.04  0.94 -0.12 -0.01  0.9  -0.13  0.11  1.01 -0.13  0.07  0.95\n",
      "  -0.04  0.09  1.07 -0.09  0.07  1.04  0.08  0.02  1.11  0.05 -0.07  0.97]\n",
      " [ 0.08  0.05  1.01 -0.15  0.15  0.86  0.03  0.08  0.97  0.03  0.05  0.89\n",
      "  -0.03  0.1   0.94  0.01  0.13  1.1  -0.07  0.12  0.87  0.04  0.05  1.1\n",
      "  -0.08  0.08  1.03  0.13 -0.2   1.09  0.05 -0.13  1.09 -0.06 -0.11  0.91\n",
      "   0.05  0.03  1.07 -0.13  0.1   0.88  0.06  0.05  1.1  -0.15  0.15  0.91\n",
      "  -0.07  0.1   0.86 -0.12  0.16  0.96 -0.14  0.09  0.86  0.03  0.08  1.02]\n",
      " [ 0.01 -0.03  0.93  0.03 -0.    1.05 -0.06 -0.02  0.9  -0.12  0.2   0.95\n",
      "  -0.11  0.08  0.94 -0.09  0.18  1.02  0.01  0.06  0.91 -0.01 -0.04  1.03\n",
      "   0.02 -0.16  0.96  0.06 -0.3   1.11  0.02 -0.28  0.95 -0.09 -0.12  0.94\n",
      "  -0.04  0.01  0.87 -0.1   0.14  0.94  0.02  0.04  1.06 -0.09  0.21  1.02\n",
      "   0.02  0.01  1.09 -0.05  0.13  1.04  0.    0.03  0.99 -0.09  0.01  0.87]\n",
      " [-0.1   0.17  0.95  0.03  0.05  1.03 -0.01 -0.13  1.04 -0.03 -0.2   0.91\n",
      "  -0.08 -0.13  0.97 -0.09 -0.12  0.92  0.05 -0.16  1.06 -0.08  0.15  0.98\n",
      "   0.04 -0.02  1.02 -0.12  0.08  0.89 -0.07 -0.    0.91 -0.1   0.13  1.02\n",
      "   0.03 -0.04  1.02  0.    0.07  1.07 -0.07  0.02  0.93 -0.   -0.04  0.97\n",
      "  -0.13  0.05  0.9   0.02 -0.04  1.05 -0.   -0.24  1.04  0.01 -0.3   0.97]\n",
      " [ 0.02  0.04  1.06 -0.09  0.21  1.02  0.02  0.01  1.09 -0.05  0.13  1.04\n",
      "   0.    0.03  0.99 -0.09  0.01  0.87 -0.01  0.    0.93 -0.13  0.1   0.92\n",
      "  -0.04  0.1   1.06 -0.09 -0.18  0.96  0.04 -0.28  1.07 -0.04  0.1   1.04\n",
      "   0.07 -0.02  1.06 -0.03 -0.02  0.89  0.02 -0.03  0.97 -0.1   0.11  0.91\n",
      "  -0.08  0.18  1.03 -0.13  0.14  0.92 -0.02  0.13  1.03 -0.    0.18  1.02]\n",
      " [-0.    0.06  0.88 -0.13  0.19  0.94 -0.05  0.18  1.04 -0.13  0.07  0.94\n",
      "  -0.1   0.19  0.97 -0.04  0.13  1.03 -0.14  0.18  0.94 -0.05  0.14  1.06\n",
      "   0.02  0.04  1.09 -0.06 -0.2   1.02  0.03 -0.31  1.05 -0.1   0.08  0.96\n",
      "  -0.    0.03  1.06  0.06 -0.05  1.08 -0.08  0.16  1.02  0.01  0.04  1.07\n",
      "   0.04 -0.01  1.01 -0.05  0.17  1.05  0.    0.06  1.07 -0.09  0.19  1.02]\n",
      " [ 0.04 -0.2   0.89  0.13 -0.07  1.01  0.09  0.    1.04  0.03  0.01  1.06\n",
      "  -0.15  0.06  0.98 -0.12  0.05  0.94 -0.07  0.09  0.95 -0.02  0.12  1.03\n",
      "  -0.02  0.12  1.05 -0.14  0.1   1.01 -0.17  0.06  0.96 -0.15  0.08  0.91\n",
      "   0.11 -0.12  0.96  0.14 -0.12  1.03  0.13 -0.08  1.06 -0.03 -0.06  1.\n",
      "  -0.05 -0.08  0.94 -0.03  0.03  0.9   0.04  0.04  0.95  0.05  0.07  1.02]\n",
      " [-0.03  0.16  1.04 -0.15  0.14  0.84  0.04  0.12  1.1   0.01 -0.06  1.\n",
      "  -0.01  0.02  1.09 -0.09 -0.    0.87 -0.09  0.01  0.89  0.11 -0.2   1.01\n",
      "  -0.01  0.09  0.86  0.07  0.1   1.1  -0.14  0.18  0.85 -0.11  0.21  0.98\n",
      "  -0.03  0.06  0.92 -0.02  0.14  1.06  0.02  0.13  1.09 -0.17  0.19  0.87\n",
      "  -0.02  0.12  0.97 -0.12  0.08  0.88  0.03  0.1   1.12  0.13 -0.23  1.09]\n",
      " [-0.11  0.02  0.9  -0.07  0.01  0.87 -0.04  0.06  1.04  0.01  0.09  1.07\n",
      "  -0.02  0.11  1.09 -0.13  0.06  0.98 -0.14  0.03  0.93 -0.14  0.02  0.89\n",
      "  -0.    0.02  0.95  0.03  0.04  1.01  0.05  0.04  1.06  0.07 -0.18  1.02\n",
      "   0.01 -0.17  1.   -0.07 -0.02  0.87 -0.03 -0.    0.89  0.02  0.02  0.94\n",
      "  -0.02  0.11  1.09 -0.06  0.11  1.08 -0.1   0.11  1.04 -0.13  0.02  0.88]\n",
      " [ 0.04  0.1   0.99 -0.16  0.16  0.85  0.    0.08  0.99 -0.08 -0.08  0.9\n",
      "   0.04 -0.15  0.97  0.13 -0.22  1.16 -0.02  0.01  0.88  0.11 -0.01  1.09\n",
      "  -0.03  0.09  1.04  0.06  0.05  1.02 -0.01  0.11  1.05  0.01  0.03  0.98\n",
      "  -0.    0.13  1.1  -0.15  0.18  0.91  0.03  0.08  1.11 -0.11  0.17  0.99\n",
      "  -0.17  0.18  0.84  0.04 -0.11  1.11 -0.09 -0.06  0.91  0.1  -0.07  1.12]\n",
      " [-0.06 -0.29  0.88  0.07 -0.09  1.04  0.02 -0.11  0.94 -0.05 -0.05  0.89\n",
      "   0.05 -0.01  1.03  0.01 -0.02  0.94 -0.02  0.13  1.03  0.03  0.02  1.02\n",
      "  -0.   -0.05  0.95 -0.02  0.12  1.04  0.01  0.07  1.06 -0.01  0.06  0.95\n",
      "  -0.   -0.16  1.03  0.04 -0.21  1.06 -0.07 -0.03  1.02  0.02 -0.1   1.06\n",
      "   0.07 -0.17  1.09 -0.09  0.16  0.99 -0.    0.09  1.03  0.05  0.02  1.07]\n",
      " [-0.1  -0.11  0.97 -0.03 -0.02  0.9  -0.09  0.11  0.92  0.01 -0.03  0.96\n",
      "  -0.07  0.05  0.88 -0.13  0.19  0.94 -0.03 -0.04  0.92 -0.11  0.03  0.91\n",
      "  -0.12  0.15  0.94 -0.07  0.04  0.91 -0.13  0.11  0.93  0.03 -0.34  0.94\n",
      "  -0.08 -0.23  0.9  -0.1  -0.15  0.94 -0.   -0.07  0.91 -0.09  0.06  0.92\n",
      "  -0.1   0.15  0.95 -0.05  0.03  0.9  -0.11  0.15  0.93 -0.1   0.22  0.93]\n",
      " [-0.08  0.02  0.89 -0.02  0.1   1.   -0.02  0.11  1.05 -0.08  0.08  1.03\n",
      "  -0.14  0.09  0.99 -0.16  0.06  0.94  0.03 -0.13  0.91  0.08 -0.13  0.93\n",
      "   0.15 -0.16  1.01  0.08 -0.11  1.06  0.   -0.05  1.02 -0.12  0.04  0.92\n",
      "  -0.1   0.03  0.89 -0.04  0.03  0.9  -0.01  0.09  1.05 -0.02  0.12  1.07\n",
      "  -0.04  0.1   1.08 -0.15  0.06  0.94 -0.15  0.05  0.91  0.01 -0.07  0.94]\n",
      " [ 0.02 -0.01  1.02 -0.05  0.15  1.03  0.03  0.12  1.06 -0.05  0.05  0.92\n",
      "   0.04 -0.18  1.04  0.02 -0.28  0.95 -0.07 -0.22  1.06  0.04 -0.17  1.01\n",
      "  -0.07 -0.02  0.94  0.04  0.02  1.04 -0.06  0.05  0.91 -0.13  0.13  0.91\n",
      "  -0.03 -0.03  0.97 -0.13  0.09  0.89 -0.08  0.18  1.02 -0.09  0.06  0.95\n",
      "  -0.11  0.19  0.96 -0.04 -0.13  0.9  -0.1  -0.03  0.91  0.01 -0.07  1.03]\n",
      " [ 0.02 -0.12  1.06 -0.08 -0.1   0.99  0.02 -0.17  1.07  0.07 -0.26  0.93\n",
      "  -0.06  0.15  1.01  0.02  0.06  1.05 -0.13  0.2   0.94 -0.07  0.19  1.05\n",
      "   0.    0.05  1.09 -0.12  0.17  0.95 -0.05  0.13  1.03 -0.01  0.11  1.07\n",
      "  -0.12  0.11  0.98 -0.04  0.06  1.05 -0.08 -0.18  0.92 -0.09 -0.15  0.97\n",
      "  -0.   -0.21  1.07 -0.08  0.08  0.92 -0.1   0.13  0.94 -0.02  0.13  1.03]\n",
      " [-0.11  0.23  0.95 -0.    0.04  0.92 -0.13  0.11  0.85  0.07  0.04  1.09\n",
      "  -0.15  0.22  0.88 -0.05  0.2   1.04 -0.12  0.01  0.85  0.01  0.19  0.86\n",
      "   0.01 -0.    1.03 -0.11  0.24  1.02  0.11 -0.16  1.15 -0.1  -0.09  0.89\n",
      "   0.14 -0.24  1.09 -0.    0.01  0.92  0.02  0.17  1.05 -0.11  0.11  0.85\n",
      "  -0.14  0.19  0.87  0.04  0.    0.98 -0.11  0.07  0.86  0.02  0.14  1.08]\n",
      " [-0.02  0.11  1.04  0.02  0.03  1.02 -0.09  0.09  0.93  0.02  0.04  1.05\n",
      "  -0.06 -0.    0.92  0.01  0.11  1.07 -0.01  0.06  0.96 -0.11  0.13  0.91\n",
      "   0.02 -0.19  0.99 -0.07 -0.17  0.9  -0.07 -0.11  0.97 -0.05 -0.13  0.92\n",
      "  -0.1  -0.04  0.93  0.02 -0.08  0.94 -0.12  0.11  0.89 -0.05  0.15  1.02\n",
      "  -0.11  0.09  0.95 -0.12  0.18  0.98 -0.01  0.08  1.06 -0.12  0.13  0.9 ]\n",
      " [ 0.02  0.04  1.05 -0.06 -0.    0.92  0.01  0.11  1.07 -0.01  0.06  0.96\n",
      "  -0.11  0.13  0.91  0.02 -0.19  0.99 -0.07 -0.17  0.9  -0.07 -0.11  0.97\n",
      "  -0.05 -0.13  0.92 -0.1  -0.04  0.93  0.02 -0.08  0.94 -0.12  0.11  0.89\n",
      "  -0.05  0.15  1.02 -0.11  0.09  0.95 -0.12  0.18  0.98 -0.01  0.08  1.06\n",
      "  -0.12  0.13  0.9  -0.04  0.14  1.03  0.02  0.06  1.02 -0.06  0.07  0.99]\n",
      " [ 0.09  0.    1.04  0.03  0.01  1.06 -0.15  0.06  0.98 -0.12  0.05  0.94\n",
      "  -0.07  0.09  0.95 -0.02  0.12  1.03 -0.02  0.12  1.05 -0.14  0.1   1.01\n",
      "  -0.17  0.06  0.96 -0.15  0.08  0.91  0.11 -0.12  0.96  0.14 -0.12  1.03\n",
      "   0.13 -0.08  1.06 -0.03 -0.06  1.   -0.05 -0.08  0.94 -0.03  0.03  0.9\n",
      "   0.04  0.04  0.95  0.05  0.07  1.02 -0.07  0.11  1.06 -0.11  0.09  1.04]\n",
      " [-0.11  0.06  0.9  -0.03  0.1   1.04 -0.11  0.08  0.93 -0.09  0.16  1.\n",
      "  -0.07  0.04  0.93 -0.15  0.16  0.96 -0.03  0.11  1.07 -0.12  0.13  0.89\n",
      "  -0.04  0.1   1.04  0.04  0.05  1.03 -0.06 -0.14  0.98  0.04 -0.17  1.03\n",
      "  -0.09 -0.04  0.95  0.02 -0.12  1.06  0.04 -0.17  1.01 -0.03  0.06  1.02\n",
      "   0.05 -0.01  1.05 -0.06  0.03  0.93  0.02  0.01  1.06 -0.03 -0.07  0.95]\n",
      " [-0.15  0.03  0.93 -0.12 -0.01  0.92 -0.12 -0.    0.91 -0.02  0.04  0.93\n",
      "  -0.    0.06  0.96  0.05  0.06  1.09  0.02  0.07  1.09  0.    0.07  1.08\n",
      "   0.04 -0.24  0.95  0.   -0.21  0.92  0.01 -0.23  0.89 -0.   -0.01  0.92\n",
      "   0.03  0.02  0.96  0.02  0.09  1.09 -0.03  0.11  1.09 -0.05  0.07  1.09\n",
      "  -0.15  0.04  0.94 -0.14  0.01  0.91 -0.14  0.01  0.89  0.01  0.    0.94]\n",
      " [-0.11  0.21  0.87  0.    0.13  1.07 -0.18  0.19  0.86 -0.09  0.03  0.99\n",
      "   0.01 -0.15  0.97  0.04 -0.07  1.1   0.15 -0.13  1.12 -0.1   0.09  0.88\n",
      "   0.11 -0.14  0.99 -0.05  0.06  0.85  0.08  0.11  1.11  0.04 -0.03  1.03\n",
      "  -0.1   0.21  0.98 -0.05  0.03  0.92 -0.16  0.16  0.84  0.02  0.12  1.06\n",
      "  -0.02 -0.02  0.97 -0.02  0.07  1.09 -0.12  0.03  0.9  -0.1  -0.03  0.86]\n",
      " [-0.07  0.19  1.05  0.    0.05  1.09 -0.12  0.17  0.95 -0.05  0.13  1.03\n",
      "  -0.01  0.11  1.07 -0.12  0.11  0.98 -0.04  0.06  1.05 -0.08 -0.18  0.92\n",
      "  -0.09 -0.15  0.97 -0.   -0.21  1.07 -0.08  0.08  0.92 -0.1   0.13  0.94\n",
      "  -0.02  0.13  1.03 -0.1   0.12  0.93 -0.13  0.2   0.95 -0.05 -0.03  0.9\n",
      "  -0.11  0.05  0.91 -0.13  0.16  0.93 -0.05  0.05  0.92 -0.11  0.1   0.92]\n",
      " [-0.09  0.17  1.   -0.    0.06  1.06 -0.02 -0.02  0.93 -0.01  0.13  1.07\n",
      "   0.    0.02  0.96 -0.12  0.14  0.92  0.03 -0.25  0.98 -0.07 -0.22  0.92\n",
      "   0.06 -0.16  1.04 -0.06 -0.08  0.92 -0.09  0.03  0.95 -0.03  0.    0.93\n",
      "  -0.13  0.1   0.89 -0.04  0.12  1.04 -0.12  0.1   0.91 -0.09  0.14  1.02\n",
      "  -0.08  0.06  0.94 -0.1   0.14  0.96  0.01  0.09  1.07 -0.1   0.03  0.92]\n",
      " [ 0.15 -0.09  1.07  0.16 -0.14  1.07 -0.03 -0.09  0.96 -0.1  -0.03  0.94\n",
      "  -0.09  0.03  0.9  -0.06 -0.01  0.91 -0.03  0.01  0.93  0.    0.09  1.05\n",
      "   0.    0.09  1.06 -0.01  0.1   1.07 -0.09  0.09  1.01 -0.12  0.08  0.99\n",
      "  -0.13  0.04  0.96 -0.01 -0.14  0.91  0.04 -0.15  0.91  0.17 -0.15  1.03\n",
      "   0.17 -0.14  1.04  0.12 -0.06  1.05 -0.16  0.09  1.   -0.16  0.06  0.95]\n",
      " [-0.13  0.18  0.92 -0.07  0.17  1.02 -0.02  0.13  1.04 -0.14  0.18  0.95\n",
      "  -0.08  0.17  1.05  0.    0.08  1.07 -0.1  -0.2   0.95 -0.03 -0.19  1.05\n",
      "  -0.08 -0.    0.92 -0.11  0.1   0.94 -0.03  0.1   1.05 -0.09  0.11  0.92\n",
      "  -0.12  0.18  0.94 -0.05  0.14  1.04 -0.11  0.07  0.91 -0.13  0.18  0.94\n",
      "  -0.07  0.07  0.92 -0.12  0.12  0.94 -0.14  0.19  0.94 -0.04 -0.23  0.9 ]\n",
      " [ 0.1   0.03  1.09 -0.14  0.21  0.88 -0.08  0.21  1.03 -0.1   0.04  0.86\n",
      "   0.01  0.14  1.1   0.02  0.09  1.   -0.1   0.21  0.98 -0.1   0.14  0.89\n",
      "  -0.13 -0.01  0.9   0.12 -0.16  1.12  0.04 -0.15  0.93  0.07 -0.04  1.08\n",
      "  -0.09  0.04  0.87 -0.14  0.23  0.91  0.01  0.05  0.94  0.    0.13  1.05\n",
      "   0.07  0.02  1.1  -0.15  0.21  0.9  -0.03  0.16  1.04 -0.15  0.14  0.84]\n",
      " [-0.01  0.15  0.99 -0.14  0.05  0.87  0.02 -0.03  0.99  0.08 -0.09  1.12\n",
      "   0.04 -0.1   0.94  0.13 -0.15  1.15 -0.05 -0.04  1.    0.09  0.06  1.06\n",
      "  -0.05  0.12  1.03  0.05  0.02  1.04 -0.02  0.11  1.07 -0.15  0.13  0.88\n",
      "   0.02  0.1   1.07 -0.15  0.15  0.95 -0.14  0.11  0.84 -0.06  0.04  1.05\n",
      "  -0.16  0.1   0.89  0.06 -0.18  1.1  -0.1  -0.06  0.92  0.01 -0.13  0.9 ]\n",
      " [ 0.07 -0.04  1.04  0.01 -0.03  1.04 -0.05  0.03  1.02 -0.12  0.04  0.94\n",
      "  -0.09  0.03  0.9  -0.03  0.12  0.98 -0.02  0.13  1.03 -0.03  0.14  1.05\n",
      "  -0.14  0.09  1.02 -0.16  0.07  0.98 -0.17  0.06  0.95  0.04 -0.12  0.92\n",
      "   0.09 -0.13  0.95  0.14 -0.13  1.    0.07 -0.07  1.04  0.03 -0.08  1.04\n",
      "  -0.13  0.05  0.96 -0.11  0.04  0.92 -0.07  0.03  0.89 -0.01  0.12  1.  ]\n",
      " [ 0.02  0.04  1.05  0.01 -0.21  1.05  0.05 -0.31  1.01  0.04 -0.09  1.07\n",
      "   0.05 -0.15  0.97 -0.07 -0.07  0.9   0.01 -0.03  0.94 -0.09  0.1   0.91\n",
      "  -0.09  0.2   1.   -0.1   0.12  0.93 -0.1   0.21  1.01 -0.13  0.16  0.95\n",
      "  -0.06  0.19  1.05  0.    0.09  1.06 -0.02 -0.08  1.06  0.03 -0.21  1.03\n",
      "  -0.04 -0.2   0.91  0.06 -0.29  1.   -0.06 -0.15  0.9  -0.09 -0.03  0.94]\n",
      " [ 0.05 -0.21  1.04  0.03 -0.24  0.99 -0.03  0.1   1.03  0.06 -0.    1.05\n",
      "  -0.04 -0.01  0.92  0.    0.04  1.05  0.   -0.04  1.   -0.05  0.1   1.03\n",
      "   0.03  0.01  1.03 -0.06  0.01  0.93  0.02  0.01  1.04 -0.   -0.06  0.96\n",
      "  -0.1   0.01  0.9   0.06 -0.3   1.03 -0.05 -0.28  0.91  0.06  0.01  1.06\n",
      "  -0.04 -0.01  0.91 -0.07  0.13  0.99 -0.13  0.1   0.92 -0.01  0.07  1.05]\n",
      " [ 0.07 -0.19  1.03 -0.11  0.02  0.9  -0.07  0.01  0.87 -0.04  0.06  1.04\n",
      "   0.01  0.09  1.07 -0.02  0.11  1.09 -0.13  0.06  0.98 -0.14  0.03  0.93\n",
      "  -0.14  0.02  0.89 -0.    0.02  0.95  0.03  0.04  1.01  0.05  0.04  1.06\n",
      "   0.07 -0.18  1.02  0.01 -0.17  1.   -0.07 -0.02  0.87 -0.03 -0.    0.89\n",
      "   0.02  0.02  0.94 -0.02  0.11  1.09 -0.06  0.11  1.08 -0.1   0.11  1.04]\n",
      " [-0.15  0.13  0.99 -0.02 -0.12  0.96  0.12 -0.15  1.06  0.16 -0.2   0.96\n",
      "   0.08 -0.03  0.99 -0.05 -0.05  0.87 -0.13  0.08  0.95 -0.19  0.09  0.94\n",
      "  -0.06  0.12  1.07 -0.08  0.11  1.07  0.01  0.09  1.09 -0.02 -0.05  0.93\n",
      "   0.    0.04  0.96 -0.13  0.04  0.9  -0.14  0.12  1.   -0.06 -0.04  0.97\n",
      "   0.08 -0.05  1.07  0.07 -0.08  1.05  0.15 -0.11  1.04  0.07 -0.15  0.9 ]\n",
      " [-0.09  0.09  1.04 -0.14  0.11  1.03 -0.17  0.1   0.99  0.03 -0.2   0.9\n",
      "   0.07 -0.18  0.9   0.14 -0.2   0.95  0.08  0.01  1.05  0.    0.03  1.04\n",
      "  -0.14  0.07  0.98 -0.13  0.04  0.95 -0.09  0.03  0.91 -0.03  0.13  0.99\n",
      "  -0.01  0.13  1.03 -0.03  0.12  1.05 -0.15  0.08  1.   -0.17  0.08  0.97\n",
      "   0.03 -0.16  0.9   0.09 -0.17  0.93  0.13 -0.16  0.97  0.07 -0.04  1.04]\n",
      " [-0.15  0.14  0.83 -0.11  0.01  0.89  0.08 -0.14  1.08 -0.1   0.01  0.95\n",
      "   0.05 -0.04  1.08 -0.05 -0.08  0.87 -0.12  0.16  0.83  0.08  0.04  1.03\n",
      "  -0.12  0.2   0.93 -0.04  0.18  1.05 -0.09  0.07  0.87  0.01  0.1   1.09\n",
      "   0.01  0.11  1.05 -0.16  0.2   0.89 -0.04  0.09  1.07 -0.1   0.03  0.89\n",
      "   0.06 -0.01  1.14  0.13 -0.23  1.06 -0.04 -0.02  1.01  0.02 -0.19  0.92]\n",
      " [-0.1   0.04  0.93  0.07 -0.08  1.05 -0.07 -0.    0.93  0.   -0.04  0.97\n",
      "  -0.13  0.13  0.92  0.03  0.02  1.05 -0.09  0.13  1.03  0.01 -0.06  1.02\n",
      "  -0.15  0.09  1.07 -0.06  0.04  0.91 -0.1   0.16  0.98 -0.09 -0.16  0.88\n",
      "  -0.   -0.13  1.04 -0.01 -0.24  0.92  0.07 -0.23  1.06 -0.08 -0.11  0.93\n",
      "   0.01 -0.12  1.05 -0.11  0.11  0.91  0.04  0.02  1.05 -0.08  0.13  1.03]\n",
      " [ 0.01  0.    0.94  0.02 -0.    0.98 -0.09 -0.04  0.9  -0.14  0.07  0.98\n",
      "  -0.16  0.09  0.94 -0.02  0.1   1.07  0.04  0.05  1.04  0.12 -0.06  1.05\n",
      "   0.06 -0.14  0.92 -0.04 -0.11  0.92 -0.01 -0.12  0.87 -0.03 -0.04  0.99\n",
      "  -0.16  0.12  0.96 -0.    0.12  1.07  0.04  0.03  1.01  0.04  0.04  1.05\n",
      "  -0.03 -0.04  0.92 -0.14  0.03  0.93 -0.14  0.01  0.9  -0.12  0.1   1.03]\n",
      " [-0.1  -0.03  0.91  0.01 -0.07  1.03 -0.09 -0.13  0.9  -0.01 -0.15  1.03\n",
      "   0.06 -0.19  1.05 -0.08  0.16  0.98  0.02  0.05  1.05 -0.15  0.16  0.95\n",
      "  -0.03  0.09  1.06  0.02 -0.01  1.03 -0.06  0.13  1.01  0.03  0.06  1.06\n",
      "  -0.06 -0.    0.92  0.01  0.06  1.04 -0.   -0.04  0.97 -0.09  0.03  1.04\n",
      "   0.06 -0.3   1.03 -0.05 -0.24  0.91  0.05  0.01  1.06 -0.   -0.01  0.94]\n",
      " [ 0.16 -0.18  0.97  0.22 -0.25  1.03 -0.02  0.09  1.05 -0.07  0.09  1.04\n",
      "  -0.09  0.08  1.02 -0.15  0.03  0.93 -0.12 -0.01  0.92 -0.12 -0.    0.91\n",
      "  -0.02  0.04  0.93 -0.    0.06  0.96  0.05  0.06  1.09  0.02  0.07  1.09\n",
      "   0.    0.07  1.08  0.04 -0.24  0.95  0.   -0.21  0.92  0.01 -0.23  0.89\n",
      "  -0.   -0.01  0.92  0.03  0.02  0.96  0.02  0.09  1.09 -0.03  0.11  1.09]\n",
      " [-0.04 -0.    0.92 -0.12  0.12  0.9   0.   -0.05  0.98 -0.13  0.09  0.9\n",
      "   0.01  0.02  1.01 -0.06  0.05  0.93 -0.12  0.13  0.93 -0.02 -0.09  0.91\n",
      "  -0.11 -0.03  0.89 -0.01 -0.02  1.03 -0.09 -0.19  0.93 -0.05 -0.18  1.\n",
      "   0.05 -0.23  0.92 -0.13  0.12  0.9  -0.02  0.1   1.04 -0.12  0.09  0.93\n",
      "  -0.06  0.15  1.01  0.02  0.01  1.05 -0.13  0.14  0.95 -0.03  0.07  1.07]\n",
      " [-0.15  0.08  1.   -0.17  0.08  0.97  0.03 -0.16  0.9   0.09 -0.17  0.93\n",
      "   0.13 -0.16  0.97  0.07 -0.04  1.04  0.01 -0.03  1.04 -0.05  0.03  1.02\n",
      "  -0.12  0.04  0.94 -0.09  0.03  0.9  -0.03  0.12  0.98 -0.02  0.13  1.03\n",
      "  -0.03  0.14  1.05 -0.14  0.09  1.02 -0.16  0.07  0.98 -0.17  0.06  0.95\n",
      "   0.04 -0.12  0.92  0.09 -0.13  0.95  0.14 -0.13  1.    0.07 -0.07  1.04]\n",
      " [ 0.06 -0.    0.99 -0.04  0.21  0.99 -0.07  0.04  0.86 -0.16  0.21  0.86\n",
      "   0.06  0.03  1.   -0.11  0.02  0.88  0.04  0.15  1.09 -0.16  0.18  0.89\n",
      "  -0.04  0.23  1.03 -0.16  0.15  0.84  0.01  0.12  1.09  0.06 -0.23  1.02\n",
      "   0.04 -0.1   1.12 -0.05 -0.03  1.07 -0.1   0.12  0.92  0.01 -0.06  0.89\n",
      "  -0.14  0.17  0.85  0.07  0.04  1.03 -0.08  0.24  0.97  0.    0.17  1.08]\n",
      " [-0.08  0.08  1.06  0.01  0.09  1.09  0.03  0.08  1.09  0.01  0.01  0.96\n",
      "   0.15 -0.18  0.96  0.04 -0.21  0.86 -0.01 -0.11  0.96 -0.1   0.01  0.92\n",
      "   0.    0.04  1.05  0.09  0.02  1.05  0.04  0.08  1.08  0.01  0.    0.94\n",
      "   0.02 -0.    0.98 -0.09 -0.04  0.9  -0.14  0.07  0.98 -0.16  0.09  0.94\n",
      "  -0.02  0.1   1.07  0.04  0.05  1.04  0.12 -0.06  1.05  0.06 -0.14  0.92]\n",
      " [ 0.05  0.03  1.05 -0.02 -0.01  0.92  0.01  0.11  1.03  0.01  0.01  0.98\n",
      "  -0.07 -0.22  1.03  0.06 -0.22  1.07 -0.02 -0.31  0.92  0.02  0.08  1.06\n",
      "   0.01 -0.02  1.   -0.07  0.06  0.94  0.04  0.05  1.04 -0.01  0.01  0.96\n",
      "  -0.14  0.15  0.92  0.01  0.03  1.02 -0.06  0.02  0.93  0.03  0.09  1.07\n",
      "  -0.02  0.04  0.95 -0.11  0.15  0.9   0.04 -0.29  1.02 -0.06 -0.19  0.93]\n",
      " [-0.05  0.02  0.93  0.07  0.01  1.15  0.   -0.16  0.92  0.15 -0.21  1.15\n",
      "  -0.01 -0.06  1.04  0.09  0.04  1.07 -0.05  0.16  1.03 -0.12  0.15  0.85\n",
      "  -0.03  0.15  1.05 -0.14  0.17  0.86 -0.05  0.15  1.07 -0.16  0.18  0.87\n",
      "  -0.06  0.06  0.93 -0.19  0.19  0.87 -0.05  0.09  0.9   0.08  0.03  1.12\n",
      "  -0.03 -0.12  0.89  0.13 -0.25  1.12 -0.11  0.06  0.84  0.09 -0.02  1.02]\n",
      " [ 0.02  0.05  0.91 -0.04 -0.05  1.01  0.04 -0.14  1.03 -0.06 -0.13  0.96\n",
      "   0.05 -0.21  1.04  0.03 -0.24  0.99 -0.03  0.1   1.03  0.06 -0.    1.05\n",
      "  -0.04 -0.01  0.92  0.    0.04  1.05  0.   -0.04  1.   -0.05  0.1   1.03\n",
      "   0.03  0.01  1.03 -0.06  0.01  0.93  0.02  0.01  1.04 -0.   -0.06  0.96\n",
      "  -0.1   0.01  0.9   0.06 -0.3   1.03 -0.05 -0.28  0.91  0.06  0.01  1.06]\n",
      " [-0.11  0.12  1.03 -0.15  0.09  1.   -0.15  0.06  0.95 -0.06 -0.02  0.92\n",
      "  -0.03 -0.    0.94  0.01  0.    0.97  0.01  0.08  1.07  0.02  0.05  1.07\n",
      "  -0.04  0.06  1.06 -0.08  0.01  0.98 -0.12 -0.01  0.92 -0.06 -0.06  0.9\n",
      "   0.16 -0.18  0.97  0.22 -0.25  1.03 -0.02  0.09  1.05 -0.07  0.09  1.04\n",
      "  -0.09  0.08  1.02 -0.15  0.03  0.93 -0.12 -0.01  0.92 -0.12 -0.    0.91]\n",
      " [-0.13  0.07  1.01 -0.06 -0.05  0.91  0.01 -0.09  0.93  0.07 -0.15  1.03\n",
      "   0.12 -0.12  1.05  0.06 -0.14  1.04 -0.12  0.03  0.94 -0.09  0.02  0.91\n",
      "  -0.03  0.02  0.91  0.02  0.13  1.05 -0.06  0.14  1.08 -0.08  0.11  0.96\n",
      "  -0.15  0.03  0.92 -0.13  0.04  0.88  0.04  0.03  1.02  0.06  0.03  1.08\n",
      "   0.01  0.05  1.09  0.   -0.2   0.98 -0.01 -0.19  0.93  0.05 -0.23  0.88]\n",
      " [-0.05 -0.28  0.87  0.07 -0.11  1.03  0.02 -0.11  0.94  0.03  0.05  1.09\n",
      "   0.04 -0.02  1.   -0.02  0.01  0.93  0.01  0.06  1.07  0.   -0.06  0.95\n",
      "  -0.08 -0.03  0.86  0.01  0.06  1.05 -0.03  0.07  0.93  0.02 -0.2   1.06\n",
      "   0.04 -0.29  0.99 -0.02 -0.24  0.92  0.06 -0.17  1.1   0.05 -0.22  0.97\n",
      "  -0.04 -0.14  0.9   0.05  0.    1.04 -0.   -0.02  0.92 -0.08  0.06  1.06]\n",
      " [-0.09 -0.2   0.92 -0.04 -0.21  0.99 -0.05  0.02  0.92 -0.11  0.11  0.91\n",
      "   0.01  0.09  0.94 -0.12  0.12  0.92 -0.07  0.15  1.02 -0.09  0.04  0.94\n",
      "  -0.12  0.17  0.96 -0.    0.08  1.06 -0.12  0.16  0.9  -0.02  0.13  1.05\n",
      "   0.02  0.06  0.99 -0.03 -0.18  1.02  0.05 -0.23  1.06 -0.09  0.06  0.96\n",
      "   0.03 -0.01  1.07  0.01 -0.05  0.98 -0.02  0.11  1.04  0.02  0.03  1.02]\n",
      " [-0.02  0.13  1.03  0.03  0.02  1.02 -0.   -0.05  0.95 -0.02  0.12  1.04\n",
      "   0.01  0.07  1.06 -0.01  0.06  0.95 -0.   -0.16  1.03  0.04 -0.21  1.06\n",
      "  -0.07 -0.03  1.02  0.02 -0.1   1.06  0.07 -0.17  1.09 -0.09  0.16  0.99\n",
      "  -0.    0.09  1.03  0.05  0.02  1.07 -0.11  0.23  1.01 -0.03  0.12  1.04\n",
      "   0.03  0.01  0.91 -0.07  0.17  1.02 -0.    0.1   1.06 -0.12  0.08  0.97]\n",
      " [ 0.03  0.08  1.09  0.01  0.01  0.96  0.15 -0.18  0.96  0.04 -0.21  0.86\n",
      "  -0.01 -0.11  0.96 -0.1   0.01  0.92  0.    0.04  1.05  0.09  0.02  1.05\n",
      "   0.04  0.08  1.08  0.01  0.    0.94  0.02 -0.    0.98 -0.09 -0.04  0.9\n",
      "  -0.14  0.07  0.98 -0.16  0.09  0.94 -0.02  0.1   1.07  0.04  0.05  1.04\n",
      "   0.12 -0.06  1.05  0.06 -0.14  0.92 -0.04 -0.11  0.92 -0.01 -0.12  0.87]\n",
      " [-0.02  0.04  1.06  0.03 -0.07  1.08 -0.06 -0.11  1.02  0.04 -0.25  1.06\n",
      "   0.08 -0.31  1.05 -0.04  0.14  1.02  0.03  0.06  1.07 -0.12  0.21  0.95\n",
      "  -0.07  0.18  1.04 -0.    0.08  1.07 -0.12  0.16  0.91 -0.08  0.16  1.\n",
      "  -0.02  0.12  1.03 -0.14  0.15  0.93 -0.09  0.17  1.02 -0.08 -0.24  0.88\n",
      "  -0.1  -0.16  0.93 -0.08 -0.18  1.   -0.03 -0.08  0.9  -0.08  0.04  0.92]\n",
      " [-0.08  0.09  0.92 -0.1   0.15  0.93 -0.03 -0.04  0.93 -0.1   0.07  0.89\n",
      "  -0.13  0.16  0.92 -0.03  0.08  0.92 -0.08  0.04  0.88 -0.12  0.13  0.96\n",
      "   0.02 -0.18  0.97 -0.04 -0.15  0.92  0.06 -0.25  1.1   0.06 -0.31  1.01\n",
      "   0.01 -0.27  0.94  0.03  0.06  1.07  0.06 -0.01  1.03  0.03 -0.03  0.94\n",
      "  -0.02  0.11  1.03  0.03  0.03  1.07 -0.11  0.19  0.96 -0.05  0.15  1.02]\n",
      " [-0.14  0.03  0.93 -0.14  0.02  0.89 -0.    0.02  0.95  0.03  0.04  1.01\n",
      "   0.05  0.04  1.06  0.07 -0.18  1.02  0.01 -0.17  1.   -0.07 -0.02  0.87\n",
      "  -0.03 -0.    0.89  0.02  0.02  0.94 -0.02  0.11  1.09 -0.06  0.11  1.08\n",
      "  -0.1   0.11  1.04 -0.13  0.02  0.88 -0.1   0.02  0.87  0.03  0.06  1.04\n",
      "   0.04  0.05  1.07 -0.02  0.1   1.06  0.   -0.18  0.97  0.   -0.2   0.93]\n",
      " [ 0.14 -0.12  1.03  0.13 -0.08  1.06 -0.03 -0.06  1.   -0.05 -0.08  0.94\n",
      "  -0.03  0.03  0.9   0.04  0.04  0.95  0.05  0.07  1.02 -0.07  0.11  1.06\n",
      "  -0.11  0.09  1.04 -0.14  0.06  1.   -0.13  0.08  0.87 -0.05  0.1   0.91\n",
      "   0.07 -0.    1.07  0.03 -0.01  1.08 -0.03 -0.02  1.05 -0.   -0.17  0.9\n",
      "   0.01 -0.17  0.87  0.07 -0.11  0.91  0.04  0.11  1.04 -0.02  0.11  1.06]\n",
      " [ 0.02 -0.03  0.97 -0.1   0.11  0.91 -0.08  0.18  1.03 -0.13  0.14  0.92\n",
      "  -0.02  0.13  1.03 -0.    0.18  1.02  0.02  0.04  1.08 -0.06  0.06  0.91\n",
      "   0.04 -0.34  0.98 -0.07 -0.28  0.88 -0.08 -0.18  0.98 -0.06 -0.06  0.89\n",
      "  -0.1   0.06  0.92 -0.02  0.08  1.05 -0.1   0.12  0.93 -0.09  0.17  0.99\n",
      "  -0.1   0.05  0.88 -0.13  0.16  0.94 -0.06  0.18  1.05 -0.13  0.14  0.95]\n",
      " [ 0.06  0.05  1.02 -0.01  0.11  1.05  0.01  0.03  0.98 -0.    0.13  1.1\n",
      "  -0.15  0.18  0.91  0.03  0.08  1.11 -0.11  0.17  0.99 -0.17  0.18  0.84\n",
      "   0.04 -0.11  1.11 -0.09 -0.06  0.91  0.1  -0.07  1.12 -0.06  0.01  0.99\n",
      "  -0.09  0.03  0.86 -0.    0.08  1.04 -0.13  0.15  0.87 -0.03  0.1   0.9\n",
      "  -0.15  0.16  0.92 -0.09  0.1   0.87  0.04  0.04  1.   -0.15  0.13  0.84]\n",
      " [-0.16  0.09  0.91 -0.09  0.12  1.05 -0.05 -0.03  1.01  0.12 -0.1   1.09\n",
      "   0.09 -0.06  1.06  0.13 -0.09  1.01  0.05 -0.13  0.87  0.02  0.02  0.92\n",
      "  -0.11  0.04  0.88 -0.04 -0.04  0.9  -0.13  0.04  0.95 -0.11  0.12  1.06\n",
      "  -0.16  0.07  0.97 -0.01  0.09  1.08  0.04  0.06  1.07  0.08 -0.01  1.09\n",
      "   0.07 -0.08  0.97  0.17 -0.16  0.99  0.07 -0.2   0.87 -0.05 -0.1   0.89]\n",
      " [ 0.06 -0.16  0.88  0.1  -0.14  0.91  0.02  0.1   1.06 -0.05  0.1   1.05\n",
      "  -0.11  0.1   1.03 -0.1   0.02  0.9  -0.04  0.01  0.92 -0.03  0.07  0.96\n",
      "   0.01  0.07  1.06 -0.06  0.07  1.04 -0.13  0.07  1.01 -0.06 -0.05  0.91\n",
      "   0.01 -0.09  0.93  0.07 -0.15  1.03  0.12 -0.12  1.05  0.06 -0.14  1.04\n",
      "  -0.12  0.03  0.94 -0.09  0.02  0.91 -0.03  0.02  0.91  0.02  0.13  1.05]\n",
      " [ 0.03  0.04  1.04  0.04  0.06  1.1  -0.01  0.04  0.95 -0.13  0.04  0.88\n",
      "   0.02 -0.17  0.88 -0.06 -0.09  0.94 -0.05 -0.1   0.9   0.01 -0.06  1.02\n",
      "   0.11 -0.03  1.07 -0.02  0.12  1.08  0.07  0.05  1.04 -0.    0.01  0.91\n",
      "   0.04 -0.    1.   -0.04 -0.05  0.9  -0.02  0.03  0.95 -0.12  0.01  0.88\n",
      "  -0.16  0.07  0.95 -0.11  0.    0.92 -0.1   0.06  1.02  0.05  0.03  1.08]\n",
      " [ 0.09 -0.22  0.88  0.07  0.03  1.04  0.03  0.06  1.07 -0.03  0.09  1.05\n",
      "  -0.13  0.06  0.97 -0.12  0.05  0.91 -0.07  0.02  0.9  -0.02  0.12  1.01\n",
      "  -0.01  0.12  1.05 -0.14  0.13  1.02 -0.16  0.09  0.97 -0.16  0.08  0.93\n",
      "   0.13 -0.18  0.94  0.18 -0.2   1.01  0.16 -0.17  1.05 -0.06  0.05  1.02\n",
      "  -0.08  0.03  0.99 -0.11  0.03  0.88 -0.03  0.03  0.9  -0.    0.04  0.93]\n",
      " [-0.02 -0.16  0.87 -0.09 -0.01  0.88 -0.14  0.11  0.95 -0.14  0.05  0.94\n",
      "  -0.07  0.14  1.08  0.03  0.06  1.07  0.01  0.08  1.11 -0.03  0.02  0.95\n",
      "  -0.14  0.02  0.91 -0.08 -0.01  0.89 -0.13  0.08  0.99  0.04  0.05  0.92\n",
      "   0.11 -0.12  1.03  0.18 -0.17  1.04  0.04  0.09  1.08  0.01  0.01  0.94\n",
      "  -0.11  0.04  0.9  -0.09 -0.02  0.9  -0.17  0.12  0.99 -0.03  0.13  1.08]\n",
      " [-0.12  0.11  0.98 -0.04  0.06  1.05 -0.08 -0.18  0.92 -0.09 -0.15  0.97\n",
      "  -0.   -0.21  1.07 -0.08  0.08  0.92 -0.1   0.13  0.94 -0.02  0.13  1.03\n",
      "  -0.1   0.12  0.93 -0.13  0.2   0.95 -0.05 -0.03  0.9  -0.11  0.05  0.91\n",
      "  -0.13  0.16  0.93 -0.05  0.05  0.92 -0.11  0.1   0.92 -0.13  0.16  0.95\n",
      "  -0.02 -0.3   0.91 -0.07 -0.23  0.92 -0.1  -0.1   0.98  0.   -0.07  0.92]\n",
      " [-0.11  0.11  0.84  0.08  0.05  1.01 -0.15  0.15  0.86  0.03  0.08  0.97\n",
      "   0.03  0.05  0.89 -0.03  0.1   0.94  0.01  0.13  1.1  -0.07  0.12  0.87\n",
      "   0.04  0.05  1.1  -0.08  0.08  1.03  0.13 -0.2   1.09  0.05 -0.13  1.09\n",
      "  -0.06 -0.11  0.91  0.05  0.03  1.07 -0.13  0.1   0.88  0.06  0.05  1.1\n",
      "  -0.15  0.15  0.91 -0.07  0.1   0.86 -0.12  0.16  0.96 -0.14  0.09  0.86]\n",
      " [ 0.02  0.11  1.06 -0.16  0.2   0.87 -0.04  0.12  1.1  -0.07 -0.1   0.91\n",
      "   0.09 -0.11  1.16  0.12 -0.18  1.06 -0.07  0.09  0.94  0.04 -0.14  0.95\n",
      "  -0.09  0.11  0.83  0.08  0.07  1.12 -0.14  0.19  0.85 -0.12  0.22  0.98\n",
      "  -0.02  0.01  0.94 -0.15  0.14  0.84  0.03  0.12  1.1  -0.17  0.18  0.87\n",
      "  -0.08  0.08  1.03 -0.04 -0.04  0.93  0.04 -0.02  1.11  0.14 -0.17  1.12]\n",
      " [ 0.05 -0.07  0.97  0.18 -0.2   0.99  0.07 -0.21  0.87 -0.02 -0.13  0.9\n",
      "  -0.09  0.03  0.86 -0.15  0.11  0.96  0.01  0.06  0.94 -0.09  0.14  1.08\n",
      "   0.01  0.1   1.09 -0.03  0.08  1.11  0.01  0.07  1.03 -0.06 -0.02  0.89\n",
      "   0.    0.01  0.95 -0.12  0.03  0.9  -0.16  0.09  0.98 -0.01 -0.17  0.92\n",
      "   0.05 -0.13  1.05 -0.12  0.1   0.98  0.02  0.06  1.07  0.06  0.05  1.  ]\n",
      " [-0.16  0.09  0.91  0.03 -0.2   0.87 -0.04 -0.14  0.94  0.09 -0.12  1.07\n",
      "  -0.03  0.06  1.02  0.07  0.05  1.09 -0.02  0.12  1.07  0.07  0.04  1.03\n",
      "  -0.03 -0.    0.91 -0.01 -0.04  0.95 -0.09 -0.04  0.89 -0.15  0.05  0.96\n",
      "  -0.18  0.09  0.93 -0.09  0.13  1.04 -0.06 -0.04  0.99  0.09 -0.09  1.07\n",
      "   0.16 -0.17  1.01  0.12 -0.05  1.07  0.09 -0.11  0.91 -0.03 -0.06  0.88]\n",
      " [-0.14  0.07  0.98 -0.16  0.09  0.94 -0.02  0.1   1.07  0.04  0.05  1.04\n",
      "   0.12 -0.06  1.05  0.06 -0.14  0.92 -0.04 -0.11  0.92 -0.01 -0.12  0.87\n",
      "  -0.03 -0.04  0.99 -0.16  0.12  0.96 -0.    0.12  1.07  0.04  0.03  1.01\n",
      "   0.04  0.04  1.05 -0.03 -0.04  0.92 -0.14  0.03  0.93 -0.14  0.01  0.9\n",
      "  -0.12  0.1   1.03 -0.15  0.11  0.99  0.02  0.07  1.08  0.06 -0.03  1.  ]\n",
      " [-0.1  -0.02  1.03 -0.01 -0.02  0.94 -0.08  0.07  0.89  0.02  0.01  0.99\n",
      "  -0.03 -0.05  0.92 -0.11  0.01  0.88  0.01  0.03  1.01 -0.04  0.06  0.93\n",
      "  -0.11  0.09  0.91  0.05 -0.34  1.02 -0.01 -0.31  0.93  0.05 -0.05  1.09\n",
      "   0.06 -0.14  1.01  0.02 -0.12  0.94  0.03  0.04  1.09  0.03 -0.03  1.\n",
      "  -0.02 -0.    0.93  0.    0.07  1.07  0.01 -0.06  0.96 -0.02  0.11  1.04]\n",
      " [ 0.16 -0.16  1.01  0.17 -0.15  1.03 -0.08  0.08  1.02 -0.1   0.08  1.\n",
      "  -0.13  0.05  0.96 -0.04  0.01  0.92 -0.01  0.03  0.95  0.    0.06  0.98\n",
      "   0.    0.09  1.05 -0.04  0.07  1.04 -0.08  0.08  0.98 -0.15  0.06  0.94\n",
      "  -0.11  0.04  0.92  0.16 -0.19  0.96  0.16 -0.17  0.99  0.18 -0.19  1.02\n",
      "  -0.03  0.07  1.04 -0.07  0.08  1.03 -0.1   0.08  1.01 -0.11  0.03  0.92]\n",
      " [-0.15  0.08  0.96 -0.15 -0.18  0.89  0.05 -0.18  0.9   0.1  -0.2   0.93\n",
      "   0.15 -0.12  1.05  0.11 -0.06  1.07  0.08 -0.05  1.06 -0.13  0.08  0.99\n",
      "  -0.14  0.06  0.96 -0.14  0.06  0.94 -0.06  0.    0.91 -0.05  0.04  0.92\n",
      "   0.04  0.04  1.05  0.04  0.04  1.07  0.02  0.06  1.07 -0.03 -0.08  1.02\n",
      "  -0.04 -0.11  0.97 -0.03 -0.14  0.94  0.07 -0.17  0.9   0.1  -0.14  0.92]\n",
      " [ 0.08  0.06  1.05 -0.03  0.03  0.92  0.02  0.15  1.08 -0.14  0.15  0.87\n",
      "  -0.13  0.22  0.97 -0.06  0.01  0.91 -0.02  0.21  1.06  0.02  0.08  1.05\n",
      "  -0.14  0.23  0.97  0.09 -0.17  1.13 -0.08 -0.1   0.9   0.13 -0.25  1.11\n",
      "   0.02 -0.04  0.93  0.04  0.11  1.05 -0.11  0.08  0.85 -0.13  0.23  0.9\n",
      "  -0.    0.05  0.94 -0.14  0.11  0.87  0.06  0.04  1.06 -0.14  0.24  0.95]\n",
      " [ 0.14 -0.19  0.96  0.05  0.05  1.04 -0.02  0.01  0.89  0.01 -0.01  0.94\n",
      "  -0.13  0.02  0.92 -0.15  0.15  1.01 -0.14  0.06  0.95 -0.05  0.09  1.08\n",
      "   0.03  0.07  1.09  0.03  0.06  1.1   0.04  0.01  1.01  0.18 -0.19  1.04\n",
      "   0.11 -0.21  0.9   0.01 -0.19  0.87 -0.04 -0.03  0.86 -0.12  0.06  0.92\n",
      "  -0.06  0.11  1.04 -0.16  0.14  0.98 -0.03  0.12  1.08  0.06  0.09  1.07]\n",
      " [ 0.03  0.11  1.05 -0.09  0.02  0.85  0.11  0.02  1.08  0.02  0.04  0.95\n",
      "   0.    0.18  1.03 -0.1   0.09  1.1  -0.17  0.21  0.89  0.01  0.02  0.97\n",
      "  -0.15  0.17  0.84  0.02  0.12  1.09 -0.18  0.22  0.87 -0.02 -0.    1.09\n",
      "  -0.06 -0.13  0.91  0.09 -0.12  1.16  0.11 -0.17  1.02 -0.04  0.09  1.01\n",
      "   0.07  0.07  1.1  -0.13  0.2   0.86  0.08  0.03  1.03 -0.04  0.06  0.91]\n",
      " [ 0.04 -0.04  0.99 -0.03 -0.02  0.93 -0.1   0.09  0.91  0.   -0.01  0.96\n",
      "  -0.04 -0.05  0.91 -0.11  0.05  1.1   0.    0.    0.97 -0.07  0.04  0.92\n",
      "   0.07 -0.31  1.07  0.02 -0.34  0.96 -0.06 -0.29  0.88  0.07 -0.09  1.04\n",
      "   0.02 -0.11  0.94 -0.05 -0.05  0.89  0.05 -0.01  1.03  0.01 -0.02  0.94\n",
      "  -0.02  0.13  1.03  0.03  0.02  1.02 -0.   -0.05  0.95 -0.02  0.12  1.04]\n",
      " [ 0.    0.04  0.98 -0.03  0.13  1.06  0.01  0.03  1.08  0.03 -0.05  1.01\n",
      "  -0.02 -0.22  1.07  0.05 -0.3   1.09  0.07 -0.36  1.02 -0.03  0.13  1.04\n",
      "   0.04  0.05  1.08  0.06 -0.03  0.94 -0.06  0.15  1.05  0.01  0.06  1.08\n",
      "  -0.13  0.18  0.92 -0.07  0.17  1.02 -0.02  0.13  1.04 -0.14  0.18  0.95\n",
      "  -0.08  0.17  1.05  0.    0.08  1.07 -0.1  -0.2   0.95 -0.03 -0.19  1.05]\n",
      " [ 0.01 -0.23  0.89 -0.   -0.01  0.92  0.03  0.02  0.96  0.02  0.09  1.09\n",
      "  -0.03  0.11  1.09 -0.05  0.07  1.09 -0.15  0.04  0.94 -0.14  0.01  0.91\n",
      "  -0.14  0.01  0.89  0.01  0.    0.94  0.02  0.02  0.96  0.05  0.02  1.08\n",
      "   0.16 -0.18  1.08  0.13 -0.2   1.06 -0.11  0.01  0.93 -0.11 -0.03  0.91\n",
      "  -0.11  0.01  0.88  0.02  0.01  0.97  0.02  0.05  1.01  0.05  0.05  1.05]\n",
      " [-0.05  0.1   1.09 -0.04  0.08  1.07  0.04  0.05  1.06 -0.04  0.03  0.91\n",
      "   0.06 -0.08  0.93 -0.06 -0.07  0.9  -0.07  0.    0.86 -0.02 -0.1   0.94\n",
      "   0.13 -0.11  1.05 -0.06  0.13  1.05  0.07  0.03  1.06 -0.01  0.01  0.93\n",
      "   0.02 -0.01  0.96 -0.11 -0.02  0.9  -0.17  0.13  1.   -0.14  0.04  0.96\n",
      "  -0.03  0.09  1.09 -0.05  0.1   1.06  0.06  0.05  1.06 -0.01 -0.02  0.93]\n",
      " [-0.13  0.13  0.91 -0.03 -0.03  0.97 -0.13  0.09  0.89 -0.08  0.18  1.02\n",
      "  -0.09  0.06  0.95 -0.11  0.19  0.96 -0.04 -0.13  0.9  -0.1  -0.03  0.91\n",
      "   0.01 -0.07  1.03 -0.09 -0.13  0.9  -0.01 -0.15  1.03  0.06 -0.19  1.05\n",
      "  -0.08  0.16  0.98  0.02  0.05  1.05 -0.15  0.16  0.95 -0.03  0.09  1.06\n",
      "   0.02 -0.01  1.03 -0.06  0.13  1.01  0.03  0.06  1.06 -0.06 -0.    0.92]\n",
      " [ 0.01 -0.16  0.91 -0.11  0.15  0.85  0.08  0.06  1.05 -0.11  0.22  0.94\n",
      "  -0.01  0.18  1.06 -0.14  0.14  0.86  0.05  0.01  1.06 -0.06  0.07  0.92\n",
      "  -0.03  0.2   1.05  0.04  0.05  1.09 -0.12  0.18  0.99 -0.05 -0.    0.95\n",
      "  -0.05 -0.12  0.88  0.14 -0.26  1.09 -0.04 -0.01  0.99  0.05  0.16  1.07\n",
      "  -0.12  0.13  0.84 -0.11  0.26  0.96 -0.04  0.06  0.9   0.03  0.13  1.08]\n",
      " [-0.12  0.11  0.9   0.01  0.05  1.04 -0.05  0.01  0.9   0.06 -0.32  1.04\n",
      "  -0.07 -0.27  0.92  0.01 -0.09  0.94 -0.1   0.11  0.94  0.06 -0.05  1.05\n",
      "  -0.04  0.13  1.05  0.01 -0.03  1.   -0.13  0.13  0.9  -0.05 -0.03  0.92\n",
      "  -0.12  0.14  0.99  0.02  0.01  0.94 -0.04  0.12  1.05 -0.    0.04  0.96\n",
      "   0.03 -0.21  1.02 -0.03 -0.26  0.9  -0.07 -0.21  0.97 -0.07 -0.06  0.94]\n",
      " [-0.11  0.19  0.99 -0.16  0.15  0.85 -0.01  0.04  1.09 -0.16  0.11  0.89\n",
      "  -0.03 -0.03  0.94 -0.09 -0.01  0.94 -0.03 -0.11  0.89  0.13 -0.2   1.08\n",
      "  -0.11  0.12  0.83  0.07  0.05  1.   -0.15  0.16  0.86  0.01  0.04  0.96\n",
      "   0.05  0.06  1.12 -0.06  0.09  0.89  0.02  0.13  1.08 -0.08  0.16  1.03\n",
      "   0.03  0.03  1.05 -0.    0.05  1.09 -0.16  0.12  0.89  0.11 -0.19  1.14]\n",
      " [ 0.09  0.08  1.09 -0.13  0.22  0.89  0.02  0.14  1.06 -0.12  0.14  0.85\n",
      "   0.08 -0.02  1.05 -0.05  0.06  0.91 -0.02  0.17  1.07  0.01  0.07  1.03\n",
      "  -0.14  0.22  0.98 -0.1   0.13  0.89 -0.09 -0.03  0.9   0.1  -0.26  1.07\n",
      "  -0.02 -0.05  1.06  0.09 -0.01  1.11 -0.12  0.12  0.87 -0.02  0.12  1.04\n",
      "  -0.08  0.11  0.84  0.09  0.    1.12  0.   -0.03  0.99 -0.03  0.2   1.05]\n",
      " [-0.14  0.05  0.93 -0.07  0.09  1.06 -0.01 -0.1   0.99  0.14 -0.14  1.06\n",
      "   0.14 -0.19  0.96  0.05  0.05  1.04 -0.02  0.01  0.89  0.01 -0.01  0.94\n",
      "  -0.13  0.02  0.92 -0.15  0.15  1.01 -0.14  0.06  0.95 -0.05  0.09  1.08\n",
      "   0.03  0.07  1.09  0.03  0.06  1.1   0.04  0.01  1.01  0.18 -0.19  1.04\n",
      "   0.11 -0.21  0.9   0.01 -0.19  0.87 -0.04 -0.03  0.86 -0.12  0.06  0.92]\n",
      " [ 0.01  0.    0.94  0.02  0.02  0.96  0.05  0.02  1.08  0.16 -0.18  1.08\n",
      "   0.13 -0.2   1.06 -0.11  0.01  0.93 -0.11 -0.03  0.91 -0.11  0.01  0.88\n",
      "   0.02  0.01  0.97  0.02  0.05  1.01  0.05  0.05  1.05 -0.04  0.08  1.07\n",
      "  -0.05  0.06  1.05 -0.14  0.06  0.92 -0.13  0.04  0.91 -0.12  0.04  0.89\n",
      "   0.16 -0.19  0.97  0.2  -0.2   1.03  0.18 -0.16  1.06 -0.02  0.02  1.02]\n",
      " [-0.14  0.15  0.87 -0.13  0.22  0.97 -0.06  0.01  0.91 -0.02  0.21  1.06\n",
      "   0.02  0.08  1.05 -0.14  0.23  0.97  0.09 -0.17  1.13 -0.08 -0.1   0.9\n",
      "   0.13 -0.25  1.11  0.02 -0.04  0.93  0.04  0.11  1.05 -0.11  0.08  0.85\n",
      "  -0.13  0.23  0.9  -0.    0.05  0.94 -0.14  0.11  0.87  0.06  0.04  1.06\n",
      "  -0.14  0.24  0.95  0.01  0.18  1.05 -0.19  0.21  0.84 -0.02  0.13  1.  ]\n",
      " [-0.1   0.11  1.05 -0.12  0.02  0.92 -0.11  0.03  0.88 -0.05  0.07  0.9\n",
      "   0.03  0.06  1.06 -0.04  0.08  1.07 -0.01 -0.17  0.98 -0.03 -0.17  0.92\n",
      "   0.04 -0.2   0.89  0.13 -0.07  1.01  0.09  0.    1.04  0.03  0.01  1.06\n",
      "  -0.15  0.06  0.98 -0.12  0.05  0.94 -0.07  0.09  0.95 -0.02  0.12  1.03\n",
      "  -0.02  0.12  1.05 -0.14  0.1   1.01 -0.17  0.06  0.96 -0.15  0.08  0.91]\n",
      " [ 0.04 -0.29  1.02 -0.06 -0.19  0.93 -0.08 -0.24  0.95 -0.04 -0.04  0.93\n",
      "  -0.13  0.05  0.91 -0.    0.02  0.99 -0.11  0.06  0.94 -0.1   0.2   0.98\n",
      "  -0.06 -0.04  0.91 -0.15  0.22  0.92 -0.04  0.12  1.03 -0.11  0.11  0.94\n",
      "  -0.09  0.16  0.98 -0.05 -0.16  0.9  -0.1  -0.12  0.94  0.   -0.1   1.04\n",
      "  -0.1  -0.05  0.9  -0.01 -0.07  1.03  0.04 -0.13  1.03 -0.03  0.14  1.05]\n",
      " [-0.06  0.11  1.   -0.12  0.09  0.91 -0.    0.08  1.04 -0.03 -0.03  0.96\n",
      "   0.02  0.02  1.05 -0.08  0.01  0.95 -0.08  0.13  1.01 -0.12  0.13  0.91\n",
      "  -0.01  0.1   1.04 -0.07 -0.11  0.97  0.04 -0.21  1.04 -0.06 -0.22  0.92\n",
      "   0.03 -0.21  0.98 -0.11 -0.06  0.91  0.05 -0.11  1.05 -0.06  0.12  1.02\n",
      "   0.02 -0.01  1.01 -0.11  0.02  1.06 -0.05  0.01  0.92 -0.12  0.11  1.  ]\n",
      " [ 0.1  -0.2   0.9   0.01 -0.2   0.92 -0.06 -0.07  0.88 -0.04  0.04  1.02\n",
      "   0.07  0.01  1.07  0.    0.11  1.08  0.04  0.03  1.   -0.08 -0.01  0.87\n",
      "  -0.04 -0.06  0.91 -0.15  0.01  0.92 -0.16  0.04  0.89 -0.12  0.13  1.01\n",
      "   0.03  0.08  1.1   0.07 -0.02  1.08  0.12 -0.09  1.01  0.02 -0.16  0.88\n",
      "   0.07 -0.19  0.86 -0.05 -0.09  0.91 -0.14  0.06  0.9  -0.12  0.13  1.03]\n",
      " [-0.08  0.05  1.1  -0.01  0.    0.95 -0.11  0.14  0.91 -0.04 -0.04  0.91\n",
      "  -0.12  0.11  0.92 -0.07  0.17  1.03 -0.13  0.18  0.94 -0.04  0.17  1.07\n",
      "   0.02  0.04  1.05  0.01 -0.21  1.05  0.05 -0.31  1.01  0.04 -0.09  1.07\n",
      "   0.05 -0.15  0.97 -0.07 -0.07  0.9   0.01 -0.03  0.94 -0.09  0.1   0.91\n",
      "  -0.09  0.2   1.   -0.1   0.12  0.93 -0.1   0.21  1.01 -0.13  0.16  0.95]\n",
      " [-0.13  0.05  0.9   0.02 -0.04  1.05 -0.   -0.24  1.04  0.01 -0.3   0.97\n",
      "  -0.09 -0.2   0.94 -0.08  0.07  0.93  0.    0.08  1.05  0.02 -0.04  0.94\n",
      "   0.04  0.01  1.05 -0.12  0.08  0.94 -0.02 -0.05  0.93 -0.14  0.14  0.96\n",
      "   0.02  0.01  1.04 -0.01  0.12  1.05 -0.02  0.    0.94 -0.13  0.14  0.93\n",
      "  -0.05 -0.27  0.92  0.   -0.25  1.04 -0.1   0.04  0.93  0.07 -0.08  1.05]\n",
      " [ 0.01 -0.2   0.96  0.02 -0.12  1.05  0.04 -0.2   1.   -0.08 -0.08  0.93\n",
      "   0.05  0.04  1.05 -0.04 -0.    0.92 -0.12  0.12  0.9   0.   -0.05  0.98\n",
      "  -0.13  0.09  0.9   0.01  0.02  1.01 -0.06  0.05  0.93 -0.12  0.13  0.93\n",
      "  -0.02 -0.09  0.91 -0.11 -0.03  0.89 -0.01 -0.02  1.03 -0.09 -0.19  0.93\n",
      "  -0.05 -0.18  1.    0.05 -0.23  0.92 -0.13  0.12  0.9  -0.02  0.1   1.04]\n",
      " [-0.01  0.06  0.96 -0.01  0.09  0.99 -0.01  0.12  1.04 -0.07  0.07  1.03\n",
      "  -0.13  0.08  1.   -0.15  0.06  0.91 -0.04 -0.06  0.9   0.   -0.07  0.91\n",
      "   0.15 -0.15  1.03  0.17 -0.16  1.06  0.11 -0.12  1.05 -0.11  0.07  1.\n",
      "  -0.13  0.05  0.96 -0.12  0.05  0.94 -0.03  0.04  0.93  0.01  0.06  0.97\n",
      "  -0.    0.08  1.04 -0.03  0.07  1.06 -0.09  0.08  1.02 -0.13  0.04  0.95]\n",
      " [-0.04  0.09  1.07 -0.04 -0.15  0.96 -0.01 -0.17  0.92  0.08 -0.09  0.93\n",
      "   0.1  -0.06  0.99  0.11 -0.06  1.05 -0.1   0.1   1.04 -0.13  0.08  1.01\n",
      "  -0.14  0.05  0.96 -0.05  0.04  0.9  -0.03  0.1   0.95  0.02  0.05  1.07\n",
      "  -0.05  0.07  1.05 -0.09  0.08  1.03 -0.06 -0.1   0.93 -0.02 -0.11  0.89\n",
      "   0.06 -0.15  0.91  0.15 -0.13  1.04  0.11 -0.08  1.07 -0.16  0.09  1.  ]\n",
      " [-0.04 -0.02  0.92 -0.11  0.04  0.92  0.05 -0.36  0.96 -0.04 -0.29  0.89\n",
      "  -0.08 -0.18  0.93  0.03 -0.06  0.93 -0.05 -0.    0.88 -0.1  -0.02  1.03\n",
      "  -0.01 -0.02  0.94 -0.08  0.07  0.89  0.02  0.01  0.99 -0.03 -0.05  0.92\n",
      "  -0.11  0.01  0.88  0.01  0.03  1.01 -0.04  0.06  0.93 -0.11  0.09  0.91\n",
      "   0.05 -0.34  1.02 -0.01 -0.31  0.93  0.05 -0.05  1.09  0.06 -0.14  1.01]\n",
      " [-0.01  0.11  1.07 -0.12  0.11  0.98 -0.04  0.06  1.05 -0.08 -0.18  0.92\n",
      "  -0.09 -0.15  0.97 -0.   -0.21  1.07 -0.08  0.08  0.92 -0.1   0.13  0.94\n",
      "  -0.02  0.13  1.03 -0.1   0.12  0.93 -0.13  0.2   0.95 -0.05 -0.03  0.9\n",
      "  -0.11  0.05  0.91 -0.13  0.16  0.93 -0.05  0.05  0.92 -0.11  0.1   0.92\n",
      "  -0.13  0.16  0.95 -0.02 -0.3   0.91 -0.07 -0.23  0.92 -0.1  -0.1   0.98]\n",
      " [-0.08  0.16  1.02  0.01  0.04  1.07  0.04 -0.01  1.01 -0.05  0.17  1.05\n",
      "   0.    0.06  1.07 -0.09  0.19  1.02 -0.01  0.06  1.07  0.01  0.07  1.01\n",
      "  -0.02 -0.16  1.05  0.05 -0.25  1.06  0.02 -0.29  0.95  0.03 -0.12  1.08\n",
      "   0.07 -0.21  1.03  0.01  0.08  1.04  0.04  0.02  1.09  0.02 -0.03  0.94\n",
      "  -0.06  0.12  1.05  0.02 -0.04  1.01 -0.04 -0.03  0.93  0.    0.08  1.07]\n",
      " [-0.04  0.06  1.04 -0.14  0.06  0.96 -0.11  0.04  0.91 -0.08  0.03  0.92\n",
      "  -0.02  0.12  0.96 -0.01  0.13  1.02 -0.09  0.09  1.04 -0.14  0.11  1.03\n",
      "  -0.17  0.1   0.99  0.03 -0.2   0.9   0.07 -0.18  0.9   0.14 -0.2   0.95\n",
      "   0.08  0.01  1.05  0.    0.03  1.04 -0.14  0.07  0.98 -0.13  0.04  0.95\n",
      "  -0.09  0.03  0.91 -0.03  0.13  0.99 -0.01  0.13  1.03 -0.03  0.12  1.05]\n",
      " [-0.17  0.18  0.87 -0.04  0.11  0.94  0.01  0.13  0.89 -0.05  0.02  0.93\n",
      "   0.07  0.01  1.15  0.   -0.16  0.92  0.15 -0.21  1.15 -0.01 -0.06  1.04\n",
      "   0.09  0.04  1.07 -0.05  0.16  1.03 -0.12  0.15  0.85 -0.03  0.15  1.05\n",
      "  -0.14  0.17  0.86 -0.05  0.15  1.07 -0.16  0.18  0.87 -0.06  0.06  0.93\n",
      "  -0.19  0.19  0.87 -0.05  0.09  0.9   0.08  0.03  1.12 -0.03 -0.12  0.89]\n",
      " [ 0.01  0.08  1.04  0.04  0.02  1.09  0.02 -0.03  0.94 -0.06  0.12  1.05\n",
      "   0.02 -0.04  1.01 -0.04 -0.03  0.93  0.    0.08  1.07 -0.01  0.04  0.94\n",
      "  -0.09  0.06  0.88  0.03 -0.15  1.   -0.02 -0.15  0.93 -0.1  -0.1   0.91\n",
      "   0.04 -0.31  0.96 -0.06 -0.22  0.88  0.05 -0.03  1.01 -0.   -0.03  0.91\n",
      "  -0.08  0.07  0.91  0.01 -0.04  0.97 -0.06  0.02  0.89 -0.12  0.16  0.94]\n",
      " [-0.05  0.06  0.85  0.08  0.11  1.11  0.04 -0.03  1.03 -0.1   0.21  0.98\n",
      "  -0.05  0.03  0.92 -0.16  0.16  0.84  0.02  0.12  1.06 -0.02 -0.02  0.97\n",
      "  -0.02  0.07  1.09 -0.12  0.03  0.9  -0.1  -0.03  0.86  0.12 -0.25  1.04\n",
      "  -0.02  0.    1.01  0.05  0.12  1.09 -0.13  0.16  0.83  0.1   0.06  0.94\n",
      "   0.    0.03  0.93 -0.02  0.17  1.06  0.05  0.13  1.09 -0.15  0.19  0.87]\n",
      " [-0.09  0.08  1.05 -0.01  0.09  1.09  0.04  0.08  1.09  0.01 -0.    0.94\n",
      "   0.16 -0.19  0.93  0.02 -0.23  0.86  0.   -0.12  1.   -0.11  0.08  0.97\n",
      "   0.06  0.07  1.07  0.05  0.02  0.96  0.05  0.04  1.03 -0.02 -0.02  0.9\n",
      "  -0.01 -0.04  0.95 -0.11 -0.03  0.9  -0.15  0.07  1.02 -0.18  0.1   0.95\n",
      "  -0.01  0.11  1.08  0.02  0.04  1.05  0.15 -0.11  1.06  0.09 -0.19  0.92]\n",
      " [-0.09  0.05  0.92 -0.   -0.02  0.97 -0.13  0.14  0.95  0.03  0.02  1.05\n",
      "  -0.1   0.15  1.02  0.01 -0.06  1.02 -0.14  0.08  0.9  -0.06  0.03  0.91\n",
      "  -0.1   0.18  0.97  0.03  0.    1.02  0.01 -0.17  1.04 -0.01 -0.32  0.93\n",
      "   0.08 -0.14  1.06 -0.08 -0.06  0.93  0.   -0.02  1.04 -0.13  0.1   0.91\n",
      "   0.03  0.05  1.04 -0.07  0.04  0.93  0.03 -0.03  1.03 -0.14  0.11  0.9 ]\n",
      " [-0.01 -0.11  1.05  0.03 -0.24  0.99  0.06 -0.19  1.1   0.03 -0.25  0.95\n",
      "  -0.09 -0.06  0.94 -0.02  0.02  0.89 -0.11  0.14  0.92  0.01  0.09  0.9\n",
      "  -0.1   0.22  1.02  0.02  0.04  1.07 -0.04  0.14  1.04  0.    0.07  0.99\n",
      "  -0.09  0.08  0.88 -0.   -0.13  0.92 -0.11  0.    0.93 -0.07 -0.01  1.03\n",
      "  -0.09 -0.11  0.96  0.01 -0.2   1.08 -0.09  0.17  0.96  0.04  0.05  1.06]\n",
      " [ 0.13 -0.14  1.08  0.17 -0.24  0.95  0.01 -0.19  0.86 -0.06 -0.    0.85\n",
      "  -0.13  0.08  0.95 -0.16  0.09  0.94 -0.07  0.13  1.09  0.06  0.04  1.04\n",
      "  -0.    0.09  1.09 -0.   -0.02  0.93 -0.12 -0.02  0.88 -0.1   0.05  0.87\n",
      "  -0.17  0.09  0.94 -0.06 -0.19  0.92  0.01 -0.11  1.05  0.15 -0.16  1.09\n",
      "   0.07 -0.01  1.07  0.1  -0.05  0.99  0.   -0.06  0.86  0.   -0.    0.92]\n",
      " [-0.12  0.03  0.94 -0.09  0.02  0.91 -0.03  0.02  0.91  0.02  0.13  1.05\n",
      "  -0.06  0.14  1.08 -0.08  0.11  0.96 -0.15  0.03  0.92 -0.13  0.04  0.88\n",
      "   0.04  0.03  1.02  0.06  0.03  1.08  0.01  0.05  1.09  0.   -0.2   0.98\n",
      "  -0.01 -0.19  0.93  0.05 -0.23  0.88  0.06  0.04  1.    0.04  0.08  1.03\n",
      "  -0.13  0.11  1.04 -0.13  0.06  1.   -0.12  0.02  0.94 -0.06  0.12  0.91]\n",
      " [-0.03  0.2   1.05 -0.13  0.09  0.86 -0.16  0.25  0.89 -0.07  0.14  0.88\n",
      "  -0.01  0.2   0.9   0.09 -0.16  1.1  -0.08  0.01  1.02  0.08 -0.09  1.1\n",
      "  -0.09 -0.03  0.85  0.13 -0.18  1.06 -0.01  0.04  0.91  0.05  0.14  1.07\n",
      "  -0.13  0.17  0.86 -0.13  0.23  0.98 -0.03  0.02  0.9  -0.18  0.19  0.84\n",
      "   0.02  0.11  1.01 -0.1   0.25  0.97  0.04  0.03  1.11 -0.16  0.1   0.9 ]\n",
      " [ 0.08 -0.17  1.06  0.   -0.15  1.02 -0.06 -0.04  0.86  0.   -0.02  0.9\n",
      "   0.03  0.1   1.06 -0.02  0.12  1.07 -0.08  0.1   1.05 -0.13  0.02  0.93\n",
      "  -0.11  0.02  0.88 -0.08  0.04  0.89  0.04  0.05  1.05  0.01  0.06  1.07\n",
      "  -0.04  0.09  1.07 -0.04 -0.15  0.96 -0.01 -0.17  0.92  0.08 -0.09  0.93\n",
      "   0.1  -0.06  0.99  0.11 -0.06  1.05 -0.1   0.1   1.04 -0.13  0.08  1.01]\n",
      " [-0.02  0.03  1.03 -0.08  0.07  0.94 -0.09  0.15  0.99  0.01  0.04  1.05\n",
      "  -0.14  0.16  0.95 -0.04  0.1   1.05  0.   -0.01  1.03 -0.04  0.13  1.02\n",
      "   0.03  0.06  1.06 -0.05  0.08  0.96  0.02 -0.15  1.02  0.01 -0.16  0.98\n",
      "   0.   -0.16  1.03  0.04 -0.14  1.05 -0.03 -0.2   0.92  0.03  0.09  1.06\n",
      "  -0.03 -0.02  0.92 -0.15  0.1   0.9  -0.02  0.03  0.96 -0.16  0.04  0.9 ]\n",
      " [-0.1  -0.    0.91 -0.03 -0.06  0.88 -0.09  0.13  0.96 -0.1   0.12  0.83\n",
      "   0.06  0.07  1.01 -0.13  0.14  0.86  0.02  0.03  0.98  0.02  0.1   1.1\n",
      "  -0.07  0.15  0.9   0.02  0.13  1.11 -0.07  0.05  0.88  0.07 -0.03  1.09\n",
      "  -0.03 -0.01  1.08  0.12 -0.2   1.06  0.07 -0.11  1.1  -0.09 -0.06  0.9\n",
      "   0.06  0.06  1.1  -0.11  0.13  0.94 -0.09  0.09  0.85 -0.08  0.08  0.86]\n",
      " [ 0.08 -0.31  1.05 -0.04  0.14  1.02  0.03  0.06  1.07 -0.12  0.21  0.95\n",
      "  -0.07  0.18  1.04 -0.    0.08  1.07 -0.12  0.16  0.91 -0.08  0.16  1.\n",
      "  -0.02  0.12  1.03 -0.14  0.15  0.93 -0.09  0.17  1.02 -0.08 -0.24  0.88\n",
      "  -0.1  -0.16  0.93 -0.08 -0.18  1.   -0.03 -0.08  0.9  -0.08  0.04  0.92\n",
      "  -0.1   0.11  0.94 -0.02 -0.    0.93 -0.09  0.07  0.89 -0.11  0.18  1.  ]\n",
      " [-0.06  0.07  0.91  0.03 -0.17  1.03  0.01 -0.2   0.96  0.02 -0.12  1.05\n",
      "   0.04 -0.2   1.   -0.08 -0.08  0.93  0.05  0.04  1.05 -0.04 -0.    0.92\n",
      "  -0.12  0.12  0.9   0.   -0.05  0.98 -0.13  0.09  0.9   0.01  0.02  1.01\n",
      "  -0.06  0.05  0.93 -0.12  0.13  0.93 -0.02 -0.09  0.91 -0.11 -0.03  0.89\n",
      "  -0.01 -0.02  1.03 -0.09 -0.19  0.93 -0.05 -0.18  1.    0.05 -0.23  0.92]\n",
      " [-0.11  0.04  0.92 -0.07  0.03  0.89 -0.01  0.12  1.   -0.02  0.14  1.04\n",
      "  -0.02  0.12  1.05 -0.13  0.08  1.02 -0.16  0.07  0.97 -0.03 -0.07  0.91\n",
      "   0.03 -0.1   0.92  0.08 -0.1   0.95  0.14 -0.15  1.05  0.06 -0.09  1.04\n",
      "   0.01 -0.09  1.02 -0.12  0.04  0.92 -0.07  0.03  0.89 -0.02  0.11  1.01\n",
      "  -0.01  0.12  1.06 -0.05  0.14  1.07 -0.15  0.04  0.99 -0.17  0.05  0.94]\n",
      " [-0.02 -0.04  0.98 -0.15  0.14  0.92 -0.07  0.01  0.92 -0.1   0.15  0.98\n",
      "   0.   -0.02  0.91 -0.03  0.03  1.04  0.01 -0.12  0.96  0.04 -0.27  1.06\n",
      "  -0.02 -0.29  0.91 -0.06 -0.16  0.99 -0.07  0.06  0.92 -0.01  0.1   1.03\n",
      "   0.04 -0.03  0.96  0.02  0.03  1.04 -0.03  0.    0.92  0.01  0.    1.04\n",
      "  -0.11  0.02  0.94 -0.05  0.12  1.03 -0.12  0.11  0.9   0.01  0.05  1.04]\n",
      " [-0.03  0.12  1.03  0.03 -0.02  1.02 -0.02  0.06  1.06 -0.02 -0.04  0.98\n",
      "  -0.15  0.14  0.92 -0.07  0.01  0.92 -0.1   0.15  0.98  0.   -0.02  0.91\n",
      "  -0.03  0.03  1.04  0.01 -0.12  0.96  0.04 -0.27  1.06 -0.02 -0.29  0.91\n",
      "  -0.06 -0.16  0.99 -0.07  0.06  0.92 -0.01  0.1   1.03  0.04 -0.03  0.96\n",
      "   0.02  0.03  1.04 -0.03  0.    0.92  0.01  0.    1.04 -0.11  0.02  0.94]\n",
      " [-0.12  0.21  0.96 -0.01  0.07  0.93 -0.12  0.09  0.85  0.01  0.14  1.1\n",
      "   0.02  0.07  1.03 -0.14  0.21  0.97 -0.06  0.09  0.92 -0.08 -0.1   0.88\n",
      "   0.14 -0.19  1.16 -0.1  -0.05  0.9  -0.06  0.13  0.99  0.   -0.05  0.91\n",
      "  -0.12  0.13  0.84  0.09  0.06  1.09 -0.15  0.19  0.86 -0.09  0.2   1.03\n",
      "  -0.07  0.05  0.88 -0.    0.15  1.09  0.02  0.11  1.06 -0.16  0.2   0.87]\n",
      " [-0.07  0.08  0.93  0.03 -0.2   1.07  0.04 -0.27  0.99 -0.02 -0.26  0.93\n",
      "   0.04 -0.15  1.08  0.07 -0.2   1.02 -0.05  0.15  1.02  0.02  0.07  1.05\n",
      "   0.06  0.01  1.04 -0.1   0.23  1.   -0.03  0.14  1.04  0.02  0.06  1.08\n",
      "  -0.12  0.17  0.95 -0.04  0.17  1.04 -0.11 -0.    0.93 -0.13  0.04  0.96\n",
      "  -0.06  0.04  1.05 -0.08 -0.21  0.91 -0.1  -0.12  0.94 -0.06 -0.09  1.03]\n",
      " [-0.07  0.05  0.88 -0.13  0.19  0.94 -0.03 -0.04  0.92 -0.11  0.03  0.91\n",
      "  -0.12  0.15  0.94 -0.07  0.04  0.91 -0.13  0.11  0.93  0.03 -0.34  0.94\n",
      "  -0.08 -0.23  0.9  -0.1  -0.15  0.94 -0.   -0.07  0.91 -0.09  0.06  0.92\n",
      "  -0.1   0.15  0.95 -0.05  0.03  0.9  -0.11  0.15  0.93 -0.1   0.22  0.93\n",
      "  -0.09 -0.03  0.86 -0.13  0.13  0.92 -0.04  0.04  0.93 -0.12  0.11  0.91]\n",
      " [-0.14  0.12  0.91  0.   -0.02  0.99 -0.1   0.05  0.95 -0.08  0.11  0.98\n",
      "  -0.07  0.06  0.92 -0.11  0.14  0.94 -0.03 -0.28  0.91 -0.09 -0.21  0.89\n",
      "  -0.   -0.21  1.04 -0.1   0.07  0.93 -0.06  0.12  1.01  0.05  0.03  1.06\n",
      "  -0.12  0.15  0.97  0.    0.08  1.04 -0.13  0.14  0.92 -0.05  0.12  1.04\n",
      "   0.02 -0.01  1.02 -0.05  0.15  1.03  0.03  0.12  1.06 -0.05  0.05  0.92]\n",
      " [-0.04  0.01  0.93 -0.12  0.08  0.9  -0.01 -0.03  0.96 -0.12  0.15  0.92\n",
      "  -0.08  0.15  1.   -0.05 -0.26  0.91 -0.08 -0.23  0.95  0.04 -0.27  1.04\n",
      "  -0.11  0.05  0.91 -0.01  0.04  1.03  0.06 -0.05  0.93 -0.09  0.16  0.99\n",
      "   0.02  0.03  1.06 -0.16  0.15  0.96 -0.02  0.07  1.05  0.03 -0.01  1.01\n",
      "  -0.04  0.13  1.03  0.02  0.08  1.05 -0.06  0.07  0.91  0.03 -0.17  1.03]\n",
      " [-0.07  0.14  1.    0.03  0.04  1.06 -0.09  0.17  0.97  0.02  0.01  1.06\n",
      "  -0.09 -0.2   0.93  0.02 -0.28  1.04  0.04 -0.27  1.01 -0.02  0.04  1.03\n",
      "   0.05 -0.    1.05 -0.04 -0.01  0.92 -0.    0.09  1.04 -0.   -0.04  0.99\n",
      "  -0.1   0.09  1.03  0.04 -0.04  1.04 -0.06 -0.01  0.94  0.02  0.09  1.06\n",
      "  -0.02  0.03  0.95 -0.11  0.11  0.9   0.03 -0.29  1.   -0.06 -0.23  0.93]\n",
      " [-0.17  0.19  0.87 -0.02  0.12  0.97 -0.12  0.08  0.88  0.03  0.1   1.12\n",
      "   0.13 -0.23  1.09 -0.05 -0.05  1.    0.04 -0.21  0.96 -0.11  0.09  0.83\n",
      "   0.09  0.07  1.1  -0.15  0.21  0.87 -0.07  0.19  1.02 -0.09  0.08  0.86\n",
      "  -0.17  0.18  0.87  0.02  0.06  1.02 -0.13  0.22  0.97  0.02  0.13  1.07\n",
      "  -0.18  0.16  0.87  0.09  0.03  1.12  0.07 -0.23  1.01  0.04 -0.11  1.09]\n",
      " [-0.02 -0.31  0.92  0.02  0.08  1.06  0.01 -0.02  1.   -0.07  0.06  0.94\n",
      "   0.04  0.05  1.04 -0.01  0.01  0.96 -0.14  0.15  0.92  0.01  0.03  1.02\n",
      "  -0.06  0.02  0.93  0.03  0.09  1.07 -0.02  0.04  0.95 -0.11  0.15  0.9\n",
      "   0.04 -0.29  1.02 -0.06 -0.19  0.93 -0.08 -0.24  0.95 -0.04 -0.04  0.93\n",
      "  -0.13  0.05  0.91 -0.    0.02  0.99 -0.11  0.06  0.94 -0.1   0.2   0.98]\n",
      " [ 0.02 -0.04  1.01 -0.04 -0.03  0.93  0.    0.08  1.07 -0.01  0.04  0.94\n",
      "  -0.09  0.06  0.88  0.03 -0.15  1.   -0.02 -0.15  0.93 -0.1  -0.1   0.91\n",
      "   0.04 -0.31  0.96 -0.06 -0.22  0.88  0.05 -0.03  1.01 -0.   -0.03  0.91\n",
      "  -0.08  0.07  0.91  0.01 -0.04  0.97 -0.06  0.02  0.89 -0.12  0.16  0.94\n",
      "  -0.02  0.    0.91 -0.1   0.01  0.89  0.02 -0.04  0.99 -0.04 -0.02  0.92]\n",
      " [-0.    0.03  1.06  0.06 -0.05  1.08 -0.08  0.16  1.02  0.01  0.04  1.07\n",
      "   0.04 -0.01  1.01 -0.05  0.17  1.05  0.    0.06  1.07 -0.09  0.19  1.02\n",
      "  -0.01  0.06  1.07  0.01  0.07  1.01 -0.02 -0.16  1.05  0.05 -0.25  1.06\n",
      "   0.02 -0.29  0.95  0.03 -0.12  1.08  0.07 -0.21  1.03  0.01  0.08  1.04\n",
      "   0.04  0.02  1.09  0.02 -0.03  0.94 -0.06  0.12  1.05  0.02 -0.04  1.01]\n",
      " [ 0.05 -0.14  0.92  0.13 -0.14  0.97  0.16 -0.15  1.03 -0.02 -0.03  1.03\n",
      "  -0.06 -0.04  0.99 -0.08 -0.06  0.89  0.02  0.05  0.93  0.06  0.06  1.\n",
      "   0.03  0.09  1.06 -0.09  0.1   1.04 -0.14  0.07  1.   -0.1   0.08  0.87\n",
      "  -0.06  0.1   0.91  0.01  0.07  1.    0.01 -0.01  1.07 -0.01 -0.04  1.04\n",
      "  -0.05 -0.1   0.99  0.06 -0.16  0.88  0.1  -0.14  0.91  0.02  0.1   1.06]\n",
      " [ 0.06  0.01  1.05  0.04 -0.04  0.93 -0.01  0.08  1.04  0.05 -0.01  1.03\n",
      "  -0.08  0.18  1.01 -0.02  0.12  1.03  0.02  0.02  1.   -0.06  0.19  1.06\n",
      "   0.    0.08  1.09  0.01  0.02  1.   -0.01 -0.24  1.06  0.05 -0.29  1.09\n",
      "   0.05 -0.38  0.99  0.    0.06  1.05  0.06 -0.02  1.09  0.06  0.18  0.98\n",
      "  -0.01  0.09  1.03  0.04  0.04  1.08 -0.12  0.25  0.97 -0.03  0.18  1.02]\n",
      " [-0.11  0.05  0.93 -0.12  0.12  0.99 -0.06 -0.26  0.9  -0.1  -0.16  0.94\n",
      "  -0.03 -0.19  1.06 -0.08  0.07  0.92 -0.09  0.16  0.98 -0.    0.06  0.88\n",
      "  -0.13  0.19  0.94 -0.05  0.18  1.04 -0.13  0.07  0.94 -0.1   0.19  0.97\n",
      "  -0.04  0.13  1.03 -0.14  0.18  0.94 -0.05  0.14  1.06  0.02  0.04  1.09\n",
      "  -0.06 -0.2   1.02  0.03 -0.31  1.05 -0.1   0.08  0.96 -0.    0.03  1.06]\n",
      " [-0.1  -0.05  0.9  -0.06  0.13  0.99  0.   -0.05  0.91 -0.12  0.13  0.84\n",
      "   0.09  0.06  1.09 -0.15  0.19  0.86 -0.09  0.2   1.03 -0.07  0.05  0.88\n",
      "  -0.    0.15  1.09  0.02  0.11  1.06 -0.16  0.2   0.87 -0.04  0.12  1.1\n",
      "  -0.07 -0.1   0.91  0.09 -0.11  1.16  0.12 -0.18  1.06 -0.07  0.09  0.94\n",
      "   0.04 -0.14  0.95 -0.09  0.11  0.83  0.08  0.07  1.12 -0.14  0.19  0.85]\n",
      " [-0.11  0.11  0.91  0.04  0.02  1.05 -0.08  0.13  1.03  0.02 -0.04  1.02\n",
      "  -0.15  0.11  0.9  -0.07  0.03  0.92 -0.09  0.12  1.01  0.01  0.02  1.\n",
      "   0.01 -0.01  1.03 -0.06 -0.05  0.91  0.05 -0.3   1.02 -0.1  -0.18  0.9\n",
      "   0.05 -0.27  1.06 -0.04  0.13  1.01  0.04 -0.03  0.99 -0.11  0.12  0.89\n",
      "  -0.08  0.05  0.93 -0.06  0.11  1.02  0.02  0.08  0.89 -0.01  0.04  1.06]\n",
      " [ 0.02  0.06  0.99 -0.03 -0.18  1.02  0.05 -0.23  1.06 -0.09  0.06  0.96\n",
      "   0.03 -0.01  1.07  0.01 -0.05  0.98 -0.02  0.11  1.04  0.02  0.03  1.02\n",
      "  -0.09  0.09  0.93  0.02  0.04  1.05 -0.06 -0.    0.92  0.01  0.11  1.07\n",
      "  -0.01  0.06  0.96 -0.11  0.13  0.91  0.02 -0.19  0.99 -0.07 -0.17  0.9\n",
      "  -0.07 -0.11  0.97 -0.05 -0.13  0.92 -0.1  -0.04  0.93  0.02 -0.08  0.94]\n",
      " [ 0.03 -0.34  0.96  0.05  0.02  1.09  0.03 -0.07  0.94 -0.08  0.05  1.1\n",
      "  -0.01  0.    0.95 -0.11  0.14  0.91 -0.04 -0.04  0.91 -0.12  0.11  0.92\n",
      "  -0.07  0.17  1.03 -0.13  0.18  0.94 -0.04  0.17  1.07  0.02  0.04  1.05\n",
      "   0.01 -0.21  1.05  0.05 -0.31  1.01  0.04 -0.09  1.07  0.05 -0.15  0.97\n",
      "  -0.07 -0.07  0.9   0.01 -0.03  0.94 -0.09  0.1   0.91 -0.09  0.2   1.  ]\n",
      " [ 0.02  0.07  1.09  0.    0.07  1.08  0.04 -0.24  0.95  0.   -0.21  0.92\n",
      "   0.01 -0.23  0.89 -0.   -0.01  0.92  0.03  0.02  0.96  0.02  0.09  1.09\n",
      "  -0.03  0.11  1.09 -0.05  0.07  1.09 -0.15  0.04  0.94 -0.14  0.01  0.91\n",
      "  -0.14  0.01  0.89  0.01  0.    0.94  0.02  0.02  0.96  0.05  0.02  1.08\n",
      "   0.16 -0.18  1.08  0.13 -0.2   1.06 -0.11  0.01  0.93 -0.11 -0.03  0.91]\n",
      " [ 0.01  0.12  1.09  0.06 -0.23  1.02  0.04 -0.1   1.12 -0.05 -0.03  1.07\n",
      "  -0.1   0.12  0.92  0.01 -0.06  0.89 -0.14  0.17  0.85  0.07  0.04  1.03\n",
      "  -0.08  0.24  0.97  0.    0.17  1.08 -0.16  0.16  0.85  0.04  0.01  1.03\n",
      "  -0.11  0.11  0.86 -0.    0.18  1.06  0.06 -0.11  1.07 -0.05  0.04  1.06\n",
      "  -0.04 -0.13  0.95 -0.09 -0.04  0.86  0.12 -0.2   1.02 -0.01  0.05  1.01]\n",
      " [-0.17  0.07  0.94  0.11 -0.2   0.92  0.14 -0.17  0.97  0.04  0.02  1.06\n",
      "  -0.    0.03  1.05 -0.06  0.05  1.03 -0.13  0.05  0.93 -0.09  0.03  0.9\n",
      "  -0.05  0.03  0.9  -0.02  0.13  1.02  0.01  0.12  1.04 -0.03  0.1   1.03\n",
      "  -0.16  0.08  0.99 -0.18  0.07  0.95  0.05 -0.14  0.92  0.13 -0.14  0.97\n",
      "   0.16 -0.15  1.03 -0.02 -0.03  1.03 -0.06 -0.04  0.99 -0.08 -0.06  0.89]\n",
      " [-0.01  0.    0.9  -0.08  0.09  0.92 -0.1   0.15  0.93 -0.03 -0.04  0.93\n",
      "  -0.1   0.07  0.89 -0.13  0.16  0.92 -0.03  0.08  0.92 -0.08  0.04  0.88\n",
      "  -0.12  0.13  0.96  0.02 -0.18  0.97 -0.04 -0.15  0.92  0.06 -0.25  1.1\n",
      "   0.06 -0.31  1.01  0.01 -0.27  0.94  0.03  0.06  1.07  0.06 -0.01  1.03\n",
      "   0.03 -0.03  0.94 -0.02  0.11  1.03  0.03  0.03  1.07 -0.11  0.19  0.96]\n",
      " [-0.14  0.22  0.98 -0.1   0.13  0.89 -0.09 -0.03  0.9   0.1  -0.26  1.07\n",
      "  -0.02 -0.05  1.06  0.09 -0.01  1.11 -0.12  0.12  0.87 -0.02  0.12  1.04\n",
      "  -0.08  0.11  0.84  0.09  0.    1.12  0.   -0.03  0.99 -0.03  0.2   1.05\n",
      "  -0.13  0.09  0.86 -0.16  0.25  0.89 -0.07  0.14  0.88 -0.01  0.2   0.9\n",
      "   0.09 -0.16  1.1  -0.08  0.01  1.02  0.08 -0.09  1.1  -0.09 -0.03  0.85]\n",
      " [-0.09  0.16  0.98  0.01  0.08  1.06 -0.01 -0.02  0.96 -0.02  0.07  1.04\n",
      "   0.02 -0.06  0.99 -0.08 -0.    0.92  0.06 -0.31  1.06 -0.05 -0.28  0.91\n",
      "   0.04  0.02  1.05  0.01 -0.02  0.96 -0.1   0.11  0.91  0.02 -0.04  1.02\n",
      "  -0.1   0.08  0.95 -0.12  0.13  0.97 -0.04  0.01  0.93 -0.12  0.08  0.9\n",
      "  -0.01 -0.03  0.96 -0.12  0.15  0.92 -0.08  0.15  1.   -0.05 -0.26  0.91]\n",
      " [-0.07 -0.28  0.88 -0.08 -0.18  0.98 -0.06 -0.06  0.89 -0.1   0.06  0.92\n",
      "  -0.02  0.08  1.05 -0.1   0.12  0.93 -0.09  0.17  0.99 -0.1   0.05  0.88\n",
      "  -0.13  0.16  0.94 -0.06  0.18  1.05 -0.13  0.14  0.95 -0.11  0.19  1.\n",
      "  -0.01  0.08  1.06 -0.1  -0.12  0.95 -0.02 -0.17  1.05 -0.08 -0.06  0.93\n",
      "  -0.07 -0.02  1.    0.02 -0.1   1.06 -0.1   0.13  0.92 -0.05  0.12  1.03]\n",
      " [-0.12  0.09  0.93 -0.06  0.15  1.01  0.02  0.01  1.05 -0.13  0.14  0.95\n",
      "  -0.03  0.07  1.07  0.01  0.    0.99 -0.02  0.11  1.04  0.04  0.01  1.02\n",
      "  -0.05 -0.25  1.    0.04 -0.24  1.04  0.   -0.36  0.93 -0.02  0.07  1.04\n",
      "   0.05 -0.03  1.01 -0.06 -0.    0.93  0.02  0.05  1.05 -0.03 -0.01  0.97\n",
      "  -0.06  0.1   1.04  0.04 -0.03  1.03 -0.07  0.    0.94  0.03  0.07  1.07]\n",
      " [ 0.09 -0.06  1.06  0.13 -0.09  1.01  0.05 -0.13  0.87  0.02  0.02  0.92\n",
      "  -0.11  0.04  0.88 -0.04 -0.04  0.9  -0.13  0.04  0.95 -0.11  0.12  1.06\n",
      "  -0.16  0.07  0.97 -0.01  0.09  1.08  0.04  0.06  1.07  0.08 -0.01  1.09\n",
      "   0.07 -0.08  0.97  0.17 -0.16  0.99  0.07 -0.2   0.87 -0.05 -0.1   0.89\n",
      "  -0.11  0.03  0.89 -0.14  0.12  0.97 -0.01  0.12  1.07 -0.09  0.14  1.06]\n",
      " [-0.04  0.08  1.07  0.04  0.05  1.06 -0.04  0.03  0.91  0.06 -0.08  0.93\n",
      "  -0.06 -0.07  0.9  -0.07  0.    0.86 -0.02 -0.1   0.94  0.13 -0.11  1.05\n",
      "  -0.06  0.13  1.05  0.07  0.03  1.06 -0.01  0.01  0.93  0.02 -0.01  0.96\n",
      "  -0.11 -0.02  0.9  -0.17  0.13  1.   -0.14  0.04  0.96 -0.03  0.09  1.09\n",
      "  -0.05  0.1   1.06  0.06  0.05  1.06 -0.01 -0.02  0.93  0.12 -0.2   0.93]\n",
      " [-0.13  0.04  0.94 -0.1   0.03  0.9  -0.04  0.14  0.95 -0.01  0.11  1.01\n",
      "  -0.02  0.13  1.03 -0.14  0.11  1.04 -0.18  0.1   1.   -0.17 -0.19  0.88\n",
      "   0.09 -0.22  0.9   0.13 -0.17  0.93  0.05  0.04  1.05  0.01  0.04  1.06\n",
      "  -0.05  0.07  1.04 -0.14  0.07  0.97 -0.11  0.03  0.92 -0.08  0.03  0.89\n",
      "  -0.04  0.14  0.97 -0.02  0.13  1.02 -0.1   0.1   1.04 -0.15  0.1   1.01]\n",
      " [-0.08  0.1   1.05 -0.13  0.02  0.93 -0.11  0.02  0.88 -0.08  0.04  0.89\n",
      "   0.04  0.05  1.05  0.01  0.06  1.07 -0.04  0.09  1.07 -0.04 -0.15  0.96\n",
      "  -0.01 -0.17  0.92  0.08 -0.09  0.93  0.1  -0.06  0.99  0.11 -0.06  1.05\n",
      "  -0.1   0.1   1.04 -0.13  0.08  1.01 -0.14  0.05  0.96 -0.05  0.04  0.9\n",
      "  -0.03  0.1   0.95  0.02  0.05  1.07 -0.05  0.07  1.05 -0.09  0.08  1.03]\n",
      " [-0.13  0.06  0.97 -0.12  0.05  0.91 -0.07  0.02  0.9  -0.02  0.12  1.01\n",
      "  -0.01  0.12  1.05 -0.14  0.13  1.02 -0.16  0.09  0.97 -0.16  0.08  0.93\n",
      "   0.13 -0.18  0.94  0.18 -0.2   1.01  0.16 -0.17  1.05 -0.06  0.05  1.02\n",
      "  -0.08  0.03  0.99 -0.11  0.03  0.88 -0.03  0.03  0.9  -0.    0.04  0.93\n",
      "  -0.01  0.11  1.05 -0.02  0.11  1.07 -0.02  0.09  1.08 -0.17  0.1   0.97]\n",
      " [-0.11  0.04  0.91 -0.08  0.03  0.92 -0.02  0.12  0.96 -0.01  0.13  1.02\n",
      "  -0.09  0.09  1.04 -0.14  0.11  1.03 -0.17  0.1   0.99  0.03 -0.2   0.9\n",
      "   0.07 -0.18  0.9   0.14 -0.2   0.95  0.08  0.01  1.05  0.    0.03  1.04\n",
      "  -0.14  0.07  0.98 -0.13  0.04  0.95 -0.09  0.03  0.91 -0.03  0.13  0.99\n",
      "  -0.01  0.13  1.03 -0.03  0.12  1.05 -0.15  0.08  1.   -0.17  0.08  0.97]\n",
      " [-0.13  0.11  0.86  0.05  0.03  0.89  0.14 -0.25  1.08 -0.06 -0.05  1.\n",
      "   0.02  0.09  1.07 -0.09  0.09  0.84  0.07  0.07  1.12  0.07  0.03  1.02\n",
      "  -0.12  0.21  0.96 -0.01  0.07  0.93 -0.12  0.09  0.85  0.01  0.14  1.1\n",
      "   0.02  0.07  1.03 -0.14  0.21  0.97 -0.06  0.09  0.92 -0.08 -0.1   0.88\n",
      "   0.14 -0.19  1.16 -0.1  -0.05  0.9  -0.06  0.13  0.99  0.   -0.05  0.91]\n",
      " [-0.12  0.1   1.03 -0.15  0.11  0.99  0.02  0.07  1.08  0.06 -0.03  1.\n",
      "   0.18 -0.17  1.01  0.09 -0.21  0.88 -0.01 -0.14  0.9  -0.15  0.05  0.89\n",
      "  -0.12  0.13  1.02 -0.18  0.14  0.98 -0.02  0.11  1.08  0.04  0.01  1.\n",
      "   0.03  0.04  1.05 -0.04 -0.04  0.91 -0.13  0.01  0.93 -0.15  0.07  0.91\n",
      "  -0.12  0.15  1.04  0.05  0.05  1.08  0.15 -0.13  1.07  0.13 -0.19  0.94]\n",
      " [-0.05 -0.1   0.99  0.06 -0.16  0.88  0.1  -0.14  0.91  0.02  0.1   1.06\n",
      "  -0.05  0.1   1.05 -0.11  0.1   1.03 -0.1   0.02  0.9  -0.04  0.01  0.92\n",
      "  -0.03  0.07  0.96  0.01  0.07  1.06 -0.06  0.07  1.04 -0.13  0.07  1.01\n",
      "  -0.06 -0.05  0.91  0.01 -0.09  0.93  0.07 -0.15  1.03  0.12 -0.12  1.05\n",
      "   0.06 -0.14  1.04 -0.12  0.03  0.94 -0.09  0.02  0.91 -0.03  0.02  0.91]\n",
      " [ 0.09 -0.2   0.89  0.15 -0.21  0.93  0.03  0.06  1.06 -0.03  0.1   1.05\n",
      "  -0.14  0.07  0.98 -0.13  0.04  0.94 -0.1   0.03  0.9  -0.04  0.14  0.95\n",
      "  -0.01  0.11  1.01 -0.02  0.13  1.03 -0.14  0.11  1.04 -0.18  0.1   1.\n",
      "  -0.17 -0.19  0.88  0.09 -0.22  0.9   0.13 -0.17  0.93  0.05  0.04  1.05\n",
      "   0.01  0.04  1.06 -0.05  0.07  1.04 -0.14  0.07  0.97 -0.11  0.03  0.92]\n",
      " [-0.06 -0.06  0.9   0.03 -0.05  1.04 -0.12  0.15  1.01  0.03  0.09  1.08\n",
      "   0.03  0.02  0.95  0.04  0.    1.01 -0.06 -0.04  0.89 -0.14  0.03  0.95\n",
      "  -0.14  0.05  0.9  -0.13  0.1   1.   -0.12  0.04  0.96  0.01  0.03  1.07\n",
      "   0.12 -0.08  1.05  0.15 -0.12  1.06  0.14 -0.15  0.95  0.03 -0.18  0.85\n",
      "  -0.02  0.    0.91 -0.14  0.06  0.89 -0.14 -0.03  0.89 -0.14  0.06  0.94]\n",
      " [-0.03 -0.14  0.97 -0.05  0.03  0.89  0.01  0.05  0.91  0.05  0.04  0.97\n",
      "  -0.06  0.13  1.08 -0.08  0.11  1.06 -0.12  0.1   1.04 -0.16  0.04  0.9\n",
      "  -0.13  0.06  0.86  0.05  0.03  1.01  0.07 -0.    1.07  0.04  0.04  1.08\n",
      "  -0.01 -0.13  1.01 -0.02 -0.19  0.97 -0.01 -0.19  0.93 -0.01  0.04  0.9\n",
      "   0.05  0.04  0.96 -0.02  0.13  1.08 -0.08  0.12  1.08 -0.11  0.11  1.06]\n",
      " [ 0.03  0.04  1.06 -0.09  0.17  0.97  0.02  0.01  1.06 -0.09 -0.2   0.93\n",
      "   0.02 -0.28  1.04  0.04 -0.27  1.01 -0.02  0.04  1.03  0.05 -0.    1.05\n",
      "  -0.04 -0.01  0.92 -0.    0.09  1.04 -0.   -0.04  0.99 -0.1   0.09  1.03\n",
      "   0.04 -0.04  1.04 -0.06 -0.01  0.94  0.02  0.09  1.06 -0.02  0.03  0.95\n",
      "  -0.11  0.11  0.9   0.03 -0.29  1.   -0.06 -0.23  0.93 -0.08 -0.18  0.97]\n",
      " [ 0.06 -0.01  1.05 -0.02 -0.01  0.93 -0.1   0.07  0.91  0.03 -0.01  1.02\n",
      "  -0.09  0.05  0.93 -0.13  0.15  0.96 -0.02 -0.04  0.95 -0.12  0.07  0.91\n",
      "   0.03  0.02  1.   -0.07  0.07  0.92 -0.13  0.14  0.91 -0.   -0.28  0.94\n",
      "  -0.08 -0.25  0.89 -0.03 -0.19  1.01 -0.06 -0.06  0.94 -0.09  0.03  0.96\n",
      "  -0.06  0.01  0.93 -0.13  0.1   0.89 -0.04  0.12  1.04 -0.12  0.06  0.94]\n",
      " [-0.18  0.09  0.93 -0.09  0.13  1.04 -0.06 -0.04  0.99  0.09 -0.09  1.07\n",
      "   0.16 -0.17  1.01  0.12 -0.05  1.07  0.09 -0.11  0.91 -0.03 -0.06  0.88\n",
      "  -0.04  0.01  0.9  -0.14  0.09  0.94 -0.11 -0.01  0.9  -0.15  0.07  1.\n",
      "  -0.03  0.12  1.09 -0.08  0.09  1.07  0.05  0.08  1.09 -0.01  0.05  0.95\n",
      "   0.11 -0.1   0.99 -0.   -0.14  0.88 -0.06 -0.08  0.95 -0.06 -0.06  0.9 ]\n",
      " [-0.05  0.24  1.02  0.04  0.09  1.14 -0.16  0.17  0.89  0.03 -0.09  1.07\n",
      "  -0.04 -0.22  0.91  0.14 -0.18  1.16  0.06 -0.    0.99 -0.04  0.21  0.99\n",
      "  -0.07  0.04  0.86 -0.16  0.21  0.86  0.06  0.03  1.   -0.11  0.02  0.88\n",
      "   0.04  0.15  1.09 -0.16  0.18  0.89 -0.04  0.23  1.03 -0.16  0.15  0.84\n",
      "   0.01  0.12  1.09  0.06 -0.23  1.02  0.04 -0.1   1.12 -0.05 -0.03  1.07]\n",
      " [-0.11  0.21  0.98 -0.03  0.06  0.92 -0.02  0.14  1.06  0.02  0.13  1.09\n",
      "  -0.17  0.19  0.87 -0.02  0.12  0.97 -0.12  0.08  0.88  0.03  0.1   1.12\n",
      "   0.13 -0.23  1.09 -0.05 -0.05  1.    0.04 -0.21  0.96 -0.11  0.09  0.83\n",
      "   0.09  0.07  1.1  -0.15  0.21  0.87 -0.07  0.19  1.02 -0.09  0.08  0.86\n",
      "  -0.17  0.18  0.87  0.02  0.06  1.02 -0.13  0.22  0.97  0.02  0.13  1.07]\n",
      " [-0.1  -0.01  0.87 -0.15  0.05  0.94 -0.15  0.05  0.91 -0.14  0.12  1.01\n",
      "   0.01  0.08  1.09  0.07 -0.13  1.06  0.17 -0.17  1.07  0.02  0.07  1.07\n",
      "   0.07  0.04  1.04 -0.   -0.01  0.89  0.04  0.01  0.97 -0.08 -0.01  0.89\n",
      "  -0.15  0.08  0.95 -0.13 -0.01  0.89 -0.12  0.07  1.02 -0.16  0.1   0.93\n",
      "  -0.07  0.12  1.06  0.04  0.07  1.1   0.12 -0.13  1.07  0.17 -0.18  1.01]\n",
      " [-0.04  0.09  1.04 -0.03 -0.05  0.94  0.03  0.05  1.05 -0.07  0.04  0.92\n",
      "  -0.08  0.12  1.02 -0.09 -0.25  0.9   0.04 -0.26  1.04 -0.02 -0.32  0.91\n",
      "   0.07 -0.05  1.03 -0.09  0.05  0.92 -0.   -0.02  0.97 -0.13  0.14  0.95\n",
      "   0.03  0.02  1.05 -0.1   0.15  1.02  0.01 -0.06  1.02 -0.14  0.08  0.9\n",
      "  -0.06  0.03  0.91 -0.1   0.18  0.97  0.03  0.    1.02  0.01 -0.17  1.04]\n",
      " [-0.09  0.08  1.04  0.07  0.    1.08  0.06 -0.07  0.96  0.16 -0.18  0.97\n",
      "   0.05 -0.19  0.86 -0.03 -0.    0.9  -0.16  0.07  0.91 -0.08  0.12  1.04\n",
      "  -0.16  0.14  1.02 -0.02  0.11  1.08  0.04 -0.    0.98  0.02  0.06  1.04\n",
      "  -0.05 -0.03  0.9  -0.01 -0.    0.93 -0.15  0.07  0.92 -0.1   0.12  1.04\n",
      "   0.01 -0.13  0.99  0.14 -0.13  1.06  0.18 -0.21  0.97  0.08  0.02  1.03]\n",
      " [-0.06  0.05  0.93 -0.12  0.13  0.93 -0.02 -0.09  0.91 -0.11 -0.03  0.89\n",
      "  -0.01 -0.02  1.03 -0.09 -0.19  0.93 -0.05 -0.18  1.    0.05 -0.23  0.92\n",
      "  -0.13  0.12  0.9  -0.02  0.1   1.04 -0.12  0.09  0.93 -0.06  0.15  1.01\n",
      "   0.02  0.01  1.05 -0.13  0.14  0.95 -0.03  0.07  1.07  0.01  0.    0.99\n",
      "  -0.02  0.11  1.04  0.04  0.01  1.02 -0.05 -0.25  1.    0.04 -0.24  1.04]\n",
      " [-0.11 -0.23  1.06 -0.01 -0.13  0.95 -0.1  -0.04  0.93  0.01 -0.01  0.99\n",
      "  -0.1   0.05  0.94 -0.1   0.14  0.98 -0.05 -0.01  0.92 -0.15  0.14  0.94\n",
      "  -0.03  0.08  1.05 -0.11  0.07  0.91 -0.04  0.13  1.02 -0.06 -0.09  0.91\n",
      "  -0.08 -0.02  0.98  0.04 -0.1   1.02 -0.06 -0.14  0.95  0.02 -0.21  1.05\n",
      "   0.03 -0.26  0.98 -0.01  0.08  1.04  0.04 -0.01  1.02 -0.07  0.14  1.02]\n",
      " [-0.01 -0.03  1.11 -0.02  0.01  0.95 -0.12 -0.03  0.89 -0.08  0.04  0.89\n",
      "  -0.16  0.11  0.95 -0.04  0.1   1.07  0.05 -0.13  1.03  0.15 -0.12  1.07\n",
      "   0.15 -0.21  0.93  0.09 -0.02  0.99 -0.05 -0.04  0.87 -0.04 -0.01  0.92\n",
      "  -0.15  0.06  0.91 -0.15  0.16  1.03 -0.13  0.06  0.98 -0.06  0.12  1.09\n",
      "   0.03  0.04  1.05  0.04  0.07  1.09 -0.    0.02  0.96  0.12 -0.11  0.99]\n",
      " [-0.1   0.13  1.07  0.03  0.07  1.08 -0.    0.09  1.1  -0.02  0.05  0.99\n",
      "  -0.12 -0.    1.01 -0.07 -0.02  0.89 -0.13  0.08  0.96  0.01 -0.16  0.92\n",
      "   0.08 -0.11  1.05  0.19 -0.19  1.02  0.06  0.06  1.08 -0.    0.01  0.93\n",
      "  -0.13  0.04  0.89 -0.1   0.    0.89 -0.18  0.14  1.   -0.15  0.05  0.98\n",
      "  -0.04  0.11  1.1  -0.    0.05  1.02  0.05  0.05  1.05 -0.04  0.02  0.92]\n",
      " [-0.09  0.11  0.92 -0.12  0.18  0.94 -0.05  0.14  1.04 -0.11  0.07  0.91\n",
      "  -0.13  0.18  0.94 -0.07  0.07  0.92 -0.12  0.12  0.94 -0.14  0.19  0.94\n",
      "  -0.04 -0.23  0.9  -0.08 -0.19  0.92 -0.11 -0.16  0.97 -0.01 -0.18  0.93\n",
      "  -0.08 -0.07  0.91 -0.1   0.06  0.99 -0.   -0.01  0.92 -0.08  0.07  0.89\n",
      "   0.03 -0.02  1.   -0.01 -0.05  0.96 -0.07  0.02  0.87  0.01  0.09  1.04]\n",
      " [ 0.03 -0.02  1.06 -0.04 -0.1   0.88  0.13 -0.07  1.11  0.07  0.03  1.02\n",
      "  -0.1   0.25  0.95 -0.01  0.04  0.92 -0.12  0.12  0.85  0.07  0.04  1.1\n",
      "  -0.16  0.09  0.96 -0.08  0.21  1.02 -0.1   0.05  0.87 -0.18  0.16  0.89\n",
      "   0.05 -0.03  1.08 -0.15  0.16  0.97  0.08 -0.13  1.11 -0.06 -0.15  0.88\n",
      "   0.14 -0.18  1.15  0.06 -0.    1.   -0.05  0.24  0.99  0.06  0.06  1.1 ]\n",
      " [ 0.04  0.01  1.02 -0.05 -0.25  1.    0.04 -0.24  1.04  0.   -0.36  0.93\n",
      "  -0.02  0.07  1.04  0.05 -0.03  1.01 -0.06 -0.    0.93  0.02  0.05  1.05\n",
      "  -0.03 -0.01  0.97 -0.06  0.1   1.04  0.04 -0.03  1.03 -0.07  0.    0.94\n",
      "   0.03  0.07  1.07  0.    0.03  0.96 -0.11  0.08  0.94  0.06 -0.23  1.03\n",
      "  -0.05 -0.29  0.9  -0.11 -0.23  1.06 -0.01 -0.13  0.95 -0.1  -0.04  0.93]\n",
      " [-0.14  0.21  0.98 -0.08  0.12  0.91 -0.    0.15  1.07  0.11 -0.2   1.15\n",
      "  -0.08 -0.07  0.97  0.03  0.1   1.04 -0.09  0.04  0.85  0.09  0.04  1.12\n",
      "   0.04  0.04  0.98 -0.03  0.17  1.02 -0.06  0.07  0.86 -0.15  0.17  0.87\n",
      "   0.04  0.    1.01 -0.11  0.21  0.87  0.    0.13  1.07 -0.18  0.19  0.86\n",
      "  -0.09  0.03  0.99  0.01 -0.15  0.97  0.04 -0.07  1.1   0.15 -0.13  1.12]\n",
      " [ 0.05 -0.36  0.98  0.01  0.08  1.03  0.06  0.01  1.05  0.04 -0.04  0.93\n",
      "  -0.01  0.08  1.04  0.05 -0.01  1.03 -0.08  0.18  1.01 -0.02  0.12  1.03\n",
      "   0.02  0.02  1.   -0.06  0.19  1.06  0.    0.08  1.09  0.01  0.02  1.\n",
      "  -0.01 -0.24  1.06  0.05 -0.29  1.09  0.05 -0.38  0.99  0.    0.06  1.05\n",
      "   0.06 -0.02  1.09  0.06  0.18  0.98 -0.01  0.09  1.03  0.04  0.04  1.08]\n",
      " [-0.07  0.02  0.94 -0.09  0.14  1.    0.03 -0.03  1.04 -0.04  0.09  1.04\n",
      "  -0.03 -0.05  0.94  0.03  0.05  1.05 -0.07  0.04  0.92 -0.08  0.12  1.02\n",
      "  -0.09 -0.25  0.9   0.04 -0.26  1.04 -0.02 -0.32  0.91  0.07 -0.05  1.03\n",
      "  -0.09  0.05  0.92 -0.   -0.02  0.97 -0.13  0.14  0.95  0.03  0.02  1.05\n",
      "  -0.1   0.15  1.02  0.01 -0.06  1.02 -0.14  0.08  0.9  -0.06  0.03  0.91]\n",
      " [-0.15  0.08  0.95 -0.13 -0.01  0.89 -0.12  0.07  1.02 -0.16  0.1   0.93\n",
      "  -0.07  0.12  1.06  0.04  0.07  1.1   0.12 -0.13  1.07  0.17 -0.18  1.01\n",
      "   0.07 -0.22  0.88  0.06 -0.08  0.9  -0.07 -0.05  0.89 -0.1   0.    0.88\n",
      "  -0.15  0.09  0.93 -0.1   0.14  1.06 -0.14  0.08  1.02 -0.04  0.12  1.11\n",
      "   0.03  0.04  1.04  0.04  0.06  1.1  -0.01  0.04  0.95 -0.13  0.04  0.88]\n",
      " [-0.12  0.12  0.99 -0.06 -0.26  0.9  -0.1  -0.16  0.94 -0.03 -0.19  1.06\n",
      "  -0.08  0.07  0.92 -0.09  0.16  0.98 -0.    0.06  0.88 -0.13  0.19  0.94\n",
      "  -0.05  0.18  1.04 -0.13  0.07  0.94 -0.1   0.19  0.97 -0.04  0.13  1.03\n",
      "  -0.14  0.18  0.94 -0.05  0.14  1.06  0.02  0.04  1.09 -0.06 -0.2   1.02\n",
      "   0.03 -0.31  1.05 -0.1   0.08  0.96 -0.    0.03  1.06  0.06 -0.05  1.08]\n",
      " [-0.04  0.18  1.05 -0.09  0.07  0.87  0.01  0.1   1.09  0.01  0.11  1.05\n",
      "  -0.16  0.2   0.89 -0.04  0.09  1.07 -0.1   0.03  0.89  0.06 -0.01  1.14\n",
      "   0.13 -0.23  1.06 -0.04 -0.02  1.01  0.02 -0.19  0.92 -0.12  0.13  0.83\n",
      "   0.09  0.05  1.07  0.01  0.05  0.96 -0.05  0.16  1.04 -0.09  0.1   0.86\n",
      "  -0.16  0.17  0.88  0.01  0.11  1.03 -0.14  0.21  0.95 -0.02  0.14  1.08]\n",
      " [ 0.03  0.08  0.97  0.03  0.05  0.89 -0.03  0.1   0.94  0.01  0.13  1.1\n",
      "  -0.07  0.12  0.87  0.04  0.05  1.1  -0.08  0.08  1.03  0.13 -0.2   1.09\n",
      "   0.05 -0.13  1.09 -0.06 -0.11  0.91  0.05  0.03  1.07 -0.13  0.1   0.88\n",
      "   0.06  0.05  1.1  -0.15  0.15  0.91 -0.07  0.1   0.86 -0.12  0.16  0.96\n",
      "  -0.14  0.09  0.86  0.03  0.08  1.02 -0.16  0.16  0.84 -0.01  0.11  0.97]\n",
      " [-0.06  0.04  1.05 -0.08 -0.21  0.91 -0.1  -0.12  0.94 -0.06 -0.09  1.03\n",
      "  -0.06  0.02  0.87 -0.1   0.11  0.91 -0.09 -0.05  0.96 -0.06  0.02  0.89\n",
      "  -0.1   0.15  0.94 -0.   -0.02  0.94 -0.07 -0.03  0.88 -0.12  0.07  0.93\n",
      "   0.   -0.01  0.97 -0.05  0.01  0.91 -0.11  0.06  0.92  0.06 -0.35  1.01\n",
      "   0.01 -0.34  0.94  0.04  0.01  1.06  0.08 -0.04  1.04  0.04 -0.09  0.94]\n",
      " [ 0.09 -0.12  1.06  0.04 -0.16  1.03 -0.01 -0.17  0.99 -0.03 -0.03  0.89\n",
      "   0.02  0.    0.92  0.01  0.1   1.07 -0.04  0.12  1.08 -0.1   0.11  1.05\n",
      "  -0.12  0.02  0.92 -0.11  0.03  0.88 -0.05  0.07  0.9   0.03  0.06  1.06\n",
      "  -0.04  0.08  1.07 -0.01 -0.17  0.98 -0.03 -0.17  0.92  0.04 -0.2   0.89\n",
      "   0.13 -0.07  1.01  0.09  0.    1.04  0.03  0.01  1.06 -0.15  0.06  0.98]\n",
      " [-0.17  0.05  0.93 -0.06  0.1   1.07 -0.09  0.08  1.04  0.07  0.    1.08\n",
      "   0.06 -0.07  0.96  0.16 -0.18  0.97  0.05 -0.19  0.86 -0.03 -0.    0.9\n",
      "  -0.16  0.07  0.91 -0.08  0.12  1.04 -0.16  0.14  1.02 -0.02  0.11  1.08\n",
      "   0.04 -0.    0.98  0.02  0.06  1.04 -0.05 -0.03  0.9  -0.01 -0.    0.93\n",
      "  -0.15  0.07  0.92 -0.1   0.12  1.04  0.01 -0.13  0.99  0.14 -0.13  1.06]\n",
      " [ 0.02 -0.12  1.05  0.04 -0.2   1.   -0.08 -0.08  0.93  0.05  0.04  1.05\n",
      "  -0.04 -0.    0.92 -0.12  0.12  0.9   0.   -0.05  0.98 -0.13  0.09  0.9\n",
      "   0.01  0.02  1.01 -0.06  0.05  0.93 -0.12  0.13  0.93 -0.02 -0.09  0.91\n",
      "  -0.11 -0.03  0.89 -0.01 -0.02  1.03 -0.09 -0.19  0.93 -0.05 -0.18  1.\n",
      "   0.05 -0.23  0.92 -0.13  0.12  0.9  -0.02  0.1   1.04 -0.12  0.09  0.93]\n",
      " [ 0.04  0.01  1.05 -0.12  0.08  0.94 -0.02 -0.05  0.93 -0.14  0.14  0.96\n",
      "   0.02  0.01  1.04 -0.01  0.12  1.05 -0.02  0.    0.94 -0.13  0.14  0.93\n",
      "  -0.05 -0.27  0.92  0.   -0.25  1.04 -0.1   0.04  0.93  0.07 -0.08  1.05\n",
      "  -0.07 -0.    0.93  0.   -0.04  0.97 -0.13  0.13  0.92  0.03  0.02  1.05\n",
      "  -0.09  0.13  1.03  0.01 -0.06  1.02 -0.15  0.09  1.07 -0.06  0.04  0.91]\n",
      " [-0.12  0.11  0.92 -0.07  0.17  1.03 -0.13  0.18  0.94 -0.04  0.17  1.07\n",
      "   0.02  0.04  1.05  0.01 -0.21  1.05  0.05 -0.31  1.01  0.04 -0.09  1.07\n",
      "   0.05 -0.15  0.97 -0.07 -0.07  0.9   0.01 -0.03  0.94 -0.09  0.1   0.91\n",
      "  -0.09  0.2   1.   -0.1   0.12  0.93 -0.1   0.21  1.01 -0.13  0.16  0.95\n",
      "  -0.06  0.19  1.05  0.    0.09  1.06 -0.02 -0.08  1.06  0.03 -0.21  1.03]\n",
      " [ 0.11 -0.17  1.02 -0.04  0.09  1.01  0.07  0.07  1.1  -0.13  0.2   0.86\n",
      "   0.08  0.03  1.03 -0.04  0.06  0.91 -0.    0.14  1.06 -0.14  0.18  0.86\n",
      "  -0.13  0.28  0.97 -0.08  0.1   0.88 -0.15  0.12  0.9   0.04 -0.04  1.05\n",
      "  -0.07  0.06  1.05  0.12 -0.14  1.14 -0.1  -0.01  0.89  0.1  -0.18  1.01\n",
      "  -0.09  0.1   0.84  0.1   0.03  1.09 -0.01  0.06  0.94 -0.    0.11  1.05]\n",
      " [ 0.11 -0.2   1.01 -0.01  0.09  0.86  0.07  0.1   1.1  -0.14  0.18  0.85\n",
      "  -0.11  0.21  0.98 -0.03  0.06  0.92 -0.02  0.14  1.06  0.02  0.13  1.09\n",
      "  -0.17  0.19  0.87 -0.02  0.12  0.97 -0.12  0.08  0.88  0.03  0.1   1.12\n",
      "   0.13 -0.23  1.09 -0.05 -0.05  1.    0.04 -0.21  0.96 -0.11  0.09  0.83\n",
      "   0.09  0.07  1.1  -0.15  0.21  0.87 -0.07  0.19  1.02 -0.09  0.08  0.86]\n",
      " [-0.02 -0.11  0.89  0.06 -0.15  0.91  0.15 -0.13  1.04  0.11 -0.08  1.07\n",
      "  -0.16  0.09  1.   -0.12  0.05  0.96 -0.11  0.04  0.93 -0.01  0.06  0.96\n",
      "  -0.01  0.09  0.99 -0.01  0.12  1.04 -0.07  0.07  1.03 -0.13  0.08  1.\n",
      "  -0.15  0.06  0.91 -0.04 -0.06  0.9   0.   -0.07  0.91  0.15 -0.15  1.03\n",
      "   0.17 -0.16  1.06  0.11 -0.12  1.05 -0.11  0.07  1.   -0.13  0.05  0.96]\n",
      " [ 0.02  0.05  1.05 -0.15  0.16  0.95 -0.03  0.09  1.06  0.02 -0.01  1.03\n",
      "  -0.06  0.13  1.01  0.03  0.06  1.06 -0.06 -0.    0.92  0.01  0.06  1.04\n",
      "  -0.   -0.04  0.97 -0.09  0.03  1.04  0.06 -0.3   1.03 -0.05 -0.24  0.91\n",
      "   0.05  0.01  1.06 -0.   -0.01  0.94 -0.09  0.08  0.9   0.01  0.    1.01\n",
      "  -0.11  0.07  0.94 -0.1   0.16  0.97 -0.05 -0.02  0.93 -0.14  0.08  0.9 ]\n",
      " [-0.01 -0.15  1.03  0.06 -0.19  1.05 -0.08  0.16  0.98  0.02  0.05  1.05\n",
      "  -0.15  0.16  0.95 -0.03  0.09  1.06  0.02 -0.01  1.03 -0.06  0.13  1.01\n",
      "   0.03  0.06  1.06 -0.06 -0.    0.92  0.01  0.06  1.04 -0.   -0.04  0.97\n",
      "  -0.09  0.03  1.04  0.06 -0.3   1.03 -0.05 -0.24  0.91  0.05  0.01  1.06\n",
      "  -0.   -0.01  0.94 -0.09  0.08  0.9   0.01  0.    1.01 -0.11  0.07  0.94]\n",
      " [-0.18  0.17  0.85  0.03  0.1   1.08  0.03 -0.22  1.01  0.05 -0.09  1.12\n",
      "   0.13 -0.06  1.11 -0.12  0.09  0.85  0.09 -0.09  1.   -0.05  0.04  0.88\n",
      "   0.07  0.1   1.1  -0.13  0.15  0.84 -0.14  0.25  0.97 -0.03  0.01  0.93\n",
      "  -0.13  0.18  0.84  0.    0.14  1.05 -0.17  0.24  0.87  0.    0.07  1.1\n",
      "  -0.11 -0.01  0.9   0.07 -0.05  1.16  0.1  -0.22  1.01 -0.01  0.    1.04]\n",
      " [ 0.1  -0.08  1.01  0.18 -0.15  1.04  0.11 -0.17  0.9  -0.01 -0.15  0.87\n",
      "  -0.08  0.    0.89 -0.15  0.09  0.95 -0.03  0.12  1.07 -0.11  0.13  1.07\n",
      "  -0.01  0.1   1.09  0.02  0.07  1.09  0.01  0.07  1.03 -0.06 -0.01  0.91\n",
      "   0.01 -0.01  0.93 -0.12  0.02  0.91 -0.11  0.09  1.02 -0.01 -0.15  0.93\n",
      "   0.1  -0.12  1.06  0.2  -0.19  1.02  0.05  0.08  1.07  0.03  0.03  0.95]\n",
      " [ 0.17 -0.17  1.07  0.02  0.07  1.07  0.07  0.04  1.04 -0.   -0.01  0.89\n",
      "   0.04  0.01  0.97 -0.08 -0.01  0.89 -0.15  0.08  0.95 -0.13 -0.01  0.89\n",
      "  -0.12  0.07  1.02 -0.16  0.1   0.93 -0.07  0.12  1.06  0.04  0.07  1.1\n",
      "   0.12 -0.13  1.07  0.17 -0.18  1.01  0.07 -0.22  0.88  0.06 -0.08  0.9\n",
      "  -0.07 -0.05  0.89 -0.1   0.    0.88 -0.15  0.09  0.93 -0.1   0.14  1.06]\n",
      " [-0.1   0.15  0.94 -0.   -0.02  0.94 -0.07 -0.03  0.88 -0.12  0.07  0.93\n",
      "   0.   -0.01  0.97 -0.05  0.01  0.91 -0.11  0.06  0.92  0.06 -0.35  1.01\n",
      "   0.01 -0.34  0.94  0.04  0.01  1.06  0.08 -0.04  1.04  0.04 -0.09  0.94\n",
      "  -0.01  0.09  1.03  0.04  0.03  1.08  0.05 -0.03  0.99 -0.06  0.18  1.03\n",
      "  -0.01  0.12  1.03 -0.14  0.18  0.94 -0.09  0.2   1.03 -0.02  0.12  1.05]\n",
      " [ 0.15 -0.18  0.96  0.04 -0.21  0.86 -0.01 -0.11  0.96 -0.1   0.01  0.92\n",
      "   0.    0.04  1.05  0.09  0.02  1.05  0.04  0.08  1.08  0.01  0.    0.94\n",
      "   0.02 -0.    0.98 -0.09 -0.04  0.9  -0.14  0.07  0.98 -0.16  0.09  0.94\n",
      "  -0.02  0.1   1.07  0.04  0.05  1.04  0.12 -0.06  1.05  0.06 -0.14  0.92\n",
      "  -0.04 -0.11  0.92 -0.01 -0.12  0.87 -0.03 -0.04  0.99 -0.16  0.12  0.96]\n",
      " [ 0.11 -0.17  0.93  0.15 -0.19  0.99  0.08 -0.05  1.06  0.   -0.01  1.06\n",
      "  -0.03  0.01  1.02 -0.13  0.05  0.94 -0.11  0.03  0.9  -0.08  0.04  0.89\n",
      "  -0.03  0.12  0.97 -0.03  0.13  1.02 -0.06  0.09  1.05 -0.1   0.09  1.03\n",
      "  -0.15  0.1   1.   -0.01 -0.14  0.91  0.03 -0.14  0.9   0.09 -0.19  0.93\n",
      "   0.13 -0.1   1.03  0.08 -0.05  1.05 -0.12  0.08  1.   -0.14  0.06  0.97]\n",
      " [-0.05 -0.01  0.92 -0.15  0.14  0.94 -0.03  0.08  1.05 -0.11  0.07  0.91\n",
      "  -0.04  0.13  1.02 -0.06 -0.09  0.91 -0.08 -0.02  0.98  0.04 -0.1   1.02\n",
      "  -0.06 -0.14  0.95  0.02 -0.21  1.05  0.03 -0.26  0.98 -0.01  0.08  1.04\n",
      "   0.04 -0.01  1.02 -0.07  0.14  1.02  0.02 -0.    1.04 -0.03 -0.01  0.95\n",
      "  -0.02  0.07  1.05  0.03  0.    1.   -0.08  0.04  0.95  0.04  0.01  1.05]\n",
      " [-0.13  0.05  0.96 -0.11  0.04  0.92 -0.07  0.03  0.89 -0.01  0.12  1.\n",
      "  -0.02  0.14  1.04 -0.02  0.12  1.05 -0.13  0.08  1.02 -0.16  0.07  0.97\n",
      "  -0.03 -0.07  0.91  0.03 -0.1   0.92  0.08 -0.1   0.95  0.14 -0.15  1.05\n",
      "   0.06 -0.09  1.04  0.01 -0.09  1.02 -0.12  0.04  0.92 -0.07  0.03  0.89\n",
      "  -0.02  0.11  1.01 -0.01  0.12  1.06 -0.05  0.14  1.07 -0.15  0.04  0.99]\n",
      " [-0.03 -0.23  0.91 -0.1  -0.11  0.94 -0.08 -0.11  0.93 -0.03 -0.03  1.06\n",
      "  -0.1   0.15  0.93  0.02  0.06  1.06  0.01 -0.03  0.93  0.03 -0.    1.05\n",
      "  -0.06 -0.02  0.9  -0.12  0.2   0.95 -0.11  0.08  0.94 -0.09  0.18  1.02\n",
      "   0.01  0.06  0.91 -0.01 -0.04  1.03  0.02 -0.16  0.96  0.06 -0.3   1.11\n",
      "   0.02 -0.28  0.95 -0.09 -0.12  0.94 -0.04  0.01  0.87 -0.1   0.14  0.94]\n",
      " [-0.03 -0.2   0.91 -0.08 -0.13  0.97 -0.09 -0.12  0.92  0.05 -0.16  1.06\n",
      "  -0.08  0.15  0.98  0.04 -0.02  1.02 -0.12  0.08  0.89 -0.07 -0.    0.91\n",
      "  -0.1   0.13  1.02  0.03 -0.04  1.02  0.    0.07  1.07 -0.07  0.02  0.93\n",
      "  -0.   -0.04  0.97 -0.13  0.05  0.9   0.02 -0.04  1.05 -0.   -0.24  1.04\n",
      "   0.01 -0.3   0.97 -0.09 -0.2   0.94 -0.08  0.07  0.93  0.    0.08  1.05]\n",
      " [-0.06  0.06  0.93 -0.03  0.12  1.03  0.03 -0.02  1.02 -0.02  0.06  1.06\n",
      "  -0.02 -0.04  0.98 -0.15  0.14  0.92 -0.07  0.01  0.92 -0.1   0.15  0.98\n",
      "   0.   -0.02  0.91 -0.03  0.03  1.04  0.01 -0.12  0.96  0.04 -0.27  1.06\n",
      "  -0.02 -0.29  0.91 -0.06 -0.16  0.99 -0.07  0.06  0.92 -0.01  0.1   1.03\n",
      "   0.04 -0.03  0.96  0.02  0.03  1.04 -0.03  0.    0.92  0.01  0.    1.04]\n",
      " [-0.08  0.11  1.06 -0.12  0.1   1.04 -0.16  0.04  0.9  -0.13  0.06  0.86\n",
      "   0.05  0.03  1.01  0.07 -0.    1.07  0.04  0.04  1.08 -0.01 -0.13  1.01\n",
      "  -0.02 -0.19  0.97 -0.01 -0.19  0.93 -0.01  0.04  0.9   0.05  0.04  0.96\n",
      "  -0.02  0.13  1.08 -0.08  0.12  1.08 -0.11  0.11  1.06 -0.14  0.04  0.88\n",
      "  -0.13  0.06  0.85 -0.09  0.1   0.87  0.05  0.05  1.08  0.01  0.06  1.1 ]\n",
      " [ 0.06  0.04  1.1   0.15 -0.16  1.08  0.18 -0.19  0.99  0.1  -0.22  0.87\n",
      "   0.03 -0.01  0.91 -0.08  0.    0.86 -0.04 -0.01  0.88 -0.14  0.05  0.91\n",
      "  -0.15  0.14  1.04 -0.13  0.01  0.95 -0.08  0.08  1.09 -0.02  0.11  1.1\n",
      "  -0.04  0.1   1.08  0.05  0.07  1.1   0.11 -0.11  1.08  0.18 -0.18  1.02\n",
      "   0.1  -0.2   0.91  0.1  -0.06  0.95  0.01 -0.1   0.86 -0.09  0.01  0.92]\n",
      " [-0.04 -0.15  0.96 -0.01 -0.17  0.92  0.08 -0.09  0.93  0.1  -0.06  0.99\n",
      "   0.11 -0.06  1.05 -0.1   0.1   1.04 -0.13  0.08  1.01 -0.14  0.05  0.96\n",
      "  -0.05  0.04  0.9  -0.03  0.1   0.95  0.02  0.05  1.07 -0.05  0.07  1.05\n",
      "  -0.09  0.08  1.03 -0.06 -0.1   0.93 -0.02 -0.11  0.89  0.06 -0.15  0.91\n",
      "   0.15 -0.13  1.04  0.11 -0.08  1.07 -0.16  0.09  1.   -0.12  0.05  0.96]\n",
      " [-0.    0.13  1.1  -0.15  0.18  0.91  0.03  0.08  1.11 -0.11  0.17  0.99\n",
      "  -0.17  0.18  0.84  0.04 -0.11  1.11 -0.09 -0.06  0.91  0.1  -0.07  1.12\n",
      "  -0.06  0.01  0.99 -0.09  0.03  0.86 -0.    0.08  1.04 -0.13  0.15  0.87\n",
      "  -0.03  0.1   0.9  -0.15  0.16  0.92 -0.09  0.1   0.87  0.04  0.04  1.\n",
      "  -0.15  0.13  0.84 -0.01  0.15  0.99 -0.14  0.05  0.87  0.02 -0.03  0.99]\n",
      " [-0.01  0.2   0.9   0.09 -0.16  1.1  -0.08  0.01  1.02  0.08 -0.09  1.1\n",
      "  -0.09 -0.03  0.85  0.13 -0.18  1.06 -0.01  0.04  0.91  0.05  0.14  1.07\n",
      "  -0.13  0.17  0.86 -0.13  0.23  0.98 -0.03  0.02  0.9  -0.18  0.19  0.84\n",
      "   0.02  0.11  1.01 -0.1   0.25  0.97  0.04  0.03  1.11 -0.16  0.1   0.9\n",
      "   0.05 -0.09  1.07  0.   -0.18  0.92  0.14 -0.1   1.1   0.07  0.05  1.03]\n",
      " [-0.12  0.11  0.91 -0.14  0.16  0.95 -0.01 -0.3   0.93 -0.08 -0.2   0.91\n",
      "  -0.1  -0.19  0.96 -0.01 -0.13  0.92 -0.08  0.01  0.93  0.03 -0.02  0.97\n",
      "  -0.03  0.01  0.91 -0.1   0.13  0.93  0.01 -0.05  0.96 -0.06 -0.03  0.89\n",
      "  -0.11  0.11  0.93 -0.01  0.07  0.95 -0.07  0.07  0.9   0.03 -0.21  1.05\n",
      "   0.02 -0.25  0.95 -0.06 -0.2   0.88  0.07 -0.21  1.04  0.03 -0.24  0.95]\n",
      " [-0.09  0.19  1.02 -0.01  0.06  1.07  0.01  0.07  1.01 -0.02 -0.16  1.05\n",
      "   0.05 -0.25  1.06  0.02 -0.29  0.95  0.03 -0.12  1.08  0.07 -0.21  1.03\n",
      "   0.01  0.08  1.04  0.04  0.02  1.09  0.02 -0.03  0.94 -0.06  0.12  1.05\n",
      "   0.02 -0.04  1.01 -0.04 -0.03  0.93  0.    0.08  1.07 -0.01  0.04  0.94\n",
      "  -0.09  0.06  0.88  0.03 -0.15  1.   -0.02 -0.15  0.93 -0.1  -0.1   0.91]\n",
      " [-0.04  0.09  1.07 -0.1   0.03  0.89  0.06 -0.01  1.14  0.13 -0.23  1.06\n",
      "  -0.04 -0.02  1.01  0.02 -0.19  0.92 -0.12  0.13  0.83  0.09  0.05  1.07\n",
      "   0.01  0.05  0.96 -0.05  0.16  1.04 -0.09  0.1   0.86 -0.16  0.17  0.88\n",
      "   0.01  0.11  1.03 -0.14  0.21  0.95 -0.02  0.14  1.08 -0.13  0.11  0.86\n",
      "   0.05  0.03  0.89  0.14 -0.25  1.08 -0.06 -0.05  1.    0.02  0.09  1.07]\n",
      " [-0.02  0.07  1.04  0.05 -0.03  1.01 -0.06 -0.    0.93  0.02  0.05  1.05\n",
      "  -0.03 -0.01  0.97 -0.06  0.1   1.04  0.04 -0.03  1.03 -0.07  0.    0.94\n",
      "   0.03  0.07  1.07  0.    0.03  0.96 -0.11  0.08  0.94  0.06 -0.23  1.03\n",
      "  -0.05 -0.29  0.9  -0.11 -0.23  1.06 -0.01 -0.13  0.95 -0.1  -0.04  0.93\n",
      "   0.01 -0.01  0.99 -0.1   0.05  0.94 -0.1   0.14  0.98 -0.05 -0.01  0.92]\n",
      " [-0.12  0.05  0.93 -0.03  0.07  0.93 -0.02  0.09  0.98 -0.03  0.13  1.02\n",
      "  -0.04  0.07  1.04 -0.09  0.1   1.01 -0.14 -0.04  0.94 -0.02 -0.11  0.91\n",
      "   0.01 -0.11  0.91  0.16 -0.16  1.02  0.15 -0.13  1.04  0.1  -0.1   1.06\n",
      "  -0.12  0.09  1.01 -0.13  0.06  0.97 -0.12  0.04  0.94 -0.03  0.05  0.94\n",
      "  -0.01  0.08  0.98 -0.    0.08  1.05 -0.04  0.07  1.05 -0.1   0.08  1.02]\n",
      " [-0.1   0.07  0.91  0.03 -0.01  1.02 -0.09  0.05  0.93 -0.13  0.15  0.96\n",
      "  -0.02 -0.04  0.95 -0.12  0.07  0.91  0.03  0.02  1.   -0.07  0.07  0.92\n",
      "  -0.13  0.14  0.91 -0.   -0.28  0.94 -0.08 -0.25  0.89 -0.03 -0.19  1.01\n",
      "  -0.06 -0.06  0.94 -0.09  0.03  0.96 -0.06  0.01  0.93 -0.13  0.1   0.89\n",
      "  -0.04  0.12  1.04 -0.12  0.06  0.94 -0.11  0.16  1.01  0.01  0.02  1.06]\n",
      " [ 0.17 -0.17  1.   -0.01  0.09  1.05 -0.07  0.07  1.03 -0.11  0.07  0.99\n",
      "  -0.08  0.03  0.91 -0.01  0.03  0.94  0.02  0.07  0.99 -0.04  0.09  1.06\n",
      "  -0.1   0.07  1.02 -0.15  0.08  0.89 -0.08  0.07  0.89 -0.02  0.07  0.95\n",
      "   0.11 -0.11  1.07  0.08 -0.17  1.06  0.   -0.15  1.02 -0.06 -0.04  0.86\n",
      "   0.   -0.02  0.9   0.03  0.1   1.06 -0.02  0.12  1.07 -0.08  0.1   1.05]\n",
      " [-0.12  0.15  0.97  0.    0.08  1.04 -0.13  0.14  0.92 -0.05  0.12  1.04\n",
      "   0.02 -0.01  1.02 -0.05  0.15  1.03  0.03  0.12  1.06 -0.05  0.05  0.92\n",
      "   0.04 -0.18  1.04  0.02 -0.28  0.95 -0.07 -0.22  1.06  0.04 -0.17  1.01\n",
      "  -0.07 -0.02  0.94  0.04  0.02  1.04 -0.06  0.05  0.91 -0.13  0.13  0.91\n",
      "  -0.03 -0.03  0.97 -0.13  0.09  0.89 -0.08  0.18  1.02 -0.09  0.06  0.95]\n",
      " [ 0.01 -0.08  1.1  -0.11 -0.04  0.89 -0.01 -0.08  0.95 -0.1  -0.    0.91\n",
      "  -0.03 -0.06  0.88 -0.09  0.13  0.96 -0.1   0.12  0.83  0.06  0.07  1.01\n",
      "  -0.13  0.14  0.86  0.02  0.03  0.98  0.02  0.1   1.1  -0.07  0.15  0.9\n",
      "   0.02  0.13  1.11 -0.07  0.05  0.88  0.07 -0.03  1.09 -0.03 -0.01  1.08\n",
      "   0.12 -0.2   1.06  0.07 -0.11  1.1  -0.09 -0.06  0.9   0.06  0.06  1.1 ]\n",
      " [ 0.01 -0.01  1.07 -0.01 -0.04  1.04 -0.05 -0.1   0.99  0.06 -0.16  0.88\n",
      "   0.1  -0.14  0.91  0.02  0.1   1.06 -0.05  0.1   1.05 -0.11  0.1   1.03\n",
      "  -0.1   0.02  0.9  -0.04  0.01  0.92 -0.03  0.07  0.96  0.01  0.07  1.06\n",
      "  -0.06  0.07  1.04 -0.13  0.07  1.01 -0.06 -0.05  0.91  0.01 -0.09  0.93\n",
      "   0.07 -0.15  1.03  0.12 -0.12  1.05  0.06 -0.14  1.04 -0.12  0.03  0.94]\n",
      " [-0.01 -0.19  0.93 -0.02  0.01  0.91  0.03  0.04  0.96  0.06  0.04  1.03\n",
      "  -0.07  0.12  1.07 -0.11  0.09  1.04 -0.14  0.08  0.99 -0.11  0.05  0.88\n",
      "  -0.09  0.08  0.89  0.05  0.03  1.06  0.04  0.05  1.08 -0.    0.07  1.07\n",
      "   0.02 -0.21  0.98 -0.02 -0.19  0.94  0.03 -0.22  0.91  0.02  0.03  0.94\n",
      "   0.05  0.04  1.    0.06  0.12  1.08 -0.1   0.12  1.04 -0.12  0.07  1.01]\n",
      " [-0.14  0.11  1.01 -0.18  0.09  0.98 -0.17  0.07  0.94  0.11 -0.2   0.92\n",
      "   0.14 -0.17  0.97  0.04  0.02  1.06 -0.    0.03  1.05 -0.06  0.05  1.03\n",
      "  -0.13  0.05  0.93 -0.09  0.03  0.9  -0.05  0.03  0.9  -0.02  0.13  1.02\n",
      "   0.01  0.12  1.04 -0.03  0.1   1.03 -0.16  0.08  0.99 -0.18  0.07  0.95\n",
      "   0.05 -0.14  0.92  0.13 -0.14  0.97  0.16 -0.15  1.03 -0.02 -0.03  1.03]\n",
      " [ 0.02  0.1   1.06 -0.05 -0.09  1.02  0.04 -0.22  1.   -0.09 -0.21  0.89\n",
      "  -0.   -0.22  0.92 -0.08 -0.06  0.97 -0.06  0.06  0.93 -0.03  0.12  1.03\n",
      "   0.03 -0.02  1.02 -0.02  0.06  1.06 -0.02 -0.04  0.98 -0.15  0.14  0.92\n",
      "  -0.07  0.01  0.92 -0.1   0.15  0.98  0.   -0.02  0.91 -0.03  0.03  1.04\n",
      "   0.01 -0.12  0.96  0.04 -0.27  1.06 -0.02 -0.29  0.91 -0.06 -0.16  0.99]\n",
      " [ 0.01  0.08  1.06 -0.01 -0.02  0.96 -0.02  0.07  1.04  0.02 -0.06  0.99\n",
      "  -0.08 -0.    0.92  0.06 -0.31  1.06 -0.05 -0.28  0.91  0.04  0.02  1.05\n",
      "   0.01 -0.02  0.96 -0.1   0.11  0.91  0.02 -0.04  1.02 -0.1   0.08  0.95\n",
      "  -0.12  0.13  0.97 -0.04  0.01  0.93 -0.12  0.08  0.9  -0.01 -0.03  0.96\n",
      "  -0.12  0.15  0.92 -0.08  0.15  1.   -0.05 -0.26  0.91 -0.08 -0.23  0.95]\n",
      " [ 0.16 -0.18  0.97  0.05 -0.19  0.86 -0.03 -0.    0.9  -0.16  0.07  0.91\n",
      "  -0.08  0.12  1.04 -0.16  0.14  1.02 -0.02  0.11  1.08  0.04 -0.    0.98\n",
      "   0.02  0.06  1.04 -0.05 -0.03  0.9  -0.01 -0.    0.93 -0.15  0.07  0.92\n",
      "  -0.1   0.12  1.04  0.01 -0.13  0.99  0.14 -0.13  1.06  0.18 -0.21  0.97\n",
      "   0.08  0.02  1.03 -0.01 -0.02  0.9  -0.13  0.    0.96 -0.12  0.02  0.89]\n",
      " [-0.16  0.16  0.84  0.02  0.12  1.06 -0.02 -0.02  0.97 -0.02  0.07  1.09\n",
      "  -0.12  0.03  0.9  -0.1  -0.03  0.86  0.12 -0.25  1.04 -0.02  0.    1.01\n",
      "   0.05  0.12  1.09 -0.13  0.16  0.83  0.1   0.06  0.94  0.    0.03  0.93\n",
      "  -0.02  0.17  1.06  0.05  0.13  1.09 -0.15  0.19  0.87 -0.01  0.07  0.96\n",
      "  -0.15  0.14  0.87  0.06  0.1   1.14 -0.18  0.19  0.87  0.01 -0.09  1.08]\n",
      " [-0.04  0.09  1.07 -0.14  0.05  0.98 -0.14  0.04  0.93 -0.12  0.01  0.91\n",
      "  -0.04  0.09  0.92  0.01  0.07  0.98  0.03  0.03  1.09  0.02 -0.02  1.08\n",
      "  -0.03 -0.01  1.05 -0.   -0.19  0.89  0.02 -0.2   0.86  0.03 -0.17  0.86\n",
      "   0.06  0.05  1.03  0.06  0.07  1.06  0.02  0.1   1.06 -0.11  0.09  1.03\n",
      "  -0.13  0.08  1.   -0.11  0.02  0.86 -0.09  0.05  0.86 -0.07  0.08  0.89]\n",
      " [-0.02  0.13  1.03 -0.14  0.11  1.04 -0.18  0.1   1.   -0.17 -0.19  0.88\n",
      "   0.09 -0.22  0.9   0.13 -0.17  0.93  0.05  0.04  1.05  0.01  0.04  1.06\n",
      "  -0.05  0.07  1.04 -0.14  0.07  0.97 -0.11  0.03  0.92 -0.08  0.03  0.89\n",
      "  -0.04  0.14  0.97 -0.02  0.13  1.02 -0.1   0.1   1.04 -0.15  0.1   1.01\n",
      "  -0.18  0.08  0.97  0.07 -0.17  0.9   0.14 -0.19  0.96  0.16 -0.15  1.02]\n",
      " [-0.   -0.22  0.92 -0.08 -0.06  0.97 -0.06  0.06  0.93 -0.03  0.12  1.03\n",
      "   0.03 -0.02  1.02 -0.02  0.06  1.06 -0.02 -0.04  0.98 -0.15  0.14  0.92\n",
      "  -0.07  0.01  0.92 -0.1   0.15  0.98  0.   -0.02  0.91 -0.03  0.03  1.04\n",
      "   0.01 -0.12  0.96  0.04 -0.27  1.06 -0.02 -0.29  0.91 -0.06 -0.16  0.99\n",
      "  -0.07  0.06  0.92 -0.01  0.1   1.03  0.04 -0.03  0.96  0.02  0.03  1.04]\n",
      " [-0.13  0.15  0.92 -0.03  0.16  1.03  0.01 -0.02  0.96  0.01  0.07  1.1\n",
      "  -0.04  0.04  0.93 -0.14  0.17  1.01 -0.07 -0.24  0.88 -0.08 -0.16  1.\n",
      "  -0.1   0.03  0.94  0.02  0.01  1.05  0.06 -0.14  0.98  0.05  0.04  1.08\n",
      "  -0.02 -0.    0.93 -0.11  0.18  0.93 -0.1   0.02  0.88 -0.1   0.2   0.98\n",
      "  -0.13  0.15  0.96 -0.03  0.11  1.04  0.    0.02  0.97  0.04 -0.22  1.04]\n",
      " [-0.05  0.06  0.91 -0.02  0.17  1.07  0.01  0.07  1.03 -0.14  0.22  0.98\n",
      "  -0.1   0.13  0.89 -0.09 -0.03  0.9   0.1  -0.26  1.07 -0.02 -0.05  1.06\n",
      "   0.09 -0.01  1.11 -0.12  0.12  0.87 -0.02  0.12  1.04 -0.08  0.11  0.84\n",
      "   0.09  0.    1.12  0.   -0.03  0.99 -0.03  0.2   1.05 -0.13  0.09  0.86\n",
      "  -0.16  0.25  0.89 -0.07  0.14  0.88 -0.01  0.2   0.9   0.09 -0.16  1.1 ]\n",
      " [-0.15  0.07  0.92 -0.1   0.12  1.04  0.01 -0.13  0.99  0.14 -0.13  1.06\n",
      "   0.18 -0.21  0.97  0.08  0.02  1.03 -0.01 -0.02  0.9  -0.13  0.    0.96\n",
      "  -0.12  0.02  0.89 -0.17  0.12  0.97 -0.14  0.02  0.93 -0.09  0.1   1.06\n",
      "  -0.01  0.1   1.1   0.    0.09  1.09  0.03  0.04  1.03 -0.07  0.03  0.9\n",
      "   0.09 -0.18  0.91 -0.01 -0.17  0.88 -0.04 -0.11  0.87 -0.07 -0.03  0.93]\n",
      " [-0.06 -0.01  0.91  0.01 -0.01  0.93 -0.12  0.02  0.91 -0.11  0.09  1.02\n",
      "  -0.01 -0.15  0.93  0.1  -0.12  1.06  0.2  -0.19  1.02  0.05  0.08  1.07\n",
      "   0.03  0.03  0.95  0.05  0.03  1.01 -0.04 -0.03  0.9  -0.15  0.05  0.97\n",
      "  -0.14 -0.02  0.91 -0.09  0.08  1.05 -0.01  0.09  1.09  0.04  0.08  1.09\n",
      "   0.01 -0.    0.94  0.16 -0.19  0.93  0.02 -0.23  0.86  0.   -0.12  1.  ]\n",
      " [-0.14  0.09  0.95  0.02  0.06  1.07 -0.07  0.15  1.06  0.07  0.02  1.05\n",
      "  -0.04  0.01  0.9  -0.01 -0.06  0.93 -0.14  0.01  0.91 -0.13  0.08  1.04\n",
      "  -0.17  0.13  0.98  0.02  0.07  1.08  0.02  0.05  1.06  0.14 -0.13  1.01\n",
      "   0.05 -0.17  0.87  0.06 -0.17  0.88 -0.08 -0.04  0.9   0.04 -0.03  1.04\n",
      "  -0.13  0.15  1.02  0.03  0.09  1.07  0.02 -0.    0.94  0.03  0.    0.99]\n",
      " [-0.05 -0.1   0.89 -0.11  0.03  0.89 -0.14  0.12  0.97 -0.01  0.12  1.07\n",
      "  -0.09  0.14  1.06  0.03  0.07  1.09  0.02 -0.04  1.09  0.01  0.04  1.01\n",
      "  -0.09 -0.01  0.89 -0.03 -0.03  0.93 -0.14  0.05  0.93 -0.07  0.09  1.06\n",
      "  -0.01 -0.1   0.99  0.14 -0.14  1.06  0.14 -0.19  0.96  0.05  0.05  1.04\n",
      "  -0.02  0.01  0.89  0.01 -0.01  0.94 -0.13  0.02  0.92 -0.15  0.15  1.01]\n",
      " [-0.13  0.08  0.85 -0.14  0.16  0.91 -0.01 -0.03  1.   -0.01 -0.18  0.88\n",
      "   0.15 -0.2   1.12 -0.07 -0.04  0.94  0.03  0.17  1.05 -0.11  0.13  0.84\n",
      "   0.08  0.06  1.05 -0.03  0.03  0.92  0.02  0.15  1.08 -0.14  0.15  0.87\n",
      "  -0.13  0.22  0.97 -0.06  0.01  0.91 -0.02  0.21  1.06  0.02  0.08  1.05\n",
      "  -0.14  0.23  0.97  0.09 -0.17  1.13 -0.08 -0.1   0.9   0.13 -0.25  1.11]\n",
      " [ 0.01 -0.19  0.87 -0.04 -0.03  0.86 -0.12  0.06  0.92 -0.06  0.11  1.04\n",
      "  -0.16  0.14  0.98 -0.03  0.12  1.08  0.06  0.09  1.07 -0.    0.09  1.09\n",
      "   0.01  0.01  0.97  0.02  0.05  1.05 -0.05  0.03  0.91 -0.16  0.09  0.91\n",
      "   0.03 -0.2   0.87 -0.04 -0.14  0.94  0.09 -0.12  1.07 -0.03  0.06  1.02\n",
      "   0.07  0.05  1.09 -0.02  0.12  1.07  0.07  0.04  1.03 -0.03 -0.    0.91]\n",
      " [-0.14  0.08  1.02 -0.04  0.12  1.11  0.03  0.04  1.04  0.04  0.06  1.1\n",
      "  -0.01  0.04  0.95 -0.13  0.04  0.88  0.02 -0.17  0.88 -0.06 -0.09  0.94\n",
      "  -0.05 -0.1   0.9   0.01 -0.06  1.02  0.11 -0.03  1.07 -0.02  0.12  1.08\n",
      "   0.07  0.05  1.04 -0.    0.01  0.91  0.04 -0.    1.   -0.04 -0.05  0.9\n",
      "  -0.02  0.03  0.95 -0.12  0.01  0.88 -0.16  0.07  0.95 -0.11  0.    0.92]\n",
      " [ 0.07  0.07  1.1  -0.13  0.2   0.86  0.08  0.03  1.03 -0.04  0.06  0.91\n",
      "  -0.    0.14  1.06 -0.14  0.18  0.86 -0.13  0.28  0.97 -0.08  0.1   0.88\n",
      "  -0.15  0.12  0.9   0.04 -0.04  1.05 -0.07  0.06  1.05  0.12 -0.14  1.14\n",
      "  -0.1  -0.01  0.89  0.1  -0.18  1.01 -0.09  0.1   0.84  0.1   0.03  1.09\n",
      "  -0.01  0.06  0.94 -0.    0.11  1.05 -0.13  0.16  0.85 -0.15  0.24  0.94]\n",
      " [-0.14  0.01  0.91 -0.13  0.08  1.04 -0.17  0.13  0.98  0.02  0.07  1.08\n",
      "   0.02  0.05  1.06  0.14 -0.13  1.01  0.05 -0.17  0.87  0.06 -0.17  0.88\n",
      "  -0.08 -0.04  0.9   0.04 -0.03  1.04 -0.13  0.15  1.02  0.03  0.09  1.07\n",
      "   0.02 -0.    0.94  0.03  0.    0.99 -0.09 -0.03  0.89 -0.07 -0.    0.9\n",
      "  -0.15  0.06  0.94 -0.06  0.09  1.06 -0.1   0.08  1.03  0.06  0.02  1.07]\n",
      " [-0.01  0.06  1.07  0.01  0.07  1.01 -0.02 -0.16  1.05  0.05 -0.25  1.06\n",
      "   0.02 -0.29  0.95  0.03 -0.12  1.08  0.07 -0.21  1.03  0.01  0.08  1.04\n",
      "   0.04  0.02  1.09  0.02 -0.03  0.94 -0.06  0.12  1.05  0.02 -0.04  1.01\n",
      "  -0.04 -0.03  0.93  0.    0.08  1.07 -0.01  0.04  0.94 -0.09  0.06  0.88\n",
      "   0.03 -0.15  1.   -0.02 -0.15  0.93 -0.1  -0.1   0.91  0.04 -0.31  0.96]\n",
      " [ 0.08 -0.11  1.06  0.   -0.05  1.02 -0.12  0.04  0.92 -0.1   0.03  0.89\n",
      "  -0.04  0.03  0.9  -0.01  0.09  1.05 -0.02  0.12  1.07 -0.04  0.1   1.08\n",
      "  -0.15  0.06  0.94 -0.15  0.05  0.91  0.01 -0.07  0.94  0.09 -0.09  0.98\n",
      "   0.12 -0.09  1.05  0.07 -0.13  1.04  0.01 -0.12  1.01 -0.03 -0.11  0.95\n",
      "  -0.06  0.03  0.89 -0.02  0.02  0.92  0.03  0.04  1.06 -0.03  0.12  1.08]\n",
      " [ 0.04  0.1   1.05 -0.09  0.09  1.03 -0.14  0.04  0.99 -0.09  0.07  0.89\n",
      "  -0.03  0.07  0.94  0.01  0.05  0.98  0.09 -0.12  1.06  0.04 -0.16  1.03\n",
      "  -0.01 -0.17  0.99 -0.03 -0.03  0.89  0.02  0.    0.92  0.01  0.1   1.07\n",
      "  -0.04  0.12  1.08 -0.1   0.11  1.05 -0.12  0.02  0.92 -0.11  0.03  0.88\n",
      "  -0.05  0.07  0.9   0.03  0.06  1.06 -0.04  0.08  1.07 -0.01 -0.17  0.98]\n",
      " [-0.04  0.    0.92  0.01  0.07  1.06 -0.02 -0.01  0.97 -0.14  0.12  0.91\n",
      "   0.   -0.02  0.99 -0.1   0.05  0.95 -0.08  0.11  0.98 -0.07  0.06  0.92\n",
      "  -0.11  0.14  0.94 -0.03 -0.28  0.91 -0.09 -0.21  0.89 -0.   -0.21  1.04\n",
      "  -0.1   0.07  0.93 -0.06  0.12  1.01  0.05  0.03  1.06 -0.12  0.15  0.97\n",
      "   0.    0.08  1.04 -0.13  0.14  0.92 -0.05  0.12  1.04  0.02 -0.01  1.02]\n",
      " [-0.11  0.1   0.92 -0.13  0.21  0.94 -0.05  0.01  0.92 -0.12  0.06  0.92\n",
      "  -0.13  0.15  0.94 -0.04 -0.05  0.94 -0.11 -0.01  0.91 -0.13  0.05  0.93\n",
      "  -0.   -0.31  0.95 -0.08 -0.21  0.9   0.05 -0.05  1.    0.   -0.04  0.92\n",
      "  -0.07  0.04  0.89  0.04 -0.04  0.99 -0.03 -0.02  0.93 -0.1   0.09  0.91\n",
      "   0.   -0.01  0.96 -0.04 -0.05  0.91 -0.11  0.05  1.1   0.    0.    0.97]\n",
      " [ 0.13 -0.1   1.07 -0.04  0.12  1.06  0.07  0.06  1.08  0.02  0.02  1.09\n",
      "   0.04  0.03  1.02 -0.03 -0.04  0.9  -0.01  0.01  0.96 -0.1  -0.01  0.87\n",
      "  -0.15  0.05  0.94 -0.15  0.05  0.91 -0.14  0.12  1.01  0.01  0.08  1.09\n",
      "   0.07 -0.13  1.06  0.17 -0.17  1.07  0.02  0.07  1.07  0.07  0.04  1.04\n",
      "  -0.   -0.01  0.89  0.04  0.01  0.97 -0.08 -0.01  0.89 -0.15  0.08  0.95]\n",
      " [ 0.03  0.08  1.1   0.07 -0.02  1.08  0.12 -0.09  1.01  0.02 -0.16  0.88\n",
      "   0.07 -0.19  0.86 -0.05 -0.09  0.91 -0.14  0.06  0.9  -0.12  0.13  1.03\n",
      "   0.04  0.08  1.08 -0.04  0.13  1.09  0.03  0.    1.   -0.07 -0.04  0.89\n",
      "  -0.06 -0.02  0.9  -0.16  0.04  0.92 -0.06  0.09  1.06 -0.1   0.1   1.04\n",
      "   0.07  0.03  1.09  0.06 -0.11  1.07  0.16 -0.21  0.95  0.02 -0.19  0.86]\n",
      " [-0.    0.11  1.08  0.06  0.04  1.02 -0.04  0.    0.88  0.01 -0.04  0.94\n",
      "  -0.12 -0.01  0.9  -0.13  0.11  1.01 -0.13  0.07  0.95 -0.04  0.09  1.07\n",
      "  -0.09  0.07  1.04  0.08  0.02  1.11  0.05 -0.07  0.97  0.18 -0.2   0.99\n",
      "   0.07 -0.21  0.87 -0.02 -0.13  0.9  -0.09  0.03  0.86 -0.15  0.11  0.96\n",
      "   0.01  0.06  0.94 -0.09  0.14  1.08  0.01  0.1   1.09 -0.03  0.08  1.11]\n",
      " [ 0.02 -0.04  0.93  0.04  0.11  1.05 -0.11  0.08  0.85 -0.13  0.23  0.9\n",
      "  -0.    0.05  0.94 -0.14  0.11  0.87  0.06  0.04  1.06 -0.14  0.24  0.95\n",
      "   0.01  0.18  1.05 -0.19  0.21  0.84 -0.02  0.13  1.   -0.05 -0.13  0.94\n",
      "   0.08 -0.09  1.16 -0.1  -0.21  1.04 -0.03  0.04  0.98  0.01 -0.16  0.91\n",
      "  -0.11  0.15  0.85  0.08  0.06  1.05 -0.11  0.22  0.94 -0.01  0.18  1.06]\n",
      " [ 0.02 -0.01  1.01 -0.11  0.02  1.06 -0.05  0.01  0.92 -0.12  0.11  1.\n",
      "  -0.11  0.12  0.91 -0.02  0.1   1.05 -0.03  0.03  0.93  0.03 -0.09  1.04\n",
      "  -0.11 -0.05  0.9  -0.01 -0.05  1.04 -0.09 -0.17  0.95  0.07 -0.26  1.07\n",
      "  -0.03  0.1   1.04  0.02 -0.04  0.97 -0.1   0.13  0.9  -0.07  0.02  0.94\n",
      "  -0.09  0.14  1.    0.03 -0.03  1.04 -0.04  0.09  1.04 -0.03 -0.05  0.94]\n",
      " [-0.14  0.05  0.87  0.02 -0.03  0.99  0.08 -0.09  1.12  0.04 -0.1   0.94\n",
      "   0.13 -0.15  1.15 -0.05 -0.04  1.    0.09  0.06  1.06 -0.05  0.12  1.03\n",
      "   0.05  0.02  1.04 -0.02  0.11  1.07 -0.15  0.13  0.88  0.02  0.1   1.07\n",
      "  -0.15  0.15  0.95 -0.14  0.11  0.84 -0.06  0.04  1.05 -0.16  0.1   0.89\n",
      "   0.06 -0.18  1.1  -0.1  -0.06  0.92  0.01 -0.13  0.9  -0.11  0.14  0.95]\n",
      " [-0.03  0.1   0.95  0.02  0.05  1.07 -0.05  0.07  1.05 -0.09  0.08  1.03\n",
      "  -0.06 -0.1   0.93 -0.02 -0.11  0.89  0.06 -0.15  0.91  0.15 -0.13  1.04\n",
      "   0.11 -0.08  1.07 -0.16  0.09  1.   -0.12  0.05  0.96 -0.11  0.04  0.93\n",
      "  -0.01  0.06  0.96 -0.01  0.09  0.99 -0.01  0.12  1.04 -0.07  0.07  1.03\n",
      "  -0.13  0.08  1.   -0.15  0.06  0.91 -0.04 -0.06  0.9   0.   -0.07  0.91]\n",
      " [-0.08 -0.21  0.91 -0.1  -0.12  0.94 -0.06 -0.09  1.03 -0.06  0.02  0.87\n",
      "  -0.1   0.11  0.91 -0.09 -0.05  0.96 -0.06  0.02  0.89 -0.1   0.15  0.94\n",
      "  -0.   -0.02  0.94 -0.07 -0.03  0.88 -0.12  0.07  0.93  0.   -0.01  0.97\n",
      "  -0.05  0.01  0.91 -0.11  0.06  0.92  0.06 -0.35  1.01  0.01 -0.34  0.94\n",
      "   0.04  0.01  1.06  0.08 -0.04  1.04  0.04 -0.09  0.94 -0.01  0.09  1.03]\n",
      " [-0.08 -0.1   0.9   0.13 -0.25  1.11  0.02 -0.04  0.93  0.04  0.11  1.05\n",
      "  -0.11  0.08  0.85 -0.13  0.23  0.9  -0.    0.05  0.94 -0.14  0.11  0.87\n",
      "   0.06  0.04  1.06 -0.14  0.24  0.95  0.01  0.18  1.05 -0.19  0.21  0.84\n",
      "  -0.02  0.13  1.   -0.05 -0.13  0.94  0.08 -0.09  1.16 -0.1  -0.21  1.04\n",
      "  -0.03  0.04  0.98  0.01 -0.16  0.91 -0.11  0.15  0.85  0.08  0.06  1.05]\n",
      " [ 0.04  0.09  1.14 -0.16  0.17  0.89  0.03 -0.09  1.07 -0.04 -0.22  0.91\n",
      "   0.14 -0.18  1.16  0.06 -0.    0.99 -0.04  0.21  0.99 -0.07  0.04  0.86\n",
      "  -0.16  0.21  0.86  0.06  0.03  1.   -0.11  0.02  0.88  0.04  0.15  1.09\n",
      "  -0.16  0.18  0.89 -0.04  0.23  1.03 -0.16  0.15  0.84  0.01  0.12  1.09\n",
      "   0.06 -0.23  1.02  0.04 -0.1   1.12 -0.05 -0.03  1.07 -0.1   0.12  0.92]\n",
      " [-0.09  0.11  0.92  0.01 -0.03  0.96 -0.07  0.05  0.88 -0.13  0.19  0.94\n",
      "  -0.03 -0.04  0.92 -0.11  0.03  0.91 -0.12  0.15  0.94 -0.07  0.04  0.91\n",
      "  -0.13  0.11  0.93  0.03 -0.34  0.94 -0.08 -0.23  0.9  -0.1  -0.15  0.94\n",
      "  -0.   -0.07  0.91 -0.09  0.06  0.92 -0.1   0.15  0.95 -0.05  0.03  0.9\n",
      "  -0.11  0.15  0.93 -0.1   0.22  0.93 -0.09 -0.03  0.86 -0.13  0.13  0.92]\n",
      " [-0.13  0.11  0.93  0.03 -0.34  0.94 -0.08 -0.23  0.9  -0.1  -0.15  0.94\n",
      "  -0.   -0.07  0.91 -0.09  0.06  0.92 -0.1   0.15  0.95 -0.05  0.03  0.9\n",
      "  -0.11  0.15  0.93 -0.1   0.22  0.93 -0.09 -0.03  0.86 -0.13  0.13  0.92\n",
      "  -0.04  0.04  0.93 -0.12  0.11  0.91 -0.14  0.16  0.95 -0.01 -0.3   0.93\n",
      "  -0.08 -0.2   0.91 -0.1  -0.19  0.96 -0.01 -0.13  0.92 -0.08  0.01  0.93]\n",
      " [-0.05  0.03  0.91 -0.16  0.09  0.91  0.03 -0.2   0.87 -0.04 -0.14  0.94\n",
      "   0.09 -0.12  1.07 -0.03  0.06  1.02  0.07  0.05  1.09 -0.02  0.12  1.07\n",
      "   0.07  0.04  1.03 -0.03 -0.    0.91 -0.01 -0.04  0.95 -0.09 -0.04  0.89\n",
      "  -0.15  0.05  0.96 -0.18  0.09  0.93 -0.09  0.13  1.04 -0.06 -0.04  0.99\n",
      "   0.09 -0.09  1.07  0.16 -0.17  1.01  0.12 -0.05  1.07  0.09 -0.11  0.91]\n",
      " [-0.16  0.14  1.02 -0.02  0.11  1.08  0.04 -0.    0.98  0.02  0.06  1.04\n",
      "  -0.05 -0.03  0.9  -0.01 -0.    0.93 -0.15  0.07  0.92 -0.1   0.12  1.04\n",
      "   0.01 -0.13  0.99  0.14 -0.13  1.06  0.18 -0.21  0.97  0.08  0.02  1.03\n",
      "  -0.01 -0.02  0.9  -0.13  0.    0.96 -0.12  0.02  0.89 -0.17  0.12  0.97\n",
      "  -0.14  0.02  0.93 -0.09  0.1   1.06 -0.01  0.1   1.1   0.    0.09  1.09]\n",
      " [-0.13  0.04  0.94 -0.07  0.12  0.89 -0.04  0.14  0.95 -0.05  0.1   1.08\n",
      "  -0.11  0.12  1.05 -0.15  0.1   1.04  0.04 -0.23  0.89  0.09 -0.2   0.89\n",
      "   0.15 -0.21  0.93  0.03  0.06  1.06 -0.03  0.1   1.05 -0.14  0.07  0.98\n",
      "  -0.13  0.04  0.94 -0.1   0.03  0.9  -0.04  0.14  0.95 -0.01  0.11  1.01\n",
      "  -0.02  0.13  1.03 -0.14  0.11  1.04 -0.18  0.1   1.   -0.17 -0.19  0.88]\n",
      " [ 0.01 -0.21  0.99  0.06 -0.11  1.11  0.12 -0.07  1.12 -0.12  0.1   0.86\n",
      "   0.09 -0.07  1.01 -0.04  0.07  0.88  0.04  0.1   1.07  0.04  0.04  1.11\n",
      "  -0.16  0.18  0.87  0.04 -0.01  1.03 -0.08  0.12  0.89 -0.02  0.17  1.05\n",
      "  -0.15  0.14  0.83 -0.11  0.01  0.89  0.08 -0.14  1.08 -0.1   0.01  0.95\n",
      "   0.05 -0.04  1.08 -0.05 -0.08  0.87 -0.12  0.16  0.83  0.08  0.04  1.03]\n",
      " [ 0.06  0.07  1.01 -0.13  0.14  0.86  0.02  0.03  0.98  0.02  0.1   1.1\n",
      "  -0.07  0.15  0.9   0.02  0.13  1.11 -0.07  0.05  0.88  0.07 -0.03  1.09\n",
      "  -0.03 -0.01  1.08  0.12 -0.2   1.06  0.07 -0.11  1.1  -0.09 -0.06  0.9\n",
      "   0.06  0.06  1.1  -0.11  0.13  0.94 -0.09  0.09  0.85 -0.08  0.08  0.86\n",
      "  -0.01  0.04  0.99 -0.07 -0.01  0.86 -0.01  0.09  0.91 -0.06 -0.02  0.93]\n",
      " [ 0.02  0.04  1.07 -0.04  0.14  1.04  0.    0.07  0.99 -0.09  0.08  0.88\n",
      "  -0.   -0.13  0.92 -0.11  0.    0.93 -0.07 -0.01  1.03 -0.09 -0.11  0.96\n",
      "   0.01 -0.2   1.08 -0.09  0.17  0.96  0.04  0.05  1.06  0.04 -0.02  0.94\n",
      "   0.03  0.03  1.09 -0.01 -0.02  0.95 -0.11  0.16  0.93 -0.06 -0.02  0.9\n",
      "  -0.13  0.14  0.93 -0.12  0.09  0.9  -0.13  0.16  0.98  0.    0.03  1.04]\n",
      " [ 0.14 -0.13  1.    0.07 -0.07  1.04  0.03 -0.08  1.04 -0.13  0.05  0.96\n",
      "  -0.11  0.04  0.92 -0.07  0.03  0.89 -0.01  0.12  1.   -0.02  0.14  1.04\n",
      "  -0.02  0.12  1.05 -0.13  0.08  1.02 -0.16  0.07  0.97 -0.03 -0.07  0.91\n",
      "   0.03 -0.1   0.92  0.08 -0.1   0.95  0.14 -0.15  1.05  0.06 -0.09  1.04\n",
      "   0.01 -0.09  1.02 -0.12  0.04  0.92 -0.07  0.03  0.89 -0.02  0.11  1.01]\n",
      " [-0.09 -0.07  0.94 -0.08  0.    0.98  0.   -0.04  1.06 -0.08  0.09  0.92\n",
      "  -0.1   0.16  0.95 -0.03  0.14  1.02 -0.11  0.1   0.92 -0.13  0.21  0.94\n",
      "  -0.05  0.01  0.92 -0.12  0.06  0.92 -0.13  0.15  0.94 -0.04 -0.05  0.94\n",
      "  -0.11 -0.01  0.91 -0.13  0.05  0.93 -0.   -0.31  0.95 -0.08 -0.21  0.9\n",
      "   0.05 -0.05  1.    0.   -0.04  0.92 -0.07  0.04  0.89  0.04 -0.04  0.99]\n",
      " [ 0.01 -0.19  0.86 -0.06 -0.    0.85 -0.13  0.08  0.95 -0.16  0.09  0.94\n",
      "  -0.07  0.13  1.09  0.06  0.04  1.04 -0.    0.09  1.09 -0.   -0.02  0.93\n",
      "  -0.12 -0.02  0.88 -0.1   0.05  0.87 -0.17  0.09  0.94 -0.06 -0.19  0.92\n",
      "   0.01 -0.11  1.05  0.15 -0.16  1.09  0.07 -0.01  1.07  0.1  -0.05  0.99\n",
      "   0.   -0.06  0.86  0.   -0.    0.92 -0.12  0.04  0.87 -0.16  0.12  0.89]\n",
      " [-0.14  0.02  0.93 -0.09  0.1   1.06 -0.01  0.1   1.1   0.    0.09  1.09\n",
      "   0.03  0.04  1.03 -0.07  0.03  0.9   0.09 -0.18  0.91 -0.01 -0.17  0.88\n",
      "  -0.04 -0.11  0.87 -0.07 -0.03  0.93  0.03  0.04  1.06 -0.11  0.15  1.04\n",
      "   0.04  0.08  1.08  0.    0.02  0.97  0.02  0.02  1.03 -0.06 -0.05  0.92\n",
      "  -0.05  0.03  0.94 -0.16  0.04  0.9  -0.13  0.11  1.   -0.1   0.02  0.97]\n",
      " [ 0.11 -0.17  1.18 -0.08 -0.11  0.9  -0.1   0.17  0.95  0.01 -0.05  0.92\n",
      "   0.02  0.1   0.85  0.1   0.03  1.09 -0.14  0.21  0.88 -0.08  0.21  1.03\n",
      "  -0.1   0.04  0.86  0.01  0.14  1.1   0.02  0.09  1.   -0.1   0.21  0.98\n",
      "  -0.1   0.14  0.89 -0.13 -0.01  0.9   0.12 -0.16  1.12  0.04 -0.15  0.93\n",
      "   0.07 -0.04  1.08 -0.09  0.04  0.87 -0.14  0.23  0.91  0.01  0.05  0.94]\n",
      " [-0.05  0.12  1.03  0.02  0.03  1.09 -0.13  0.2   0.96 -0.05  0.15  1.06\n",
      "  -0.12  0.11  0.94 -0.09  0.17  0.99 -0.02  0.08  1.04 -0.12  0.    0.93\n",
      "  -0.05 -0.01  1.05  0.02 -0.12  1.06 -0.08 -0.1   0.99  0.02 -0.17  1.07\n",
      "   0.07 -0.26  0.93 -0.06  0.15  1.01  0.02  0.06  1.05 -0.13  0.2   0.94\n",
      "  -0.07  0.19  1.05  0.    0.05  1.09 -0.12  0.17  0.95 -0.05  0.13  1.03]\n",
      " [-0.08 -0.05  0.9  -0.13  0.05  0.96 -0.16  0.04  0.92 -0.1   0.11  1.03\n",
      "   0.04  0.09  1.09  0.05  0.02  1.07  0.1  -0.08  1.01  0.18 -0.15  1.04\n",
      "   0.11 -0.17  0.9  -0.01 -0.15  0.87 -0.08  0.    0.89 -0.15  0.09  0.95\n",
      "  -0.03  0.12  1.07 -0.11  0.13  1.07 -0.01  0.1   1.09  0.02  0.07  1.09\n",
      "   0.01  0.07  1.03 -0.06 -0.01  0.91  0.01 -0.01  0.93 -0.12  0.02  0.91]\n",
      " [ 0.02  0.01  1.05  0.06 -0.14  0.98  0.05  0.04  1.08 -0.02 -0.    0.93\n",
      "  -0.11  0.18  0.93 -0.1   0.02  0.88 -0.1   0.2   0.98 -0.13  0.15  0.96\n",
      "  -0.03  0.11  1.04  0.    0.02  0.97  0.04 -0.22  1.04 -0.03 -0.23  0.91\n",
      "  -0.1  -0.11  0.94 -0.08 -0.11  0.93 -0.03 -0.03  1.06 -0.1   0.15  0.93\n",
      "   0.02  0.06  1.06  0.01 -0.03  0.93  0.03 -0.    1.05 -0.06 -0.02  0.9 ]\n",
      " [-0.15  0.13  0.88  0.02  0.1   1.07 -0.15  0.15  0.95 -0.14  0.11  0.84\n",
      "  -0.06  0.04  1.05 -0.16  0.1   0.89  0.06 -0.18  1.1  -0.1  -0.06  0.92\n",
      "   0.01 -0.13  0.9  -0.11  0.14  0.95 -0.11  0.11  0.84  0.08  0.05  1.01\n",
      "  -0.15  0.15  0.86  0.03  0.08  0.97  0.03  0.05  0.89 -0.03  0.1   0.94\n",
      "   0.01  0.13  1.1  -0.07  0.12  0.87  0.04  0.05  1.1  -0.08  0.08  1.03]\n",
      " [-0.15  0.15  0.85  0.02  0.03  1.03 -0.14  0.23  0.97  0.    0.19  1.07\n",
      "  -0.18  0.17  0.85  0.03  0.1   1.08  0.03 -0.22  1.01  0.05 -0.09  1.12\n",
      "   0.13 -0.06  1.11 -0.12  0.09  0.85  0.09 -0.09  1.   -0.05  0.04  0.88\n",
      "   0.07  0.1   1.1  -0.13  0.15  0.84 -0.14  0.25  0.97 -0.03  0.01  0.93\n",
      "  -0.13  0.18  0.84  0.    0.14  1.05 -0.17  0.24  0.87  0.    0.07  1.1 ]\n",
      " [ 0.03  0.02  1.   -0.07 -0.06  0.9  -0.07  0.02  0.92 -0.16  0.07  0.93\n",
      "  -0.06 -0.05  0.9  -0.04  0.02  1.04  0.12 -0.04  1.07  0.15 -0.1   1.05\n",
      "   0.14 -0.15  0.95 -0.02 -0.16  0.87 -0.09 -0.01  0.88 -0.14  0.11  0.95\n",
      "  -0.14  0.05  0.94 -0.07  0.14  1.08  0.03  0.06  1.07  0.01  0.08  1.11\n",
      "  -0.03  0.02  0.95 -0.14  0.02  0.91 -0.08 -0.01  0.89 -0.13  0.08  0.99]\n",
      " [-0.02  0.08  1.06 -0.03 -0.02  0.95 -0.13  0.15  0.97 -0.11  0.06  0.94\n",
      "  -0.06  0.09  1.05 -0.04 -0.04  0.93  0.04  0.04  1.04 -0.1   0.09  0.92\n",
      "  -0.   -0.32  0.91 -0.06 -0.22  0.97  0.05 -0.31  1.04  0.01 -0.03  1.05\n",
      "  -0.01 -0.09  0.92 -0.08  0.09  0.97 -0.12  0.07  0.92 -0.02  0.08  1.05\n",
      "  -0.03 -0.02  0.97  0.02 -0.01  1.06 -0.12  0.05  0.93 -0.03  0.03  0.93]\n",
      " [ 0.18 -0.21  0.97  0.08  0.02  1.03 -0.01 -0.02  0.9  -0.13  0.    0.96\n",
      "  -0.12  0.02  0.89 -0.17  0.12  0.97 -0.14  0.02  0.93 -0.09  0.1   1.06\n",
      "  -0.01  0.1   1.1   0.    0.09  1.09  0.03  0.04  1.03 -0.07  0.03  0.9\n",
      "   0.09 -0.18  0.91 -0.01 -0.17  0.88 -0.04 -0.11  0.87 -0.07 -0.03  0.93\n",
      "   0.03  0.04  1.06 -0.11  0.15  1.04  0.04  0.08  1.08  0.    0.02  0.97]\n",
      " [-0.08  0.08  1.03 -0.11  0.06  1.01 -0.1   0.04  0.9  -0.07  0.03  0.89\n",
      "  -0.02  0.02  0.91 -0.02  0.13  1.01 -0.01  0.12  1.03 -0.01  0.11  1.05\n",
      "  -0.15  0.12  1.02 -0.16  0.08  1.    0.03 -0.21  0.89  0.08 -0.19  0.9\n",
      "   0.15 -0.21  0.93  0.06  0.02  1.05  0.    0.07  1.05 -0.04  0.06  1.04\n",
      "  -0.14  0.06  0.96 -0.11  0.04  0.91 -0.08  0.03  0.92 -0.02  0.12  0.96]\n",
      " [-0.16  0.24  0.91 -0.07  0.15  0.92 -0.03  0.19  1.05  0.08 -0.08  1.15\n",
      "  -0.13  0.03  0.92  0.03 -0.02  1.06 -0.04 -0.1   0.88  0.13 -0.07  1.11\n",
      "   0.07  0.03  1.02 -0.1   0.25  0.95 -0.01  0.04  0.92 -0.12  0.12  0.85\n",
      "   0.07  0.04  1.1  -0.16  0.09  0.96 -0.08  0.21  1.02 -0.1   0.05  0.87\n",
      "  -0.18  0.16  0.89  0.05 -0.03  1.08 -0.15  0.16  0.97  0.08 -0.13  1.11]\n",
      " [-0.12  0.08  0.91 -0.06  0.16  1.   -0.09  0.01  0.95 -0.13  0.19  0.96\n",
      "   0.    0.06  1.06 -0.12  0.12  0.9  -0.04  0.14  1.02 -0.07 -0.1   0.92\n",
      "  -0.09 -0.04  0.96  0.01 -0.11  1.02 -0.1  -0.13  0.91 -0.   -0.13  1.04\n",
      "   0.06 -0.23  1.05 -0.08  0.16  0.97  0.02  0.05  1.04  0.01 -0.04  0.91\n",
      "  -0.05  0.15  1.05  0.04 -0.03  1.05 -0.09  0.16  0.98  0.01  0.08  1.06]\n",
      " [ 0.01  0.11  1.07  0.    0.04  0.98 -0.03  0.13  1.06  0.01  0.03  1.08\n",
      "   0.03 -0.05  1.01 -0.02 -0.22  1.07  0.05 -0.3   1.09  0.07 -0.36  1.02\n",
      "  -0.03  0.13  1.04  0.04  0.05  1.08  0.06 -0.03  0.94 -0.06  0.15  1.05\n",
      "   0.01  0.06  1.08 -0.13  0.18  0.92 -0.07  0.17  1.02 -0.02  0.13  1.04\n",
      "  -0.14  0.18  0.95 -0.08  0.17  1.05  0.    0.08  1.07 -0.1  -0.2   0.95]\n",
      " [-0.13  0.08  0.99 -0.14  0.06  0.96 -0.14  0.06  0.94 -0.06  0.    0.91\n",
      "  -0.05  0.04  0.92  0.04  0.04  1.05  0.04  0.04  1.07  0.02  0.06  1.07\n",
      "  -0.03 -0.08  1.02 -0.04 -0.11  0.97 -0.03 -0.14  0.94  0.07 -0.17  0.9\n",
      "   0.1  -0.14  0.92  0.02  0.09  1.07  0.01  0.08  1.08 -0.04  0.09  1.07\n",
      "  -0.14  0.05  0.98 -0.14  0.04  0.93 -0.12  0.01  0.91 -0.04  0.09  0.92]\n",
      " [-0.11  0.22  0.94 -0.01  0.18  1.06 -0.14  0.14  0.86  0.05  0.01  1.06\n",
      "  -0.06  0.07  0.92 -0.03  0.2   1.05  0.04  0.05  1.09 -0.12  0.18  0.99\n",
      "  -0.05 -0.    0.95 -0.05 -0.12  0.88  0.14 -0.26  1.09 -0.04 -0.01  0.99\n",
      "   0.05  0.16  1.07 -0.12  0.13  0.84 -0.11  0.26  0.96 -0.04  0.06  0.9\n",
      "   0.03  0.13  1.08  0.03  0.05  1.04 -0.13  0.23  0.97 -0.06 -0.01  0.91]\n",
      " [ 0.    0.03  1.04 -0.05 -0.19  1.05  0.07 -0.32  1.09 -0.02 -0.32  0.94\n",
      "   0.07 -0.06  1.   -0.05 -0.01  0.88  0.   -0.02  0.94 -0.1   0.13  0.93\n",
      "  -0.07  0.19  1.03 -0.13  0.15  0.92 -0.03  0.16  1.03  0.01 -0.02  0.96\n",
      "   0.01  0.07  1.1  -0.04  0.04  0.93 -0.14  0.17  1.01 -0.07 -0.24  0.88\n",
      "  -0.08 -0.16  1.   -0.1   0.03  0.94  0.02  0.01  1.05  0.06 -0.14  0.98]\n",
      " [ 0.03 -0.2   0.87 -0.04 -0.14  0.94  0.09 -0.12  1.07 -0.03  0.06  1.02\n",
      "   0.07  0.05  1.09 -0.02  0.12  1.07  0.07  0.04  1.03 -0.03 -0.    0.91\n",
      "  -0.01 -0.04  0.95 -0.09 -0.04  0.89 -0.15  0.05  0.96 -0.18  0.09  0.93\n",
      "  -0.09  0.13  1.04 -0.06 -0.04  0.99  0.09 -0.09  1.07  0.16 -0.17  1.01\n",
      "   0.12 -0.05  1.07  0.09 -0.11  0.91 -0.03 -0.06  0.88 -0.04  0.01  0.9 ]\n",
      " [-0.09 -0.04  0.89 -0.15  0.05  0.96 -0.18  0.09  0.93 -0.09  0.13  1.04\n",
      "  -0.06 -0.04  0.99  0.09 -0.09  1.07  0.16 -0.17  1.01  0.12 -0.05  1.07\n",
      "   0.09 -0.11  0.91 -0.03 -0.06  0.88 -0.04  0.01  0.9  -0.14  0.09  0.94\n",
      "  -0.11 -0.01  0.9  -0.15  0.07  1.   -0.03  0.12  1.09 -0.08  0.09  1.07\n",
      "   0.05  0.08  1.09 -0.01  0.05  0.95  0.11 -0.1   0.99 -0.   -0.14  0.88]\n",
      " [-0.07  0.15  0.9   0.02  0.13  1.11 -0.07  0.05  0.88  0.07 -0.03  1.09\n",
      "  -0.03 -0.01  1.08  0.12 -0.2   1.06  0.07 -0.11  1.1  -0.09 -0.06  0.9\n",
      "   0.06  0.06  1.1  -0.11  0.13  0.94 -0.09  0.09  0.85 -0.08  0.08  0.86\n",
      "  -0.01  0.04  0.99 -0.07 -0.01  0.86 -0.01  0.09  0.91 -0.06 -0.02  0.93\n",
      "  -0.11 -0.02  0.96 -0.12  0.2   0.91 -0.01  0.05  0.94 -0.11  0.1   0.86]\n",
      " [ 0.13 -0.2   1.09  0.05 -0.13  1.09 -0.06 -0.11  0.91  0.05  0.03  1.07\n",
      "  -0.13  0.1   0.88  0.06  0.05  1.1  -0.15  0.15  0.91 -0.07  0.1   0.86\n",
      "  -0.12  0.16  0.96 -0.14  0.09  0.86  0.03  0.08  1.02 -0.16  0.16  0.84\n",
      "  -0.01  0.11  0.97 -0.1  -0.06  0.9   0.02 -0.15  0.98  0.12 -0.15  1.15\n",
      "  -0.   -0.    0.9   0.12 -0.04  1.1  -0.02  0.04  1.03  0.07  0.07  1.02]\n",
      " [ 0.02  0.07  1.05 -0.05  0.02  0.9   0.04 -0.06  0.97 -0.06 -0.06  0.88\n",
      "  -0.1   0.01  0.96 -0.01 -0.15  0.89  0.01 -0.11  0.99  0.13 -0.1   1.07\n",
      "  -0.04  0.12  1.06  0.07  0.06  1.08  0.02  0.02  1.09  0.04  0.03  1.02\n",
      "  -0.03 -0.04  0.9  -0.01  0.01  0.96 -0.1  -0.01  0.87 -0.15  0.05  0.94\n",
      "  -0.15  0.05  0.91 -0.14  0.12  1.01  0.01  0.08  1.09  0.07 -0.13  1.06]\n",
      " [-0.13  0.16  0.92 -0.03  0.08  0.92 -0.08  0.04  0.88 -0.12  0.13  0.96\n",
      "   0.02 -0.18  0.97 -0.04 -0.15  0.92  0.06 -0.25  1.1   0.06 -0.31  1.01\n",
      "   0.01 -0.27  0.94  0.03  0.06  1.07  0.06 -0.01  1.03  0.03 -0.03  0.94\n",
      "  -0.02  0.11  1.03  0.03  0.03  1.07 -0.11  0.19  0.96 -0.05  0.15  1.02\n",
      "  -0.    0.11  1.05 -0.13  0.13  0.95 -0.07  0.1   1.04 -0.    0.02  1.05]\n",
      " [ 0.01 -0.15  0.97  0.04 -0.07  1.1   0.15 -0.13  1.12 -0.1   0.09  0.88\n",
      "   0.11 -0.14  0.99 -0.05  0.06  0.85  0.08  0.11  1.11  0.04 -0.03  1.03\n",
      "  -0.1   0.21  0.98 -0.05  0.03  0.92 -0.16  0.16  0.84  0.02  0.12  1.06\n",
      "  -0.02 -0.02  0.97 -0.02  0.07  1.09 -0.12  0.03  0.9  -0.1  -0.03  0.86\n",
      "   0.12 -0.25  1.04 -0.02  0.    1.01  0.05  0.12  1.09 -0.13  0.16  0.83]\n",
      " [-0.06  0.03  0.91 -0.1   0.18  0.97  0.03  0.    1.02  0.01 -0.17  1.04\n",
      "  -0.01 -0.32  0.93  0.08 -0.14  1.06 -0.08 -0.06  0.93  0.   -0.02  1.04\n",
      "  -0.13  0.1   0.91  0.03  0.05  1.04 -0.07  0.04  0.93  0.03 -0.03  1.03\n",
      "  -0.14  0.11  0.9  -0.06  0.01  0.91 -0.11  0.18  0.94  0.01  0.04  1.05\n",
      "  -0.05  0.02  1.02  0.03 -0.21  0.99 -0.1  -0.07  0.88 -0.01 -0.27  0.92]\n",
      " [ 0.01  0.12  1.04 -0.03  0.1   1.03 -0.16  0.08  0.99 -0.18  0.07  0.95\n",
      "   0.05 -0.14  0.92  0.13 -0.14  0.97  0.16 -0.15  1.03 -0.02 -0.03  1.03\n",
      "  -0.06 -0.04  0.99 -0.08 -0.06  0.89  0.02  0.05  0.93  0.06  0.06  1.\n",
      "   0.03  0.09  1.06 -0.09  0.1   1.04 -0.14  0.07  1.   -0.1   0.08  0.87\n",
      "  -0.06  0.1   0.91  0.01  0.07  1.    0.01 -0.01  1.07 -0.01 -0.04  1.04]\n",
      " [-0.06 -0.17  1.01  0.05 -0.33  1.08  0.03 -0.34  0.96  0.05  0.02  1.09\n",
      "   0.03 -0.07  0.94 -0.08  0.05  1.1  -0.01  0.    0.95 -0.11  0.14  0.91\n",
      "  -0.04 -0.04  0.91 -0.12  0.11  0.92 -0.07  0.17  1.03 -0.13  0.18  0.94\n",
      "  -0.04  0.17  1.07  0.02  0.04  1.05  0.01 -0.21  1.05  0.05 -0.31  1.01\n",
      "   0.04 -0.09  1.07  0.05 -0.15  0.97 -0.07 -0.07  0.9   0.01 -0.03  0.94]\n",
      " [ 0.04 -0.22  1.04 -0.03 -0.23  0.91 -0.1  -0.11  0.94 -0.08 -0.11  0.93\n",
      "  -0.03 -0.03  1.06 -0.1   0.15  0.93  0.02  0.06  1.06  0.01 -0.03  0.93\n",
      "   0.03 -0.    1.05 -0.06 -0.02  0.9  -0.12  0.2   0.95 -0.11  0.08  0.94\n",
      "  -0.09  0.18  1.02  0.01  0.06  0.91 -0.01 -0.04  1.03  0.02 -0.16  0.96\n",
      "   0.06 -0.3   1.11  0.02 -0.28  0.95 -0.09 -0.12  0.94 -0.04  0.01  0.87]\n",
      " [ 0.1  -0.2   0.93  0.15 -0.12  1.05  0.11 -0.06  1.07  0.08 -0.05  1.06\n",
      "  -0.13  0.08  0.99 -0.14  0.06  0.96 -0.14  0.06  0.94 -0.06  0.    0.91\n",
      "  -0.05  0.04  0.92  0.04  0.04  1.05  0.04  0.04  1.07  0.02  0.06  1.07\n",
      "  -0.03 -0.08  1.02 -0.04 -0.11  0.97 -0.03 -0.14  0.94  0.07 -0.17  0.9\n",
      "   0.1  -0.14  0.92  0.02  0.09  1.07  0.01  0.08  1.08 -0.04  0.09  1.07]\n",
      " [ 0.04  0.04  1.08 -0.01 -0.13  1.01 -0.02 -0.19  0.97 -0.01 -0.19  0.93\n",
      "  -0.01  0.04  0.9   0.05  0.04  0.96 -0.02  0.13  1.08 -0.08  0.12  1.08\n",
      "  -0.11  0.11  1.06 -0.14  0.04  0.88 -0.13  0.06  0.85 -0.09  0.1   0.87\n",
      "   0.05  0.05  1.08  0.01  0.06  1.1  -0.06  0.08  0.99 -0.02 -0.17  0.94\n",
      "   0.03 -0.22  0.88  0.03  0.04  0.96  0.06  0.06  1.02  0.04  0.09  1.05]\n",
      " [ 0.04 -0.01  1.01 -0.05  0.17  1.05  0.    0.06  1.07 -0.09  0.19  1.02\n",
      "  -0.01  0.06  1.07  0.01  0.07  1.01 -0.02 -0.16  1.05  0.05 -0.25  1.06\n",
      "   0.02 -0.29  0.95  0.03 -0.12  1.08  0.07 -0.21  1.03  0.01  0.08  1.04\n",
      "   0.04  0.02  1.09  0.02 -0.03  0.94 -0.06  0.12  1.05  0.02 -0.04  1.01\n",
      "  -0.04 -0.03  0.93  0.    0.08  1.07 -0.01  0.04  0.94 -0.09  0.06  0.88]\n",
      " [-0.12  0.13  0.83  0.09  0.05  1.07  0.01  0.05  0.96 -0.05  0.16  1.04\n",
      "  -0.09  0.1   0.86 -0.16  0.17  0.88  0.01  0.11  1.03 -0.14  0.21  0.95\n",
      "  -0.02  0.14  1.08 -0.13  0.11  0.86  0.05  0.03  0.89  0.14 -0.25  1.08\n",
      "  -0.06 -0.05  1.    0.02  0.09  1.07 -0.09  0.09  0.84  0.07  0.07  1.12\n",
      "   0.07  0.03  1.02 -0.12  0.21  0.96 -0.01  0.07  0.93 -0.12  0.09  0.85]\n",
      " [-0.11  0.01  0.88 -0.15  0.13  0.96 -0.15  0.06  0.94 -0.1   0.13  1.07\n",
      "   0.03  0.07  1.08 -0.    0.09  1.1  -0.02  0.05  0.99 -0.12 -0.    1.01\n",
      "  -0.07 -0.02  0.89 -0.13  0.08  0.96  0.01 -0.16  0.92  0.08 -0.11  1.05\n",
      "   0.19 -0.19  1.02  0.06  0.06  1.08 -0.    0.01  0.93 -0.13  0.04  0.89\n",
      "  -0.1   0.    0.89 -0.18  0.14  1.   -0.15  0.05  0.98 -0.04  0.11  1.1 ]\n",
      " [-0.02 -0.13  1.02 -0.08  0.05  0.95 -0.1   0.17  0.97 -0.07 -0.02  0.92\n",
      "  -0.16  0.14  0.92 -0.01  0.1   1.04 -0.12  0.06  0.92 -0.07  0.14  1.\n",
      "   0.03  0.04  1.06 -0.09  0.17  0.97  0.02  0.01  1.06 -0.09 -0.2   0.93\n",
      "   0.02 -0.28  1.04  0.04 -0.27  1.01 -0.02  0.04  1.03  0.05 -0.    1.05\n",
      "  -0.04 -0.01  0.92 -0.    0.09  1.04 -0.   -0.04  0.99 -0.1   0.09  1.03]\n",
      " [ 0.12 -0.06  1.05 -0.16  0.09  1.   -0.16  0.06  0.95 -0.14  0.03  0.92\n",
      "  -0.02  0.02  0.96  0.01  0.04  0.99  0.01  0.07  1.07 -0.02  0.06  1.06\n",
      "  -0.07  0.08  1.02 -0.09 -0.04  0.93 -0.03 -0.11  0.91  0.01 -0.11  0.91\n",
      "   0.19 -0.18  1.04  0.22 -0.21  1.05 -0.11  0.12  1.03 -0.15  0.09  1.\n",
      "  -0.15  0.06  0.95 -0.06 -0.02  0.92 -0.03 -0.    0.94  0.01  0.    0.97]\n",
      " [ 0.22 -0.21  1.05 -0.11  0.12  1.03 -0.15  0.09  1.   -0.15  0.06  0.95\n",
      "  -0.06 -0.02  0.92 -0.03 -0.    0.94  0.01  0.    0.97  0.01  0.08  1.07\n",
      "   0.02  0.05  1.07 -0.04  0.06  1.06 -0.08  0.01  0.98 -0.12 -0.01  0.92\n",
      "  -0.06 -0.06  0.9   0.16 -0.18  0.97  0.22 -0.25  1.03 -0.02  0.09  1.05\n",
      "  -0.07  0.09  1.04 -0.09  0.08  1.02 -0.15  0.03  0.93 -0.12 -0.01  0.92]\n",
      " [ 0.05 -0.28  0.98 -0.04  0.08  1.03  0.05 -0.    1.04  0.01 -0.03  0.93\n",
      "  -0.01  0.09  1.04  0.02 -0.02  1.02 -0.01 -0.06  0.96 -0.01  0.11  1.04\n",
      "   0.01  0.04  1.   -0.04  0.02  1.05  0.02 -0.01  1.07  0.02 -0.1   0.98\n",
      "   0.   -0.23  1.06  0.06 -0.3   1.1   0.05 -0.36  0.98  0.01  0.08  1.03\n",
      "   0.06  0.01  1.05  0.04 -0.04  0.93 -0.01  0.08  1.04  0.05 -0.01  1.03]\n",
      " [-0.14  0.02  0.91 -0.08 -0.01  0.89 -0.13  0.08  0.99  0.04  0.05  0.92\n",
      "   0.11 -0.12  1.03  0.18 -0.17  1.04  0.04  0.09  1.08  0.01  0.01  0.94\n",
      "  -0.11  0.04  0.9  -0.09 -0.02  0.9  -0.17  0.12  0.99 -0.03  0.13  1.08\n",
      "  -0.02  0.09  1.1   0.03  0.05  1.03  0.06  0.06  1.08 -0.02 -0.01  0.93\n",
      "  -0.16  0.05  0.92 -0.   -0.2   0.88  0.   -0.15  0.99  0.14 -0.12  1.07]\n",
      " [-0.06 -0.05  0.91  0.05 -0.3   1.02 -0.1  -0.18  0.9   0.05 -0.27  1.06\n",
      "  -0.04  0.13  1.01  0.04 -0.03  0.99 -0.11  0.12  0.89 -0.08  0.05  0.93\n",
      "  -0.06  0.11  1.02  0.02  0.08  0.89 -0.01  0.04  1.06 -0.07 -0.01  0.93\n",
      "   0.01  0.    0.99 -0.12  0.12  0.89  0.    0.05  1.04 -0.02 -0.2   1.01\n",
      "   0.04 -0.32  1.01 -0.08 -0.25  0.9  -0.05 -0.06  0.92 -0.06  0.11  1.  ]\n",
      " [-0.04  0.03  0.91  0.06 -0.08  0.93 -0.06 -0.07  0.9  -0.07  0.    0.86\n",
      "  -0.02 -0.1   0.94  0.13 -0.11  1.05 -0.06  0.13  1.05  0.07  0.03  1.06\n",
      "  -0.01  0.01  0.93  0.02 -0.01  0.96 -0.11 -0.02  0.9  -0.17  0.13  1.\n",
      "  -0.14  0.04  0.96 -0.03  0.09  1.09 -0.05  0.1   1.06  0.06  0.05  1.06\n",
      "  -0.01 -0.02  0.93  0.12 -0.2   0.93  0.02 -0.21  0.87  0.01 -0.12  1.  ]\n",
      " [-0.11  0.03  0.92 -0.08  0.03  0.9  -0.04  0.13  0.97  0.    0.11  1.02\n",
      "  -0.01  0.11  1.04 -0.14  0.11  1.01 -0.18  0.09  0.98 -0.17  0.07  0.94\n",
      "   0.11 -0.2   0.92  0.14 -0.17  0.97  0.04  0.02  1.06 -0.    0.03  1.05\n",
      "  -0.06  0.05  1.03 -0.13  0.05  0.93 -0.09  0.03  0.9  -0.05  0.03  0.9\n",
      "  -0.02  0.13  1.02  0.01  0.12  1.04 -0.03  0.1   1.03 -0.16  0.08  0.99]\n",
      " [ 0.04 -0.02  0.97 -0.08 -0.01  0.88 -0.14  0.05  0.95 -0.13 -0.01  0.9\n",
      "  -0.14  0.08  0.99 -0.02  0.08  1.11 -0.07  0.12  1.06  0.06  0.04  1.1\n",
      "   0.15 -0.16  1.08  0.18 -0.19  0.99  0.1  -0.22  0.87  0.03 -0.01  0.91\n",
      "  -0.08  0.    0.86 -0.04 -0.01  0.88 -0.14  0.05  0.91 -0.15  0.14  1.04\n",
      "  -0.13  0.01  0.95 -0.08  0.08  1.09 -0.02  0.11  1.1  -0.04  0.1   1.08]\n",
      " [-0.06  0.09  0.89  0.02  0.13  1.08 -0.08  0.16  1.03  0.03  0.03  1.05\n",
      "  -0.    0.05  1.09 -0.16  0.12  0.89  0.11 -0.19  1.14 -0.06 -0.07  0.99\n",
      "   0.07  0.03  1.1  -0.04  0.15  1.01 -0.12  0.14  0.84 -0.01  0.1   1.06\n",
      "  -0.14  0.17  0.87 -0.03  0.08  0.9  -0.15  0.18  0.92 -0.1   0.07  0.86\n",
      "   0.04  0.1   0.99 -0.16  0.16  0.85  0.    0.08  0.99 -0.08 -0.08  0.9 ]\n",
      " [-0.   -0.13  0.92 -0.11  0.    0.93 -0.07 -0.01  1.03 -0.09 -0.11  0.96\n",
      "   0.01 -0.2   1.08 -0.09  0.17  0.96  0.04  0.05  1.06  0.04 -0.02  0.94\n",
      "   0.03  0.03  1.09 -0.01 -0.02  0.95 -0.11  0.16  0.93 -0.06 -0.02  0.9\n",
      "  -0.13  0.14  0.93 -0.12  0.09  0.9  -0.13  0.16  0.98  0.    0.03  1.04\n",
      "  -0.05 -0.19  1.05  0.07 -0.32  1.09 -0.02 -0.32  0.94  0.07 -0.06  1.  ]\n",
      " [ 0.02 -0.22  1.05 -0.12  0.15  0.88 -0.01  0.08  1.04  0.05  0.02  1.01\n",
      "  -0.06  0.12  1.01  0.03 -0.    1.05 -0.05  0.01  0.93 -0.    0.09  1.06\n",
      "  -0.01  0.02  0.95 -0.11  0.03  1.05  0.03  0.04  1.   -0.08  0.09  0.92\n",
      "   0.05 -0.28  1.05 -0.04 -0.27  0.91 -0.09 -0.21  0.94 -0.01 -0.05  0.94\n",
      "  -0.11  0.06  0.9  -0.03  0.1   1.04 -0.11  0.08  0.93 -0.09  0.16  1.  ]\n",
      " [-0.09 -0.16  0.88 -0.   -0.13  1.04 -0.01 -0.24  0.92  0.07 -0.23  1.06\n",
      "  -0.08 -0.11  0.93  0.01 -0.12  1.05 -0.11  0.11  0.91  0.04  0.02  1.05\n",
      "  -0.08  0.13  1.03  0.02 -0.04  1.02 -0.15  0.11  0.9  -0.07  0.03  0.92\n",
      "  -0.09  0.12  1.01  0.01  0.02  1.    0.01 -0.01  1.03 -0.06 -0.05  0.91\n",
      "   0.05 -0.3   1.02 -0.1  -0.18  0.9   0.05 -0.27  1.06 -0.04  0.13  1.01]\n",
      " [ 0.06 -0.01  1.02 -0.03 -0.05  0.9  -0.12  0.03  0.94 -0.01 -0.17  0.89\n",
      "   0.03 -0.11  1.03  0.18 -0.14  0.97  0.03  0.1   1.07  0.04  0.05  0.99\n",
      "   0.05  0.04  1.02 -0.05 -0.02  0.9  -0.16  0.06  0.94 -0.14 -0.    0.92\n",
      "  -0.08  0.08  1.06  0.01  0.09  1.09  0.03  0.08  1.09  0.01  0.01  0.96\n",
      "   0.15 -0.18  0.96  0.04 -0.21  0.86 -0.01 -0.11  0.96 -0.1   0.01  0.92]\n",
      " [-0.02  0.12  1.05 -0.11 -0.15  0.93 -0.08 -0.14  1.01 -0.01 -0.23  1.05\n",
      "  -0.09 -0.01  0.93 -0.09  0.07  0.97 -0.01  0.    0.9  -0.08  0.09  0.92\n",
      "  -0.1   0.15  0.93 -0.03 -0.04  0.93 -0.1   0.07  0.89 -0.13  0.16  0.92\n",
      "  -0.03  0.08  0.92 -0.08  0.04  0.88 -0.12  0.13  0.96  0.02 -0.18  0.97\n",
      "  -0.04 -0.15  0.92  0.06 -0.25  1.1   0.06 -0.31  1.01  0.01 -0.27  0.94]\n",
      " [-0.06  0.05  1.03 -0.13  0.05  0.93 -0.09  0.03  0.9  -0.05  0.03  0.9\n",
      "  -0.02  0.13  1.02  0.01  0.12  1.04 -0.03  0.1   1.03 -0.16  0.08  0.99\n",
      "  -0.18  0.07  0.95  0.05 -0.14  0.92  0.13 -0.14  0.97  0.16 -0.15  1.03\n",
      "  -0.02 -0.03  1.03 -0.06 -0.04  0.99 -0.08 -0.06  0.89  0.02  0.05  0.93\n",
      "   0.06  0.06  1.    0.03  0.09  1.06 -0.09  0.1   1.04 -0.14  0.07  1.  ]\n",
      " [-0.11  0.03  0.91 -0.12  0.15  0.94 -0.07  0.04  0.91 -0.13  0.11  0.93\n",
      "   0.03 -0.34  0.94 -0.08 -0.23  0.9  -0.1  -0.15  0.94 -0.   -0.07  0.91\n",
      "  -0.09  0.06  0.92 -0.1   0.15  0.95 -0.05  0.03  0.9  -0.11  0.15  0.93\n",
      "  -0.1   0.22  0.93 -0.09 -0.03  0.86 -0.13  0.13  0.92 -0.04  0.04  0.93\n",
      "  -0.12  0.11  0.91 -0.14  0.16  0.95 -0.01 -0.3   0.93 -0.08 -0.2   0.91]\n",
      " [-0.02  0.09  1.09  0.03 -0.02  0.97  0.02  0.07  1.05 -0.05  0.02  0.9\n",
      "   0.04 -0.06  0.97 -0.06 -0.06  0.88 -0.1   0.01  0.96 -0.01 -0.15  0.89\n",
      "   0.01 -0.11  0.99  0.13 -0.1   1.07 -0.04  0.12  1.06  0.07  0.06  1.08\n",
      "   0.02  0.02  1.09  0.04  0.03  1.02 -0.03 -0.04  0.9  -0.01  0.01  0.96\n",
      "  -0.1  -0.01  0.87 -0.15  0.05  0.94 -0.15  0.05  0.91 -0.14  0.12  1.01]\n",
      " [-0.04 -0.04  0.9  -0.13  0.04  0.95 -0.11  0.12  1.06 -0.16  0.07  0.97\n",
      "  -0.01  0.09  1.08  0.04  0.06  1.07  0.08 -0.01  1.09  0.07 -0.08  0.97\n",
      "   0.17 -0.16  0.99  0.07 -0.2   0.87 -0.05 -0.1   0.89 -0.11  0.03  0.89\n",
      "  -0.14  0.12  0.97 -0.01  0.12  1.07 -0.09  0.14  1.06  0.03  0.07  1.09\n",
      "   0.02 -0.04  1.09  0.01  0.04  1.01 -0.09 -0.01  0.89 -0.03 -0.03  0.93]\n",
      " [ 0.13 -0.23  1.09 -0.05 -0.05  1.    0.04 -0.21  0.96 -0.11  0.09  0.83\n",
      "   0.09  0.07  1.1  -0.15  0.21  0.87 -0.07  0.19  1.02 -0.09  0.08  0.86\n",
      "  -0.17  0.18  0.87  0.02  0.06  1.02 -0.13  0.22  0.97  0.02  0.13  1.07\n",
      "  -0.18  0.16  0.87  0.09  0.03  1.12  0.07 -0.23  1.01  0.04 -0.11  1.09\n",
      "  -0.02 -0.14  1.1  -0.13  0.15  0.86  0.1  -0.01  1.06  0.02  0.05  0.93]\n",
      " [-0.06  0.06  0.91  0.04 -0.34  0.98 -0.07 -0.28  0.88 -0.08 -0.18  0.98\n",
      "  -0.06 -0.06  0.89 -0.1   0.06  0.92 -0.02  0.08  1.05 -0.1   0.12  0.93\n",
      "  -0.09  0.17  0.99 -0.1   0.05  0.88 -0.13  0.16  0.94 -0.06  0.18  1.05\n",
      "  -0.13  0.14  0.95 -0.11  0.19  1.   -0.01  0.08  1.06 -0.1  -0.12  0.95\n",
      "  -0.02 -0.17  1.05 -0.08 -0.06  0.93 -0.07 -0.02  1.    0.02 -0.1   1.06]\n",
      " [ 0.02 -0.03  0.94 -0.06  0.12  1.05  0.02 -0.04  1.01 -0.04 -0.03  0.93\n",
      "   0.    0.08  1.07 -0.01  0.04  0.94 -0.09  0.06  0.88  0.03 -0.15  1.\n",
      "  -0.02 -0.15  0.93 -0.1  -0.1   0.91  0.04 -0.31  0.96 -0.06 -0.22  0.88\n",
      "   0.05 -0.03  1.01 -0.   -0.03  0.91 -0.08  0.07  0.91  0.01 -0.04  0.97\n",
      "  -0.06  0.02  0.89 -0.12  0.16  0.94 -0.02  0.    0.91 -0.1   0.01  0.89]\n",
      " [-0.11  0.06  1.01 -0.1   0.04  0.9  -0.07  0.03  0.89 -0.02  0.02  0.91\n",
      "  -0.02  0.13  1.01 -0.01  0.12  1.03 -0.01  0.11  1.05 -0.15  0.12  1.02\n",
      "  -0.16  0.08  1.    0.03 -0.21  0.89  0.08 -0.19  0.9   0.15 -0.21  0.93\n",
      "   0.06  0.02  1.05  0.    0.07  1.05 -0.04  0.06  1.04 -0.14  0.06  0.96\n",
      "  -0.11  0.04  0.91 -0.08  0.03  0.92 -0.02  0.12  0.96 -0.01  0.13  1.02]\n",
      " [-0.08  0.16  1.   -0.05 -0.25  0.93 -0.06 -0.21  0.96  0.04 -0.24  0.92\n",
      "  -0.11  0.08  0.93  0.01  0.06  1.05 -0.13  0.1   0.9  -0.04  0.13  1.04\n",
      "   0.03  0.01  1.04 -0.09  0.17  1.   -0.    0.06  1.06 -0.02 -0.02  0.93\n",
      "  -0.01  0.13  1.07  0.    0.02  0.96 -0.12  0.14  0.92  0.03 -0.25  0.98\n",
      "  -0.07 -0.22  0.92  0.06 -0.16  1.04 -0.06 -0.08  0.92 -0.09  0.03  0.95]\n",
      " [-0.07  0.04  0.93  0.03 -0.03  1.03 -0.14  0.11  0.9  -0.06  0.01  0.91\n",
      "  -0.11  0.18  0.94  0.01  0.04  1.05 -0.05  0.02  1.02  0.03 -0.21  0.99\n",
      "  -0.1  -0.07  0.88 -0.01 -0.27  0.92 -0.07 -0.12  0.97  0.08 -0.24  0.93\n",
      "  -0.02  0.11  1.03  0.04 -0.01  0.99 -0.02  0.08  1.06 -0.02 -0.03  0.97\n",
      "  -0.14  0.12  0.92 -0.06 -0.02  0.91 -0.1   0.15  0.96  0.02  0.02  1.07]\n",
      " [-0.    0.04  0.93 -0.01  0.11  1.05 -0.02  0.11  1.07 -0.02  0.09  1.08\n",
      "  -0.17  0.1   0.97 -0.17  0.09  0.94 -0.16  0.09  0.9   0.11 -0.17  0.93\n",
      "   0.15 -0.19  0.99  0.08 -0.05  1.06  0.   -0.01  1.06 -0.03  0.01  1.02\n",
      "  -0.13  0.05  0.94 -0.11  0.03  0.9  -0.08  0.04  0.89 -0.03  0.12  0.97\n",
      "  -0.03  0.13  1.02 -0.06  0.09  1.05 -0.1   0.09  1.03 -0.15  0.1   1.  ]\n",
      " [-0.1   0.03  0.89 -0.04  0.03  0.9  -0.01  0.09  1.05 -0.02  0.12  1.07\n",
      "  -0.04  0.1   1.08 -0.15  0.06  0.94 -0.15  0.05  0.91  0.01 -0.07  0.94\n",
      "   0.09 -0.09  0.98  0.12 -0.09  1.05  0.07 -0.13  1.04  0.01 -0.12  1.01\n",
      "  -0.03 -0.11  0.95 -0.06  0.03  0.89 -0.02  0.02  0.92  0.03  0.04  1.06\n",
      "  -0.03  0.12  1.08 -0.06  0.09  1.06 -0.16  0.04  0.94 -0.14  0.04  0.89]\n",
      " [-0.15  0.06  0.98 -0.12  0.05  0.94 -0.07  0.09  0.95 -0.02  0.12  1.03\n",
      "  -0.02  0.12  1.05 -0.14  0.1   1.01 -0.17  0.06  0.96 -0.15  0.08  0.91\n",
      "   0.11 -0.12  0.96  0.14 -0.12  1.03  0.13 -0.08  1.06 -0.03 -0.06  1.\n",
      "  -0.05 -0.08  0.94 -0.03  0.03  0.9   0.04  0.04  0.95  0.05  0.07  1.02\n",
      "  -0.07  0.11  1.06 -0.11  0.09  1.04 -0.14  0.06  1.   -0.13  0.08  0.87]\n",
      " [ 0.06 -0.01  1.05  0.03 -0.01  1.05 -0.13  0.05  0.95 -0.12  0.05  0.91\n",
      "  -0.08  0.02  0.89 -0.02  0.1   1.   -0.02  0.11  1.05 -0.08  0.08  1.03\n",
      "  -0.14  0.09  0.99 -0.16  0.06  0.94  0.03 -0.13  0.91  0.08 -0.13  0.93\n",
      "   0.15 -0.16  1.01  0.08 -0.11  1.06  0.   -0.05  1.02 -0.12  0.04  0.92\n",
      "  -0.1   0.03  0.89 -0.04  0.03  0.9  -0.01  0.09  1.05 -0.02  0.12  1.07]\n",
      " [-0.02 -0.04  0.95 -0.    0.08  1.07  0.    0.06  0.99 -0.05  0.05  0.92\n",
      "   0.04 -0.07  1.08  0.01 -0.15  0.96 -0.08 -0.11  0.88  0.07 -0.31  1.03\n",
      "   0.   -0.29  0.94  0.04  0.05  1.1   0.06 -0.03  0.97 -0.02 -0.    0.9\n",
      "   0.02  0.03  1.09  0.03 -0.05  0.98 -0.03 -0.01  0.93  0.01  0.11  1.07\n",
      "   0.    0.04  0.98 -0.03  0.13  1.06  0.01  0.03  1.08  0.03 -0.05  1.01]\n",
      " [ 0.18 -0.19  0.99  0.1  -0.22  0.87  0.03 -0.01  0.91 -0.08  0.    0.86\n",
      "  -0.04 -0.01  0.88 -0.14  0.05  0.91 -0.15  0.14  1.04 -0.13  0.01  0.95\n",
      "  -0.08  0.08  1.09 -0.02  0.11  1.1  -0.04  0.1   1.08  0.05  0.07  1.1\n",
      "   0.11 -0.11  1.08  0.18 -0.18  1.02  0.1  -0.2   0.91  0.1  -0.06  0.95\n",
      "   0.01 -0.1   0.86 -0.09  0.01  0.92 -0.13  0.04  0.88 -0.15  0.12  0.98]\n",
      " [-0.08  0.04  0.85  0.09  0.09  1.09 -0.14  0.2   0.85 -0.05  0.22  1.01\n",
      "  -0.07  0.06  0.86 -0.15  0.15  0.85  0.02  0.03  1.03 -0.14  0.23  0.97\n",
      "   0.    0.19  1.07 -0.18  0.17  0.85  0.03  0.1   1.08  0.03 -0.22  1.01\n",
      "   0.05 -0.09  1.12  0.13 -0.06  1.11 -0.12  0.09  0.85  0.09 -0.09  1.\n",
      "  -0.05  0.04  0.88  0.07  0.1   1.1  -0.13  0.15  0.84 -0.14  0.25  0.97]\n",
      " [ 0.02  0.08  1.05 -0.14  0.23  0.97  0.09 -0.17  1.13 -0.08 -0.1   0.9\n",
      "   0.13 -0.25  1.11  0.02 -0.04  0.93  0.04  0.11  1.05 -0.11  0.08  0.85\n",
      "  -0.13  0.23  0.9  -0.    0.05  0.94 -0.14  0.11  0.87  0.06  0.04  1.06\n",
      "  -0.14  0.24  0.95  0.01  0.18  1.05 -0.19  0.21  0.84 -0.02  0.13  1.\n",
      "  -0.05 -0.13  0.94  0.08 -0.09  1.16 -0.1  -0.21  1.04 -0.03  0.04  0.98]\n",
      " [-0.12  0.08  1.   -0.14  0.06  0.97 -0.12  0.05  0.93 -0.03  0.07  0.93\n",
      "  -0.02  0.09  0.98 -0.03  0.13  1.02 -0.04  0.07  1.04 -0.09  0.1   1.01\n",
      "  -0.14 -0.04  0.94 -0.02 -0.11  0.91  0.01 -0.11  0.91  0.16 -0.16  1.02\n",
      "   0.15 -0.13  1.04  0.1  -0.1   1.06 -0.12  0.09  1.01 -0.13  0.06  0.97\n",
      "  -0.12  0.04  0.94 -0.03  0.05  0.94 -0.01  0.08  0.98 -0.    0.08  1.05]\n",
      " [ 0.09  0.02  1.02 -0.1   0.21  0.96  0.04  0.14  1.09 -0.13  0.16  0.85\n",
      "   0.09  0.02  1.05 -0.03 -0.01  0.93 -0.03  0.2   1.06  0.04  0.09  1.09\n",
      "  -0.18  0.23  0.88 -0.02  0.1   0.97 -0.04 -0.18  0.92  0.11 -0.17  1.18\n",
      "  -0.08 -0.11  0.9  -0.1   0.17  0.95  0.01 -0.05  0.92  0.02  0.1   0.85\n",
      "   0.1   0.03  1.09 -0.14  0.21  0.88 -0.08  0.21  1.03 -0.1   0.04  0.86]\n",
      " [-0.16  0.2   0.89 -0.04  0.09  1.07 -0.1   0.03  0.89  0.06 -0.01  1.14\n",
      "   0.13 -0.23  1.06 -0.04 -0.02  1.01  0.02 -0.19  0.92 -0.12  0.13  0.83\n",
      "   0.09  0.05  1.07  0.01  0.05  0.96 -0.05  0.16  1.04 -0.09  0.1   0.86\n",
      "  -0.16  0.17  0.88  0.01  0.11  1.03 -0.14  0.21  0.95 -0.02  0.14  1.08\n",
      "  -0.13  0.11  0.86  0.05  0.03  0.89  0.14 -0.25  1.08 -0.06 -0.05  1.  ]\n",
      " [ 0.03  0.12  1.06 -0.05  0.05  0.92  0.04 -0.18  1.04  0.02 -0.28  0.95\n",
      "  -0.07 -0.22  1.06  0.04 -0.17  1.01 -0.07 -0.02  0.94  0.04  0.02  1.04\n",
      "  -0.06  0.05  0.91 -0.13  0.13  0.91 -0.03 -0.03  0.97 -0.13  0.09  0.89\n",
      "  -0.08  0.18  1.02 -0.09  0.06  0.95 -0.11  0.19  0.96 -0.04 -0.13  0.9\n",
      "  -0.1  -0.03  0.91  0.01 -0.07  1.03 -0.09 -0.13  0.9  -0.01 -0.15  1.03]\n",
      " [ 0.04 -0.1   1.12 -0.05 -0.03  1.07 -0.1   0.12  0.92  0.01 -0.06  0.89\n",
      "  -0.14  0.17  0.85  0.07  0.04  1.03 -0.08  0.24  0.97  0.    0.17  1.08\n",
      "  -0.16  0.16  0.85  0.04  0.01  1.03 -0.11  0.11  0.86 -0.    0.18  1.06\n",
      "   0.06 -0.11  1.07 -0.05  0.04  1.06 -0.04 -0.13  0.95 -0.09 -0.04  0.86\n",
      "   0.12 -0.2   1.02 -0.01  0.05  1.01  0.08  0.13  1.1  -0.14  0.18  0.85]\n",
      " [-0.03  0.19  1.04 -0.13  0.1   0.85 -0.16  0.18  0.88 -0.02  0.06  0.97\n",
      "  -0.05  0.24  1.02  0.04  0.09  1.14 -0.16  0.17  0.89  0.03 -0.09  1.07\n",
      "  -0.04 -0.22  0.91  0.14 -0.18  1.16  0.06 -0.    0.99 -0.04  0.21  0.99\n",
      "  -0.07  0.04  0.86 -0.16  0.21  0.86  0.06  0.03  1.   -0.11  0.02  0.88\n",
      "   0.04  0.15  1.09 -0.16  0.18  0.89 -0.04  0.23  1.03 -0.16  0.15  0.84]\n",
      " [ 0.14 -0.13  1.01  0.05 -0.17  0.87  0.06 -0.17  0.88 -0.08 -0.04  0.9\n",
      "   0.04 -0.03  1.04 -0.13  0.15  1.02  0.03  0.09  1.07  0.02 -0.    0.94\n",
      "   0.03  0.    0.99 -0.09 -0.03  0.89 -0.07 -0.    0.9  -0.15  0.06  0.94\n",
      "  -0.06  0.09  1.06 -0.1   0.08  1.03  0.06  0.02  1.07  0.07 -0.06  0.96\n",
      "   0.18 -0.19  1.    0.07 -0.19  0.87 -0.    0.    0.93 -0.14  0.04  0.89]\n",
      " [ 0.02  0.08  1.09 -0.01  0.05  0.98  0.06 -0.01  1.02 -0.03 -0.05  0.9\n",
      "  -0.12  0.03  0.94 -0.01 -0.17  0.89  0.03 -0.11  1.03  0.18 -0.14  0.97\n",
      "   0.03  0.1   1.07  0.04  0.05  0.99  0.05  0.04  1.02 -0.05 -0.02  0.9\n",
      "  -0.16  0.06  0.94 -0.14 -0.    0.92 -0.08  0.08  1.06  0.01  0.09  1.09\n",
      "   0.03  0.08  1.09  0.01  0.01  0.96  0.15 -0.18  0.96  0.04 -0.21  0.86]\n",
      " [-0.12  0.05  0.93 -0.03  0.03  0.93 -0.1   0.17  0.95  0.03  0.05  1.03\n",
      "  -0.01 -0.13  1.04 -0.03 -0.2   0.91 -0.08 -0.13  0.97 -0.09 -0.12  0.92\n",
      "   0.05 -0.16  1.06 -0.08  0.15  0.98  0.04 -0.02  1.02 -0.12  0.08  0.89\n",
      "  -0.07 -0.    0.91 -0.1   0.13  1.02  0.03 -0.04  1.02  0.    0.07  1.07\n",
      "  -0.07  0.02  0.93 -0.   -0.04  0.97 -0.13  0.05  0.9   0.02 -0.04  1.05]\n",
      " [-0.11  0.02  0.86 -0.09  0.05  0.86 -0.07  0.08  0.89  0.07  0.01  1.08\n",
      "   0.06  0.01  1.1   0.03  0.02  1.1  -0.   -0.16  0.97  0.   -0.2   0.94\n",
      "  -0.04  0.    0.89 -0.03  0.02  0.91  0.02  0.02  0.95 -0.02  0.11  1.08\n",
      "  -0.04  0.11  1.09 -0.07  0.11  1.07 -0.14  0.03  0.95 -0.14  0.02  0.92\n",
      "  -0.03  0.01  0.93  0.01  0.02  0.95  0.04  0.    1.    0.15 -0.2   1.07]\n",
      " [ 0.14 -0.2   1.1  -0.07 -0.02  0.93  0.05  0.13  1.05 -0.12  0.14  0.84\n",
      "   0.08  0.04  1.08 -0.01  0.03  0.96 -0.03  0.19  1.04 -0.13  0.1   0.85\n",
      "  -0.16  0.18  0.88 -0.02  0.06  0.97 -0.05  0.24  1.02  0.04  0.09  1.14\n",
      "  -0.16  0.17  0.89  0.03 -0.09  1.07 -0.04 -0.22  0.91  0.14 -0.18  1.16\n",
      "   0.06 -0.    0.99 -0.04  0.21  0.99 -0.07  0.04  0.86 -0.16  0.21  0.86]\n",
      " [-0.09  0.17  1.02 -0.08 -0.24  0.88 -0.1  -0.16  0.93 -0.08 -0.18  1.\n",
      "  -0.03 -0.08  0.9  -0.08  0.04  0.92 -0.1   0.11  0.94 -0.02 -0.    0.93\n",
      "  -0.09  0.07  0.89 -0.11  0.18  1.   -0.01 -0.06  0.95 -0.08 -0.03  0.87\n",
      "   0.01  0.08  1.07 -0.01  0.04  0.97 -0.07  0.08  0.93  0.03 -0.2   1.07\n",
      "   0.04 -0.27  0.99 -0.02 -0.26  0.93  0.04 -0.15  1.08  0.07 -0.2   1.02]\n",
      " [-0.05  0.01  0.92 -0.12  0.06  0.92 -0.13  0.15  0.94 -0.04 -0.05  0.94\n",
      "  -0.11 -0.01  0.91 -0.13  0.05  0.93 -0.   -0.31  0.95 -0.08 -0.21  0.9\n",
      "   0.05 -0.05  1.    0.   -0.04  0.92 -0.07  0.04  0.89  0.04 -0.04  0.99\n",
      "  -0.03 -0.02  0.93 -0.1   0.09  0.91  0.   -0.01  0.96 -0.04 -0.05  0.91\n",
      "  -0.11  0.05  1.1   0.    0.    0.97 -0.07  0.04  0.92  0.07 -0.31  1.07]\n",
      " [ 0.    0.03  0.99 -0.09  0.01  0.87 -0.01  0.    0.93 -0.13  0.1   0.92\n",
      "  -0.04  0.1   1.06 -0.09 -0.18  0.96  0.04 -0.28  1.07 -0.04  0.1   1.04\n",
      "   0.07 -0.02  1.06 -0.03 -0.02  0.89  0.02 -0.03  0.97 -0.1   0.11  0.91\n",
      "  -0.08  0.18  1.03 -0.13  0.14  0.92 -0.02  0.13  1.03 -0.    0.18  1.02\n",
      "   0.02  0.04  1.08 -0.06  0.06  0.91  0.04 -0.34  0.98 -0.07 -0.28  0.88]\n",
      " [-0.08  0.1   1.05 -0.13  0.1   1.01 -0.13  0.05  0.97 -0.07  0.03  0.9\n",
      "  -0.04  0.08  0.93 -0.02  0.11  0.99 -0.04  0.08  1.06 -0.1   0.11  1.02\n",
      "  -0.15 -0.18  0.93  0.03 -0.2   0.9   0.09 -0.21  0.92  0.12 -0.09  1.03\n",
      "   0.06 -0.01  1.05  0.03 -0.01  1.05 -0.13  0.05  0.95 -0.12  0.05  0.91\n",
      "  -0.08  0.02  0.89 -0.02  0.1   1.   -0.02  0.11  1.05 -0.08  0.08  1.03]\n",
      " [-0.18  0.17  0.86 -0.01  0.06  1.01 -0.11  0.22  0.99  0.01  0.16  1.1\n",
      "  -0.18  0.18  0.88  0.04  0.04  1.1   0.04 -0.25  0.99  0.06 -0.09  1.1\n",
      "   0.11  0.07  1.09 -0.12  0.2   0.86  0.06  0.01  0.98 -0.08  0.08  0.86\n",
      "   0.07  0.09  1.11 -0.14  0.18  0.86 -0.12  0.23  0.97 -0.05  0.03  0.92\n",
      "  -0.03  0.2   0.86  0.02  0.09  1.07 -0.18  0.22  0.91  0.03 -0.07  1.11]\n",
      " [-0.05  0.07  0.9   0.03  0.06  1.06 -0.04  0.08  1.07 -0.01 -0.17  0.98\n",
      "  -0.03 -0.17  0.92  0.04 -0.2   0.89  0.13 -0.07  1.01  0.09  0.    1.04\n",
      "   0.03  0.01  1.06 -0.15  0.06  0.98 -0.12  0.05  0.94 -0.07  0.09  0.95\n",
      "  -0.02  0.12  1.03 -0.02  0.12  1.05 -0.14  0.1   1.01 -0.17  0.06  0.96\n",
      "  -0.15  0.08  0.91  0.11 -0.12  0.96  0.14 -0.12  1.03  0.13 -0.08  1.06]\n",
      " [-0.15  0.09  1.07 -0.06  0.04  0.91 -0.1   0.16  0.98 -0.09 -0.16  0.88\n",
      "  -0.   -0.13  1.04 -0.01 -0.24  0.92  0.07 -0.23  1.06 -0.08 -0.11  0.93\n",
      "   0.01 -0.12  1.05 -0.11  0.11  0.91  0.04  0.02  1.05 -0.08  0.13  1.03\n",
      "   0.02 -0.04  1.02 -0.15  0.11  0.9  -0.07  0.03  0.92 -0.09  0.12  1.01\n",
      "   0.01  0.02  1.    0.01 -0.01  1.03 -0.06 -0.05  0.91  0.05 -0.3   1.02]\n",
      " [-0.06 -0.16  0.99 -0.07  0.06  0.92 -0.01  0.1   1.03  0.04 -0.03  0.96\n",
      "   0.02  0.03  1.04 -0.03  0.    0.92  0.01  0.    1.04 -0.11  0.02  0.94\n",
      "  -0.05  0.12  1.03 -0.12  0.11  0.9   0.01  0.05  1.04 -0.05  0.01  0.9\n",
      "   0.06 -0.32  1.04 -0.07 -0.27  0.92  0.01 -0.09  0.94 -0.1   0.11  0.94\n",
      "   0.06 -0.05  1.05 -0.04  0.13  1.05  0.01 -0.03  1.   -0.13  0.13  0.9 ]\n",
      " [ 0.    0.09  1.04 -0.01  0.09  1.05 -0.01  0.09  1.07 -0.11  0.08  1.\n",
      "  -0.15  0.08  0.96 -0.15 -0.18  0.89  0.05 -0.18  0.9   0.1  -0.2   0.93\n",
      "   0.15 -0.12  1.05  0.11 -0.06  1.07  0.08 -0.05  1.06 -0.13  0.08  0.99\n",
      "  -0.14  0.06  0.96 -0.14  0.06  0.94 -0.06  0.    0.91 -0.05  0.04  0.92\n",
      "   0.04  0.04  1.05  0.04  0.04  1.07  0.02  0.06  1.07 -0.03 -0.08  1.02]\n",
      " [-0.01  0.1   1.06 -0.14  0.17  0.87 -0.03  0.08  0.9  -0.15  0.18  0.92\n",
      "  -0.1   0.07  0.86  0.04  0.1   0.99 -0.16  0.16  0.85  0.    0.08  0.99\n",
      "  -0.08 -0.08  0.9   0.04 -0.15  0.97  0.13 -0.22  1.16 -0.02  0.01  0.88\n",
      "   0.11 -0.01  1.09 -0.03  0.09  1.04  0.06  0.05  1.02 -0.01  0.11  1.05\n",
      "   0.01  0.03  0.98 -0.    0.13  1.1  -0.15  0.18  0.91  0.03  0.08  1.11]\n",
      " [ 0.04 -0.22  1.   -0.09 -0.21  0.89 -0.   -0.22  0.92 -0.08 -0.06  0.97\n",
      "  -0.06  0.06  0.93 -0.03  0.12  1.03  0.03 -0.02  1.02 -0.02  0.06  1.06\n",
      "  -0.02 -0.04  0.98 -0.15  0.14  0.92 -0.07  0.01  0.92 -0.1   0.15  0.98\n",
      "   0.   -0.02  0.91 -0.03  0.03  1.04  0.01 -0.12  0.96  0.04 -0.27  1.06\n",
      "  -0.02 -0.29  0.91 -0.06 -0.16  0.99 -0.07  0.06  0.92 -0.01  0.1   1.03]\n",
      " [ 0.03  0.02  0.96  0.02  0.09  1.09 -0.03  0.11  1.09 -0.05  0.07  1.09\n",
      "  -0.15  0.04  0.94 -0.14  0.01  0.91 -0.14  0.01  0.89  0.01  0.    0.94\n",
      "   0.02  0.02  0.96  0.05  0.02  1.08  0.16 -0.18  1.08  0.13 -0.2   1.06\n",
      "  -0.11  0.01  0.93 -0.11 -0.03  0.91 -0.11  0.01  0.88  0.02  0.01  0.97\n",
      "   0.02  0.05  1.01  0.05  0.05  1.05 -0.04  0.08  1.07 -0.05  0.06  1.05]\n",
      " [-0.05 -0.1   0.9   0.01 -0.06  1.02  0.11 -0.03  1.07 -0.02  0.12  1.08\n",
      "   0.07  0.05  1.04 -0.    0.01  0.91  0.04 -0.    1.   -0.04 -0.05  0.9\n",
      "  -0.02  0.03  0.95 -0.12  0.01  0.88 -0.16  0.07  0.95 -0.11  0.    0.92\n",
      "  -0.1   0.06  1.02  0.05  0.03  1.08  0.07 -0.11  1.05  0.18 -0.15  1.06\n",
      "   0.14  0.1   1.07  0.06  0.05  1.02 -0.02  0.    0.88  0.04 -0.02  0.97]\n",
      " [ 0.14 -0.25  1.08 -0.06 -0.05  1.    0.02  0.09  1.07 -0.09  0.09  0.84\n",
      "   0.07  0.07  1.12  0.07  0.03  1.02 -0.12  0.21  0.96 -0.01  0.07  0.93\n",
      "  -0.12  0.09  0.85  0.01  0.14  1.1   0.02  0.07  1.03 -0.14  0.21  0.97\n",
      "  -0.06  0.09  0.92 -0.08 -0.1   0.88  0.14 -0.19  1.16 -0.1  -0.05  0.9\n",
      "  -0.06  0.13  0.99  0.   -0.05  0.91 -0.12  0.13  0.84  0.09  0.06  1.09]\n",
      " [-0.05  0.03  1.02 -0.12  0.04  0.94 -0.09  0.03  0.9  -0.03  0.12  0.98\n",
      "  -0.02  0.13  1.03 -0.03  0.14  1.05 -0.14  0.09  1.02 -0.16  0.07  0.98\n",
      "  -0.17  0.06  0.95  0.04 -0.12  0.92  0.09 -0.13  0.95  0.14 -0.13  1.\n",
      "   0.07 -0.07  1.04  0.03 -0.08  1.04 -0.13  0.05  0.96 -0.11  0.04  0.92\n",
      "  -0.07  0.03  0.89 -0.01  0.12  1.   -0.02  0.14  1.04 -0.02  0.12  1.05]\n",
      " [ 0.02  0.1   1.07 -0.15  0.15  0.95 -0.14  0.11  0.84 -0.06  0.04  1.05\n",
      "  -0.16  0.1   0.89  0.06 -0.18  1.1  -0.1  -0.06  0.92  0.01 -0.13  0.9\n",
      "  -0.11  0.14  0.95 -0.11  0.11  0.84  0.08  0.05  1.01 -0.15  0.15  0.86\n",
      "   0.03  0.08  0.97  0.03  0.05  0.89 -0.03  0.1   0.94  0.01  0.13  1.1\n",
      "  -0.07  0.12  0.87  0.04  0.05  1.1  -0.08  0.08  1.03  0.13 -0.2   1.09]\n",
      " [-0.09  0.11  0.83  0.08  0.07  1.12 -0.14  0.19  0.85 -0.12  0.22  0.98\n",
      "  -0.02  0.01  0.94 -0.15  0.14  0.84  0.03  0.12  1.1  -0.17  0.18  0.87\n",
      "  -0.08  0.08  1.03 -0.04 -0.04  0.93  0.04 -0.02  1.11  0.14 -0.17  1.12\n",
      "  -0.07  0.02  0.97  0.09  0.07  1.11 -0.09  0.18  0.98 -0.1   0.12  0.83\n",
      "  -0.07  0.19  1.02 -0.14  0.15  0.86  0.03  0.    0.98 -0.17  0.18  0.87]\n",
      " [-0.13  0.2   0.86  0.08  0.03  1.03 -0.04  0.06  0.91 -0.    0.14  1.06\n",
      "  -0.14  0.18  0.86 -0.13  0.28  0.97 -0.08  0.1   0.88 -0.15  0.12  0.9\n",
      "   0.04 -0.04  1.05 -0.07  0.06  1.05  0.12 -0.14  1.14 -0.1  -0.01  0.89\n",
      "   0.1  -0.18  1.01 -0.09  0.1   0.84  0.1   0.03  1.09 -0.01  0.06  0.94\n",
      "  -0.    0.11  1.05 -0.13  0.16  0.85 -0.15  0.24  0.94 -0.06  0.06  0.91]\n",
      " [ 0.11 -0.12  0.96  0.14 -0.12  1.03  0.13 -0.08  1.06 -0.03 -0.06  1.\n",
      "  -0.05 -0.08  0.94 -0.03  0.03  0.9   0.04  0.04  0.95  0.05  0.07  1.02\n",
      "  -0.07  0.11  1.06 -0.11  0.09  1.04 -0.14  0.06  1.   -0.13  0.08  0.87\n",
      "  -0.05  0.1   0.91  0.07 -0.    1.07  0.03 -0.01  1.08 -0.03 -0.02  1.05\n",
      "  -0.   -0.17  0.9   0.01 -0.17  0.87  0.07 -0.11  0.91  0.04  0.11  1.04]\n",
      " [-0.03  0.05  0.94 -0.01  0.08  0.98 -0.    0.08  1.05 -0.04  0.07  1.05\n",
      "  -0.1   0.08  1.02 -0.11 -0.    0.94 -0.07 -0.05  0.92  0.01 -0.08  0.92\n",
      "   0.17 -0.16  1.03  0.16 -0.16  1.04 -0.1   0.09  1.02 -0.12  0.06  0.99\n",
      "  -0.13  0.05  0.96 -0.04  0.02  0.93 -0.02  0.05  0.94  0.    0.07  0.98\n",
      "  -0.01  0.08  1.05 -0.05  0.07  1.05 -0.15  0.06  0.97 -0.14  0.03  0.94]\n",
      " [-0.16  0.04  0.89 -0.12  0.13  1.01  0.03  0.08  1.1   0.07 -0.02  1.08\n",
      "   0.12 -0.09  1.01  0.02 -0.16  0.88  0.07 -0.19  0.86 -0.05 -0.09  0.91\n",
      "  -0.14  0.06  0.9  -0.12  0.13  1.03  0.04  0.08  1.08 -0.04  0.13  1.09\n",
      "   0.03  0.    1.   -0.07 -0.04  0.89 -0.06 -0.02  0.9  -0.16  0.04  0.92\n",
      "  -0.06  0.09  1.06 -0.1   0.1   1.04  0.07  0.03  1.09  0.06 -0.11  1.07]\n",
      " [-0.   -0.31  0.95 -0.08 -0.21  0.9   0.05 -0.05  1.    0.   -0.04  0.92\n",
      "  -0.07  0.04  0.89  0.04 -0.04  0.99 -0.03 -0.02  0.93 -0.1   0.09  0.91\n",
      "   0.   -0.01  0.96 -0.04 -0.05  0.91 -0.11  0.05  1.1   0.    0.    0.97\n",
      "  -0.07  0.04  0.92  0.07 -0.31  1.07  0.02 -0.34  0.96 -0.06 -0.29  0.88\n",
      "   0.07 -0.09  1.04  0.02 -0.11  0.94 -0.05 -0.05  0.89  0.05 -0.01  1.03]\n",
      " [ 0.07 -0.07  1.04  0.03 -0.08  1.04 -0.13  0.05  0.96 -0.11  0.04  0.92\n",
      "  -0.07  0.03  0.89 -0.01  0.12  1.   -0.02  0.14  1.04 -0.02  0.12  1.05\n",
      "  -0.13  0.08  1.02 -0.16  0.07  0.97 -0.03 -0.07  0.91  0.03 -0.1   0.92\n",
      "   0.08 -0.1   0.95  0.14 -0.15  1.05  0.06 -0.09  1.04  0.01 -0.09  1.02\n",
      "  -0.12  0.04  0.92 -0.07  0.03  0.89 -0.02  0.11  1.01 -0.01  0.12  1.06]\n",
      " [-0.07 -0.19  0.95  0.03 -0.22  1.05  0.04 -0.26  1.   -0.02  0.12  1.03\n",
      "   0.04  0.01  1.02 -0.07  0.06  0.93  0.03  0.05  1.06 -0.06  0.02  0.92\n",
      "   0.    0.08  1.06 -0.01 -0.02  0.95 -0.12  0.09  0.91  0.01  0.02  0.98\n",
      "  -0.1   0.1   0.91 -0.08  0.16  1.   -0.05 -0.25  0.93 -0.06 -0.21  0.96\n",
      "   0.04 -0.24  0.92 -0.11  0.08  0.93  0.01  0.06  1.05 -0.13  0.1   0.9 ]\n",
      " [ 0.08 -0.05  1.06 -0.13  0.08  0.99 -0.14  0.06  0.96 -0.14  0.06  0.94\n",
      "  -0.06  0.    0.91 -0.05  0.04  0.92  0.04  0.04  1.05  0.04  0.04  1.07\n",
      "   0.02  0.06  1.07 -0.03 -0.08  1.02 -0.04 -0.11  0.97 -0.03 -0.14  0.94\n",
      "   0.07 -0.17  0.9   0.1  -0.14  0.92  0.02  0.09  1.07  0.01  0.08  1.08\n",
      "  -0.04  0.09  1.07 -0.14  0.05  0.98 -0.14  0.04  0.93 -0.12  0.01  0.91]\n",
      " [-0.04 -0.14  0.94  0.09 -0.12  1.07 -0.03  0.06  1.02  0.07  0.05  1.09\n",
      "  -0.02  0.12  1.07  0.07  0.04  1.03 -0.03 -0.    0.91 -0.01 -0.04  0.95\n",
      "  -0.09 -0.04  0.89 -0.15  0.05  0.96 -0.18  0.09  0.93 -0.09  0.13  1.04\n",
      "  -0.06 -0.04  0.99  0.09 -0.09  1.07  0.16 -0.17  1.01  0.12 -0.05  1.07\n",
      "   0.09 -0.11  0.91 -0.03 -0.06  0.88 -0.04  0.01  0.9  -0.14  0.09  0.94]\n",
      " [-0.1  -0.01  0.89  0.1  -0.18  1.01 -0.09  0.1   0.84  0.1   0.03  1.09\n",
      "  -0.01  0.06  0.94 -0.    0.11  1.05 -0.13  0.16  0.85 -0.15  0.24  0.94\n",
      "  -0.06  0.06  0.91 -0.01  0.2   1.06  0.02  0.    1.05 -0.11  0.2   1.02\n",
      "  -0.12  0.08  1.15 -0.09 -0.06  0.9   0.09 -0.25  1.02 -0.07  0.04  0.85\n",
      "   0.09  0.08  1.09 -0.13  0.22  0.89  0.02  0.14  1.06 -0.12  0.14  0.85]\n",
      " [ 0.01  0.07  1.07 -0.02  0.06  1.06 -0.07  0.08  1.02 -0.09 -0.04  0.93\n",
      "  -0.03 -0.11  0.91  0.01 -0.11  0.91  0.19 -0.18  1.04  0.22 -0.21  1.05\n",
      "  -0.11  0.12  1.03 -0.15  0.09  1.   -0.15  0.06  0.95 -0.06 -0.02  0.92\n",
      "  -0.03 -0.    0.94  0.01  0.    0.97  0.01  0.08  1.07  0.02  0.05  1.07\n",
      "  -0.04  0.06  1.06 -0.08  0.01  0.98 -0.12 -0.01  0.92 -0.06 -0.06  0.9 ]\n",
      " [-0.05 -0.05  0.89  0.05 -0.01  1.03  0.01 -0.02  0.94 -0.02  0.13  1.03\n",
      "   0.03  0.02  1.02 -0.   -0.05  0.95 -0.02  0.12  1.04  0.01  0.07  1.06\n",
      "  -0.01  0.06  0.95 -0.   -0.16  1.03  0.04 -0.21  1.06 -0.07 -0.03  1.02\n",
      "   0.02 -0.1   1.06  0.07 -0.17  1.09 -0.09  0.16  0.99 -0.    0.09  1.03\n",
      "   0.05  0.02  1.07 -0.11  0.23  1.01 -0.03  0.12  1.04  0.03  0.01  0.91]\n",
      " [ 0.03 -0.05  1.04 -0.12  0.15  1.01  0.03  0.09  1.08  0.03  0.02  0.95\n",
      "   0.04  0.    1.01 -0.06 -0.04  0.89 -0.14  0.03  0.95 -0.14  0.05  0.9\n",
      "  -0.13  0.1   1.   -0.12  0.04  0.96  0.01  0.03  1.07  0.12 -0.08  1.05\n",
      "   0.15 -0.12  1.06  0.14 -0.15  0.95  0.03 -0.18  0.85 -0.02  0.    0.91\n",
      "  -0.14  0.06  0.89 -0.14 -0.03  0.89 -0.14  0.06  0.94 -0.08  0.13  1.08]\n",
      " [-0.09  0.08  0.9   0.01  0.    1.01 -0.11  0.07  0.94 -0.1   0.16  0.97\n",
      "  -0.05 -0.02  0.93 -0.14  0.08  0.9   0.    0.04  0.97 -0.1   0.1   0.94\n",
      "  -0.07  0.18  0.98 -0.05 -0.27  0.91 -0.07 -0.2   0.93  0.   -0.22  1.03\n",
      "  -0.08 -0.01  0.92 -0.02  0.03  1.03 -0.08  0.07  0.94 -0.09  0.15  0.99\n",
      "   0.01  0.04  1.05 -0.14  0.16  0.95 -0.04  0.1   1.05  0.   -0.01  1.03]\n",
      " [-0.03  0.03  0.93 -0.1   0.17  0.95  0.03  0.05  1.03 -0.01 -0.13  1.04\n",
      "  -0.03 -0.2   0.91 -0.08 -0.13  0.97 -0.09 -0.12  0.92  0.05 -0.16  1.06\n",
      "  -0.08  0.15  0.98  0.04 -0.02  1.02 -0.12  0.08  0.89 -0.07 -0.    0.91\n",
      "  -0.1   0.13  1.02  0.03 -0.04  1.02  0.    0.07  1.07 -0.07  0.02  0.93\n",
      "  -0.   -0.04  0.97 -0.13  0.05  0.9   0.02 -0.04  1.05 -0.   -0.24  1.04]\n",
      " [-0.02  0.11  1.04  0.01  0.07  1.05 -0.02  0.04  0.92  0.02 -0.15  1.04\n",
      "   0.03 -0.26  0.99 -0.03 -0.22  0.92  0.07 -0.21  1.1   0.05 -0.28  0.98\n",
      "  -0.04  0.08  1.03  0.05 -0.    1.04  0.01 -0.03  0.93 -0.01  0.09  1.04\n",
      "   0.02 -0.02  1.02 -0.01 -0.06  0.96 -0.01  0.11  1.04  0.01  0.04  1.\n",
      "  -0.04  0.02  1.05  0.02 -0.01  1.07  0.02 -0.1   0.98  0.   -0.23  1.06]\n",
      " [-0.12  0.05  0.94 -0.07  0.09  0.95 -0.02  0.12  1.03 -0.02  0.12  1.05\n",
      "  -0.14  0.1   1.01 -0.17  0.06  0.96 -0.15  0.08  0.91  0.11 -0.12  0.96\n",
      "   0.14 -0.12  1.03  0.13 -0.08  1.06 -0.03 -0.06  1.   -0.05 -0.08  0.94\n",
      "  -0.03  0.03  0.9   0.04  0.04  0.95  0.05  0.07  1.02 -0.07  0.11  1.06\n",
      "  -0.11  0.09  1.04 -0.14  0.06  1.   -0.13  0.08  0.87 -0.05  0.1   0.91]\n",
      " [ 0.05  0.03  1.07 -0.13  0.1   0.88  0.06  0.05  1.1  -0.15  0.15  0.91\n",
      "  -0.07  0.1   0.86 -0.12  0.16  0.96 -0.14  0.09  0.86  0.03  0.08  1.02\n",
      "  -0.16  0.16  0.84 -0.01  0.11  0.97 -0.1  -0.06  0.9   0.02 -0.15  0.98\n",
      "   0.12 -0.15  1.15 -0.   -0.    0.9   0.12 -0.04  1.1  -0.02  0.04  1.03\n",
      "   0.07  0.07  1.02  0.02  0.06  1.05 -0.14  0.14  0.87  0.01  0.11  1.1 ]\n",
      " [ 0.08 -0.22  0.87 -0.03 -0.13  0.89 -0.09  0.01  0.88 -0.13  0.09  0.94\n",
      "  -0.09  0.12  1.04 -0.15  0.11  0.98 -0.07  0.12  1.08  0.07  0.06  1.08\n",
      "  -0.02  0.11  1.09  0.02  0.06  1.06  0.01  0.05  1.11  0.07  0.    1.04\n",
      "   0.01 -0.05  0.94  0.19 -0.2   0.98  0.11 -0.22  0.88 -0.02 -0.16  0.87\n",
      "  -0.04  0.01  0.89 -0.14  0.04  0.88 -0.06 -0.02  0.89 -0.13  0.04  0.94]\n",
      " [-0.12  0.11  1.   -0.11  0.12  0.91 -0.02  0.1   1.05 -0.03  0.03  0.93\n",
      "   0.03 -0.09  1.04 -0.11 -0.05  0.9  -0.01 -0.05  1.04 -0.09 -0.17  0.95\n",
      "   0.07 -0.26  1.07 -0.03  0.1   1.04  0.02 -0.04  0.97 -0.1   0.13  0.9\n",
      "  -0.07  0.02  0.94 -0.09  0.14  1.    0.03 -0.03  1.04 -0.04  0.09  1.04\n",
      "  -0.03 -0.05  0.94  0.03  0.05  1.05 -0.07  0.04  0.92 -0.08  0.12  1.02]\n",
      " [-0.1   0.19  0.97 -0.04  0.13  1.03 -0.14  0.18  0.94 -0.05  0.14  1.06\n",
      "   0.02  0.04  1.09 -0.06 -0.2   1.02  0.03 -0.31  1.05 -0.1   0.08  0.96\n",
      "  -0.    0.03  1.06  0.06 -0.05  1.08 -0.08  0.16  1.02  0.01  0.04  1.07\n",
      "   0.04 -0.01  1.01 -0.05  0.17  1.05  0.    0.06  1.07 -0.09  0.19  1.02\n",
      "  -0.01  0.06  1.07  0.01  0.07  1.01 -0.02 -0.16  1.05  0.05 -0.25  1.06]\n",
      " [-0.09  0.02  0.85  0.11  0.02  1.08  0.02  0.04  0.95  0.    0.18  1.03\n",
      "  -0.1   0.09  1.1  -0.17  0.21  0.89  0.01  0.02  0.97 -0.15  0.17  0.84\n",
      "   0.02  0.12  1.09 -0.18  0.22  0.87 -0.02 -0.    1.09 -0.06 -0.13  0.91\n",
      "   0.09 -0.12  1.16  0.11 -0.17  1.02 -0.04  0.09  1.01  0.07  0.07  1.1\n",
      "  -0.13  0.2   0.86  0.08  0.03  1.03 -0.04  0.06  0.91 -0.    0.14  1.06]\n",
      " [-0.14  0.19  0.87  0.04  0.    0.98 -0.11  0.07  0.86  0.02  0.14  1.08\n",
      "  -0.16  0.18  0.87 -0.14  0.21  0.98 -0.08  0.12  0.91 -0.    0.15  1.07\n",
      "   0.11 -0.2   1.15 -0.08 -0.07  0.97  0.03  0.1   1.04 -0.09  0.04  0.85\n",
      "   0.09  0.04  1.12  0.04  0.04  0.98 -0.03  0.17  1.02 -0.06  0.07  0.86\n",
      "  -0.15  0.17  0.87  0.04  0.    1.01 -0.11  0.21  0.87  0.    0.13  1.07]\n",
      " [-0.13  0.16  0.94 -0.06  0.18  1.05 -0.13  0.14  0.95 -0.11  0.19  1.\n",
      "  -0.01  0.08  1.06 -0.1  -0.12  0.95 -0.02 -0.17  1.05 -0.08 -0.06  0.93\n",
      "  -0.07 -0.02  1.    0.02 -0.1   1.06 -0.1   0.13  0.92 -0.05  0.12  1.03\n",
      "   0.02  0.03  1.09 -0.13  0.2   0.96 -0.05  0.15  1.06 -0.12  0.11  0.94\n",
      "  -0.09  0.17  0.99 -0.02  0.08  1.04 -0.12  0.    0.93 -0.05 -0.01  1.05]\n",
      " [-0.06 -0.09  0.91 -0.08 -0.02  0.98  0.04 -0.1   1.02 -0.06 -0.14  0.95\n",
      "   0.02 -0.21  1.05  0.03 -0.26  0.98 -0.01  0.08  1.04  0.04 -0.01  1.02\n",
      "  -0.07  0.14  1.02  0.02 -0.    1.04 -0.03 -0.01  0.95 -0.02  0.07  1.05\n",
      "   0.03  0.    1.   -0.08  0.04  0.95  0.04  0.01  1.05 -0.03 -0.03  0.93\n",
      "  -0.11  0.03  1.04  0.04 -0.3   1.01 -0.07 -0.23  0.93  0.06 -0.01  1.05]\n",
      " [-0.07  0.02  0.93 -0.   -0.04  0.97 -0.13  0.05  0.9   0.02 -0.04  1.05\n",
      "  -0.   -0.24  1.04  0.01 -0.3   0.97 -0.09 -0.2   0.94 -0.08  0.07  0.93\n",
      "   0.    0.08  1.05  0.02 -0.04  0.94  0.04  0.01  1.05 -0.12  0.08  0.94\n",
      "  -0.02 -0.05  0.93 -0.14  0.14  0.96  0.02  0.01  1.04 -0.01  0.12  1.05\n",
      "  -0.02  0.    0.94 -0.13  0.14  0.93 -0.05 -0.27  0.92  0.   -0.25  1.04]\n",
      " [-0.11  0.03  0.88 -0.05  0.07  0.9   0.03  0.06  1.06 -0.04  0.08  1.07\n",
      "  -0.01 -0.17  0.98 -0.03 -0.17  0.92  0.04 -0.2   0.89  0.13 -0.07  1.01\n",
      "   0.09  0.    1.04  0.03  0.01  1.06 -0.15  0.06  0.98 -0.12  0.05  0.94\n",
      "  -0.07  0.09  0.95 -0.02  0.12  1.03 -0.02  0.12  1.05 -0.14  0.1   1.01\n",
      "  -0.17  0.06  0.96 -0.15  0.08  0.91  0.11 -0.12  0.96  0.14 -0.12  1.03]\n",
      " [-0.09  0.18  1.02  0.01  0.06  0.91 -0.01 -0.04  1.03  0.02 -0.16  0.96\n",
      "   0.06 -0.3   1.11  0.02 -0.28  0.95 -0.09 -0.12  0.94 -0.04  0.01  0.87\n",
      "  -0.1   0.14  0.94  0.02  0.04  1.06 -0.09  0.21  1.02  0.02  0.01  1.09\n",
      "  -0.05  0.13  1.04  0.    0.03  0.99 -0.09  0.01  0.87 -0.01  0.    0.93\n",
      "  -0.13  0.1   0.92 -0.04  0.1   1.06 -0.09 -0.18  0.96  0.04 -0.28  1.07]\n",
      " [ 0.02  0.14  1.06 -0.12  0.14  0.85  0.08 -0.02  1.05 -0.05  0.06  0.91\n",
      "  -0.02  0.17  1.07  0.01  0.07  1.03 -0.14  0.22  0.98 -0.1   0.13  0.89\n",
      "  -0.09 -0.03  0.9   0.1  -0.26  1.07 -0.02 -0.05  1.06  0.09 -0.01  1.11\n",
      "  -0.12  0.12  0.87 -0.02  0.12  1.04 -0.08  0.11  0.84  0.09  0.    1.12\n",
      "   0.   -0.03  0.99 -0.03  0.2   1.05 -0.13  0.09  0.86 -0.16  0.25  0.89]\n",
      " [ 0.01 -0.06  1.02 -0.14  0.08  0.9  -0.06  0.03  0.91 -0.1   0.18  0.97\n",
      "   0.03  0.    1.02  0.01 -0.17  1.04 -0.01 -0.32  0.93  0.08 -0.14  1.06\n",
      "  -0.08 -0.06  0.93  0.   -0.02  1.04 -0.13  0.1   0.91  0.03  0.05  1.04\n",
      "  -0.07  0.04  0.93  0.03 -0.03  1.03 -0.14  0.11  0.9  -0.06  0.01  0.91\n",
      "  -0.11  0.18  0.94  0.01  0.04  1.05 -0.05  0.02  1.02  0.03 -0.21  0.99]\n",
      " [ 0.    0.03  1.04 -0.14  0.07  0.98 -0.13  0.04  0.95 -0.09  0.03  0.91\n",
      "  -0.03  0.13  0.99 -0.01  0.13  1.03 -0.03  0.12  1.05 -0.15  0.08  1.\n",
      "  -0.17  0.08  0.97  0.03 -0.16  0.9   0.09 -0.17  0.93  0.13 -0.16  0.97\n",
      "   0.07 -0.04  1.04  0.01 -0.03  1.04 -0.05  0.03  1.02 -0.12  0.04  0.94\n",
      "  -0.09  0.03  0.9  -0.03  0.12  0.98 -0.02  0.13  1.03 -0.03  0.14  1.05]\n",
      " [-0.07  0.18  1.04 -0.    0.08  1.07 -0.12  0.16  0.91 -0.08  0.16  1.\n",
      "  -0.02  0.12  1.03 -0.14  0.15  0.93 -0.09  0.17  1.02 -0.08 -0.24  0.88\n",
      "  -0.1  -0.16  0.93 -0.08 -0.18  1.   -0.03 -0.08  0.9  -0.08  0.04  0.92\n",
      "  -0.1   0.11  0.94 -0.02 -0.    0.93 -0.09  0.07  0.89 -0.11  0.18  1.\n",
      "  -0.01 -0.06  0.95 -0.08 -0.03  0.87  0.01  0.08  1.07 -0.01  0.04  0.97]\n",
      " [-0.12 -0.03  0.89 -0.08  0.04  0.89 -0.16  0.11  0.95 -0.04  0.1   1.07\n",
      "   0.05 -0.13  1.03  0.15 -0.12  1.07  0.15 -0.21  0.93  0.09 -0.02  0.99\n",
      "  -0.05 -0.04  0.87 -0.04 -0.01  0.92 -0.15  0.06  0.91 -0.15  0.16  1.03\n",
      "  -0.13  0.06  0.98 -0.06  0.12  1.09  0.03  0.04  1.05  0.04  0.07  1.09\n",
      "  -0.    0.02  0.96  0.12 -0.11  0.99  0.04 -0.18  0.88 -0.03 -0.11  0.93]\n",
      " [-0.01  0.15  1.07 -0.17  0.18  0.86 -0.08 -0.01  0.98  0.01 -0.21  0.99\n",
      "   0.06 -0.11  1.11  0.12 -0.07  1.12 -0.12  0.1   0.86  0.09 -0.07  1.01\n",
      "  -0.04  0.07  0.88  0.04  0.1   1.07  0.04  0.04  1.11 -0.16  0.18  0.87\n",
      "   0.04 -0.01  1.03 -0.08  0.12  0.89 -0.02  0.17  1.05 -0.15  0.14  0.83\n",
      "  -0.11  0.01  0.89  0.08 -0.14  1.08 -0.1   0.01  0.95  0.05 -0.04  1.08]\n",
      " [ 0.04 -0.06  0.97 -0.06 -0.06  0.88 -0.1   0.01  0.96 -0.01 -0.15  0.89\n",
      "   0.01 -0.11  0.99  0.13 -0.1   1.07 -0.04  0.12  1.06  0.07  0.06  1.08\n",
      "   0.02  0.02  1.09  0.04  0.03  1.02 -0.03 -0.04  0.9  -0.01  0.01  0.96\n",
      "  -0.1  -0.01  0.87 -0.15  0.05  0.94 -0.15  0.05  0.91 -0.14  0.12  1.01\n",
      "   0.01  0.08  1.09  0.07 -0.13  1.06  0.17 -0.17  1.07  0.02  0.07  1.07]\n",
      " [-0.06  0.06  0.93 -0.19  0.19  0.87 -0.05  0.09  0.9   0.08  0.03  1.12\n",
      "  -0.03 -0.12  0.89  0.13 -0.25  1.12 -0.11  0.06  0.84  0.09 -0.02  1.02\n",
      "   0.04  0.06  1.06  0.02  0.07  0.94  0.05  0.05  1.11 -0.12  0.2   0.95\n",
      "   0.05  0.09  1.1  -0.11  0.18  1.01 -0.15  0.13  1.05 -0.02  0.14  1.06\n",
      "  -0.18  0.18  0.86  0.09 -0.16  1.12 -0.1  -0.05  0.97 -0.06 -0.1   0.88]\n",
      " [ 0.12 -0.1   1.09  0.09 -0.06  1.06  0.13 -0.09  1.01  0.05 -0.13  0.87\n",
      "   0.02  0.02  0.92 -0.11  0.04  0.88 -0.04 -0.04  0.9  -0.13  0.04  0.95\n",
      "  -0.11  0.12  1.06 -0.16  0.07  0.97 -0.01  0.09  1.08  0.04  0.06  1.07\n",
      "   0.08 -0.01  1.09  0.07 -0.08  0.97  0.17 -0.16  0.99  0.07 -0.2   0.87\n",
      "  -0.05 -0.1   0.89 -0.11  0.03  0.89 -0.14  0.12  0.97 -0.01  0.12  1.07]\n",
      " [-0.09 -0.04  0.89 -0.11 -0.02  0.92 -0.08  0.01  1.02 -0.06 -0.22  0.9\n",
      "  -0.09 -0.14  0.94 -0.06 -0.12  1.03 -0.07  0.04  0.88 -0.1   0.12  0.91\n",
      "  -0.01 -0.03  0.95 -0.08  0.06  0.89 -0.12  0.17  0.95 -0.02 -0.01  0.92\n",
      "  -0.09  0.    0.87 -0.12  0.11  0.95 -0.01 -0.04  0.93 -0.07  0.    0.9\n",
      "   0.07 -0.34  1.07  0.04 -0.35  0.95 -0.05 -0.28  0.88  0.06 -0.04  1.03]\n",
      " [-0.14  0.09  0.89 -0.01  0.05  1.07 -0.1   0.17  0.96  0.02  0.04  1.03\n",
      "  -0.11  0.12  0.93 -0.02 -0.21  0.93 -0.1  -0.09  0.94 -0.09 -0.15  0.93\n",
      "  -0.   -0.14  1.04  0.04 -0.22  0.99  0.04  0.02  1.05 -0.04  0.    0.92\n",
      "  -0.07  0.15  1.   -0.15  0.11  0.9  -0.02  0.05  1.06 -0.04 -0.03  0.96\n",
      "   0.02  0.02  1.04 -0.11  0.07  0.93 -0.04 -0.04  0.91 -0.09  0.08  0.99]\n",
      " [ 0.06 -0.32  1.04 -0.07 -0.27  0.92  0.01 -0.09  0.94 -0.1   0.11  0.94\n",
      "   0.06 -0.05  1.05 -0.04  0.13  1.05  0.01 -0.03  1.   -0.13  0.13  0.9\n",
      "  -0.05 -0.03  0.92 -0.12  0.14  0.99  0.02  0.01  0.94 -0.04  0.12  1.05\n",
      "  -0.    0.04  0.96  0.03 -0.21  1.02 -0.03 -0.26  0.9  -0.07 -0.21  0.97\n",
      "  -0.07 -0.06  0.94 -0.01 -0.03  1.05  0.04 -0.14  0.99  0.01  0.04  1.05]\n",
      " [ 0.04 -0.01  1.04  0.12 -0.03  1.05  0.05  0.08  1.07 -0.    0.    0.95\n",
      "   0.01 -0.02  1.   -0.08 -0.05  0.9  -0.13  0.05  0.96 -0.16  0.04  0.92\n",
      "  -0.1   0.11  1.03  0.04  0.09  1.09  0.05  0.02  1.07  0.1  -0.08  1.01\n",
      "   0.18 -0.15  1.04  0.11 -0.17  0.9  -0.01 -0.15  0.87 -0.08  0.    0.89\n",
      "  -0.15  0.09  0.95 -0.03  0.12  1.07 -0.11  0.13  1.07 -0.01  0.1   1.09]\n",
      " [ 0.02 -0.02  1.08 -0.03 -0.01  1.05 -0.   -0.19  0.89  0.02 -0.2   0.86\n",
      "   0.03 -0.17  0.86  0.06  0.05  1.03  0.06  0.07  1.06  0.02  0.1   1.06\n",
      "  -0.11  0.09  1.03 -0.13  0.08  1.   -0.11  0.02  0.86 -0.09  0.05  0.86\n",
      "  -0.07  0.08  0.89  0.07  0.01  1.08  0.06  0.01  1.1   0.03  0.02  1.1\n",
      "  -0.   -0.16  0.97  0.   -0.2   0.94 -0.04  0.    0.89 -0.03  0.02  0.91]\n",
      " [-0.18  0.18  0.88  0.04  0.04  1.1   0.04 -0.25  0.99  0.06 -0.09  1.1\n",
      "   0.11  0.07  1.09 -0.12  0.2   0.86  0.06  0.01  0.98 -0.08  0.08  0.86\n",
      "   0.07  0.09  1.11 -0.14  0.18  0.86 -0.12  0.23  0.97 -0.05  0.03  0.92\n",
      "  -0.03  0.2   0.86  0.02  0.09  1.07 -0.18  0.22  0.91  0.03 -0.07  1.11\n",
      "  -0.07 -0.15  0.9   0.11 -0.17  1.17  0.1  -0.09  1.01 -0.04  0.14  1.  ]\n",
      " [-0.13  0.05  0.97 -0.07  0.03  0.9  -0.04  0.08  0.93 -0.02  0.11  0.99\n",
      "  -0.04  0.08  1.06 -0.1   0.11  1.02 -0.15 -0.18  0.93  0.03 -0.2   0.9\n",
      "   0.09 -0.21  0.92  0.12 -0.09  1.03  0.06 -0.01  1.05  0.03 -0.01  1.05\n",
      "  -0.13  0.05  0.95 -0.12  0.05  0.91 -0.08  0.02  0.89 -0.02  0.1   1.\n",
      "  -0.02  0.11  1.05 -0.08  0.08  1.03 -0.14  0.09  0.99 -0.16  0.06  0.94]\n",
      " [ 0.17 -0.14  1.04  0.12 -0.06  1.05 -0.16  0.09  1.   -0.16  0.06  0.95\n",
      "  -0.14  0.03  0.92 -0.02  0.02  0.96  0.01  0.04  0.99  0.01  0.07  1.07\n",
      "  -0.02  0.06  1.06 -0.07  0.08  1.02 -0.09 -0.04  0.93 -0.03 -0.11  0.91\n",
      "   0.01 -0.11  0.91  0.19 -0.18  1.04  0.22 -0.21  1.05 -0.11  0.12  1.03\n",
      "  -0.15  0.09  1.   -0.15  0.06  0.95 -0.06 -0.02  0.92 -0.03 -0.    0.94]\n",
      " [-0.06 -0.02  0.89  0.    0.01  0.95 -0.12  0.03  0.9  -0.16  0.09  0.98\n",
      "  -0.01 -0.17  0.92  0.05 -0.13  1.05 -0.12  0.1   0.98  0.02  0.06  1.07\n",
      "   0.06  0.05  1.    0.05  0.05  1.08  0.02 -0.02  0.93 -0.12  0.02  0.88\n",
      "  -0.08 -0.04  0.88 -0.14  0.04  0.94 -0.16  0.09  0.92 -0.12  0.13  1.03\n",
      "   0.03  0.07  1.11  0.1  -0.11  1.07  0.17 -0.2   1.01  0.07 -0.2   0.87]\n",
      " [-0.08  0.01  1.02  0.08 -0.09  1.1  -0.09 -0.03  0.85  0.13 -0.18  1.06\n",
      "  -0.01  0.04  0.91  0.05  0.14  1.07 -0.13  0.17  0.86 -0.13  0.23  0.98\n",
      "  -0.03  0.02  0.9  -0.18  0.19  0.84  0.02  0.11  1.01 -0.1   0.25  0.97\n",
      "   0.04  0.03  1.11 -0.16  0.1   0.9   0.05 -0.09  1.07  0.   -0.18  0.92\n",
      "   0.14 -0.1   1.1   0.07  0.05  1.03 -0.11  0.23  0.95 -0.    0.04  0.92]\n",
      " [ 0.    0.08  1.07 -0.1  -0.2   0.95 -0.03 -0.19  1.05 -0.08 -0.    0.92\n",
      "  -0.11  0.1   0.94 -0.03  0.1   1.05 -0.09  0.11  0.92 -0.12  0.18  0.94\n",
      "  -0.05  0.14  1.04 -0.11  0.07  0.91 -0.13  0.18  0.94 -0.07  0.07  0.92\n",
      "  -0.12  0.12  0.94 -0.14  0.19  0.94 -0.04 -0.23  0.9  -0.08 -0.19  0.92\n",
      "  -0.11 -0.16  0.97 -0.01 -0.18  0.93 -0.08 -0.07  0.91 -0.1   0.06  0.99]\n",
      " [ 0.02  0.05  1.01  0.05  0.05  1.05 -0.04  0.08  1.07 -0.05  0.06  1.05\n",
      "  -0.14  0.06  0.92 -0.13  0.04  0.91 -0.12  0.04  0.89  0.16 -0.19  0.97\n",
      "   0.2  -0.2   1.03  0.18 -0.16  1.06 -0.02  0.02  1.02 -0.05  0.01  1.\n",
      "  -0.11  0.03  0.9  -0.08  0.01  0.89 -0.03 -0.    0.92  0.    0.09  1.04\n",
      "  -0.01  0.09  1.05 -0.01  0.09  1.07 -0.11  0.08  1.   -0.15  0.08  0.96]\n",
      " [-0.08 -0.01  0.88 -0.14  0.05  0.95 -0.13 -0.01  0.9  -0.14  0.08  0.99\n",
      "  -0.02  0.08  1.11 -0.07  0.12  1.06  0.06  0.04  1.1   0.15 -0.16  1.08\n",
      "   0.18 -0.19  0.99  0.1  -0.22  0.87  0.03 -0.01  0.91 -0.08  0.    0.86\n",
      "  -0.04 -0.01  0.88 -0.14  0.05  0.91 -0.15  0.14  1.04 -0.13  0.01  0.95\n",
      "  -0.08  0.08  1.09 -0.02  0.11  1.1  -0.04  0.1   1.08  0.05  0.07  1.1 ]\n",
      " [-0.1   0.17  0.97 -0.07 -0.02  0.92 -0.16  0.14  0.92 -0.01  0.1   1.04\n",
      "  -0.12  0.06  0.92 -0.07  0.14  1.    0.03  0.04  1.06 -0.09  0.17  0.97\n",
      "   0.02  0.01  1.06 -0.09 -0.2   0.93  0.02 -0.28  1.04  0.04 -0.27  1.01\n",
      "  -0.02  0.04  1.03  0.05 -0.    1.05 -0.04 -0.01  0.92 -0.    0.09  1.04\n",
      "  -0.   -0.04  0.99 -0.1   0.09  1.03  0.04 -0.04  1.04 -0.06 -0.01  0.94]\n",
      " [ 0.05  0.08  1.09 -0.01  0.05  0.95  0.11 -0.1   0.99 -0.   -0.14  0.88\n",
      "  -0.06 -0.08  0.95 -0.06 -0.06  0.9   0.03 -0.05  1.04 -0.12  0.15  1.01\n",
      "   0.03  0.09  1.08  0.03  0.02  0.95  0.04  0.    1.01 -0.06 -0.04  0.89\n",
      "  -0.14  0.03  0.95 -0.14  0.05  0.9  -0.13  0.1   1.   -0.12  0.04  0.96\n",
      "   0.01  0.03  1.07  0.12 -0.08  1.05  0.15 -0.12  1.06  0.14 -0.15  0.95]\n",
      " [ 0.06  0.01  1.06 -0.04 -0.01  0.91 -0.07  0.13  0.99 -0.13  0.1   0.92\n",
      "  -0.01  0.07  1.05 -0.04 -0.01  0.94  0.02 -0.    1.05 -0.1   0.04  0.94\n",
      "  -0.06  0.1   0.94 -0.12  0.14  0.92  0.02  0.07  1.06 -0.   -0.21  1.03\n",
      "  -0.01 -0.31  0.95 -0.09 -0.21  0.93 -0.08 -0.04  0.93 -0.    0.01  1.04\n",
      "   0.03 -0.11  0.98  0.05  0.    1.05 -0.06  0.04  0.93  0.03 -0.08  1.02]\n",
      " [ 0.01  0.08  1.11 -0.03  0.02  0.95 -0.14  0.02  0.91 -0.08 -0.01  0.89\n",
      "  -0.13  0.08  0.99  0.04  0.05  0.92  0.11 -0.12  1.03  0.18 -0.17  1.04\n",
      "   0.04  0.09  1.08  0.01  0.01  0.94 -0.11  0.04  0.9  -0.09 -0.02  0.9\n",
      "  -0.17  0.12  0.99 -0.03  0.13  1.08 -0.02  0.09  1.1   0.03  0.05  1.03\n",
      "   0.06  0.06  1.08 -0.02 -0.01  0.93 -0.16  0.05  0.92 -0.   -0.2   0.88]\n",
      " [ 0.    0.05  1.09 -0.12  0.17  0.95 -0.05  0.13  1.03 -0.01  0.11  1.07\n",
      "  -0.12  0.11  0.98 -0.04  0.06  1.05 -0.08 -0.18  0.92 -0.09 -0.15  0.97\n",
      "  -0.   -0.21  1.07 -0.08  0.08  0.92 -0.1   0.13  0.94 -0.02  0.13  1.03\n",
      "  -0.1   0.12  0.93 -0.13  0.2   0.95 -0.05 -0.03  0.9  -0.11  0.05  0.91\n",
      "  -0.13  0.16  0.93 -0.05  0.05  0.92 -0.11  0.1   0.92 -0.13  0.16  0.95]\n",
      " [-0.12  0.04  0.92 -0.1   0.03  0.89 -0.04  0.03  0.9  -0.01  0.09  1.05\n",
      "  -0.02  0.12  1.07 -0.04  0.1   1.08 -0.15  0.06  0.94 -0.15  0.05  0.91\n",
      "   0.01 -0.07  0.94  0.09 -0.09  0.98  0.12 -0.09  1.05  0.07 -0.13  1.04\n",
      "   0.01 -0.12  1.01 -0.03 -0.11  0.95 -0.06  0.03  0.89 -0.02  0.02  0.92\n",
      "   0.03  0.04  1.06 -0.03  0.12  1.08 -0.06  0.09  1.06 -0.16  0.04  0.94]\n",
      " [ 0.03  0.01  1.04 -0.09  0.17  1.   -0.    0.06  1.06 -0.02 -0.02  0.93\n",
      "  -0.01  0.13  1.07  0.    0.02  0.96 -0.12  0.14  0.92  0.03 -0.25  0.98\n",
      "  -0.07 -0.22  0.92  0.06 -0.16  1.04 -0.06 -0.08  0.92 -0.09  0.03  0.95\n",
      "  -0.03  0.    0.93 -0.13  0.1   0.89 -0.04  0.12  1.04 -0.12  0.1   0.91\n",
      "  -0.09  0.14  1.02 -0.08  0.06  0.94 -0.1   0.14  0.96  0.01  0.09  1.07]\n",
      " [ 0.08 -0.31  1.05 -0.04 -0.31  0.89  0.06 -0.08  0.98 -0.07  0.    0.87\n",
      "  -0.09  0.15  0.97 -0.1   0.16  0.93 -0.06  0.17  1.05  0.04  0.18  0.92\n",
      "  -0.03  0.15  1.03  0.01 -0.    0.97  0.01  0.08  1.09 -0.02  0.06  0.94\n",
      "  -0.13  0.14  0.94 -0.02 -0.27  0.9  -0.1  -0.17  0.93  0.   -0.23  1.06\n",
      "  -0.09  0.08  0.98  0.05 -0.03  1.07 -0.06  0.15  1.03  0.05  0.02  1.09]\n",
      " [-0.06 -0.08  0.92 -0.09  0.03  0.95 -0.03  0.    0.93 -0.13  0.1   0.89\n",
      "  -0.04  0.12  1.04 -0.12  0.1   0.91 -0.09  0.14  1.02 -0.08  0.06  0.94\n",
      "  -0.1   0.14  0.96  0.01  0.09  1.07 -0.1   0.03  0.92  0.01 -0.01  1.03\n",
      "   0.02 -0.14  0.99 -0.01 -0.16  1.04  0.05 -0.23  1.05 -0.07 -0.18  1.\n",
      "   0.05  0.03  1.05 -0.04  0.    0.92  0.01  0.07  1.06 -0.02 -0.01  0.97]\n",
      " [ 0.06 -0.23  1.03 -0.05 -0.29  0.9  -0.11 -0.23  1.06 -0.01 -0.13  0.95\n",
      "  -0.1  -0.04  0.93  0.01 -0.01  0.99 -0.1   0.05  0.94 -0.1   0.14  0.98\n",
      "  -0.05 -0.01  0.92 -0.15  0.14  0.94 -0.03  0.08  1.05 -0.11  0.07  0.91\n",
      "  -0.04  0.13  1.02 -0.06 -0.09  0.91 -0.08 -0.02  0.98  0.04 -0.1   1.02\n",
      "  -0.06 -0.14  0.95  0.02 -0.21  1.05  0.03 -0.26  0.98 -0.01  0.08  1.04]\n",
      " [-0.1   0.1   1.04 -0.13  0.08  1.01 -0.14  0.05  0.96 -0.05  0.04  0.9\n",
      "  -0.03  0.1   0.95  0.02  0.05  1.07 -0.05  0.07  1.05 -0.09  0.08  1.03\n",
      "  -0.06 -0.1   0.93 -0.02 -0.11  0.89  0.06 -0.15  0.91  0.15 -0.13  1.04\n",
      "   0.11 -0.08  1.07 -0.16  0.09  1.   -0.12  0.05  0.96 -0.11  0.04  0.93\n",
      "  -0.01  0.06  0.96 -0.01  0.09  0.99 -0.01  0.12  1.04 -0.07  0.07  1.03]\n",
      " [ 0.18 -0.18  1.02  0.1  -0.2   0.91  0.1  -0.06  0.95  0.01 -0.1   0.86\n",
      "  -0.09  0.01  0.92 -0.13  0.04  0.88 -0.15  0.12  0.98 -0.04  0.12  0.94\n",
      "  -0.11  0.1   1.08 -0.01  0.1   1.1   0.01  0.08  1.1  -0.    0.06  0.99\n",
      "  -0.12  0.04  0.87  0.02 -0.13  0.89 -0.05 -0.1   0.94  0.02 -0.04  1.07\n",
      "  -0.   -0.05  1.01  0.12 -0.06  1.06 -0.    0.11  1.08  0.06  0.04  1.02]\n",
      " [-0.14  0.18  0.86 -0.13  0.28  0.97 -0.08  0.1   0.88 -0.15  0.12  0.9\n",
      "   0.04 -0.04  1.05 -0.07  0.06  1.05  0.12 -0.14  1.14 -0.1  -0.01  0.89\n",
      "   0.1  -0.18  1.01 -0.09  0.1   0.84  0.1   0.03  1.09 -0.01  0.06  0.94\n",
      "  -0.    0.11  1.05 -0.13  0.16  0.85 -0.15  0.24  0.94 -0.06  0.06  0.91\n",
      "  -0.01  0.2   1.06  0.02  0.    1.05 -0.11  0.2   1.02 -0.12  0.08  1.15]\n",
      " [ 0.01  0.13  1.1  -0.07  0.12  0.87  0.04  0.05  1.1  -0.08  0.08  1.03\n",
      "   0.13 -0.2   1.09  0.05 -0.13  1.09 -0.06 -0.11  0.91  0.05  0.03  1.07\n",
      "  -0.13  0.1   0.88  0.06  0.05  1.1  -0.15  0.15  0.91 -0.07  0.1   0.86\n",
      "  -0.12  0.16  0.96 -0.14  0.09  0.86  0.03  0.08  1.02 -0.16  0.16  0.84\n",
      "  -0.01  0.11  0.97 -0.1  -0.06  0.9   0.02 -0.15  0.98  0.12 -0.15  1.15]\n",
      " [-0.04 -0.12  0.98  0.05 -0.18  1.05 -0.14  0.13  0.91  0.02  0.08  1.04\n",
      "   0.    0.04  0.97 -0.04  0.13  1.04  0.03 -0.02  1.02 -0.11  0.09  0.95\n",
      "   0.03  0.04  1.04 -0.06  0.03  0.91  0.01  0.05  1.06 -0.03 -0.02  0.94\n",
      "  -0.13  0.08  0.89  0.05 -0.29  1.   -0.09 -0.2   0.92 -0.04 -0.21  0.99\n",
      "  -0.05  0.02  0.92 -0.11  0.11  0.91  0.01  0.09  0.94 -0.12  0.12  0.92]\n",
      " [-0.13  0.16  0.83  0.1   0.06  0.94  0.    0.03  0.93 -0.02  0.17  1.06\n",
      "   0.05  0.13  1.09 -0.15  0.19  0.87 -0.01  0.07  0.96 -0.15  0.14  0.87\n",
      "   0.06  0.1   1.14 -0.18  0.19  0.87  0.01 -0.09  1.08 -0.01 -0.22  0.91\n",
      "  -0.11  0.11  0.84  0.09  0.02  1.02 -0.1   0.21  0.96  0.04  0.14  1.09\n",
      "  -0.13  0.16  0.85  0.09  0.02  1.05 -0.03 -0.01  0.93 -0.03  0.2   1.06]\n",
      " [-0.07 -0.1   0.92 -0.09 -0.04  0.96  0.01 -0.11  1.02 -0.1  -0.13  0.91\n",
      "  -0.   -0.13  1.04  0.06 -0.23  1.05 -0.08  0.16  0.97  0.02  0.05  1.04\n",
      "   0.01 -0.04  0.91 -0.05  0.15  1.05  0.04 -0.03  1.05 -0.09  0.16  0.98\n",
      "   0.01  0.08  1.06 -0.01 -0.02  0.96 -0.02  0.07  1.04  0.02 -0.06  0.99\n",
      "  -0.08 -0.    0.92  0.06 -0.31  1.06 -0.05 -0.28  0.91  0.04  0.02  1.05]\n",
      " [ 0.04 -0.01  1.04 -0.1   0.07  0.95  0.03  0.06  1.01 -0.08  0.05  0.95\n",
      "  -0.08  0.19  0.98 -0.07 -0.08  0.92 -0.09  0.06  0.97  0.04 -0.08  1.03\n",
      "  -0.04 -0.12  0.98  0.05 -0.18  1.05 -0.14  0.13  0.91  0.02  0.08  1.04\n",
      "   0.    0.04  0.97 -0.04  0.13  1.04  0.03 -0.02  1.02 -0.11  0.09  0.95\n",
      "   0.03  0.04  1.04 -0.06  0.03  0.91  0.01  0.05  1.06 -0.03 -0.02  0.94]\n",
      " [ 0.03  0.    1.   -0.08  0.04  0.95  0.04  0.01  1.05 -0.03 -0.03  0.93\n",
      "  -0.11  0.03  1.04  0.04 -0.3   1.01 -0.07 -0.23  0.93  0.06 -0.01  1.05\n",
      "  -0.02 -0.01  0.93 -0.1   0.07  0.91  0.03 -0.01  1.02 -0.09  0.05  0.93\n",
      "  -0.13  0.15  0.96 -0.02 -0.04  0.95 -0.12  0.07  0.91  0.03  0.02  1.\n",
      "  -0.07  0.07  0.92 -0.13  0.14  0.91 -0.   -0.28  0.94 -0.08 -0.25  0.89]\n",
      " [-0.09  0.09  1.01 -0.12  0.08  0.99 -0.13  0.04  0.96 -0.01 -0.14  0.91\n",
      "   0.04 -0.15  0.91  0.17 -0.15  1.03  0.17 -0.14  1.04  0.12 -0.06  1.05\n",
      "  -0.16  0.09  1.   -0.16  0.06  0.95 -0.14  0.03  0.92 -0.02  0.02  0.96\n",
      "   0.01  0.04  0.99  0.01  0.07  1.07 -0.02  0.06  1.06 -0.07  0.08  1.02\n",
      "  -0.09 -0.04  0.93 -0.03 -0.11  0.91  0.01 -0.11  0.91  0.19 -0.18  1.04]\n",
      " [ 0.    0.08  1.07 -0.01  0.04  0.94 -0.09  0.06  0.88  0.03 -0.15  1.\n",
      "  -0.02 -0.15  0.93 -0.1  -0.1   0.91  0.04 -0.31  0.96 -0.06 -0.22  0.88\n",
      "   0.05 -0.03  1.01 -0.   -0.03  0.91 -0.08  0.07  0.91  0.01 -0.04  0.97\n",
      "  -0.06  0.02  0.89 -0.12  0.16  0.94 -0.02  0.    0.91 -0.1   0.01  0.89\n",
      "   0.02 -0.04  0.99 -0.04 -0.02  0.92 -0.11  0.04  0.92  0.05 -0.36  0.96]\n",
      " [-0.11  0.19  0.96 -0.04 -0.13  0.9  -0.1  -0.03  0.91  0.01 -0.07  1.03\n",
      "  -0.09 -0.13  0.9  -0.01 -0.15  1.03  0.06 -0.19  1.05 -0.08  0.16  0.98\n",
      "   0.02  0.05  1.05 -0.15  0.16  0.95 -0.03  0.09  1.06  0.02 -0.01  1.03\n",
      "  -0.06  0.13  1.01  0.03  0.06  1.06 -0.06 -0.    0.92  0.01  0.06  1.04\n",
      "  -0.   -0.04  0.97 -0.09  0.03  1.04  0.06 -0.3   1.03 -0.05 -0.24  0.91]\n",
      " [-0.12  0.04  0.96  0.01  0.03  1.07  0.12 -0.08  1.05  0.15 -0.12  1.06\n",
      "   0.14 -0.15  0.95  0.03 -0.18  0.85 -0.02  0.    0.91 -0.14  0.06  0.89\n",
      "  -0.14 -0.03  0.89 -0.14  0.06  0.94 -0.08  0.13  1.08 -0.13  0.1   1.\n",
      "  -0.01  0.08  1.09  0.01  0.06  1.04  0.08 -0.    1.09  0.05 -0.04  0.97\n",
      "  -0.08 -0.06  0.99  0.08 -0.22  0.87 -0.03 -0.13  0.89 -0.09  0.01  0.88]\n",
      " [ 0.01  0.06  1.08 -0.13  0.18  0.92 -0.07  0.17  1.02 -0.02  0.13  1.04\n",
      "  -0.14  0.18  0.95 -0.08  0.17  1.05  0.    0.08  1.07 -0.1  -0.2   0.95\n",
      "  -0.03 -0.19  1.05 -0.08 -0.    0.92 -0.11  0.1   0.94 -0.03  0.1   1.05\n",
      "  -0.09  0.11  0.92 -0.12  0.18  0.94 -0.05  0.14  1.04 -0.11  0.07  0.91\n",
      "  -0.13  0.18  0.94 -0.07  0.07  0.92 -0.12  0.12  0.94 -0.14  0.19  0.94]\n",
      " [-0.08 -0.23  0.95  0.04 -0.27  1.04 -0.11  0.05  0.91 -0.01  0.04  1.03\n",
      "   0.06 -0.05  0.93 -0.09  0.16  0.99  0.02  0.03  1.06 -0.16  0.15  0.96\n",
      "  -0.02  0.07  1.05  0.03 -0.01  1.01 -0.04  0.13  1.03  0.02  0.08  1.05\n",
      "  -0.06  0.07  0.91  0.03 -0.17  1.03  0.01 -0.2   0.96  0.02 -0.12  1.05\n",
      "   0.04 -0.2   1.   -0.08 -0.08  0.93  0.05  0.04  1.05 -0.04 -0.    0.92]\n",
      " [-0.13  0.15  0.96 -0.02 -0.04  0.95 -0.12  0.07  0.91  0.03  0.02  1.\n",
      "  -0.07  0.07  0.92 -0.13  0.14  0.91 -0.   -0.28  0.94 -0.08 -0.25  0.89\n",
      "  -0.03 -0.19  1.01 -0.06 -0.06  0.94 -0.09  0.03  0.96 -0.06  0.01  0.93\n",
      "  -0.13  0.1   0.89 -0.04  0.12  1.04 -0.12  0.06  0.94 -0.11  0.16  1.01\n",
      "   0.01  0.02  1.06 -0.13  0.13  0.91 -0.02  0.09  1.05  0.02  0.05  0.91]\n",
      " [-0.18  0.22  0.91  0.03 -0.07  1.11 -0.07 -0.15  0.9   0.11 -0.17  1.17\n",
      "   0.1  -0.09  1.01 -0.04  0.14  1.   -0.01 -0.03  0.9  -0.13  0.17  0.84\n",
      "   0.09  0.03  1.05 -0.03  0.01  0.93 -0.01  0.18  1.06 -0.14  0.14  0.87\n",
      "  -0.16  0.24  0.91 -0.07  0.15  0.92 -0.03  0.19  1.05  0.08 -0.08  1.15\n",
      "  -0.13  0.03  0.92  0.03 -0.02  1.06 -0.04 -0.1   0.88  0.13 -0.07  1.11]\n",
      " [-0.12  0.25  0.97 -0.03  0.18  1.02  0.01  0.07  1.05 -0.12  0.19  0.98\n",
      "  -0.02  0.17  1.04  0.01  0.06  1.08 -0.1  -0.05  0.99 -0.01 -0.12  1.05\n",
      "  -0.09 -0.07  0.94 -0.08  0.    0.98  0.   -0.04  1.06 -0.08  0.09  0.92\n",
      "  -0.1   0.16  0.95 -0.03  0.14  1.02 -0.11  0.1   0.92 -0.13  0.21  0.94\n",
      "  -0.05  0.01  0.92 -0.12  0.06  0.92 -0.13  0.15  0.94 -0.04 -0.05  0.94]\n",
      " [-0.18  0.14  0.98 -0.02  0.11  1.08  0.04  0.01  1.    0.03  0.04  1.05\n",
      "  -0.04 -0.04  0.91 -0.13  0.01  0.93 -0.15  0.07  0.91 -0.12  0.15  1.04\n",
      "   0.05  0.05  1.08  0.15 -0.13  1.07  0.13 -0.19  0.94  0.02 -0.04  0.98\n",
      "  -0.05 -0.06  0.86 -0.12  0.07  0.95 -0.18  0.1   0.93 -0.08  0.15  1.05\n",
      "   0.06  0.06  1.07 -0.03  0.12  1.09  0.01 -0.02  0.98 -0.09 -0.04  0.89]\n",
      " [ 0.05  0.03  1.01 -0.04 -0.03  0.9  -0.15  0.05  0.97 -0.14 -0.02  0.91\n",
      "  -0.09  0.08  1.05 -0.01  0.09  1.09  0.04  0.08  1.09  0.01 -0.    0.94\n",
      "   0.16 -0.19  0.93  0.02 -0.23  0.86  0.   -0.12  1.   -0.11  0.08  0.97\n",
      "   0.06  0.07  1.07  0.05  0.02  0.96  0.05  0.04  1.03 -0.02 -0.02  0.9\n",
      "  -0.01 -0.04  0.95 -0.11 -0.03  0.9  -0.15  0.07  1.02 -0.18  0.1   0.95]\n",
      " [-0.11  0.09  1.02 -0.12  0.06  0.99 -0.08  0.01  0.91 -0.03  0.02  0.93\n",
      "  -0.    0.06  0.95 -0.01  0.12  1.02 -0.01  0.08  1.04 -0.05  0.07  1.05\n",
      "  -0.16  0.07  0.97 -0.13  0.05  0.94 -0.09 -0.2   0.94  0.18 -0.2   0.99\n",
      "   0.18 -0.16  1.03 -0.07  0.1   1.03 -0.1   0.07  1.02 -0.13  0.06  0.98\n",
      "  -0.07  0.02  0.9  -0.03  0.03  0.92  0.01  0.05  0.96 -0.02  0.11  1.05]\n",
      " [-0.1  -0.03  0.86  0.12 -0.25  1.04 -0.02  0.    1.01  0.05  0.12  1.09\n",
      "  -0.13  0.16  0.83  0.1   0.06  0.94  0.    0.03  0.93 -0.02  0.17  1.06\n",
      "   0.05  0.13  1.09 -0.15  0.19  0.87 -0.01  0.07  0.96 -0.15  0.14  0.87\n",
      "   0.06  0.1   1.14 -0.18  0.19  0.87  0.01 -0.09  1.08 -0.01 -0.22  0.91\n",
      "  -0.11  0.11  0.84  0.09  0.02  1.02 -0.1   0.21  0.96  0.04  0.14  1.09]\n",
      " [-0.   -0.03  0.91 -0.08  0.07  0.91  0.01 -0.04  0.97 -0.06  0.02  0.89\n",
      "  -0.12  0.16  0.94 -0.02  0.    0.91 -0.1   0.01  0.89  0.02 -0.04  0.99\n",
      "  -0.04 -0.02  0.92 -0.11  0.04  0.92  0.05 -0.36  0.96 -0.04 -0.29  0.89\n",
      "  -0.08 -0.18  0.93  0.03 -0.06  0.93 -0.05 -0.    0.88 -0.1  -0.02  1.03\n",
      "  -0.01 -0.02  0.94 -0.08  0.07  0.89  0.02  0.01  0.99 -0.03 -0.05  0.92]\n",
      " [ 0.04  0.01  1.02 -0.07  0.06  0.93  0.03  0.05  1.06 -0.06  0.02  0.92\n",
      "   0.    0.08  1.06 -0.01 -0.02  0.95 -0.12  0.09  0.91  0.01  0.02  0.98\n",
      "  -0.1   0.1   0.91 -0.08  0.16  1.   -0.05 -0.25  0.93 -0.06 -0.21  0.96\n",
      "   0.04 -0.24  0.92 -0.11  0.08  0.93  0.01  0.06  1.05 -0.13  0.1   0.9\n",
      "  -0.04  0.13  1.04  0.03  0.01  1.04 -0.09  0.17  1.   -0.    0.06  1.06]\n",
      " [ 0.05 -0.17  0.87  0.06 -0.17  0.88 -0.08 -0.04  0.9   0.04 -0.03  1.04\n",
      "  -0.13  0.15  1.02  0.03  0.09  1.07  0.02 -0.    0.94  0.03  0.    0.99\n",
      "  -0.09 -0.03  0.89 -0.07 -0.    0.9  -0.15  0.06  0.94 -0.06  0.09  1.06\n",
      "  -0.1   0.08  1.03  0.06  0.02  1.07  0.07 -0.06  0.96  0.18 -0.19  1.\n",
      "   0.07 -0.19  0.87 -0.    0.    0.93 -0.14  0.04  0.89 -0.15  0.14  1.  ]\n",
      " [-0.11  0.1   0.92 -0.13  0.16  0.95 -0.02 -0.3   0.91 -0.07 -0.23  0.92\n",
      "  -0.1  -0.1   0.98  0.   -0.07  0.92 -0.08  0.02  0.91  0.02 -0.03  1.01\n",
      "  -0.02  0.01  0.94 -0.1   0.09  0.9   0.01  0.    0.98 -0.03 -0.05  0.93\n",
      "  -0.1   0.03  0.88  0.02  0.04  1.01 -0.03  0.05  0.93  0.03 -0.27  1.06\n",
      "   0.05 -0.31  1.02  0.   -0.32  0.94  0.03 -0.07  1.07  0.07 -0.12  1.06]\n",
      " [ 0.12 -0.2   0.93  0.02 -0.21  0.87  0.01 -0.12  1.   -0.14  0.09  0.95\n",
      "   0.02  0.06  1.07 -0.07  0.15  1.06  0.07  0.02  1.05 -0.04  0.01  0.9\n",
      "  -0.01 -0.06  0.93 -0.14  0.01  0.91 -0.13  0.08  1.04 -0.17  0.13  0.98\n",
      "   0.02  0.07  1.08  0.02  0.05  1.06  0.14 -0.13  1.01  0.05 -0.17  0.87\n",
      "   0.06 -0.17  0.88 -0.08 -0.04  0.9   0.04 -0.03  1.04 -0.13  0.15  1.02]\n",
      " [-0.15  0.15  0.86  0.03  0.08  0.97  0.03  0.05  0.89 -0.03  0.1   0.94\n",
      "   0.01  0.13  1.1  -0.07  0.12  0.87  0.04  0.05  1.1  -0.08  0.08  1.03\n",
      "   0.13 -0.2   1.09  0.05 -0.13  1.09 -0.06 -0.11  0.91  0.05  0.03  1.07\n",
      "  -0.13  0.1   0.88  0.06  0.05  1.1  -0.15  0.15  0.91 -0.07  0.1   0.86\n",
      "  -0.12  0.16  0.96 -0.14  0.09  0.86  0.03  0.08  1.02 -0.16  0.16  0.84]\n",
      " [-0.1   0.16  0.97 -0.05 -0.02  0.93 -0.14  0.08  0.9   0.    0.04  0.97\n",
      "  -0.1   0.1   0.94 -0.07  0.18  0.98 -0.05 -0.27  0.91 -0.07 -0.2   0.93\n",
      "   0.   -0.22  1.03 -0.08 -0.01  0.92 -0.02  0.03  1.03 -0.08  0.07  0.94\n",
      "  -0.09  0.15  0.99  0.01  0.04  1.05 -0.14  0.16  0.95 -0.04  0.1   1.05\n",
      "   0.   -0.01  1.03 -0.04  0.13  1.02  0.03  0.06  1.06 -0.05  0.08  0.96]\n",
      " [-0.08  0.16  0.98  0.02  0.05  1.05 -0.15  0.16  0.95 -0.03  0.09  1.06\n",
      "   0.02 -0.01  1.03 -0.06  0.13  1.01  0.03  0.06  1.06 -0.06 -0.    0.92\n",
      "   0.01  0.06  1.04 -0.   -0.04  0.97 -0.09  0.03  1.04  0.06 -0.3   1.03\n",
      "  -0.05 -0.24  0.91  0.05  0.01  1.06 -0.   -0.01  0.94 -0.09  0.08  0.9\n",
      "   0.01  0.    1.01 -0.11  0.07  0.94 -0.1   0.16  0.97 -0.05 -0.02  0.93]\n",
      " [-0.09 -0.    0.87 -0.09  0.01  0.89  0.11 -0.2   1.01 -0.01  0.09  0.86\n",
      "   0.07  0.1   1.1  -0.14  0.18  0.85 -0.11  0.21  0.98 -0.03  0.06  0.92\n",
      "  -0.02  0.14  1.06  0.02  0.13  1.09 -0.17  0.19  0.87 -0.02  0.12  0.97\n",
      "  -0.12  0.08  0.88  0.03  0.1   1.12  0.13 -0.23  1.09 -0.05 -0.05  1.\n",
      "   0.04 -0.21  0.96 -0.11  0.09  0.83  0.09  0.07  1.1  -0.15  0.21  0.87]\n",
      " [-0.02 -0.33  0.93  0.07 -0.1   1.06 -0.05 -0.05  0.93 -0.04  0.05  1.03\n",
      "  -0.11  0.09  0.89 -0.01  0.07  1.04 -0.02 -0.01  0.94  0.    0.    1.05\n",
      "  -0.1   0.05  0.95 -0.02  0.03  0.95 -0.14  0.13  0.91  0.02  0.1   1.06\n",
      "  -0.05 -0.09  1.02  0.04 -0.22  1.   -0.09 -0.21  0.89 -0.   -0.22  0.92\n",
      "  -0.08 -0.06  0.97 -0.06  0.06  0.93 -0.03  0.12  1.03  0.03 -0.02  1.02]\n",
      " [-0.13  0.2   0.96 -0.05  0.15  1.06 -0.12  0.11  0.94 -0.09  0.17  0.99\n",
      "  -0.02  0.08  1.04 -0.12  0.    0.93 -0.05 -0.01  1.05  0.02 -0.12  1.06\n",
      "  -0.08 -0.1   0.99  0.02 -0.17  1.07  0.07 -0.26  0.93 -0.06  0.15  1.01\n",
      "   0.02  0.06  1.05 -0.13  0.2   0.94 -0.07  0.19  1.05  0.    0.05  1.09\n",
      "  -0.12  0.17  0.95 -0.05  0.13  1.03 -0.01  0.11  1.07 -0.12  0.11  0.98]\n",
      " [ 0.07 -0.01  1.01 -0.01 -0.16  1.04  0.11 -0.2   0.89 -0.01 -0.16  0.88\n",
      "  -0.11  0.02  0.87 -0.16  0.12  0.96 -0.    0.1   1.08 -0.08  0.14  1.06\n",
      "   0.04  0.05  1.06 -0.01 -0.03  1.11 -0.02  0.01  0.95 -0.12 -0.03  0.89\n",
      "  -0.08  0.04  0.89 -0.16  0.11  0.95 -0.04  0.1   1.07  0.05 -0.13  1.03\n",
      "   0.15 -0.12  1.07  0.15 -0.21  0.93  0.09 -0.02  0.99 -0.05 -0.04  0.87]\n",
      " [-0.   -0.15  0.98 -0.01 -0.19  0.93 -0.02  0.01  0.91  0.03  0.04  0.96\n",
      "   0.06  0.04  1.03 -0.07  0.12  1.07 -0.11  0.09  1.04 -0.14  0.08  0.99\n",
      "  -0.11  0.05  0.88 -0.09  0.08  0.89  0.05  0.03  1.06  0.04  0.05  1.08\n",
      "  -0.    0.07  1.07  0.02 -0.21  0.98 -0.02 -0.19  0.94  0.03 -0.22  0.91\n",
      "   0.02  0.03  0.94  0.05  0.04  1.    0.06  0.12  1.08 -0.1   0.12  1.04]\n",
      " [-0.14  0.06  0.9  -0.12  0.13  1.03  0.04  0.08  1.08 -0.04  0.13  1.09\n",
      "   0.03  0.    1.   -0.07 -0.04  0.89 -0.06 -0.02  0.9  -0.16  0.04  0.92\n",
      "  -0.06  0.09  1.06 -0.1   0.1   1.04  0.07  0.03  1.09  0.06 -0.11  1.07\n",
      "   0.16 -0.21  0.95  0.02 -0.19  0.86 -0.06  0.    0.87 -0.14  0.09  0.95\n",
      "  -0.02  0.11  1.07 -0.1   0.15  1.07  0.05  0.05  1.06 -0.02 -0.04  0.92]\n",
      " [-0.11  0.02  0.88 -0.08  0.04  0.89  0.04  0.05  1.05  0.01  0.06  1.07\n",
      "  -0.04  0.09  1.07 -0.04 -0.15  0.96 -0.01 -0.17  0.92  0.08 -0.09  0.93\n",
      "   0.1  -0.06  0.99  0.11 -0.06  1.05 -0.1   0.1   1.04 -0.13  0.08  1.01\n",
      "  -0.14  0.05  0.96 -0.05  0.04  0.9  -0.03  0.1   0.95  0.02  0.05  1.07\n",
      "  -0.05  0.07  1.05 -0.09  0.08  1.03 -0.06 -0.1   0.93 -0.02 -0.11  0.89]\n",
      " [-0.09  0.06  0.92 -0.1   0.15  0.95 -0.05  0.03  0.9  -0.11  0.15  0.93\n",
      "  -0.1   0.22  0.93 -0.09 -0.03  0.86 -0.13  0.13  0.92 -0.04  0.04  0.93\n",
      "  -0.12  0.11  0.91 -0.14  0.16  0.95 -0.01 -0.3   0.93 -0.08 -0.2   0.91\n",
      "  -0.1  -0.19  0.96 -0.01 -0.13  0.92 -0.08  0.01  0.93  0.03 -0.02  0.97\n",
      "  -0.03  0.01  0.91 -0.1   0.13  0.93  0.01 -0.05  0.96 -0.06 -0.03  0.89]\n",
      " [ 0.01 -0.08  0.92  0.17 -0.16  1.03  0.16 -0.16  1.04 -0.1   0.09  1.02\n",
      "  -0.12  0.06  0.99 -0.13  0.05  0.96 -0.04  0.02  0.93 -0.02  0.05  0.94\n",
      "   0.    0.07  0.98 -0.01  0.08  1.05 -0.05  0.07  1.05 -0.15  0.06  0.97\n",
      "  -0.14  0.03  0.94 -0.11  0.02  0.92  0.16 -0.21  0.96  0.17 -0.17  1.01\n",
      "   0.18 -0.19  1.03 -0.06  0.08  1.03 -0.11  0.09  1.01 -0.12  0.06  0.92]\n",
      " [-0.09  0.2   1.   -0.1   0.12  0.93 -0.1   0.21  1.01 -0.13  0.16  0.95\n",
      "  -0.06  0.19  1.05  0.    0.09  1.06 -0.02 -0.08  1.06  0.03 -0.21  1.03\n",
      "  -0.04 -0.2   0.91  0.06 -0.29  1.   -0.06 -0.15  0.9  -0.09 -0.03  0.94\n",
      "  -0.09  0.1   0.89 -0.07  0.16  1.02 -0.12  0.16  0.93 -0.05  0.2   1.05\n",
      "   0.04 -0.02  0.95 -0.02  0.09  1.05 -0.01  0.06  0.94  0.03 -0.04  1.07]\n",
      " [ 0.05 -0.19  0.86 -0.03 -0.    0.9  -0.16  0.07  0.91 -0.08  0.12  1.04\n",
      "  -0.16  0.14  1.02 -0.02  0.11  1.08  0.04 -0.    0.98  0.02  0.06  1.04\n",
      "  -0.05 -0.03  0.9  -0.01 -0.    0.93 -0.15  0.07  0.92 -0.1   0.12  1.04\n",
      "   0.01 -0.13  0.99  0.14 -0.13  1.06  0.18 -0.21  0.97  0.08  0.02  1.03\n",
      "  -0.01 -0.02  0.9  -0.13  0.    0.96 -0.12  0.02  0.89 -0.17  0.12  0.97]\n",
      " [-0.09  0.08  0.86 -0.17  0.18  0.87  0.02  0.06  1.02 -0.13  0.22  0.97\n",
      "   0.02  0.13  1.07 -0.18  0.16  0.87  0.09  0.03  1.12  0.07 -0.23  1.01\n",
      "   0.04 -0.11  1.09 -0.02 -0.14  1.1  -0.13  0.15  0.86  0.1  -0.01  1.06\n",
      "   0.02  0.05  0.93 -0.02  0.12  1.05 -0.09  0.09  0.85 -0.14  0.16  0.86\n",
      "   0.02  0.04  1.03 -0.13  0.19  0.97 -0.01  0.15  1.07 -0.17  0.18  0.86]\n",
      " [ 0.12 -0.09  1.03  0.06 -0.01  1.05  0.03 -0.01  1.05 -0.13  0.05  0.95\n",
      "  -0.12  0.05  0.91 -0.08  0.02  0.89 -0.02  0.1   1.   -0.02  0.11  1.05\n",
      "  -0.08  0.08  1.03 -0.14  0.09  0.99 -0.16  0.06  0.94  0.03 -0.13  0.91\n",
      "   0.08 -0.13  0.93  0.15 -0.16  1.01  0.08 -0.11  1.06  0.   -0.05  1.02\n",
      "  -0.12  0.04  0.92 -0.1   0.03  0.89 -0.04  0.03  0.9  -0.01  0.09  1.05]\n",
      " [-0.08  0.03  0.9  -0.04  0.13  0.97  0.    0.11  1.02 -0.01  0.11  1.04\n",
      "  -0.14  0.11  1.01 -0.18  0.09  0.98 -0.17  0.07  0.94  0.11 -0.2   0.92\n",
      "   0.14 -0.17  0.97  0.04  0.02  1.06 -0.    0.03  1.05 -0.06  0.05  1.03\n",
      "  -0.13  0.05  0.93 -0.09  0.03  0.9  -0.05  0.03  0.9  -0.02  0.13  1.02\n",
      "   0.01  0.12  1.04 -0.03  0.1   1.03 -0.16  0.08  0.99 -0.18  0.07  0.95]\n",
      " [ 0.02  0.02  1.   -0.06  0.19  1.06  0.    0.08  1.09  0.01  0.02  1.\n",
      "  -0.01 -0.24  1.06  0.05 -0.29  1.09  0.05 -0.38  0.99  0.    0.06  1.05\n",
      "   0.06 -0.02  1.09  0.06  0.18  0.98 -0.01  0.09  1.03  0.04  0.04  1.08\n",
      "  -0.12  0.25  0.97 -0.03  0.18  1.02  0.01  0.07  1.05 -0.12  0.19  0.98\n",
      "  -0.02  0.17  1.04  0.01  0.06  1.08 -0.1  -0.05  0.99 -0.01 -0.12  1.05]\n",
      " [-0.03  0.08  0.92 -0.08  0.04  0.88 -0.12  0.13  0.96  0.02 -0.18  0.97\n",
      "  -0.04 -0.15  0.92  0.06 -0.25  1.1   0.06 -0.31  1.01  0.01 -0.27  0.94\n",
      "   0.03  0.06  1.07  0.06 -0.01  1.03  0.03 -0.03  0.94 -0.02  0.11  1.03\n",
      "   0.03  0.03  1.07 -0.11  0.19  0.96 -0.05  0.15  1.02 -0.    0.11  1.05\n",
      "  -0.13  0.13  0.95 -0.07  0.1   1.04 -0.    0.02  1.05 -0.09 -0.16  0.93]\n",
      " [ 0.05 -0.13  1.09 -0.06 -0.11  0.91  0.05  0.03  1.07 -0.13  0.1   0.88\n",
      "   0.06  0.05  1.1  -0.15  0.15  0.91 -0.07  0.1   0.86 -0.12  0.16  0.96\n",
      "  -0.14  0.09  0.86  0.03  0.08  1.02 -0.16  0.16  0.84 -0.01  0.11  0.97\n",
      "  -0.1  -0.06  0.9   0.02 -0.15  0.98  0.12 -0.15  1.15 -0.   -0.    0.9\n",
      "   0.12 -0.04  1.1  -0.02  0.04  1.03  0.07  0.07  1.02  0.02  0.06  1.05]\n",
      " [-0.    0.08  1.04 -0.13  0.15  0.87 -0.03  0.1   0.9  -0.15  0.16  0.92\n",
      "  -0.09  0.1   0.87  0.04  0.04  1.   -0.15  0.13  0.84 -0.01  0.15  0.99\n",
      "  -0.14  0.05  0.87  0.02 -0.03  0.99  0.08 -0.09  1.12  0.04 -0.1   0.94\n",
      "   0.13 -0.15  1.15 -0.05 -0.04  1.    0.09  0.06  1.06 -0.05  0.12  1.03\n",
      "   0.05  0.02  1.04 -0.02  0.11  1.07 -0.15  0.13  0.88  0.02  0.1   1.07]\n",
      " [-0.05 -0.    0.88 -0.1  -0.02  1.03 -0.01 -0.02  0.94 -0.08  0.07  0.89\n",
      "   0.02  0.01  0.99 -0.03 -0.05  0.92 -0.11  0.01  0.88  0.01  0.03  1.01\n",
      "  -0.04  0.06  0.93 -0.11  0.09  0.91  0.05 -0.34  1.02 -0.01 -0.31  0.93\n",
      "   0.05 -0.05  1.09  0.06 -0.14  1.01  0.02 -0.12  0.94  0.03  0.04  1.09\n",
      "   0.03 -0.03  1.   -0.02 -0.    0.93  0.    0.07  1.07  0.01 -0.06  0.96]\n",
      " [ 0.02  0.04  1.09 -0.02 -0.23  1.06  0.08 -0.31  1.05 -0.04 -0.31  0.89\n",
      "   0.06 -0.08  0.98 -0.07  0.    0.87 -0.09  0.15  0.97 -0.1   0.16  0.93\n",
      "  -0.06  0.17  1.05  0.04  0.18  0.92 -0.03  0.15  1.03  0.01 -0.    0.97\n",
      "   0.01  0.08  1.09 -0.02  0.06  0.94 -0.13  0.14  0.94 -0.02 -0.27  0.9\n",
      "  -0.1  -0.17  0.93  0.   -0.23  1.06 -0.09  0.08  0.98  0.05 -0.03  1.07]\n",
      " [ 0.06 -0.14  0.92 -0.04 -0.11  0.92 -0.01 -0.12  0.87 -0.03 -0.04  0.99\n",
      "  -0.16  0.12  0.96 -0.    0.12  1.07  0.04  0.03  1.01  0.04  0.04  1.05\n",
      "  -0.03 -0.04  0.92 -0.14  0.03  0.93 -0.14  0.01  0.9  -0.12  0.1   1.03\n",
      "  -0.15  0.11  0.99  0.02  0.07  1.08  0.06 -0.03  1.    0.18 -0.17  1.01\n",
      "   0.09 -0.21  0.88 -0.01 -0.14  0.9  -0.15  0.05  0.89 -0.12  0.13  1.02]\n",
      " [-0.02  0.08  1.04 -0.12  0.    0.93 -0.05 -0.01  1.05  0.02 -0.12  1.06\n",
      "  -0.08 -0.1   0.99  0.02 -0.17  1.07  0.07 -0.26  0.93 -0.06  0.15  1.01\n",
      "   0.02  0.06  1.05 -0.13  0.2   0.94 -0.07  0.19  1.05  0.    0.05  1.09\n",
      "  -0.12  0.17  0.95 -0.05  0.13  1.03 -0.01  0.11  1.07 -0.12  0.11  0.98\n",
      "  -0.04  0.06  1.05 -0.08 -0.18  0.92 -0.09 -0.15  0.97 -0.   -0.21  1.07]\n",
      " [-0.13  0.04  0.89 -0.1   0.    0.89 -0.18  0.14  1.   -0.15  0.05  0.98\n",
      "  -0.04  0.11  1.1  -0.    0.05  1.02  0.05  0.05  1.05 -0.04  0.02  0.92\n",
      "  -0.15  0.09  0.94  0.01 -0.19  0.88  0.03 -0.12  1.02 -0.08  0.07  1.\n",
      "   0.06  0.05  1.07  0.05  0.    0.95  0.04  0.03  1.01 -0.06 -0.01  0.89\n",
      "  -0.15  0.08  0.97 -0.14 -0.    0.92 -0.09  0.09  1.07  0.    0.09  1.02]\n",
      " [ 0.03  0.01  1.06 -0.15  0.06  0.98 -0.12  0.05  0.94 -0.07  0.09  0.95\n",
      "  -0.02  0.12  1.03 -0.02  0.12  1.05 -0.14  0.1   1.01 -0.17  0.06  0.96\n",
      "  -0.15  0.08  0.91  0.11 -0.12  0.96  0.14 -0.12  1.03  0.13 -0.08  1.06\n",
      "  -0.03 -0.06  1.   -0.05 -0.08  0.94 -0.03  0.03  0.9   0.04  0.04  0.95\n",
      "   0.05  0.07  1.02 -0.07  0.11  1.06 -0.11  0.09  1.04 -0.14  0.06  1.  ]\n",
      " [ 0.04 -0.03  1.04 -0.13  0.15  1.02  0.03  0.09  1.07  0.02 -0.    0.94\n",
      "   0.03  0.    0.99 -0.09 -0.03  0.89 -0.07 -0.    0.9  -0.15  0.06  0.94\n",
      "  -0.06  0.09  1.06 -0.1   0.08  1.03  0.06  0.02  1.07  0.07 -0.06  0.96\n",
      "   0.18 -0.19  1.    0.07 -0.19  0.87 -0.    0.    0.93 -0.14  0.04  0.89\n",
      "  -0.15  0.14  1.   -0.16  0.09  0.97 -0.07  0.14  1.08  0.04  0.03  1.03]\n",
      " [-0.12  0.02  0.88 -0.08 -0.04  0.88 -0.14  0.04  0.94 -0.16  0.09  0.92\n",
      "  -0.12  0.13  1.03  0.03  0.07  1.11  0.1  -0.11  1.07  0.17 -0.2   1.01\n",
      "   0.07 -0.2   0.87  0.04 -0.05  0.9  -0.08 -0.01  0.88 -0.06  0.04  0.88\n",
      "  -0.15  0.14  1.02  0.01  0.1   1.08 -0.04  0.1   1.12  0.02  0.02  1.02\n",
      "  -0.05 -0.06  0.89 -0.05  0.04  0.92 -0.16  0.09  0.91 -0.09  0.12  1.05]\n",
      " [ 0.03 -0.01  1.02 -0.09  0.05  0.93 -0.13  0.15  0.96 -0.02 -0.04  0.95\n",
      "  -0.12  0.07  0.91  0.03  0.02  1.   -0.07  0.07  0.92 -0.13  0.14  0.91\n",
      "  -0.   -0.28  0.94 -0.08 -0.25  0.89 -0.03 -0.19  1.01 -0.06 -0.06  0.94\n",
      "  -0.09  0.03  0.96 -0.06  0.01  0.93 -0.13  0.1   0.89 -0.04  0.12  1.04\n",
      "  -0.12  0.06  0.94 -0.11  0.16  1.01  0.01  0.02  1.06 -0.13  0.13  0.91]\n",
      " [-0.1   0.13  1.02  0.03 -0.04  1.02  0.    0.07  1.07 -0.07  0.02  0.93\n",
      "  -0.   -0.04  0.97 -0.13  0.05  0.9   0.02 -0.04  1.05 -0.   -0.24  1.04\n",
      "   0.01 -0.3   0.97 -0.09 -0.2   0.94 -0.08  0.07  0.93  0.    0.08  1.05\n",
      "   0.02 -0.04  0.94  0.04  0.01  1.05 -0.12  0.08  0.94 -0.02 -0.05  0.93\n",
      "  -0.14  0.14  0.96  0.02  0.01  1.04 -0.01  0.12  1.05 -0.02  0.    0.94]\n",
      " [-0.09 -0.01  0.93 -0.09  0.07  0.97 -0.01  0.    0.9  -0.08  0.09  0.92\n",
      "  -0.1   0.15  0.93 -0.03 -0.04  0.93 -0.1   0.07  0.89 -0.13  0.16  0.92\n",
      "  -0.03  0.08  0.92 -0.08  0.04  0.88 -0.12  0.13  0.96  0.02 -0.18  0.97\n",
      "  -0.04 -0.15  0.92  0.06 -0.25  1.1   0.06 -0.31  1.01  0.01 -0.27  0.94\n",
      "   0.03  0.06  1.07  0.06 -0.01  1.03  0.03 -0.03  0.94 -0.02  0.11  1.03]\n",
      " [ 0.06 -0.03  0.94 -0.06  0.15  1.05  0.01  0.06  1.08 -0.13  0.18  0.92\n",
      "  -0.07  0.17  1.02 -0.02  0.13  1.04 -0.14  0.18  0.95 -0.08  0.17  1.05\n",
      "   0.    0.08  1.07 -0.1  -0.2   0.95 -0.03 -0.19  1.05 -0.08 -0.    0.92\n",
      "  -0.11  0.1   0.94 -0.03  0.1   1.05 -0.09  0.11  0.92 -0.12  0.18  0.94\n",
      "  -0.05  0.14  1.04 -0.11  0.07  0.91 -0.13  0.18  0.94 -0.07  0.07  0.92]\n",
      " [ 0.04 -0.03  0.96  0.02  0.03  1.04 -0.03  0.    0.92  0.01  0.    1.04\n",
      "  -0.11  0.02  0.94 -0.05  0.12  1.03 -0.12  0.11  0.9   0.01  0.05  1.04\n",
      "  -0.05  0.01  0.9   0.06 -0.32  1.04 -0.07 -0.27  0.92  0.01 -0.09  0.94\n",
      "  -0.1   0.11  0.94  0.06 -0.05  1.05 -0.04  0.13  1.05  0.01 -0.03  1.\n",
      "  -0.13  0.13  0.9  -0.05 -0.03  0.92 -0.12  0.14  0.99  0.02  0.01  0.94]\n",
      " [-0.15  0.18  0.91  0.03  0.08  1.11 -0.11  0.17  0.99 -0.17  0.18  0.84\n",
      "   0.04 -0.11  1.11 -0.09 -0.06  0.91  0.1  -0.07  1.12 -0.06  0.01  0.99\n",
      "  -0.09  0.03  0.86 -0.    0.08  1.04 -0.13  0.15  0.87 -0.03  0.1   0.9\n",
      "  -0.15  0.16  0.92 -0.09  0.1   0.87  0.04  0.04  1.   -0.15  0.13  0.84\n",
      "  -0.01  0.15  0.99 -0.14  0.05  0.87  0.02 -0.03  0.99  0.08 -0.09  1.12]\n",
      " [ 0.05 -0.13  1.05 -0.12  0.1   0.98  0.02  0.06  1.07  0.06  0.05  1.\n",
      "   0.05  0.05  1.08  0.02 -0.02  0.93 -0.12  0.02  0.88 -0.08 -0.04  0.88\n",
      "  -0.14  0.04  0.94 -0.16  0.09  0.92 -0.12  0.13  1.03  0.03  0.07  1.11\n",
      "   0.1  -0.11  1.07  0.17 -0.2   1.01  0.07 -0.2   0.87  0.04 -0.05  0.9\n",
      "  -0.08 -0.01  0.88 -0.06  0.04  0.88 -0.15  0.14  1.02  0.01  0.1   1.08]\n",
      " [ 0.01  0.13  0.89 -0.05  0.02  0.93  0.07  0.01  1.15  0.   -0.16  0.92\n",
      "   0.15 -0.21  1.15 -0.01 -0.06  1.04  0.09  0.04  1.07 -0.05  0.16  1.03\n",
      "  -0.12  0.15  0.85 -0.03  0.15  1.05 -0.14  0.17  0.86 -0.05  0.15  1.07\n",
      "  -0.16  0.18  0.87 -0.06  0.06  0.93 -0.19  0.19  0.87 -0.05  0.09  0.9\n",
      "   0.08  0.03  1.12 -0.03 -0.12  0.89  0.13 -0.25  1.12 -0.11  0.06  0.84]\n",
      " [-0.13  0.13  0.95 -0.07  0.1   1.04 -0.    0.02  1.05 -0.09 -0.16  0.93\n",
      "  -0.06 -0.16  1.01  0.01 -0.03  0.88 -0.09  0.09  0.92 -0.1   0.14  0.95\n",
      "  -0.03  0.02  0.92 -0.1   0.12  0.92 -0.13  0.19  0.94 -0.03 -0.03  0.92\n",
      "  -0.1   0.01  0.89 -0.13  0.14  0.91 -0.03  0.04  0.93 -0.1   0.09  0.9\n",
      "   0.06 -0.31  1.05  0.02 -0.33  0.96 -0.04 -0.29  0.89  0.06 -0.07  1.1 ]\n",
      " [-0.08  0.04  0.89 -0.16  0.11  0.95 -0.04  0.1   1.07  0.05 -0.13  1.03\n",
      "   0.15 -0.12  1.07  0.15 -0.21  0.93  0.09 -0.02  0.99 -0.05 -0.04  0.87\n",
      "  -0.04 -0.01  0.92 -0.15  0.06  0.91 -0.15  0.16  1.03 -0.13  0.06  0.98\n",
      "  -0.06  0.12  1.09  0.03  0.04  1.05  0.04  0.07  1.09 -0.    0.02  0.96\n",
      "   0.12 -0.11  0.99  0.04 -0.18  0.88 -0.03 -0.11  0.93 -0.02 -0.1   0.89]\n",
      " [-0.17  0.12  0.84 -0.12  0.12  0.95 -0.01 -0.05  1.    0.02  0.05  1.1\n",
      "   0.14 -0.2   1.1  -0.07 -0.02  0.93  0.05  0.13  1.05 -0.12  0.14  0.84\n",
      "   0.08  0.04  1.08 -0.01  0.03  0.96 -0.03  0.19  1.04 -0.13  0.1   0.85\n",
      "  -0.16  0.18  0.88 -0.02  0.06  0.97 -0.05  0.24  1.02  0.04  0.09  1.14\n",
      "  -0.16  0.17  0.89  0.03 -0.09  1.07 -0.04 -0.22  0.91  0.14 -0.18  1.16]\n",
      " [-0.   -0.21  1.03 -0.01 -0.31  0.95 -0.09 -0.21  0.93 -0.08 -0.04  0.93\n",
      "  -0.    0.01  1.04  0.03 -0.11  0.98  0.05  0.    1.05 -0.06  0.04  0.93\n",
      "   0.03 -0.08  1.02 -0.14  0.09  0.89 -0.01  0.05  1.07 -0.1   0.17  0.96\n",
      "   0.02  0.04  1.03 -0.11  0.12  0.93 -0.02 -0.21  0.93 -0.1  -0.09  0.94\n",
      "  -0.09 -0.15  0.93 -0.   -0.14  1.04  0.04 -0.22  0.99  0.04  0.02  1.05]\n",
      " [-0.02  0.07  1.09 -0.12  0.03  0.9  -0.1  -0.03  0.86  0.12 -0.25  1.04\n",
      "  -0.02  0.    1.01  0.05  0.12  1.09 -0.13  0.16  0.83  0.1   0.06  0.94\n",
      "   0.    0.03  0.93 -0.02  0.17  1.06  0.05  0.13  1.09 -0.15  0.19  0.87\n",
      "  -0.01  0.07  0.96 -0.15  0.14  0.87  0.06  0.1   1.14 -0.18  0.19  0.87\n",
      "   0.01 -0.09  1.08 -0.01 -0.22  0.91 -0.11  0.11  0.84  0.09  0.02  1.02]\n",
      " [-0.11 -0.03  0.92  0.01 -0.31  0.94 -0.07 -0.2   0.93 -0.1  -0.11  0.97\n",
      "  -0.03 -0.02  0.9  -0.09  0.11  0.92  0.01 -0.03  0.96 -0.07  0.05  0.88\n",
      "  -0.13  0.19  0.94 -0.03 -0.04  0.92 -0.11  0.03  0.91 -0.12  0.15  0.94\n",
      "  -0.07  0.04  0.91 -0.13  0.11  0.93  0.03 -0.34  0.94 -0.08 -0.23  0.9\n",
      "  -0.1  -0.15  0.94 -0.   -0.07  0.91 -0.09  0.06  0.92 -0.1   0.15  0.95]\n",
      " [ 0.06 -0.01  0.98  0.1  -0.02  1.04 -0.08  0.1   1.05 -0.13  0.1   1.01\n",
      "  -0.13  0.05  0.97 -0.07  0.03  0.9  -0.04  0.08  0.93 -0.02  0.11  0.99\n",
      "  -0.04  0.08  1.06 -0.1   0.11  1.02 -0.15 -0.18  0.93  0.03 -0.2   0.9\n",
      "   0.09 -0.21  0.92  0.12 -0.09  1.03  0.06 -0.01  1.05  0.03 -0.01  1.05\n",
      "  -0.13  0.05  0.95 -0.12  0.05  0.91 -0.08  0.02  0.89 -0.02  0.1   1.  ]\n",
      " [ 0.09 -0.11  0.91 -0.03 -0.06  0.88 -0.04  0.01  0.9  -0.14  0.09  0.94\n",
      "  -0.11 -0.01  0.9  -0.15  0.07  1.   -0.03  0.12  1.09 -0.08  0.09  1.07\n",
      "   0.05  0.08  1.09 -0.01  0.05  0.95  0.11 -0.1   0.99 -0.   -0.14  0.88\n",
      "  -0.06 -0.08  0.95 -0.06 -0.06  0.9   0.03 -0.05  1.04 -0.12  0.15  1.01\n",
      "   0.03  0.09  1.08  0.03  0.02  0.95  0.04  0.    1.01 -0.06 -0.04  0.89]\n",
      " [ 0.12 -0.05  1.07  0.09 -0.11  0.91 -0.03 -0.06  0.88 -0.04  0.01  0.9\n",
      "  -0.14  0.09  0.94 -0.11 -0.01  0.9  -0.15  0.07  1.   -0.03  0.12  1.09\n",
      "  -0.08  0.09  1.07  0.05  0.08  1.09 -0.01  0.05  0.95  0.11 -0.1   0.99\n",
      "  -0.   -0.14  0.88 -0.06 -0.08  0.95 -0.06 -0.06  0.9   0.03 -0.05  1.04\n",
      "  -0.12  0.15  1.01  0.03  0.09  1.08  0.03  0.02  0.95  0.04  0.    1.01]\n",
      " [-0.01  0.11  1.04 -0.14  0.11  1.01 -0.18  0.09  0.98 -0.17  0.07  0.94\n",
      "   0.11 -0.2   0.92  0.14 -0.17  0.97  0.04  0.02  1.06 -0.    0.03  1.05\n",
      "  -0.06  0.05  1.03 -0.13  0.05  0.93 -0.09  0.03  0.9  -0.05  0.03  0.9\n",
      "  -0.02  0.13  1.02  0.01  0.12  1.04 -0.03  0.1   1.03 -0.16  0.08  0.99\n",
      "  -0.18  0.07  0.95  0.05 -0.14  0.92  0.13 -0.14  0.97  0.16 -0.15  1.03]\n",
      " [ 0.02 -0.14  0.99 -0.01 -0.16  1.04  0.05 -0.23  1.05 -0.07 -0.18  1.\n",
      "   0.05  0.03  1.05 -0.04  0.    0.92  0.01  0.07  1.06 -0.02 -0.01  0.97\n",
      "  -0.14  0.12  0.91  0.   -0.02  0.99 -0.1   0.05  0.95 -0.08  0.11  0.98\n",
      "  -0.07  0.06  0.92 -0.11  0.14  0.94 -0.03 -0.28  0.91 -0.09 -0.21  0.89\n",
      "  -0.   -0.21  1.04 -0.1   0.07  0.93 -0.06  0.12  1.01  0.05  0.03  1.06]\n",
      " [-0.03  0.08  1.05 -0.11  0.07  0.91 -0.04  0.13  1.02 -0.06 -0.09  0.91\n",
      "  -0.08 -0.02  0.98  0.04 -0.1   1.02 -0.06 -0.14  0.95  0.02 -0.21  1.05\n",
      "   0.03 -0.26  0.98 -0.01  0.08  1.04  0.04 -0.01  1.02 -0.07  0.14  1.02\n",
      "   0.02 -0.    1.04 -0.03 -0.01  0.95 -0.02  0.07  1.05  0.03  0.    1.\n",
      "  -0.08  0.04  0.95  0.04  0.01  1.05 -0.03 -0.03  0.93 -0.11  0.03  1.04]\n",
      " [ 0.13 -0.23  1.06 -0.04 -0.02  1.01  0.02 -0.19  0.92 -0.12  0.13  0.83\n",
      "   0.09  0.05  1.07  0.01  0.05  0.96 -0.05  0.16  1.04 -0.09  0.1   0.86\n",
      "  -0.16  0.17  0.88  0.01  0.11  1.03 -0.14  0.21  0.95 -0.02  0.14  1.08\n",
      "  -0.13  0.11  0.86  0.05  0.03  0.89  0.14 -0.25  1.08 -0.06 -0.05  1.\n",
      "   0.02  0.09  1.07 -0.09  0.09  0.84  0.07  0.07  1.12  0.07  0.03  1.02]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Spotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 42\n",
    "scoring = 'f1_macro'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "\n",
    "#models.append(('XGB', XGBClassifier(random_state=seed)))\n",
    "models.append(('GNB', GaussianNB(var_smoothing=2e-9)))\n",
    "models.append(('LR',  LogisticRegression(random_state=seed)))\n",
    "models.append(('CART' , DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('SVC' , SVC(gamma=0.5, random_state=seed)))\n",
    "models.append(('RF', RandomForestClassifier(random_state=seed, n_estimators = 50)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB - 0,78 0,03\n",
      "LR - 0,48 0,02\n",
      "CART - 0,78 0,03\n",
      "SVC - 0,96 0,01\n",
      "RF - 0,99 0,01\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    # Dividere dati in n = num_folds\n",
    "    kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = np.array([])\n",
    "    for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "        X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "        X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "        model.fit(X_cross_train, y_cross_train)  \n",
    "        y_pred = model.predict(X_cross_test)\n",
    "        f1s = f1_score(y_cross_test, y_pred, average=\"weighted\")\n",
    "        cv_results = np.append(cv_results, [f1s])\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #msg = \"%s - %f - %f\" % (name, cv_results.mean(), cv_results.std())\n",
    "    msg = \"{} - {:.2f} {:.2f}\".format(name, cv_results.mean(), cv_results.std()).replace('.', ',')\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "fig.suptitle('Algorithms Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione dei modelli sul Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model GNB: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.68      0.70       156\n",
      "           1       0.69      0.72      0.71       156\n",
      "           2       0.99      0.97      0.98       156\n",
      "           3       0.84      0.76      0.80       156\n",
      "           4       0.79      0.87      0.83       156\n",
      "\n",
      "    accuracy                           0.80       780\n",
      "   macro avg       0.80      0.80      0.80       780\n",
      "weighted avg       0.80      0.80      0.80       780\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model LR: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.30      0.34       156\n",
      "           1       0.38      0.51      0.43       156\n",
      "           2       0.86      0.98      0.92       156\n",
      "           3       0.18      0.08      0.11       156\n",
      "           4       0.55      0.71      0.62       156\n",
      "\n",
      "    accuracy                           0.52       780\n",
      "   macro avg       0.47      0.52      0.48       780\n",
      "weighted avg       0.47      0.52      0.48       780\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model CART: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       156\n",
      "           1       0.80      0.78      0.79       156\n",
      "           2       0.83      0.75      0.79       156\n",
      "           3       0.72      0.74      0.73       156\n",
      "           4       0.79      0.82      0.81       156\n",
      "\n",
      "    accuracy                           0.79       780\n",
      "   macro avg       0.79      0.79      0.79       780\n",
      "weighted avg       0.79      0.79      0.79       780\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model SVC: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       156\n",
      "           1       0.97      0.99      0.98       156\n",
      "           2       0.99      0.99      0.99       156\n",
      "           3       0.99      0.88      0.93       156\n",
      "           4       0.92      0.98      0.95       156\n",
      "\n",
      "    accuracy                           0.97       780\n",
      "   macro avg       0.97      0.97      0.97       780\n",
      "weighted avg       0.97      0.97      0.97       780\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model RF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       156\n",
      "           1       0.99      1.00      1.00       156\n",
      "           2       1.00      0.99      1.00       156\n",
      "           3       0.99      0.97      0.98       156\n",
      "           4       0.97      0.99      0.98       156\n",
      "\n",
      "    accuracy                           0.99       780\n",
      "   macro avg       0.99      0.99      0.99       780\n",
      "weighted avg       0.99      0.99      0.99       780\n",
      "\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def classification_report_csv(report, model_name):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    index = 0\n",
    "    row = lines[-4].split('    ')\n",
    "    accuracy = row[-2]\n",
    "    for line in lines[2:-5]:\n",
    "        row = {}\n",
    "        row_data = line.split('      ')\n",
    "        row['class'] = uniques[index]\n",
    "        row['precision'] = float(row_data[2]) \n",
    "        row['recall'] = float(row_data[3]) \n",
    "        row['f1_score'] = float(row_data[4])\n",
    "        row['accuracy'] = accuracy\n",
    "        report_data.append(row)\n",
    "        index += 1\n",
    "    dataframe = pd.DataFrame.from_dict(report_data)\n",
    "    dataframe.to_csv(tasks[choosenIndex]+ '/classificationReports'+ '/'+'classification_report' + model_name +  '.csv', index = False)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train,  y_train)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    print(f\"Model {name}: \")\n",
    "    report = classification_report(y_test, pred_test)\n",
    "    print(report)\n",
    "    classification_report_csv(report, name)\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNetwork():\n",
    "    n = 50\n",
    "    model = Sequential(name=\"Sequential-NN\")\n",
    "    model.add(layers.Dense(X.shape[1], activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(layers.Dense(np.unique(y).size * n, activation='relu'))\n",
    "    model.add(layers.Dense(np.unique(y).size, activation='softmax'))\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 387us/step - loss: 1.6053 - accuracy: 0.2393\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 361us/step - loss: 1.5749 - accuracy: 0.3205\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 1.5354 - accuracy: 0.3672\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 1.4803 - accuracy: 0.4195\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 1.4100 - accuracy: 0.4712\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 1.3371 - accuracy: 0.4939\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 1.2686 - accuracy: 0.5228\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 1.2076 - accuracy: 0.5335\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 1.1572 - accuracy: 0.5538\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 378us/step - loss: 1.1181 - accuracy: 0.5680\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 364us/step - loss: 1.0833 - accuracy: 0.5680\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 1.0525 - accuracy: 0.5940\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 1.0263 - accuracy: 0.6033\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.9999 - accuracy: 0.6129\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 367us/step - loss: 0.9740 - accuracy: 0.6122\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.9512 - accuracy: 0.6232\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.9287 - accuracy: 0.6328\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.9076 - accuracy: 0.6542\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.8862 - accuracy: 0.6485\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.8643 - accuracy: 0.6606\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 366us/step - loss: 0.8457 - accuracy: 0.6667\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.8261 - accuracy: 0.6802\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 364us/step - loss: 0.8118 - accuracy: 0.6774\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.7951 - accuracy: 0.6873\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.7789 - accuracy: 0.7012\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.7612 - accuracy: 0.7069\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 367us/step - loss: 0.7470 - accuracy: 0.7169\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.7327 - accuracy: 0.7301\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 384us/step - loss: 0.7171 - accuracy: 0.7354\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.7049 - accuracy: 0.7422\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.6915 - accuracy: 0.7397\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.6805 - accuracy: 0.7571\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.6678 - accuracy: 0.7610\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 380us/step - loss: 0.6530 - accuracy: 0.7739\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.6419 - accuracy: 0.7828\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.6306 - accuracy: 0.7771\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 360us/step - loss: 0.6186 - accuracy: 0.7885\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 363us/step - loss: 0.6068 - accuracy: 0.7906\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 386us/step - loss: 0.5977 - accuracy: 0.7920\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.5847 - accuracy: 0.8027\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 363us/step - loss: 0.5762 - accuracy: 0.7949\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 362us/step - loss: 0.5649 - accuracy: 0.8080\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 363us/step - loss: 0.5557 - accuracy: 0.8105\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.5460 - accuracy: 0.8162\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.5369 - accuracy: 0.8237\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 378us/step - loss: 0.5274 - accuracy: 0.8194\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 367us/step - loss: 0.5168 - accuracy: 0.8340\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.5092 - accuracy: 0.8323\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 530us/step - loss: 0.5004 - accuracy: 0.8362\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 367us/step - loss: 0.4915 - accuracy: 0.8380\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.4877 - accuracy: 0.8365\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.4743 - accuracy: 0.8422\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 393us/step - loss: 0.4659 - accuracy: 0.8490\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.4616 - accuracy: 0.8447\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 386us/step - loss: 0.4501 - accuracy: 0.8547\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 384us/step - loss: 0.4431 - accuracy: 0.8586\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.4368 - accuracy: 0.8593\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 366us/step - loss: 0.4290 - accuracy: 0.8558\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.4215 - accuracy: 0.8593\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.4160 - accuracy: 0.8575\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.4075 - accuracy: 0.8675\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.3981 - accuracy: 0.8704\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.3938 - accuracy: 0.8711\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 383us/step - loss: 0.3881 - accuracy: 0.8746\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.3811 - accuracy: 0.8800\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.3729 - accuracy: 0.8821\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.3663 - accuracy: 0.8839\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.3620 - accuracy: 0.8857\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.3539 - accuracy: 0.8892\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.3457 - accuracy: 0.8925\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.3411 - accuracy: 0.8871\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 368us/step - loss: 0.3356 - accuracy: 0.8942\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.3288 - accuracy: 0.8953\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 366us/step - loss: 0.3238 - accuracy: 0.8964\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.3155 - accuracy: 0.9021\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 380us/step - loss: 0.3103 - accuracy: 0.9028\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.3046 - accuracy: 0.9085\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.2984 - accuracy: 0.9074\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 365us/step - loss: 0.2929 - accuracy: 0.9120\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.2878 - accuracy: 0.9074\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.2824 - accuracy: 0.9138\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 364us/step - loss: 0.2756 - accuracy: 0.9163\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.2723 - accuracy: 0.9177\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 366us/step - loss: 0.2653 - accuracy: 0.9217\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 392us/step - loss: 0.2614 - accuracy: 0.9220\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.2561 - accuracy: 0.9227\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.2499 - accuracy: 0.9298\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.2449 - accuracy: 0.9274\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.2393 - accuracy: 0.9320\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 364us/step - loss: 0.2335 - accuracy: 0.9330\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.2291 - accuracy: 0.9345\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.2254 - accuracy: 0.9416\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 367us/step - loss: 0.2217 - accuracy: 0.9423\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.2167 - accuracy: 0.9459\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.2115 - accuracy: 0.9452\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 365us/step - loss: 0.2071 - accuracy: 0.9423\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.2039 - accuracy: 0.9427\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.2004 - accuracy: 0.9505\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.1945 - accuracy: 0.9533\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.1918 - accuracy: 0.9494\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 355us/step - loss: 1.6015 - accuracy: 0.2518\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 1.5705 - accuracy: 0.3568\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 1.5256 - accuracy: 0.3907\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 367us/step - loss: 1.4616 - accuracy: 0.4291\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 1.3850 - accuracy: 0.4719\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 1.3081 - accuracy: 0.4932\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 1.2398 - accuracy: 0.5442\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 1.1847 - accuracy: 0.5602\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 366us/step - loss: 1.1414 - accuracy: 0.5787\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 1.1032 - accuracy: 0.5837\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 366us/step - loss: 1.0719 - accuracy: 0.5990\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 1.0419 - accuracy: 0.6172\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 364us/step - loss: 1.0155 - accuracy: 0.6325\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.9904 - accuracy: 0.6467\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.9635 - accuracy: 0.6645\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 363us/step - loss: 0.9393 - accuracy: 0.6738\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.9160 - accuracy: 0.6802\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 365us/step - loss: 0.8934 - accuracy: 0.6969\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 366us/step - loss: 0.8716 - accuracy: 0.7155\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.8496 - accuracy: 0.7254\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.8263 - accuracy: 0.7290\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 385us/step - loss: 0.8058 - accuracy: 0.7343\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.7857 - accuracy: 0.7557\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.7657 - accuracy: 0.7657\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.7463 - accuracy: 0.7760\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.7269 - accuracy: 0.7885\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.7067 - accuracy: 0.7877\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.6880 - accuracy: 0.8027\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 366us/step - loss: 0.6710 - accuracy: 0.8066\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.6519 - accuracy: 0.8130\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 393us/step - loss: 0.6322 - accuracy: 0.8209\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.6153 - accuracy: 0.8234\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.5988 - accuracy: 0.8301\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.5817 - accuracy: 0.8426\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.5637 - accuracy: 0.8429\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.5479 - accuracy: 0.8469\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.5329 - accuracy: 0.8558\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.5167 - accuracy: 0.8568\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.5032 - accuracy: 0.8618\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.4868 - accuracy: 0.8714\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.4737 - accuracy: 0.8700\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.4600 - accuracy: 0.8771\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 378us/step - loss: 0.4473 - accuracy: 0.8722\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.4336 - accuracy: 0.8811\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.4213 - accuracy: 0.8850\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.4092 - accuracy: 0.8857\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.3982 - accuracy: 0.8875\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.3857 - accuracy: 0.8935\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.3771 - accuracy: 0.8985\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.3649 - accuracy: 0.8974\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.3567 - accuracy: 0.9003\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.3477 - accuracy: 0.9028\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.3388 - accuracy: 0.9014\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.3309 - accuracy: 0.9049\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.3220 - accuracy: 0.9031\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.3152 - accuracy: 0.9074\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.3067 - accuracy: 0.9113\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 367us/step - loss: 0.3010 - accuracy: 0.9138\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.2925 - accuracy: 0.9110\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.2863 - accuracy: 0.9167\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 367us/step - loss: 0.2777 - accuracy: 0.9177\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 0.2739 - accuracy: 0.9131\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 394us/step - loss: 0.2674 - accuracy: 0.9170\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.2619 - accuracy: 0.9206\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.2555 - accuracy: 0.9220\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.2507 - accuracy: 0.9241\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.2451 - accuracy: 0.9274\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 367us/step - loss: 0.2417 - accuracy: 0.9252\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 385us/step - loss: 0.2350 - accuracy: 0.9281\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.2307 - accuracy: 0.9274\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.2254 - accuracy: 0.9323\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.2215 - accuracy: 0.9338\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 366us/step - loss: 0.2158 - accuracy: 0.9359\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.2145 - accuracy: 0.9348\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.2091 - accuracy: 0.9380\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.2053 - accuracy: 0.9363\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 366us/step - loss: 0.2013 - accuracy: 0.9380\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 395us/step - loss: 0.1979 - accuracy: 0.9380\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.1926 - accuracy: 0.9437\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.1902 - accuracy: 0.9437\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 365us/step - loss: 0.1861 - accuracy: 0.9430\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.1819 - accuracy: 0.9448\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 365us/step - loss: 0.1788 - accuracy: 0.9459\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.1747 - accuracy: 0.9469\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.1727 - accuracy: 0.9480\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.1697 - accuracy: 0.9498\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.1660 - accuracy: 0.9494\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 380us/step - loss: 0.1627 - accuracy: 0.9494\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.1597 - accuracy: 0.9533\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 378us/step - loss: 0.1561 - accuracy: 0.9523\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.1541 - accuracy: 0.9516\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 366us/step - loss: 0.1501 - accuracy: 0.9551\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 378us/step - loss: 0.1474 - accuracy: 0.9576\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.1449 - accuracy: 0.9590\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 392us/step - loss: 0.1426 - accuracy: 0.9608\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 396us/step - loss: 0.1393 - accuracy: 0.9612\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.1379 - accuracy: 0.9587\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.1349 - accuracy: 0.9615\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.1325 - accuracy: 0.9651\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.1286 - accuracy: 0.9662\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 1.6080 - accuracy: 0.2265\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 364us/step - loss: 1.5879 - accuracy: 0.2959\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 1.5612 - accuracy: 0.3736\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 1.5190 - accuracy: 0.4167\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 380us/step - loss: 1.4593 - accuracy: 0.4409\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 1.3876 - accuracy: 0.4822\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 1.3089 - accuracy: 0.5214\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 364us/step - loss: 1.2407 - accuracy: 0.5417\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 1.1815 - accuracy: 0.5637\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 1.1322 - accuracy: 0.5869\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 1.0916 - accuracy: 0.6111\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 1.0552 - accuracy: 0.6207\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 1.0233 - accuracy: 0.6300\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 375us/step - loss: 0.9937 - accuracy: 0.6375\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.9651 - accuracy: 0.6478\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.9362 - accuracy: 0.6667\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.9103 - accuracy: 0.6774\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.8862 - accuracy: 0.6841\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.8605 - accuracy: 0.6937\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.8373 - accuracy: 0.6944\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.8160 - accuracy: 0.7105\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.7941 - accuracy: 0.7112\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 397us/step - loss: 0.7731 - accuracy: 0.7286\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.7509 - accuracy: 0.7386\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 365us/step - loss: 0.7312 - accuracy: 0.7447\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.7127 - accuracy: 0.7585\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.6945 - accuracy: 0.7553\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.6774 - accuracy: 0.7817\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.6604 - accuracy: 0.7771\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.6441 - accuracy: 0.7920\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.6281 - accuracy: 0.7924\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 365us/step - loss: 0.6116 - accuracy: 0.7981\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.5965 - accuracy: 0.8095\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.5835 - accuracy: 0.8102\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 367us/step - loss: 0.5707 - accuracy: 0.8155\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.5553 - accuracy: 0.8209\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.5446 - accuracy: 0.8262\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.5311 - accuracy: 0.8298\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.5198 - accuracy: 0.8280\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.5056 - accuracy: 0.8397\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.4947 - accuracy: 0.8383\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.4847 - accuracy: 0.8472\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.4720 - accuracy: 0.8561\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.4636 - accuracy: 0.8504\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.4519 - accuracy: 0.8543\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.4423 - accuracy: 0.8561\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.4319 - accuracy: 0.8572\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.4226 - accuracy: 0.8629\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.4139 - accuracy: 0.8643\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.4055 - accuracy: 0.8661\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.3967 - accuracy: 0.8757\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 378us/step - loss: 0.3877 - accuracy: 0.8750\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.3799 - accuracy: 0.8786\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 367us/step - loss: 0.3716 - accuracy: 0.8850\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 390us/step - loss: 0.3648 - accuracy: 0.8843\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 387us/step - loss: 0.3580 - accuracy: 0.8821\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.3490 - accuracy: 0.8864\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.3436 - accuracy: 0.8903\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.3366 - accuracy: 0.8917\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.3286 - accuracy: 0.8949\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 385us/step - loss: 0.3231 - accuracy: 0.9031\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.3188 - accuracy: 0.9014\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.3106 - accuracy: 0.8996\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.3028 - accuracy: 0.9049\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.3010 - accuracy: 0.9053\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.2941 - accuracy: 0.9078\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.2888 - accuracy: 0.9088\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.2818 - accuracy: 0.9110\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.2802 - accuracy: 0.9117\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.2748 - accuracy: 0.9138\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.2700 - accuracy: 0.9120\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.2647 - accuracy: 0.9142\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 364us/step - loss: 0.2597 - accuracy: 0.9202\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.2553 - accuracy: 0.9177\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.2505 - accuracy: 0.9259\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.2480 - accuracy: 0.9217\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.2423 - accuracy: 0.9256\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 365us/step - loss: 0.2390 - accuracy: 0.9266\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 0.2335 - accuracy: 0.9309\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.2323 - accuracy: 0.9270\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 365us/step - loss: 0.2270 - accuracy: 0.9327\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.2247 - accuracy: 0.9295\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.2192 - accuracy: 0.9348\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.2164 - accuracy: 0.9355\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.2133 - accuracy: 0.9338\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.2105 - accuracy: 0.9380\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.2073 - accuracy: 0.9359\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.2035 - accuracy: 0.9409\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.2004 - accuracy: 0.9416\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.1972 - accuracy: 0.9409\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 365us/step - loss: 0.1943 - accuracy: 0.9437\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 381us/step - loss: 0.1900 - accuracy: 0.9437\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.1875 - accuracy: 0.9476\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.1851 - accuracy: 0.9455\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.1832 - accuracy: 0.9469\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.1789 - accuracy: 0.9484\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.1797 - accuracy: 0.9476\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 367us/step - loss: 0.1743 - accuracy: 0.9476\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.1725 - accuracy: 0.9523\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.1707 - accuracy: 0.9526\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 364us/step - loss: 1.6061 - accuracy: 0.2550\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 1.5733 - accuracy: 0.3223\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 1.5218 - accuracy: 0.3739\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 1.4543 - accuracy: 0.4355\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 365us/step - loss: 1.3722 - accuracy: 0.4751\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 1.2976 - accuracy: 0.5110\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 378us/step - loss: 1.2296 - accuracy: 0.5296\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 1.1739 - accuracy: 0.5630\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 1.1285 - accuracy: 0.5823\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 1.0890 - accuracy: 0.6019\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 1.0519 - accuracy: 0.6161\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 1.0219 - accuracy: 0.6264\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.9902 - accuracy: 0.6314\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.9612 - accuracy: 0.6506\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 387us/step - loss: 0.9340 - accuracy: 0.6521\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.9056 - accuracy: 0.6674\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.8787 - accuracy: 0.6923\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.8527 - accuracy: 0.6998\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.8340 - accuracy: 0.6980\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.8114 - accuracy: 0.7190\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.7896 - accuracy: 0.7293\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 365us/step - loss: 0.7698 - accuracy: 0.7290\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 378us/step - loss: 0.7510 - accuracy: 0.7397\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.7305 - accuracy: 0.7500\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.7170 - accuracy: 0.7571\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.6990 - accuracy: 0.7614\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.6833 - accuracy: 0.7628\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.6679 - accuracy: 0.7753\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.6536 - accuracy: 0.7785\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.6394 - accuracy: 0.7867\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.6259 - accuracy: 0.7913\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.6138 - accuracy: 0.7963\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.5996 - accuracy: 0.8031\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 383us/step - loss: 0.5881 - accuracy: 0.8009\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.5744 - accuracy: 0.8070\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.5628 - accuracy: 0.8088\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.5533 - accuracy: 0.8091\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.5410 - accuracy: 0.8155\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.5304 - accuracy: 0.8209\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 0.5206 - accuracy: 0.8205\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.5116 - accuracy: 0.8305\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.5031 - accuracy: 0.8340\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.4893 - accuracy: 0.8387\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.4824 - accuracy: 0.8369\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.4736 - accuracy: 0.8433\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.4638 - accuracy: 0.8479\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.4535 - accuracy: 0.8515\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.4459 - accuracy: 0.8522\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.4341 - accuracy: 0.8579\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.4265 - accuracy: 0.8600\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.4183 - accuracy: 0.8682\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 0.4095 - accuracy: 0.8711\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.4028 - accuracy: 0.8718\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.3944 - accuracy: 0.8718\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.3846 - accuracy: 0.8807\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.3751 - accuracy: 0.8896\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.3685 - accuracy: 0.8882\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.3608 - accuracy: 0.8907\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.3512 - accuracy: 0.8925\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.3465 - accuracy: 0.8974\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 365us/step - loss: 0.3394 - accuracy: 0.8981\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 387us/step - loss: 0.3326 - accuracy: 0.9024\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 372us/step - loss: 0.3249 - accuracy: 0.8989\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.3184 - accuracy: 0.9067\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.3112 - accuracy: 0.9071\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 378us/step - loss: 0.3055 - accuracy: 0.9088\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.2974 - accuracy: 0.9184\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.2918 - accuracy: 0.9181\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.2882 - accuracy: 0.9202\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.2821 - accuracy: 0.9192\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.2762 - accuracy: 0.9181\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.2711 - accuracy: 0.9245\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.2641 - accuracy: 0.9241\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.2611 - accuracy: 0.9238\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.2553 - accuracy: 0.9281\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.2515 - accuracy: 0.9259\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.2444 - accuracy: 0.9316\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.2407 - accuracy: 0.9295\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 378us/step - loss: 0.2357 - accuracy: 0.9313\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.2322 - accuracy: 0.9334\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.2264 - accuracy: 0.9348\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.2230 - accuracy: 0.9352\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.2184 - accuracy: 0.9377\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.2142 - accuracy: 0.9427\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.2100 - accuracy: 0.9409\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.2071 - accuracy: 0.9423\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.2039 - accuracy: 0.9459\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 378us/step - loss: 0.1988 - accuracy: 0.9427\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.1962 - accuracy: 0.9487\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.1929 - accuracy: 0.9473\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.1890 - accuracy: 0.9494\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.1874 - accuracy: 0.9480\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.1830 - accuracy: 0.9505\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.1790 - accuracy: 0.9512\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.1765 - accuracy: 0.9541\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 378us/step - loss: 0.1743 - accuracy: 0.9509\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.1710 - accuracy: 0.9548\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 0.1680 - accuracy: 0.9548\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.1653 - accuracy: 0.9544\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.1624 - accuracy: 0.9566\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 366us/step - loss: 1.6051 - accuracy: 0.2521\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 362us/step - loss: 1.5780 - accuracy: 0.3618\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 378us/step - loss: 1.5391 - accuracy: 0.4074\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 1.4796 - accuracy: 0.4405\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 365us/step - loss: 1.4019 - accuracy: 0.4551\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 1.3166 - accuracy: 0.5196\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 380us/step - loss: 1.2432 - accuracy: 0.5299\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 1.1833 - accuracy: 0.5994\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 1.1331 - accuracy: 0.6214\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 365us/step - loss: 1.0923 - accuracy: 0.6471\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 365us/step - loss: 1.0503 - accuracy: 0.6660\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 1.0166 - accuracy: 0.6855\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.9821 - accuracy: 0.6845\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.9523 - accuracy: 0.7016\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.9214 - accuracy: 0.7101\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.8929 - accuracy: 0.7140\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 364us/step - loss: 0.8681 - accuracy: 0.7382\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.8426 - accuracy: 0.7400\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.8167 - accuracy: 0.7425\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.7945 - accuracy: 0.7582\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.7733 - accuracy: 0.7571\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 367us/step - loss: 0.7551 - accuracy: 0.7724\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 0.7347 - accuracy: 0.7678\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 367us/step - loss: 0.7171 - accuracy: 0.7746\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.7008 - accuracy: 0.7831\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 367us/step - loss: 0.6851 - accuracy: 0.7824\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.6693 - accuracy: 0.7877\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.6575 - accuracy: 0.7856\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.6431 - accuracy: 0.7952\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.6290 - accuracy: 0.7984\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.6144 - accuracy: 0.8052\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.6035 - accuracy: 0.8027\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 0.5897 - accuracy: 0.8159\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 373us/step - loss: 0.5815 - accuracy: 0.8123\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.5677 - accuracy: 0.8180\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.5566 - accuracy: 0.8202\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.5471 - accuracy: 0.8223\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.5370 - accuracy: 0.8276\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.5267 - accuracy: 0.8333\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.5165 - accuracy: 0.8340\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.5064 - accuracy: 0.8444\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.4971 - accuracy: 0.8433\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.4850 - accuracy: 0.8462\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.4751 - accuracy: 0.8497\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.4671 - accuracy: 0.8515\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 0.4584 - accuracy: 0.8583\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 395us/step - loss: 0.4493 - accuracy: 0.8615\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 0.4401 - accuracy: 0.8693\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.4315 - accuracy: 0.8657\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.4240 - accuracy: 0.8657\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 388us/step - loss: 0.4148 - accuracy: 0.8750\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 0.4082 - accuracy: 0.8761\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.3982 - accuracy: 0.8771\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 406us/step - loss: 0.3920 - accuracy: 0.8803\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 384us/step - loss: 0.3870 - accuracy: 0.8846\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.3803 - accuracy: 0.8860\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.3725 - accuracy: 0.8942\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 0.3661 - accuracy: 0.8896\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 387us/step - loss: 0.3591 - accuracy: 0.8932\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.3546 - accuracy: 0.8967\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.3483 - accuracy: 0.8939\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 388us/step - loss: 0.3443 - accuracy: 0.8960\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.3368 - accuracy: 0.8935\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 392us/step - loss: 0.3315 - accuracy: 0.8999\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 386us/step - loss: 0.3271 - accuracy: 0.9046\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 0.3203 - accuracy: 0.9038\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.3148 - accuracy: 0.9046\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.3123 - accuracy: 0.9074\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.3065 - accuracy: 0.9063\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 387us/step - loss: 0.3038 - accuracy: 0.9092\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 384us/step - loss: 0.2982 - accuracy: 0.9127\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 392us/step - loss: 0.2941 - accuracy: 0.9131\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 398us/step - loss: 0.2872 - accuracy: 0.9152\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 393us/step - loss: 0.2845 - accuracy: 0.9149\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 388us/step - loss: 0.2801 - accuracy: 0.9170\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.2775 - accuracy: 0.9163\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.2727 - accuracy: 0.9220\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 392us/step - loss: 0.2707 - accuracy: 0.9227\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 387us/step - loss: 0.2641 - accuracy: 0.9227\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.2624 - accuracy: 0.9238\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.2576 - accuracy: 0.9227\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 385us/step - loss: 0.2551 - accuracy: 0.9266\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.2497 - accuracy: 0.9241\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 384us/step - loss: 0.2481 - accuracy: 0.9277\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 0.2434 - accuracy: 0.9288\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.2417 - accuracy: 0.9295\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.2388 - accuracy: 0.9320\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 395us/step - loss: 0.2346 - accuracy: 0.9338\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 396us/step - loss: 0.2340 - accuracy: 0.9320\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 399us/step - loss: 0.2294 - accuracy: 0.9334\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 402us/step - loss: 0.2261 - accuracy: 0.9291\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.2240 - accuracy: 0.9345\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.2199 - accuracy: 0.9355\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 388us/step - loss: 0.2177 - accuracy: 0.9373\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 398us/step - loss: 0.2151 - accuracy: 0.9359\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 396us/step - loss: 0.2129 - accuracy: 0.9370\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 390us/step - loss: 0.2113 - accuracy: 0.9405\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.2061 - accuracy: 0.9366\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 400us/step - loss: 0.2059 - accuracy: 0.9405\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 384us/step - loss: 0.2014 - accuracy: 0.9412\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 1.6026 - accuracy: 0.2472\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 1.5653 - accuracy: 0.3462\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 1.5178 - accuracy: 0.3789\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 1.4547 - accuracy: 0.4035\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 371us/step - loss: 1.3804 - accuracy: 0.4487\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 1.3063 - accuracy: 0.4736\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 383us/step - loss: 1.2416 - accuracy: 0.5231\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 390us/step - loss: 1.1879 - accuracy: 0.5495\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 1.1427 - accuracy: 0.5776\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 378us/step - loss: 1.1033 - accuracy: 0.5947\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 1.0703 - accuracy: 0.6157\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 393us/step - loss: 1.0390 - accuracy: 0.6172\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 374us/step - loss: 1.0097 - accuracy: 0.6407\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.9820 - accuracy: 0.6599\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 380us/step - loss: 0.9535 - accuracy: 0.6578\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 385us/step - loss: 0.9236 - accuracy: 0.6909\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.8960 - accuracy: 0.6973\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 384us/step - loss: 0.8705 - accuracy: 0.6987\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.8438 - accuracy: 0.7151\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 383us/step - loss: 0.8161 - accuracy: 0.7247\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 386us/step - loss: 0.7928 - accuracy: 0.7418\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.7686 - accuracy: 0.7514\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 0.7467 - accuracy: 0.7607\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 386us/step - loss: 0.7228 - accuracy: 0.7596\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 387us/step - loss: 0.7028 - accuracy: 0.7703\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.6802 - accuracy: 0.7835\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 383us/step - loss: 0.6630 - accuracy: 0.7899\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 384us/step - loss: 0.6437 - accuracy: 0.7885\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 383us/step - loss: 0.6231 - accuracy: 0.7977\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.6072 - accuracy: 0.7999\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.5883 - accuracy: 0.8141\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 380us/step - loss: 0.5722 - accuracy: 0.8134\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.5562 - accuracy: 0.8212\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 392us/step - loss: 0.5412 - accuracy: 0.8230\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.5278 - accuracy: 0.8316\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 385us/step - loss: 0.5138 - accuracy: 0.8323\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.4994 - accuracy: 0.8394\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 384us/step - loss: 0.4867 - accuracy: 0.8405\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.4752 - accuracy: 0.8479\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 385us/step - loss: 0.4606 - accuracy: 0.8515\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.4508 - accuracy: 0.8526\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 378us/step - loss: 0.4385 - accuracy: 0.8615\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 399us/step - loss: 0.4284 - accuracy: 0.8689\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.4192 - accuracy: 0.8647\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.4096 - accuracy: 0.8743\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 386us/step - loss: 0.3975 - accuracy: 0.8761\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.3857 - accuracy: 0.8793\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 380us/step - loss: 0.3792 - accuracy: 0.8800\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 384us/step - loss: 0.3722 - accuracy: 0.8846\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 396us/step - loss: 0.3619 - accuracy: 0.8843\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 0.3560 - accuracy: 0.8885\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 0.3461 - accuracy: 0.8885\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.3382 - accuracy: 0.8967\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.3298 - accuracy: 0.8974\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.3256 - accuracy: 0.8960\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.3165 - accuracy: 0.9056\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 378us/step - loss: 0.3123 - accuracy: 0.9017\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.3066 - accuracy: 0.9042\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.3003 - accuracy: 0.9074\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 0.2939 - accuracy: 0.9095\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 388us/step - loss: 0.2879 - accuracy: 0.9092\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.2828 - accuracy: 0.9099\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.2760 - accuracy: 0.9152\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.2714 - accuracy: 0.9160\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.2674 - accuracy: 0.9142\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 373us/step - loss: 0.2622 - accuracy: 0.9195\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 0.2585 - accuracy: 0.9202\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 370us/step - loss: 0.2536 - accuracy: 0.9231\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 0.2504 - accuracy: 0.9277\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 383us/step - loss: 0.2451 - accuracy: 0.9256\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 371us/step - loss: 0.2416 - accuracy: 0.9263\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 0.2387 - accuracy: 0.9277\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.2342 - accuracy: 0.9249\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 369us/step - loss: 0.2316 - accuracy: 0.9274\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 0.2271 - accuracy: 0.9295\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 0.2251 - accuracy: 0.9313\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.2187 - accuracy: 0.9327\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.2159 - accuracy: 0.9316\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 400us/step - loss: 0.2112 - accuracy: 0.9373\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 404us/step - loss: 0.2119 - accuracy: 0.9366\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 402us/step - loss: 0.2073 - accuracy: 0.9359\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 405us/step - loss: 0.2033 - accuracy: 0.9405\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 404us/step - loss: 0.2023 - accuracy: 0.9387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 412us/step - loss: 0.1988 - accuracy: 0.9416\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 408us/step - loss: 0.1958 - accuracy: 0.9416\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 410us/step - loss: 0.1915 - accuracy: 0.9434\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 407us/step - loss: 0.1894 - accuracy: 0.9430\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 417us/step - loss: 0.1884 - accuracy: 0.9423\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.1849 - accuracy: 0.9444\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 402us/step - loss: 0.1843 - accuracy: 0.9437\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 404us/step - loss: 0.1813 - accuracy: 0.9494\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 401us/step - loss: 0.1780 - accuracy: 0.9484\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 410us/step - loss: 0.1773 - accuracy: 0.9441\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 408us/step - loss: 0.1750 - accuracy: 0.9437\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 406us/step - loss: 0.1721 - accuracy: 0.9480\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 404us/step - loss: 0.1683 - accuracy: 0.9533\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 399us/step - loss: 0.1676 - accuracy: 0.9476\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 405us/step - loss: 0.1676 - accuracy: 0.9501\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 409us/step - loss: 0.1625 - accuracy: 0.9476\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 406us/step - loss: 0.1626 - accuracy: 0.9523\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 380us/step - loss: 1.6035 - accuracy: 0.2443\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 377us/step - loss: 1.5744 - accuracy: 0.3422\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 396us/step - loss: 1.5380 - accuracy: 0.3746\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 394us/step - loss: 1.4831 - accuracy: 0.4038\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 400us/step - loss: 1.4148 - accuracy: 0.4441\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 402us/step - loss: 1.3364 - accuracy: 0.4719\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 388us/step - loss: 1.2647 - accuracy: 0.5050\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 1.2070 - accuracy: 0.5192\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 403us/step - loss: 1.1611 - accuracy: 0.5506\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 395us/step - loss: 1.1232 - accuracy: 0.5563\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 401us/step - loss: 1.0898 - accuracy: 0.5762\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 396us/step - loss: 1.0604 - accuracy: 0.6011\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 396us/step - loss: 1.0340 - accuracy: 0.5986\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 413us/step - loss: 1.0093 - accuracy: 0.6125\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 404us/step - loss: 0.9857 - accuracy: 0.6261\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 396us/step - loss: 0.9633 - accuracy: 0.6385\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 402us/step - loss: 0.9428 - accuracy: 0.6581\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 406us/step - loss: 0.9207 - accuracy: 0.6681\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 393us/step - loss: 0.9023 - accuracy: 0.6717\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 405us/step - loss: 0.8802 - accuracy: 0.6880\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 413us/step - loss: 0.8610 - accuracy: 0.7105\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 421us/step - loss: 0.8437 - accuracy: 0.7044\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.8240 - accuracy: 0.7194\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 412us/step - loss: 0.8029 - accuracy: 0.7322\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 409us/step - loss: 0.7875 - accuracy: 0.7511\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 396us/step - loss: 0.7711 - accuracy: 0.7411\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 396us/step - loss: 0.7511 - accuracy: 0.7632\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 393us/step - loss: 0.7341 - accuracy: 0.7710\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 393us/step - loss: 0.7197 - accuracy: 0.7703\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 401us/step - loss: 0.7027 - accuracy: 0.7767\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 403us/step - loss: 0.6863 - accuracy: 0.7888\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 402us/step - loss: 0.6710 - accuracy: 0.7895\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 407us/step - loss: 0.6569 - accuracy: 0.7977\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.6417 - accuracy: 0.8013\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 397us/step - loss: 0.6247 - accuracy: 0.8045\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.6100 - accuracy: 0.8095\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 386us/step - loss: 0.5966 - accuracy: 0.8159\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 396us/step - loss: 0.5835 - accuracy: 0.8241\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 395us/step - loss: 0.5707 - accuracy: 0.8223\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 390us/step - loss: 0.5562 - accuracy: 0.8287\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 388us/step - loss: 0.5449 - accuracy: 0.8291\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.5306 - accuracy: 0.8355\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 392us/step - loss: 0.5192 - accuracy: 0.8405\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 393us/step - loss: 0.5072 - accuracy: 0.8422\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 390us/step - loss: 0.4953 - accuracy: 0.8440\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 387us/step - loss: 0.4846 - accuracy: 0.8497\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 387us/step - loss: 0.4738 - accuracy: 0.8551\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 395us/step - loss: 0.4629 - accuracy: 0.8565\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 388us/step - loss: 0.4529 - accuracy: 0.8622\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.4402 - accuracy: 0.8643\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 399us/step - loss: 0.4320 - accuracy: 0.8722\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 385us/step - loss: 0.4242 - accuracy: 0.8697\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 388us/step - loss: 0.4124 - accuracy: 0.8697\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 394us/step - loss: 0.4050 - accuracy: 0.8768\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 392us/step - loss: 0.3975 - accuracy: 0.8800\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.3872 - accuracy: 0.8828\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 394us/step - loss: 0.3792 - accuracy: 0.8846\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 407us/step - loss: 0.3696 - accuracy: 0.8828\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.3627 - accuracy: 0.8892\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 408us/step - loss: 0.3567 - accuracy: 0.8900\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.3496 - accuracy: 0.8921\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 394us/step - loss: 0.3421 - accuracy: 0.8946\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 401us/step - loss: 0.3364 - accuracy: 0.8914\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 400us/step - loss: 0.3286 - accuracy: 0.8985\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 397us/step - loss: 0.3213 - accuracy: 0.9017\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 386us/step - loss: 0.3167 - accuracy: 0.9035\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 403us/step - loss: 0.3105 - accuracy: 0.9042\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 409us/step - loss: 0.3044 - accuracy: 0.9071\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 402us/step - loss: 0.2991 - accuracy: 0.9113\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 403us/step - loss: 0.2925 - accuracy: 0.9095\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 404us/step - loss: 0.2864 - accuracy: 0.9120\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 403us/step - loss: 0.2816 - accuracy: 0.9127\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 399us/step - loss: 0.2778 - accuracy: 0.9135\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.2730 - accuracy: 0.9160\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 401us/step - loss: 0.2682 - accuracy: 0.9167\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 385us/step - loss: 0.2636 - accuracy: 0.9199\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 398us/step - loss: 0.2588 - accuracy: 0.9209\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 397us/step - loss: 0.2547 - accuracy: 0.9220\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 398us/step - loss: 0.2495 - accuracy: 0.9234\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 397us/step - loss: 0.2447 - accuracy: 0.9274\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 390us/step - loss: 0.2416 - accuracy: 0.9284\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.2376 - accuracy: 0.9288\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 388us/step - loss: 0.2334 - accuracy: 0.9277\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.2312 - accuracy: 0.9274\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 393us/step - loss: 0.2261 - accuracy: 0.9281\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 383us/step - loss: 0.2229 - accuracy: 0.9334\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 390us/step - loss: 0.2193 - accuracy: 0.9345\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 386us/step - loss: 0.2168 - accuracy: 0.9348\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 385us/step - loss: 0.2134 - accuracy: 0.9377\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 393us/step - loss: 0.2103 - accuracy: 0.9387\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 386us/step - loss: 0.2067 - accuracy: 0.9423\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.2044 - accuracy: 0.9387\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 387us/step - loss: 0.1992 - accuracy: 0.9409\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 394us/step - loss: 0.1977 - accuracy: 0.9416\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 385us/step - loss: 0.1948 - accuracy: 0.9423\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.1912 - accuracy: 0.9423\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 387us/step - loss: 0.1880 - accuracy: 0.9462\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 385us/step - loss: 0.1862 - accuracy: 0.9466\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 393us/step - loss: 0.1839 - accuracy: 0.9448\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 381us/step - loss: 0.1795 - accuracy: 0.9516\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 363us/step - loss: 1.6056 - accuracy: 0.2457\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 368us/step - loss: 1.5823 - accuracy: 0.3219\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 375us/step - loss: 1.5503 - accuracy: 0.3554\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 376us/step - loss: 1.5027 - accuracy: 0.3942\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 1.4393 - accuracy: 0.4402\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 392us/step - loss: 1.3664 - accuracy: 0.4804\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 1.2911 - accuracy: 0.5196\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 393us/step - loss: 1.2258 - accuracy: 0.5424\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 392us/step - loss: 1.1737 - accuracy: 0.5556\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 385us/step - loss: 1.1294 - accuracy: 0.5830\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 385us/step - loss: 1.0924 - accuracy: 0.5926\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 384us/step - loss: 1.0620 - accuracy: 0.6029\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 392us/step - loss: 1.0323 - accuracy: 0.6168\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 386us/step - loss: 1.0049 - accuracy: 0.6239\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 388us/step - loss: 0.9793 - accuracy: 0.6378\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.9544 - accuracy: 0.6428\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 388us/step - loss: 0.9318 - accuracy: 0.6496\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 407us/step - loss: 0.9084 - accuracy: 0.6688\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.8843 - accuracy: 0.6855\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 388us/step - loss: 0.8592 - accuracy: 0.6909\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 392us/step - loss: 0.8391 - accuracy: 0.7058\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 392us/step - loss: 0.8172 - accuracy: 0.7215\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 385us/step - loss: 0.7952 - accuracy: 0.7457\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 388us/step - loss: 0.7741 - accuracy: 0.7418\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 394us/step - loss: 0.7565 - accuracy: 0.7504\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 386us/step - loss: 0.7369 - accuracy: 0.7628\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 392us/step - loss: 0.7189 - accuracy: 0.7742\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 383us/step - loss: 0.6992 - accuracy: 0.7767\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 387us/step - loss: 0.6828 - accuracy: 0.7785\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.6658 - accuracy: 0.7963\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 384us/step - loss: 0.6503 - accuracy: 0.7974\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.6356 - accuracy: 0.8041\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.6188 - accuracy: 0.8091\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 393us/step - loss: 0.6048 - accuracy: 0.8134\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 388us/step - loss: 0.5930 - accuracy: 0.8212\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 0.5765 - accuracy: 0.8198\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 394us/step - loss: 0.5626 - accuracy: 0.8241\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 391us/step - loss: 0.5500 - accuracy: 0.8358\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 402us/step - loss: 0.5367 - accuracy: 0.8326\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 396us/step - loss: 0.5252 - accuracy: 0.8383\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 405us/step - loss: 0.5148 - accuracy: 0.8358\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 399us/step - loss: 0.5016 - accuracy: 0.8458\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 387us/step - loss: 0.4919 - accuracy: 0.8454\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 388us/step - loss: 0.4798 - accuracy: 0.8508\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 401us/step - loss: 0.4700 - accuracy: 0.8540\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 409us/step - loss: 0.4610 - accuracy: 0.8575\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 400us/step - loss: 0.4492 - accuracy: 0.8597\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 399us/step - loss: 0.4407 - accuracy: 0.8640\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 398us/step - loss: 0.4326 - accuracy: 0.8665\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 411us/step - loss: 0.4216 - accuracy: 0.8657\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.4147 - accuracy: 0.8675\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 401us/step - loss: 0.4050 - accuracy: 0.8757\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 410us/step - loss: 0.3953 - accuracy: 0.8782\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 401us/step - loss: 0.3903 - accuracy: 0.8739\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 413us/step - loss: 0.3802 - accuracy: 0.8811\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 422us/step - loss: 0.3751 - accuracy: 0.8825\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 416us/step - loss: 0.3684 - accuracy: 0.8803\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 409us/step - loss: 0.3600 - accuracy: 0.8853\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 419us/step - loss: 0.3540 - accuracy: 0.8850\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 411us/step - loss: 0.3479 - accuracy: 0.8903\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 414us/step - loss: 0.3431 - accuracy: 0.8882\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 428us/step - loss: 0.3341 - accuracy: 0.8946\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 421us/step - loss: 0.3285 - accuracy: 0.8949\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 427us/step - loss: 0.3258 - accuracy: 0.8949\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 430us/step - loss: 0.3195 - accuracy: 0.8996\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 426us/step - loss: 0.3126 - accuracy: 0.9010\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 417us/step - loss: 0.3087 - accuracy: 0.9028\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.3042 - accuracy: 0.9003\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 416us/step - loss: 0.3002 - accuracy: 0.9031\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 413us/step - loss: 0.2939 - accuracy: 0.9028\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 417us/step - loss: 0.2922 - accuracy: 0.9060\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 420us/step - loss: 0.2874 - accuracy: 0.9074\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 418us/step - loss: 0.2807 - accuracy: 0.9113\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.2804 - accuracy: 0.9092\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 417us/step - loss: 0.2719 - accuracy: 0.9138\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 414us/step - loss: 0.2713 - accuracy: 0.9142\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 416us/step - loss: 0.2660 - accuracy: 0.9131\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 411us/step - loss: 0.2601 - accuracy: 0.9145\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 406us/step - loss: 0.2580 - accuracy: 0.9142\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 410us/step - loss: 0.2552 - accuracy: 0.9195\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.2514 - accuracy: 0.9213\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.2486 - accuracy: 0.9199\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 414us/step - loss: 0.2451 - accuracy: 0.9170\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.2397 - accuracy: 0.9213\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 411us/step - loss: 0.2374 - accuracy: 0.9184\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 423us/step - loss: 0.2350 - accuracy: 0.9266\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 414us/step - loss: 0.2316 - accuracy: 0.9217\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 418us/step - loss: 0.2306 - accuracy: 0.9227\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 419us/step - loss: 0.2258 - accuracy: 0.9277\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 414us/step - loss: 0.2259 - accuracy: 0.9249\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 413us/step - loss: 0.2215 - accuracy: 0.9270\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 416us/step - loss: 0.2181 - accuracy: 0.9256\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 417us/step - loss: 0.2149 - accuracy: 0.9302\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 416us/step - loss: 0.2115 - accuracy: 0.9345\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.2107 - accuracy: 0.9345\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 426us/step - loss: 0.2077 - accuracy: 0.9341\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 416us/step - loss: 0.2038 - accuracy: 0.9352\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 414us/step - loss: 0.2013 - accuracy: 0.9355\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 421us/step - loss: 0.2012 - accuracy: 0.9348\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 412us/step - loss: 0.2003 - accuracy: 0.9320\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 382us/step - loss: 1.6057 - accuracy: 0.2436\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 404us/step - loss: 1.5743 - accuracy: 0.3390\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 408us/step - loss: 1.5350 - accuracy: 0.3693\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 410us/step - loss: 1.4776 - accuracy: 0.4188\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 409us/step - loss: 1.4064 - accuracy: 0.4537\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 407us/step - loss: 1.3347 - accuracy: 0.4541\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 408us/step - loss: 1.2691 - accuracy: 0.5078\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 407us/step - loss: 1.2165 - accuracy: 0.5078\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 406us/step - loss: 1.1750 - accuracy: 0.5381\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 403us/step - loss: 1.1416 - accuracy: 0.5584\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 404us/step - loss: 1.1158 - accuracy: 0.5548\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 407us/step - loss: 1.0878 - accuracy: 0.5805\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 406us/step - loss: 1.0655 - accuracy: 0.5830\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 403us/step - loss: 1.0438 - accuracy: 0.5926\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 401us/step - loss: 1.0247 - accuracy: 0.5940\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 406us/step - loss: 1.0051 - accuracy: 0.6282\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 434us/step - loss: 0.9840 - accuracy: 0.6179\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 424us/step - loss: 0.9697 - accuracy: 0.6325\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 432us/step - loss: 0.9497 - accuracy: 0.6481\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 424us/step - loss: 0.9322 - accuracy: 0.6514\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 419us/step - loss: 0.9122 - accuracy: 0.6645\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 416us/step - loss: 0.8949 - accuracy: 0.6720\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 401us/step - loss: 0.8758 - accuracy: 0.6902\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.8564 - accuracy: 0.6984\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 411us/step - loss: 0.8385 - accuracy: 0.7026\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 411us/step - loss: 0.8191 - accuracy: 0.7251\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 397us/step - loss: 0.8018 - accuracy: 0.7254\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 404us/step - loss: 0.7813 - accuracy: 0.7415\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 405us/step - loss: 0.7635 - accuracy: 0.7493\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 405us/step - loss: 0.7469 - accuracy: 0.7546\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 412us/step - loss: 0.7302 - accuracy: 0.7589\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 397us/step - loss: 0.7097 - accuracy: 0.7696\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 393us/step - loss: 0.6946 - accuracy: 0.7735\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 419us/step - loss: 0.6747 - accuracy: 0.7838\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 414us/step - loss: 0.6595 - accuracy: 0.7803\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 412us/step - loss: 0.6409 - accuracy: 0.7913\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 408us/step - loss: 0.6272 - accuracy: 0.7906\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 412us/step - loss: 0.6100 - accuracy: 0.7927\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 410us/step - loss: 0.5960 - accuracy: 0.8048\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 414us/step - loss: 0.5825 - accuracy: 0.8088\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 422us/step - loss: 0.5667 - accuracy: 0.8088\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 425us/step - loss: 0.5535 - accuracy: 0.8148\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 427us/step - loss: 0.5401 - accuracy: 0.8184\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 410us/step - loss: 0.5276 - accuracy: 0.8244\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 431us/step - loss: 0.5167 - accuracy: 0.8326\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 414us/step - loss: 0.5041 - accuracy: 0.8348\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.4915 - accuracy: 0.8373\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 412us/step - loss: 0.4817 - accuracy: 0.8433\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 412us/step - loss: 0.4725 - accuracy: 0.8422\n",
      "Epoch 50/100\n",
      "281/281 [==============================] - 0s 419us/step - loss: 0.4609 - accuracy: 0.8433\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 413us/step - loss: 0.4519 - accuracy: 0.8568\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 418us/step - loss: 0.4423 - accuracy: 0.8497\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 416us/step - loss: 0.4343 - accuracy: 0.8593\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 401us/step - loss: 0.4277 - accuracy: 0.8597\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 419us/step - loss: 0.4167 - accuracy: 0.8668\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 420us/step - loss: 0.4101 - accuracy: 0.8654\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.4031 - accuracy: 0.8689\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 411us/step - loss: 0.3954 - accuracy: 0.8693\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 419us/step - loss: 0.3886 - accuracy: 0.8729\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.3814 - accuracy: 0.8778\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 402us/step - loss: 0.3770 - accuracy: 0.8782\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 410us/step - loss: 0.3688 - accuracy: 0.8800\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 416us/step - loss: 0.3632 - accuracy: 0.8850\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.3562 - accuracy: 0.8828\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 417us/step - loss: 0.3515 - accuracy: 0.8843\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 412us/step - loss: 0.3465 - accuracy: 0.8885\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 413us/step - loss: 0.3393 - accuracy: 0.8900\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 413us/step - loss: 0.3356 - accuracy: 0.8892\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 412us/step - loss: 0.3298 - accuracy: 0.8903\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 404us/step - loss: 0.3244 - accuracy: 0.8974\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 399us/step - loss: 0.3200 - accuracy: 0.8957\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 407us/step - loss: 0.3160 - accuracy: 0.9017\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 407us/step - loss: 0.3100 - accuracy: 0.9006\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 413us/step - loss: 0.3051 - accuracy: 0.8992\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 409us/step - loss: 0.3007 - accuracy: 0.9017\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 411us/step - loss: 0.2975 - accuracy: 0.9046\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 409us/step - loss: 0.2948 - accuracy: 0.9010\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 406us/step - loss: 0.2902 - accuracy: 0.9021\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 423us/step - loss: 0.2846 - accuracy: 0.9060\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 422us/step - loss: 0.2808 - accuracy: 0.9095\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 409us/step - loss: 0.2757 - accuracy: 0.9110\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 419us/step - loss: 0.2719 - accuracy: 0.9120\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 428us/step - loss: 0.2687 - accuracy: 0.9110\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 424us/step - loss: 0.2648 - accuracy: 0.9142\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 420us/step - loss: 0.2626 - accuracy: 0.9145\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 408us/step - loss: 0.2587 - accuracy: 0.9217\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 431us/step - loss: 0.2529 - accuracy: 0.9163\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 426us/step - loss: 0.2511 - accuracy: 0.9199\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 431us/step - loss: 0.2495 - accuracy: 0.9217\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 421us/step - loss: 0.2438 - accuracy: 0.9220\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 426us/step - loss: 0.2393 - accuracy: 0.9241\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 398us/step - loss: 0.2381 - accuracy: 0.9220\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 407us/step - loss: 0.2352 - accuracy: 0.9231\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.2312 - accuracy: 0.9274\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 411us/step - loss: 0.2276 - accuracy: 0.9277\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 416us/step - loss: 0.2254 - accuracy: 0.9281\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 412us/step - loss: 0.2232 - accuracy: 0.9302\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 410us/step - loss: 0.2199 - accuracy: 0.9313\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 437us/step - loss: 0.2145 - accuracy: 0.9323\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 445us/step - loss: 0.2131 - accuracy: 0.9345\n",
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 0s 389us/step - loss: 1.6088 - accuracy: 0.2397\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 0s 372us/step - loss: 1.5662 - accuracy: 0.3429\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 0s 379us/step - loss: 1.5099 - accuracy: 0.3989\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 0s 383us/step - loss: 1.4333 - accuracy: 0.4309\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 0s 393us/step - loss: 1.3502 - accuracy: 0.4733\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 0s 404us/step - loss: 1.2745 - accuracy: 0.5146\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 0s 416us/step - loss: 1.2127 - accuracy: 0.5410\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 0s 410us/step - loss: 1.1614 - accuracy: 0.5645\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 0s 404us/step - loss: 1.1187 - accuracy: 0.5780\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 0s 395us/step - loss: 1.0820 - accuracy: 0.6100\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 0s 406us/step - loss: 1.0471 - accuracy: 0.6218\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 0s 405us/step - loss: 1.0152 - accuracy: 0.6271\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - 0s 408us/step - loss: 0.9822 - accuracy: 0.6571\n",
      "Epoch 14/100\n",
      "281/281 [==============================] - 0s 405us/step - loss: 0.9520 - accuracy: 0.6699\n",
      "Epoch 15/100\n",
      "281/281 [==============================] - 0s 408us/step - loss: 0.9236 - accuracy: 0.6777\n",
      "Epoch 16/100\n",
      "281/281 [==============================] - 0s 421us/step - loss: 0.8908 - accuracy: 0.7112\n",
      "Epoch 17/100\n",
      "281/281 [==============================] - 0s 424us/step - loss: 0.8635 - accuracy: 0.7151\n",
      "Epoch 18/100\n",
      "281/281 [==============================] - 0s 426us/step - loss: 0.8322 - accuracy: 0.7354\n",
      "Epoch 19/100\n",
      "281/281 [==============================] - 0s 419us/step - loss: 0.8029 - accuracy: 0.7489\n",
      "Epoch 20/100\n",
      "281/281 [==============================] - 0s 416us/step - loss: 0.7744 - accuracy: 0.7603\n",
      "Epoch 21/100\n",
      "281/281 [==============================] - 0s 408us/step - loss: 0.7475 - accuracy: 0.7742\n",
      "Epoch 22/100\n",
      "281/281 [==============================] - 0s 413us/step - loss: 0.7196 - accuracy: 0.7863\n",
      "Epoch 23/100\n",
      "281/281 [==============================] - 0s 408us/step - loss: 0.6944 - accuracy: 0.7974\n",
      "Epoch 24/100\n",
      "281/281 [==============================] - 0s 404us/step - loss: 0.6693 - accuracy: 0.8080\n",
      "Epoch 25/100\n",
      "281/281 [==============================] - 0s 407us/step - loss: 0.6429 - accuracy: 0.8173\n",
      "Epoch 26/100\n",
      "281/281 [==============================] - 0s 409us/step - loss: 0.6190 - accuracy: 0.8273\n",
      "Epoch 27/100\n",
      "281/281 [==============================] - 0s 412us/step - loss: 0.5978 - accuracy: 0.8337\n",
      "Epoch 28/100\n",
      "281/281 [==============================] - 0s 414us/step - loss: 0.5785 - accuracy: 0.8316\n",
      "Epoch 29/100\n",
      "281/281 [==============================] - 0s 431us/step - loss: 0.5575 - accuracy: 0.8394\n",
      "Epoch 30/100\n",
      "281/281 [==============================] - 0s 432us/step - loss: 0.5389 - accuracy: 0.8494\n",
      "Epoch 31/100\n",
      "281/281 [==============================] - 0s 405us/step - loss: 0.5189 - accuracy: 0.8547\n",
      "Epoch 32/100\n",
      "281/281 [==============================] - 0s 403us/step - loss: 0.5032 - accuracy: 0.8547\n",
      "Epoch 33/100\n",
      "281/281 [==============================] - 0s 397us/step - loss: 0.4873 - accuracy: 0.8615\n",
      "Epoch 34/100\n",
      "281/281 [==============================] - 0s 400us/step - loss: 0.4721 - accuracy: 0.8657\n",
      "Epoch 35/100\n",
      "281/281 [==============================] - 0s 402us/step - loss: 0.4568 - accuracy: 0.8739\n",
      "Epoch 36/100\n",
      "281/281 [==============================] - 0s 414us/step - loss: 0.4422 - accuracy: 0.8707\n",
      "Epoch 37/100\n",
      "281/281 [==============================] - 0s 418us/step - loss: 0.4293 - accuracy: 0.8754\n",
      "Epoch 38/100\n",
      "281/281 [==============================] - 0s 411us/step - loss: 0.4191 - accuracy: 0.8811\n",
      "Epoch 39/100\n",
      "281/281 [==============================] - 0s 419us/step - loss: 0.4043 - accuracy: 0.8843\n",
      "Epoch 40/100\n",
      "281/281 [==============================] - 0s 413us/step - loss: 0.3939 - accuracy: 0.8818\n",
      "Epoch 41/100\n",
      "281/281 [==============================] - 0s 431us/step - loss: 0.3818 - accuracy: 0.8882\n",
      "Epoch 42/100\n",
      "281/281 [==============================] - 0s 435us/step - loss: 0.3721 - accuracy: 0.8907\n",
      "Epoch 43/100\n",
      "281/281 [==============================] - 0s 434us/step - loss: 0.3622 - accuracy: 0.8949\n",
      "Epoch 44/100\n",
      "281/281 [==============================] - 0s 427us/step - loss: 0.3502 - accuracy: 0.8985\n",
      "Epoch 45/100\n",
      "281/281 [==============================] - 0s 426us/step - loss: 0.3414 - accuracy: 0.9003\n",
      "Epoch 46/100\n",
      "281/281 [==============================] - 0s 431us/step - loss: 0.3345 - accuracy: 0.8996\n",
      "Epoch 47/100\n",
      "281/281 [==============================] - 0s 419us/step - loss: 0.3250 - accuracy: 0.9046\n",
      "Epoch 48/100\n",
      "281/281 [==============================] - 0s 429us/step - loss: 0.3186 - accuracy: 0.9085\n",
      "Epoch 49/100\n",
      "281/281 [==============================] - 0s 432us/step - loss: 0.3084 - accuracy: 0.9078\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 425us/step - loss: 0.3014 - accuracy: 0.9088\n",
      "Epoch 51/100\n",
      "281/281 [==============================] - 0s 429us/step - loss: 0.2942 - accuracy: 0.9117\n",
      "Epoch 52/100\n",
      "281/281 [==============================] - 0s 428us/step - loss: 0.2877 - accuracy: 0.9138\n",
      "Epoch 53/100\n",
      "281/281 [==============================] - 0s 426us/step - loss: 0.2789 - accuracy: 0.9170\n",
      "Epoch 54/100\n",
      "281/281 [==============================] - 0s 424us/step - loss: 0.2752 - accuracy: 0.9192\n",
      "Epoch 55/100\n",
      "281/281 [==============================] - 0s 425us/step - loss: 0.2647 - accuracy: 0.9199\n",
      "Epoch 56/100\n",
      "281/281 [==============================] - 0s 425us/step - loss: 0.2591 - accuracy: 0.9259\n",
      "Epoch 57/100\n",
      "281/281 [==============================] - 0s 413us/step - loss: 0.2528 - accuracy: 0.9252\n",
      "Epoch 58/100\n",
      "281/281 [==============================] - 0s 411us/step - loss: 0.2462 - accuracy: 0.9259\n",
      "Epoch 59/100\n",
      "281/281 [==============================] - 0s 426us/step - loss: 0.2393 - accuracy: 0.9284\n",
      "Epoch 60/100\n",
      "281/281 [==============================] - 0s 423us/step - loss: 0.2339 - accuracy: 0.9284\n",
      "Epoch 61/100\n",
      "281/281 [==============================] - 0s 428us/step - loss: 0.2280 - accuracy: 0.9345\n",
      "Epoch 62/100\n",
      "281/281 [==============================] - 0s 426us/step - loss: 0.2220 - accuracy: 0.9338\n",
      "Epoch 63/100\n",
      "281/281 [==============================] - 0s 422us/step - loss: 0.2169 - accuracy: 0.9359\n",
      "Epoch 64/100\n",
      "281/281 [==============================] - 0s 420us/step - loss: 0.2124 - accuracy: 0.9370\n",
      "Epoch 65/100\n",
      "281/281 [==============================] - 0s 420us/step - loss: 0.2065 - accuracy: 0.9377\n",
      "Epoch 66/100\n",
      "281/281 [==============================] - 0s 420us/step - loss: 0.2018 - accuracy: 0.9416\n",
      "Epoch 67/100\n",
      "281/281 [==============================] - 0s 424us/step - loss: 0.1942 - accuracy: 0.9455\n",
      "Epoch 68/100\n",
      "281/281 [==============================] - 0s 425us/step - loss: 0.1923 - accuracy: 0.9441\n",
      "Epoch 69/100\n",
      "281/281 [==============================] - 0s 445us/step - loss: 0.1878 - accuracy: 0.9476\n",
      "Epoch 70/100\n",
      "281/281 [==============================] - 0s 427us/step - loss: 0.1827 - accuracy: 0.9494\n",
      "Epoch 71/100\n",
      "281/281 [==============================] - 0s 424us/step - loss: 0.1771 - accuracy: 0.9498\n",
      "Epoch 72/100\n",
      "281/281 [==============================] - 0s 426us/step - loss: 0.1725 - accuracy: 0.9509\n",
      "Epoch 73/100\n",
      "281/281 [==============================] - 0s 420us/step - loss: 0.1693 - accuracy: 0.9523\n",
      "Epoch 74/100\n",
      "281/281 [==============================] - 0s 421us/step - loss: 0.1641 - accuracy: 0.9548\n",
      "Epoch 75/100\n",
      "281/281 [==============================] - 0s 425us/step - loss: 0.1638 - accuracy: 0.9537\n",
      "Epoch 76/100\n",
      "281/281 [==============================] - 0s 414us/step - loss: 0.1581 - accuracy: 0.9594\n",
      "Epoch 77/100\n",
      "281/281 [==============================] - 0s 403us/step - loss: 0.1538 - accuracy: 0.9601\n",
      "Epoch 78/100\n",
      "281/281 [==============================] - 0s 408us/step - loss: 0.1488 - accuracy: 0.9566\n",
      "Epoch 79/100\n",
      "281/281 [==============================] - 0s 413us/step - loss: 0.1476 - accuracy: 0.9605\n",
      "Epoch 80/100\n",
      "281/281 [==============================] - 0s 413us/step - loss: 0.1425 - accuracy: 0.9626\n",
      "Epoch 81/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.1399 - accuracy: 0.9623\n",
      "Epoch 82/100\n",
      "281/281 [==============================] - 0s 409us/step - loss: 0.1352 - accuracy: 0.9658\n",
      "Epoch 83/100\n",
      "281/281 [==============================] - 0s 407us/step - loss: 0.1336 - accuracy: 0.9633\n",
      "Epoch 84/100\n",
      "281/281 [==============================] - 0s 412us/step - loss: 0.1296 - accuracy: 0.9644\n",
      "Epoch 85/100\n",
      "281/281 [==============================] - 0s 408us/step - loss: 0.1277 - accuracy: 0.9676\n",
      "Epoch 86/100\n",
      "281/281 [==============================] - 0s 409us/step - loss: 0.1239 - accuracy: 0.9658\n",
      "Epoch 87/100\n",
      "281/281 [==============================] - 0s 411us/step - loss: 0.1223 - accuracy: 0.9690\n",
      "Epoch 88/100\n",
      "281/281 [==============================] - 0s 412us/step - loss: 0.1192 - accuracy: 0.9687\n",
      "Epoch 89/100\n",
      "281/281 [==============================] - 0s 410us/step - loss: 0.1166 - accuracy: 0.9690\n",
      "Epoch 90/100\n",
      "281/281 [==============================] - 0s 410us/step - loss: 0.1127 - accuracy: 0.9697\n",
      "Epoch 91/100\n",
      "281/281 [==============================] - 0s 407us/step - loss: 0.1116 - accuracy: 0.9694\n",
      "Epoch 92/100\n",
      "281/281 [==============================] - 0s 408us/step - loss: 0.1090 - accuracy: 0.9708\n",
      "Epoch 93/100\n",
      "281/281 [==============================] - 0s 408us/step - loss: 0.1056 - accuracy: 0.9719\n",
      "Epoch 94/100\n",
      "281/281 [==============================] - 0s 411us/step - loss: 0.1071 - accuracy: 0.9722\n",
      "Epoch 95/100\n",
      "281/281 [==============================] - 0s 408us/step - loss: 0.1027 - accuracy: 0.9701\n",
      "Epoch 96/100\n",
      "281/281 [==============================] - 0s 404us/step - loss: 0.1008 - accuracy: 0.9715\n",
      "Epoch 97/100\n",
      "281/281 [==============================] - 0s 415us/step - loss: 0.0982 - accuracy: 0.9736\n",
      "Epoch 98/100\n",
      "281/281 [==============================] - 0s 416us/step - loss: 0.0951 - accuracy: 0.9765\n",
      "Epoch 99/100\n",
      "281/281 [==============================] - 0s 414us/step - loss: 0.0930 - accuracy: 0.9751\n",
      "Epoch 100/100\n",
      "281/281 [==============================] - 0s 411us/step - loss: 0.0925 - accuracy: 0.9744\n",
      "Average score of Cross Validation: 0.9181249201219236\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 10\n",
    "num_folds = 10\n",
    "\n",
    "kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "cv_results = np.array([])\n",
    "for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "    X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "    X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "    model = getNetwork()\n",
    "    model.fit(X_cross_train, y_cross_train, epochs=EPOCHS, batch_size=BATCH_SIZE)  \n",
    "    y_pred = model.predict(X_cross_test)\n",
    "    predictions_categorical = np.argmax(y_pred, axis=1)\n",
    "    f1s = f1_score(y_cross_test, predictions_categorical, average=\"weighted\")\n",
    "    cv_results = np.append(cv_results, [f1s])\n",
    "\n",
    "print(f'Average score of Cross Validation: {cv_results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 250)               15250     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 20,165\n",
      "Trainable params: 20,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "234/234 [==============================] - 0s 718us/step - loss: 1.6016 - accuracy: 0.2350 - val_loss: 1.5894 - val_accuracy: 0.3128\n",
      "Epoch 2/100\n",
      "234/234 [==============================] - 0s 467us/step - loss: 1.5753 - accuracy: 0.3496 - val_loss: 1.5633 - val_accuracy: 0.2641\n",
      "Epoch 3/100\n",
      "234/234 [==============================] - 0s 461us/step - loss: 1.5413 - accuracy: 0.4021 - val_loss: 1.5229 - val_accuracy: 0.3936\n",
      "Epoch 4/100\n",
      "234/234 [==============================] - 0s 482us/step - loss: 1.4950 - accuracy: 0.4295 - val_loss: 1.4734 - val_accuracy: 0.3590\n",
      "Epoch 5/100\n",
      "234/234 [==============================] - 0s 487us/step - loss: 1.4335 - accuracy: 0.4329 - val_loss: 1.4079 - val_accuracy: 0.4718\n",
      "Epoch 6/100\n",
      "234/234 [==============================] - 0s 486us/step - loss: 1.3663 - accuracy: 0.4791 - val_loss: 1.3418 - val_accuracy: 0.4410\n",
      "Epoch 7/100\n",
      "234/234 [==============================] - 0s 487us/step - loss: 1.2988 - accuracy: 0.5038 - val_loss: 1.2801 - val_accuracy: 0.4859\n",
      "Epoch 8/100\n",
      "234/234 [==============================] - 0s 486us/step - loss: 1.2417 - accuracy: 0.5291 - val_loss: 1.2300 - val_accuracy: 0.4936\n",
      "Epoch 9/100\n",
      "234/234 [==============================] - 0s 514us/step - loss: 1.1938 - accuracy: 0.5521 - val_loss: 1.1896 - val_accuracy: 0.5218\n",
      "Epoch 10/100\n",
      "234/234 [==============================] - 0s 499us/step - loss: 1.1556 - accuracy: 0.5556 - val_loss: 1.1477 - val_accuracy: 0.5526\n",
      "Epoch 11/100\n",
      "234/234 [==============================] - 0s 505us/step - loss: 1.1255 - accuracy: 0.5808 - val_loss: 1.1241 - val_accuracy: 0.5385\n",
      "Epoch 12/100\n",
      "234/234 [==============================] - 0s 509us/step - loss: 1.0957 - accuracy: 0.5953 - val_loss: 1.0948 - val_accuracy: 0.5782\n",
      "Epoch 13/100\n",
      "234/234 [==============================] - 0s 508us/step - loss: 1.0705 - accuracy: 0.6184 - val_loss: 1.0694 - val_accuracy: 0.6103\n",
      "Epoch 14/100\n",
      "234/234 [==============================] - 0s 492us/step - loss: 1.0468 - accuracy: 0.6286 - val_loss: 1.0476 - val_accuracy: 0.6103\n",
      "Epoch 15/100\n",
      "234/234 [==============================] - 0s 486us/step - loss: 1.0237 - accuracy: 0.6342 - val_loss: 1.0241 - val_accuracy: 0.6205\n",
      "Epoch 16/100\n",
      "234/234 [==============================] - 0s 484us/step - loss: 1.0019 - accuracy: 0.6479 - val_loss: 1.0076 - val_accuracy: 0.6333\n",
      "Epoch 17/100\n",
      "234/234 [==============================] - 0s 495us/step - loss: 0.9794 - accuracy: 0.6573 - val_loss: 0.9942 - val_accuracy: 0.5936\n",
      "Epoch 18/100\n",
      "234/234 [==============================] - 0s 494us/step - loss: 0.9627 - accuracy: 0.6632 - val_loss: 0.9874 - val_accuracy: 0.5603\n",
      "Epoch 19/100\n",
      "234/234 [==============================] - 0s 512us/step - loss: 0.9425 - accuracy: 0.6620 - val_loss: 0.9550 - val_accuracy: 0.6615\n",
      "Epoch 20/100\n",
      "234/234 [==============================] - 0s 515us/step - loss: 0.9218 - accuracy: 0.6919 - val_loss: 0.9344 - val_accuracy: 0.6782\n",
      "Epoch 21/100\n",
      "234/234 [==============================] - 0s 525us/step - loss: 0.9027 - accuracy: 0.6915 - val_loss: 0.9202 - val_accuracy: 0.6577\n",
      "Epoch 22/100\n",
      "234/234 [==============================] - 0s 524us/step - loss: 0.8840 - accuracy: 0.7124 - val_loss: 0.8978 - val_accuracy: 0.6603\n",
      "Epoch 23/100\n",
      "234/234 [==============================] - 0s 528us/step - loss: 0.8657 - accuracy: 0.7068 - val_loss: 0.8819 - val_accuracy: 0.7128\n",
      "Epoch 24/100\n",
      "234/234 [==============================] - 0s 514us/step - loss: 0.8471 - accuracy: 0.7214 - val_loss: 0.8600 - val_accuracy: 0.7244\n",
      "Epoch 25/100\n",
      "234/234 [==============================] - 0s 522us/step - loss: 0.8283 - accuracy: 0.7342 - val_loss: 0.8543 - val_accuracy: 0.7013\n",
      "Epoch 26/100\n",
      "234/234 [==============================] - 0s 535us/step - loss: 0.8097 - accuracy: 0.7440 - val_loss: 0.8267 - val_accuracy: 0.7346\n",
      "Epoch 27/100\n",
      "234/234 [==============================] - 0s 519us/step - loss: 0.7936 - accuracy: 0.7517 - val_loss: 0.8113 - val_accuracy: 0.7372\n",
      "Epoch 28/100\n",
      "234/234 [==============================] - 0s 523us/step - loss: 0.7755 - accuracy: 0.7573 - val_loss: 0.7945 - val_accuracy: 0.7410\n",
      "Epoch 29/100\n",
      "234/234 [==============================] - 0s 516us/step - loss: 0.7595 - accuracy: 0.7585 - val_loss: 0.7829 - val_accuracy: 0.7333\n",
      "Epoch 30/100\n",
      "234/234 [==============================] - 0s 496us/step - loss: 0.7428 - accuracy: 0.7726 - val_loss: 0.7641 - val_accuracy: 0.7526\n",
      "Epoch 31/100\n",
      "234/234 [==============================] - 0s 515us/step - loss: 0.7266 - accuracy: 0.7675 - val_loss: 0.7520 - val_accuracy: 0.7474\n",
      "Epoch 32/100\n",
      "234/234 [==============================] - 0s 499us/step - loss: 0.7112 - accuracy: 0.7769 - val_loss: 0.7368 - val_accuracy: 0.7551\n",
      "Epoch 33/100\n",
      "234/234 [==============================] - 0s 504us/step - loss: 0.6982 - accuracy: 0.7842 - val_loss: 0.7186 - val_accuracy: 0.7628\n",
      "Epoch 34/100\n",
      "234/234 [==============================] - 0s 499us/step - loss: 0.6819 - accuracy: 0.7880 - val_loss: 0.7058 - val_accuracy: 0.7654\n",
      "Epoch 35/100\n",
      "234/234 [==============================] - 0s 511us/step - loss: 0.6664 - accuracy: 0.7953 - val_loss: 0.6964 - val_accuracy: 0.7692\n",
      "Epoch 36/100\n",
      "234/234 [==============================] - 0s 510us/step - loss: 0.6561 - accuracy: 0.7919 - val_loss: 0.6774 - val_accuracy: 0.7744\n",
      "Epoch 37/100\n",
      "234/234 [==============================] - 0s 520us/step - loss: 0.6400 - accuracy: 0.8026 - val_loss: 0.6758 - val_accuracy: 0.7782\n",
      "Epoch 38/100\n",
      "234/234 [==============================] - 0s 526us/step - loss: 0.6286 - accuracy: 0.8021 - val_loss: 0.6654 - val_accuracy: 0.7859\n",
      "Epoch 39/100\n",
      "234/234 [==============================] - 0s 519us/step - loss: 0.6172 - accuracy: 0.8030 - val_loss: 0.6391 - val_accuracy: 0.7833\n",
      "Epoch 40/100\n",
      "234/234 [==============================] - 0s 514us/step - loss: 0.6070 - accuracy: 0.8085 - val_loss: 0.6457 - val_accuracy: 0.7923\n",
      "Epoch 41/100\n",
      "234/234 [==============================] - 0s 505us/step - loss: 0.5933 - accuracy: 0.8115 - val_loss: 0.6150 - val_accuracy: 0.8000\n",
      "Epoch 42/100\n",
      "234/234 [==============================] - 0s 512us/step - loss: 0.5800 - accuracy: 0.8192 - val_loss: 0.6045 - val_accuracy: 0.8038\n",
      "Epoch 43/100\n",
      "234/234 [==============================] - 0s 514us/step - loss: 0.5697 - accuracy: 0.8141 - val_loss: 0.5953 - val_accuracy: 0.7974\n",
      "Epoch 44/100\n",
      "234/234 [==============================] - 0s 494us/step - loss: 0.5566 - accuracy: 0.8235 - val_loss: 0.5834 - val_accuracy: 0.8167\n",
      "Epoch 45/100\n",
      "234/234 [==============================] - 0s 504us/step - loss: 0.5482 - accuracy: 0.8256 - val_loss: 0.5882 - val_accuracy: 0.8077\n",
      "Epoch 46/100\n",
      "234/234 [==============================] - 0s 497us/step - loss: 0.5369 - accuracy: 0.8329 - val_loss: 0.5602 - val_accuracy: 0.8231\n",
      "Epoch 47/100\n",
      "234/234 [==============================] - 0s 500us/step - loss: 0.5240 - accuracy: 0.8389 - val_loss: 0.5582 - val_accuracy: 0.8128\n",
      "Epoch 48/100\n",
      "234/234 [==============================] - 0s 526us/step - loss: 0.5176 - accuracy: 0.8368 - val_loss: 0.5415 - val_accuracy: 0.8308\n",
      "Epoch 49/100\n",
      "234/234 [==============================] - 0s 535us/step - loss: 0.5068 - accuracy: 0.8368 - val_loss: 0.5541 - val_accuracy: 0.8090\n",
      "Epoch 50/100\n",
      "234/234 [==============================] - 0s 557us/step - loss: 0.4975 - accuracy: 0.8440 - val_loss: 0.5258 - val_accuracy: 0.8359\n",
      "Epoch 51/100\n",
      "234/234 [==============================] - 0s 558us/step - loss: 0.4896 - accuracy: 0.8440 - val_loss: 0.5161 - val_accuracy: 0.8308\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234/234 [==============================] - 0s 536us/step - loss: 0.4812 - accuracy: 0.8453 - val_loss: 0.5161 - val_accuracy: 0.8295\n",
      "Epoch 53/100\n",
      "234/234 [==============================] - 0s 538us/step - loss: 0.4725 - accuracy: 0.8496 - val_loss: 0.5004 - val_accuracy: 0.8449\n",
      "Epoch 54/100\n",
      "234/234 [==============================] - 0s 531us/step - loss: 0.4611 - accuracy: 0.8560 - val_loss: 0.4911 - val_accuracy: 0.8449\n",
      "Epoch 55/100\n",
      "234/234 [==============================] - 0s 541us/step - loss: 0.4532 - accuracy: 0.8564 - val_loss: 0.4805 - val_accuracy: 0.8654\n",
      "Epoch 56/100\n",
      "234/234 [==============================] - 0s 532us/step - loss: 0.4479 - accuracy: 0.8573 - val_loss: 0.4813 - val_accuracy: 0.8526\n",
      "Epoch 57/100\n",
      "234/234 [==============================] - 0s 534us/step - loss: 0.4373 - accuracy: 0.8598 - val_loss: 0.4777 - val_accuracy: 0.8526\n",
      "Epoch 58/100\n",
      "234/234 [==============================] - 0s 524us/step - loss: 0.4283 - accuracy: 0.8624 - val_loss: 0.4522 - val_accuracy: 0.8590\n",
      "Epoch 59/100\n",
      "234/234 [==============================] - 0s 524us/step - loss: 0.4223 - accuracy: 0.8632 - val_loss: 0.4462 - val_accuracy: 0.8641\n",
      "Epoch 60/100\n",
      "234/234 [==============================] - 0s 504us/step - loss: 0.4151 - accuracy: 0.8650 - val_loss: 0.4505 - val_accuracy: 0.8538\n",
      "Epoch 61/100\n",
      "234/234 [==============================] - 0s 509us/step - loss: 0.4073 - accuracy: 0.8731 - val_loss: 0.4330 - val_accuracy: 0.8731\n",
      "Epoch 62/100\n",
      "234/234 [==============================] - 0s 517us/step - loss: 0.4007 - accuracy: 0.8688 - val_loss: 0.4308 - val_accuracy: 0.8692\n",
      "Epoch 63/100\n",
      "234/234 [==============================] - 0s 514us/step - loss: 0.3926 - accuracy: 0.8744 - val_loss: 0.4296 - val_accuracy: 0.8718\n",
      "Epoch 64/100\n",
      "234/234 [==============================] - 0s 509us/step - loss: 0.3867 - accuracy: 0.8748 - val_loss: 0.4130 - val_accuracy: 0.8782\n",
      "Epoch 65/100\n",
      "234/234 [==============================] - 0s 516us/step - loss: 0.3804 - accuracy: 0.8756 - val_loss: 0.4122 - val_accuracy: 0.8833\n",
      "Epoch 66/100\n",
      "234/234 [==============================] - 0s 520us/step - loss: 0.3751 - accuracy: 0.8778 - val_loss: 0.3987 - val_accuracy: 0.8756\n",
      "Epoch 67/100\n",
      "234/234 [==============================] - 0s 511us/step - loss: 0.3686 - accuracy: 0.8778 - val_loss: 0.3965 - val_accuracy: 0.8769\n",
      "Epoch 68/100\n",
      "234/234 [==============================] - 0s 511us/step - loss: 0.3614 - accuracy: 0.8778 - val_loss: 0.3826 - val_accuracy: 0.8833\n",
      "Epoch 69/100\n",
      "234/234 [==============================] - 0s 502us/step - loss: 0.3573 - accuracy: 0.8846 - val_loss: 0.3907 - val_accuracy: 0.8795\n",
      "Epoch 70/100\n",
      "234/234 [==============================] - 0s 510us/step - loss: 0.3494 - accuracy: 0.8821 - val_loss: 0.3738 - val_accuracy: 0.8846\n",
      "Epoch 71/100\n",
      "234/234 [==============================] - 0s 518us/step - loss: 0.3445 - accuracy: 0.8855 - val_loss: 0.3675 - val_accuracy: 0.8897\n",
      "Epoch 72/100\n",
      "234/234 [==============================] - 0s 524us/step - loss: 0.3381 - accuracy: 0.8923 - val_loss: 0.3619 - val_accuracy: 0.8910\n",
      "Epoch 73/100\n",
      "234/234 [==============================] - 0s 515us/step - loss: 0.3313 - accuracy: 0.8915 - val_loss: 0.3605 - val_accuracy: 0.8910\n",
      "Epoch 74/100\n",
      "234/234 [==============================] - 0s 518us/step - loss: 0.3267 - accuracy: 0.8932 - val_loss: 0.3524 - val_accuracy: 0.9013\n",
      "Epoch 75/100\n",
      "234/234 [==============================] - 0s 521us/step - loss: 0.3220 - accuracy: 0.8970 - val_loss: 0.3503 - val_accuracy: 0.8949\n",
      "Epoch 76/100\n",
      "234/234 [==============================] - 0s 512us/step - loss: 0.3175 - accuracy: 0.8927 - val_loss: 0.3411 - val_accuracy: 0.8987\n",
      "Epoch 77/100\n",
      "234/234 [==============================] - 0s 519us/step - loss: 0.3121 - accuracy: 0.8962 - val_loss: 0.3410 - val_accuracy: 0.9000\n",
      "Epoch 78/100\n",
      "234/234 [==============================] - 0s 522us/step - loss: 0.3063 - accuracy: 0.8996 - val_loss: 0.3310 - val_accuracy: 0.9000\n",
      "Epoch 79/100\n",
      "234/234 [==============================] - 0s 527us/step - loss: 0.3040 - accuracy: 0.8949 - val_loss: 0.3265 - val_accuracy: 0.9000\n",
      "Epoch 80/100\n",
      "234/234 [==============================] - 0s 518us/step - loss: 0.2984 - accuracy: 0.9043 - val_loss: 0.3218 - val_accuracy: 0.9051\n",
      "Epoch 81/100\n",
      "234/234 [==============================] - 0s 512us/step - loss: 0.2929 - accuracy: 0.9060 - val_loss: 0.3266 - val_accuracy: 0.9026\n",
      "Epoch 82/100\n",
      "234/234 [==============================] - 0s 517us/step - loss: 0.2878 - accuracy: 0.9060 - val_loss: 0.3205 - val_accuracy: 0.9013\n",
      "Epoch 83/100\n",
      "234/234 [==============================] - 0s 520us/step - loss: 0.2827 - accuracy: 0.9094 - val_loss: 0.3066 - val_accuracy: 0.9064\n",
      "Epoch 84/100\n",
      "234/234 [==============================] - 0s 516us/step - loss: 0.2800 - accuracy: 0.9094 - val_loss: 0.3106 - val_accuracy: 0.9038\n",
      "Epoch 85/100\n",
      "234/234 [==============================] - 0s 516us/step - loss: 0.2746 - accuracy: 0.9137 - val_loss: 0.2971 - val_accuracy: 0.9115\n",
      "Epoch 86/100\n",
      "234/234 [==============================] - 0s 512us/step - loss: 0.2714 - accuracy: 0.9103 - val_loss: 0.3030 - val_accuracy: 0.9077\n",
      "Epoch 87/100\n",
      "234/234 [==============================] - 0s 507us/step - loss: 0.2654 - accuracy: 0.9124 - val_loss: 0.2934 - val_accuracy: 0.9038\n",
      "Epoch 88/100\n",
      "234/234 [==============================] - 0s 497us/step - loss: 0.2633 - accuracy: 0.9209 - val_loss: 0.2915 - val_accuracy: 0.9077\n",
      "Epoch 89/100\n",
      "234/234 [==============================] - 0s 497us/step - loss: 0.2581 - accuracy: 0.9184 - val_loss: 0.2817 - val_accuracy: 0.9141\n",
      "Epoch 90/100\n",
      "234/234 [==============================] - 0s 500us/step - loss: 0.2547 - accuracy: 0.9205 - val_loss: 0.2853 - val_accuracy: 0.9064\n",
      "Epoch 91/100\n",
      "234/234 [==============================] - 0s 534us/step - loss: 0.2498 - accuracy: 0.9239 - val_loss: 0.2827 - val_accuracy: 0.9128\n",
      "Epoch 92/100\n",
      "234/234 [==============================] - 0s 508us/step - loss: 0.2466 - accuracy: 0.9261 - val_loss: 0.2746 - val_accuracy: 0.9167\n",
      "Epoch 93/100\n",
      "234/234 [==============================] - 0s 515us/step - loss: 0.2427 - accuracy: 0.9269 - val_loss: 0.2689 - val_accuracy: 0.9179\n",
      "Epoch 94/100\n",
      "234/234 [==============================] - 0s 509us/step - loss: 0.2377 - accuracy: 0.9265 - val_loss: 0.2663 - val_accuracy: 0.9167\n",
      "Epoch 95/100\n",
      "234/234 [==============================] - 0s 511us/step - loss: 0.2336 - accuracy: 0.9316 - val_loss: 0.2681 - val_accuracy: 0.9192\n",
      "Epoch 96/100\n",
      "234/234 [==============================] - 0s 502us/step - loss: 0.2334 - accuracy: 0.9299 - val_loss: 0.2570 - val_accuracy: 0.9218\n",
      "Epoch 97/100\n",
      "234/234 [==============================] - 0s 504us/step - loss: 0.2279 - accuracy: 0.9269 - val_loss: 0.2655 - val_accuracy: 0.9192\n",
      "Epoch 98/100\n",
      "234/234 [==============================] - 0s 501us/step - loss: 0.2234 - accuracy: 0.9303 - val_loss: 0.2502 - val_accuracy: 0.9218\n",
      "Epoch 99/100\n",
      "234/234 [==============================] - 0s 523us/step - loss: 0.2197 - accuracy: 0.9342 - val_loss: 0.2449 - val_accuracy: 0.9256\n",
      "Epoch 100/100\n",
      "234/234 [==============================] - 0s 509us/step - loss: 0.2153 - accuracy: 0.9359 - val_loss: 0.2417 - val_accuracy: 0.9231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93       156\n",
      "           1       0.96      0.97      0.96       156\n",
      "           2       0.96      0.97      0.96       156\n",
      "           3       0.88      0.76      0.82       156\n",
      "           4       0.87      0.92      0.90       156\n",
      "\n",
      "    accuracy                           0.92       780\n",
      "   macro avg       0.92      0.92      0.92       780\n",
      "weighted avg       0.92      0.92      0.92       780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "model = getNetwork()\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.25)\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "report = classification_report(y_test, pred)\n",
    "classification_report_csv(report, \"NN\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Models in C code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpocqp13po/assets\n"
     ]
    }
   ],
   "source": [
    "# Neural network with TinyMLGen\n",
    "with open(tasks[choosenIndex] + '/exportedModels/NNmodel.h', 'w') as f:\n",
    "    f.write(tiny.port(model, optimize=False))\n",
    "\n",
    "# Classifiers with MicroMLGen\n",
    "for name, model in models:\n",
    "    prepath = tasks[choosenIndex] + '/exportedModels/'\n",
    "    path = prepath + name + '.h'\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(port(model, optimize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione Inferance Rate medio (Intensità)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = read_csv(\"InfTimeReport.csv\")\n",
    "g = sbs.barplot(x=csv['Algoritmo'], y=csv['InfTime'])\n",
    "g.set_yscale(\"log\")\n",
    "plt.ylabel(\"Inference Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoria occupata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = read_csv(\"MemOccupationReport.csv\")\n",
    "g = sbs.barplot(x=csv['Algoritmo'], y=csv['MemOccupata'])\n",
    "g.set_yscale('log')\n",
    "plt.ylabel(\"MemOccupata in Byte\")\n",
    "plt.show()\n",
    "# SVC in overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
