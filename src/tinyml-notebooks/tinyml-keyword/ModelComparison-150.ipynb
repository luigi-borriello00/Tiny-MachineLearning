{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Comparison for TinyML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "import pickle\n",
    "from pandas import read_csv\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,  classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold,StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
    "\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dense, Input, concatenate, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import tensorflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from micromlgen import port\n",
    "import tinymlgen as tiny\n",
    "\n",
    "import warnings\n",
    "import seaborn as sbs\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/X2.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open('data/y2.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 2\n",
    "samples = 150\n",
    "X = X[:labels*samples]\n",
    "y = y[:labels*samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y).tolist()\n",
    "for i in range(len(classes)):\n",
    "    y = np.where(y==classes[i], i, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([int(el) for el in y])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Spotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 42\n",
    "scoring = 'f1_macro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "\n",
    "#models.append(('XGB', XGBClassifier(random_state=seed)))\n",
    "models.append(('GNB', GaussianNB(var_smoothing=2e-9)))\n",
    "models.append(('LR', LogisticRegression(random_state=seed)))\n",
    "models.append(('CART' , DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('SVC' , SVC(gamma=0.05, random_state=seed)))\n",
    "models.append(('RF', RandomForestClassifier(random_state=seed, n_estimators = 50)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB - 0,63 0,08\n",
      "LR - 0,68 0,08\n",
      "CART - 0,71 0,05\n",
      "SVC - 0,81 0,06\n",
      "RF - 0,80 0,06\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    # Dividere dati in n = num_folds\n",
    "    kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = np.array([])\n",
    "    for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "        X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "        X_cross_train = scaler.fit_transform(X_cross_train)\n",
    "        X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "        X_cross_test = scaler.transform(X_cross_test)\n",
    "        model.fit(X_cross_train, y_cross_train)  \n",
    "        y_pred = model.predict(X_cross_test)\n",
    "        f1s = f1_score(y_cross_test, y_pred, average=\"weighted\")\n",
    "        cv_results = np.append(cv_results, [f1s])\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #msg = \"%s - %f - %f\" % (name, cv_results.mean(), cv_results.std())\n",
    "    msg = \"{} - {:.2f} {:.2f}\".format(name, cv_results.mean(), cv_results.std()).replace('.', ',')\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "fig.suptitle('Algorithms Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valutazione dei migliori algoritmi su test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione modelli sul Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model GNB: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.87      0.78        30\n",
      "           1       0.83      0.63      0.72        30\n",
      "\n",
      "    accuracy                           0.75        60\n",
      "   macro avg       0.76      0.75      0.75        60\n",
      "weighted avg       0.76      0.75      0.75        60\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model LR: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67        30\n",
      "           1       0.67      0.67      0.67        30\n",
      "\n",
      "    accuracy                           0.67        60\n",
      "   macro avg       0.67      0.67      0.67        60\n",
      "weighted avg       0.67      0.67      0.67        60\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model CART: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.77      0.78        30\n",
      "           1       0.77      0.80      0.79        30\n",
      "\n",
      "    accuracy                           0.78        60\n",
      "   macro avg       0.78      0.78      0.78        60\n",
      "weighted avg       0.78      0.78      0.78        60\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model SVC: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.60      0.73        30\n",
      "           1       0.71      0.97      0.82        30\n",
      "\n",
      "    accuracy                           0.78        60\n",
      "   macro avg       0.83      0.78      0.78        60\n",
      "weighted avg       0.83      0.78      0.78        60\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model RF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.85        30\n",
      "           1       0.86      0.83      0.85        30\n",
      "\n",
      "    accuracy                           0.85        60\n",
      "   macro avg       0.85      0.85      0.85        60\n",
      "weighted avg       0.85      0.85      0.85        60\n",
      "\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "for name, model in models:\n",
    "    model.fit(X_train,  y_train)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    print(f\"Model {name}: \")\n",
    "    print(classification_report(y_test, pred_test))\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione Inferance Rate medio (|X_test| = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = read_csv(\"InfTimeReport.csv\")\n",
    "g = sbs.barplot(x=csv['Algoritmo'], y=csv['InfTime'])\n",
    "g.set_yscale(\"log\")\n",
    "plt.ylabel(\"Inference Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memoria occupata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = read_csv(\"MemOccupationReport.csv\")\n",
    "sbs.barplot(x=csv['Algoritmo'], y=csv['MemOccupata2'])\n",
    "plt.ylabel(\"MemOccupata in Byte\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getNetwork():\n",
    "    model = Sequential(name=\"Sequential-NN\")\n",
    "    model.add(layers.Dense(X.shape[1], activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Dense(np.unique(y).size, activation='softmax'))\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "    # SGB\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 3,363\n",
      "Trainable params: 3,363\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "41/41 [==============================] - 0s 496us/step - loss: 1.1479 - accuracy: 0.3241\n",
      "Epoch 2/500\n",
      "41/41 [==============================] - 0s 486us/step - loss: 1.1312 - accuracy: 0.3457\n",
      "Epoch 3/500\n",
      "41/41 [==============================] - 0s 513us/step - loss: 1.0842 - accuracy: 0.3549\n",
      "Epoch 4/500\n",
      "41/41 [==============================] - 0s 446us/step - loss: 1.0609 - accuracy: 0.3704\n",
      "Epoch 5/500\n",
      "41/41 [==============================] - 0s 491us/step - loss: 1.0395 - accuracy: 0.4043\n",
      "Epoch 6/500\n",
      "41/41 [==============================] - 0s 465us/step - loss: 1.0436 - accuracy: 0.4259\n",
      "Epoch 7/500\n",
      "41/41 [==============================] - 0s 492us/step - loss: 1.0204 - accuracy: 0.4228\n",
      "Epoch 8/500\n",
      "41/41 [==============================] - 0s 481us/step - loss: 1.0124 - accuracy: 0.4043\n",
      "Epoch 9/500\n",
      "41/41 [==============================] - 0s 503us/step - loss: 1.0068 - accuracy: 0.4414\n",
      "Epoch 10/500\n",
      "41/41 [==============================] - 0s 454us/step - loss: 1.0074 - accuracy: 0.4506\n",
      "Epoch 11/500\n",
      "41/41 [==============================] - 0s 466us/step - loss: 0.9773 - accuracy: 0.5154\n",
      "Epoch 12/500\n",
      "41/41 [==============================] - 0s 485us/step - loss: 0.9992 - accuracy: 0.4938\n",
      "Epoch 13/500\n",
      "41/41 [==============================] - 0s 444us/step - loss: 0.9471 - accuracy: 0.5154\n",
      "Epoch 14/500\n",
      "41/41 [==============================] - 0s 462us/step - loss: 0.9922 - accuracy: 0.5000\n",
      "Epoch 15/500\n",
      "41/41 [==============================] - 0s 471us/step - loss: 0.9601 - accuracy: 0.4938\n",
      "Epoch 16/500\n",
      "41/41 [==============================] - 0s 522us/step - loss: 0.9210 - accuracy: 0.5833\n",
      "Epoch 17/500\n",
      "41/41 [==============================] - 0s 491us/step - loss: 0.9566 - accuracy: 0.4753\n",
      "Epoch 18/500\n",
      "41/41 [==============================] - 0s 471us/step - loss: 0.9402 - accuracy: 0.5216\n",
      "Epoch 19/500\n",
      "41/41 [==============================] - 0s 486us/step - loss: 0.9296 - accuracy: 0.5247\n",
      "Epoch 20/500\n",
      "41/41 [==============================] - 0s 463us/step - loss: 0.9338 - accuracy: 0.5370\n",
      "Epoch 21/500\n",
      "41/41 [==============================] - 0s 490us/step - loss: 0.9247 - accuracy: 0.5586\n",
      "Epoch 22/500\n",
      "41/41 [==============================] - 0s 494us/step - loss: 0.9098 - accuracy: 0.5772\n",
      "Epoch 23/500\n",
      "41/41 [==============================] - 0s 479us/step - loss: 0.9183 - accuracy: 0.5525\n",
      "Epoch 24/500\n",
      "41/41 [==============================] - 0s 496us/step - loss: 0.9178 - accuracy: 0.5741\n",
      "Epoch 25/500\n",
      "41/41 [==============================] - 0s 494us/step - loss: 0.9017 - accuracy: 0.6173\n",
      "Epoch 26/500\n",
      "41/41 [==============================] - 0s 501us/step - loss: 0.9122 - accuracy: 0.5833\n",
      "Epoch 27/500\n",
      "41/41 [==============================] - 0s 464us/step - loss: 0.9188 - accuracy: 0.5617\n",
      "Epoch 28/500\n",
      "41/41 [==============================] - 0s 481us/step - loss: 0.8968 - accuracy: 0.5988\n",
      "Epoch 29/500\n",
      "41/41 [==============================] - 0s 491us/step - loss: 0.8886 - accuracy: 0.5586\n",
      "Epoch 30/500\n",
      "41/41 [==============================] - 0s 498us/step - loss: 0.8913 - accuracy: 0.6296\n",
      "Epoch 31/500\n",
      "41/41 [==============================] - 0s 498us/step - loss: 0.8710 - accuracy: 0.5864\n",
      "Epoch 32/500\n",
      "41/41 [==============================] - 0s 479us/step - loss: 0.8839 - accuracy: 0.6049\n",
      "Epoch 33/500\n",
      "41/41 [==============================] - 0s 456us/step - loss: 0.8965 - accuracy: 0.5802\n",
      "Epoch 34/500\n",
      "41/41 [==============================] - 0s 476us/step - loss: 0.8874 - accuracy: 0.5833\n",
      "Epoch 35/500\n",
      "41/41 [==============================] - 0s 470us/step - loss: 0.8851 - accuracy: 0.5741\n",
      "Epoch 36/500\n",
      "41/41 [==============================] - 0s 500us/step - loss: 0.8599 - accuracy: 0.6204\n",
      "Epoch 37/500\n",
      "41/41 [==============================] - 0s 459us/step - loss: 0.8474 - accuracy: 0.6296\n",
      "Epoch 38/500\n",
      "41/41 [==============================] - 0s 500us/step - loss: 0.8790 - accuracy: 0.5988\n",
      "Epoch 39/500\n",
      "41/41 [==============================] - 0s 460us/step - loss: 0.8551 - accuracy: 0.6080\n",
      "Epoch 40/500\n",
      "41/41 [==============================] - 0s 482us/step - loss: 0.8714 - accuracy: 0.5988\n",
      "Epoch 41/500\n",
      "41/41 [==============================] - 0s 491us/step - loss: 0.8615 - accuracy: 0.6235\n",
      "Epoch 42/500\n",
      "41/41 [==============================] - 0s 467us/step - loss: 0.8605 - accuracy: 0.6420\n",
      "Epoch 43/500\n",
      "41/41 [==============================] - 0s 465us/step - loss: 0.8304 - accuracy: 0.6481\n",
      "Epoch 44/500\n",
      "41/41 [==============================] - 0s 465us/step - loss: 0.8184 - accuracy: 0.6512\n",
      "Epoch 45/500\n",
      "41/41 [==============================] - 0s 477us/step - loss: 0.8420 - accuracy: 0.6327\n",
      "Epoch 46/500\n",
      "41/41 [==============================] - 0s 502us/step - loss: 0.8442 - accuracy: 0.6512\n",
      "Epoch 47/500\n",
      "41/41 [==============================] - 0s 471us/step - loss: 0.8393 - accuracy: 0.5988\n",
      "Epoch 48/500\n",
      "41/41 [==============================] - 0s 497us/step - loss: 0.8472 - accuracy: 0.6296\n",
      "Epoch 49/500\n",
      "41/41 [==============================] - 0s 475us/step - loss: 0.8208 - accuracy: 0.6636\n",
      "Epoch 50/500\n",
      "41/41 [==============================] - 0s 486us/step - loss: 0.8427 - accuracy: 0.6019\n",
      "Epoch 51/500\n",
      "41/41 [==============================] - 0s 494us/step - loss: 0.8508 - accuracy: 0.6265\n",
      "Epoch 52/500\n",
      "41/41 [==============================] - 0s 486us/step - loss: 0.8455 - accuracy: 0.6173\n",
      "Epoch 53/500\n",
      "41/41 [==============================] - 0s 492us/step - loss: 0.8215 - accuracy: 0.6451\n",
      "Epoch 54/500\n",
      "41/41 [==============================] - 0s 535us/step - loss: 0.7937 - accuracy: 0.6728\n",
      "Epoch 55/500\n",
      "41/41 [==============================] - 0s 508us/step - loss: 0.8116 - accuracy: 0.6605\n",
      "Epoch 56/500\n",
      "41/41 [==============================] - 0s 507us/step - loss: 0.8328 - accuracy: 0.6111\n",
      "Epoch 57/500\n",
      "41/41 [==============================] - 0s 518us/step - loss: 0.8310 - accuracy: 0.6543\n",
      "Epoch 58/500\n",
      "41/41 [==============================] - 0s 509us/step - loss: 0.7920 - accuracy: 0.6420\n",
      "Epoch 59/500\n",
      "41/41 [==============================] - 0s 528us/step - loss: 0.8327 - accuracy: 0.6451\n",
      "Epoch 60/500\n",
      "41/41 [==============================] - 0s 511us/step - loss: 0.8034 - accuracy: 0.6481\n",
      "Epoch 61/500\n",
      "41/41 [==============================] - 0s 499us/step - loss: 0.8271 - accuracy: 0.6728\n",
      "Epoch 62/500\n",
      "41/41 [==============================] - 0s 515us/step - loss: 0.8105 - accuracy: 0.6821\n",
      "Epoch 63/500\n",
      "41/41 [==============================] - 0s 508us/step - loss: 0.7931 - accuracy: 0.6543\n",
      "Epoch 64/500\n",
      "41/41 [==============================] - 0s 488us/step - loss: 0.8133 - accuracy: 0.6574\n",
      "Epoch 65/500\n",
      "41/41 [==============================] - 0s 509us/step - loss: 0.8053 - accuracy: 0.6636\n",
      "Epoch 66/500\n",
      "41/41 [==============================] - 0s 520us/step - loss: 0.7893 - accuracy: 0.6667\n",
      "Epoch 67/500\n",
      "41/41 [==============================] - 0s 555us/step - loss: 0.7716 - accuracy: 0.6852\n",
      "Epoch 68/500\n",
      "41/41 [==============================] - 0s 509us/step - loss: 0.8262 - accuracy: 0.6420\n",
      "Epoch 69/500\n",
      "41/41 [==============================] - 0s 601us/step - loss: 0.8094 - accuracy: 0.6728\n",
      "Epoch 70/500\n",
      "41/41 [==============================] - 0s 504us/step - loss: 0.8176 - accuracy: 0.6265\n",
      "Epoch 71/500\n",
      "41/41 [==============================] - 0s 528us/step - loss: 0.7801 - accuracy: 0.6636\n",
      "Epoch 72/500\n",
      "41/41 [==============================] - 0s 520us/step - loss: 0.8054 - accuracy: 0.6728\n",
      "Epoch 73/500\n",
      "41/41 [==============================] - 0s 472us/step - loss: 0.7891 - accuracy: 0.6512\n",
      "Epoch 74/500\n",
      "41/41 [==============================] - 0s 513us/step - loss: 0.7749 - accuracy: 0.6667\n",
      "Epoch 75/500\n",
      "41/41 [==============================] - 0s 568us/step - loss: 0.7996 - accuracy: 0.6698\n",
      "Epoch 76/500\n",
      "41/41 [==============================] - 0s 494us/step - loss: 0.8167 - accuracy: 0.6451\n",
      "Epoch 77/500\n",
      "41/41 [==============================] - 0s 515us/step - loss: 0.7868 - accuracy: 0.6636\n",
      "Epoch 78/500\n",
      "41/41 [==============================] - 0s 525us/step - loss: 0.7951 - accuracy: 0.6543\n",
      "Epoch 79/500\n",
      "41/41 [==============================] - 0s 636us/step - loss: 0.7709 - accuracy: 0.6883\n",
      "Epoch 80/500\n",
      "41/41 [==============================] - 0s 480us/step - loss: 0.7560 - accuracy: 0.6636\n",
      "Epoch 81/500\n",
      "41/41 [==============================] - 0s 496us/step - loss: 0.7655 - accuracy: 0.6790\n",
      "Epoch 82/500\n",
      "41/41 [==============================] - 0s 470us/step - loss: 0.8017 - accuracy: 0.6512\n",
      "Epoch 83/500\n",
      "41/41 [==============================] - 0s 507us/step - loss: 0.7789 - accuracy: 0.6605\n",
      "Epoch 84/500\n",
      "41/41 [==============================] - 0s 512us/step - loss: 0.7792 - accuracy: 0.6636\n",
      "Epoch 85/500\n",
      "41/41 [==============================] - 0s 500us/step - loss: 0.7635 - accuracy: 0.6883\n",
      "Epoch 86/500\n",
      "41/41 [==============================] - 0s 488us/step - loss: 0.7972 - accuracy: 0.6821\n",
      "Epoch 87/500\n",
      "41/41 [==============================] - 0s 498us/step - loss: 0.7712 - accuracy: 0.6698\n",
      "Epoch 88/500\n",
      "41/41 [==============================] - 0s 510us/step - loss: 0.7764 - accuracy: 0.6512\n",
      "Epoch 89/500\n",
      "41/41 [==============================] - 0s 463us/step - loss: 0.7553 - accuracy: 0.6636\n",
      "Epoch 90/500\n",
      "41/41 [==============================] - 0s 505us/step - loss: 0.7739 - accuracy: 0.6512\n",
      "Epoch 91/500\n",
      "41/41 [==============================] - 0s 506us/step - loss: 0.7564 - accuracy: 0.6914\n",
      "Epoch 92/500\n",
      "41/41 [==============================] - 0s 467us/step - loss: 0.7595 - accuracy: 0.7037\n",
      "Epoch 93/500\n",
      "41/41 [==============================] - 0s 511us/step - loss: 0.7461 - accuracy: 0.7006\n",
      "Epoch 94/500\n",
      "41/41 [==============================] - 0s 492us/step - loss: 0.7735 - accuracy: 0.6944\n",
      "Epoch 95/500\n",
      "41/41 [==============================] - 0s 518us/step - loss: 0.7801 - accuracy: 0.6605\n",
      "Epoch 96/500\n",
      "41/41 [==============================] - 0s 464us/step - loss: 0.7528 - accuracy: 0.6914\n",
      "Epoch 97/500\n",
      "41/41 [==============================] - 0s 472us/step - loss: 0.7694 - accuracy: 0.6883\n",
      "Epoch 98/500\n",
      "41/41 [==============================] - 0s 441us/step - loss: 0.7275 - accuracy: 0.7006\n",
      "Epoch 99/500\n",
      "41/41 [==============================] - 0s 485us/step - loss: 0.7522 - accuracy: 0.6790\n",
      "Epoch 100/500\n",
      "41/41 [==============================] - 0s 495us/step - loss: 0.7405 - accuracy: 0.6944\n",
      "Epoch 101/500\n",
      "41/41 [==============================] - 0s 474us/step - loss: 0.7556 - accuracy: 0.6944\n",
      "Epoch 102/500\n",
      "41/41 [==============================] - 0s 477us/step - loss: 0.7339 - accuracy: 0.7191\n",
      "Epoch 103/500\n",
      "41/41 [==============================] - 0s 498us/step - loss: 0.7324 - accuracy: 0.6944\n",
      "Epoch 104/500\n",
      "41/41 [==============================] - 0s 478us/step - loss: 0.7751 - accuracy: 0.6852\n",
      "Epoch 105/500\n",
      "41/41 [==============================] - 0s 483us/step - loss: 0.7442 - accuracy: 0.7068\n",
      "Epoch 106/500\n",
      "41/41 [==============================] - 0s 498us/step - loss: 0.7660 - accuracy: 0.6975\n",
      "Epoch 107/500\n",
      "41/41 [==============================] - 0s 503us/step - loss: 0.7347 - accuracy: 0.6821\n",
      "Epoch 108/500\n",
      "41/41 [==============================] - 0s 511us/step - loss: 0.7364 - accuracy: 0.7130\n",
      "Epoch 109/500\n",
      "41/41 [==============================] - 0s 496us/step - loss: 0.7464 - accuracy: 0.7006\n",
      "Epoch 110/500\n",
      "41/41 [==============================] - 0s 518us/step - loss: 0.7540 - accuracy: 0.6821\n",
      "Epoch 111/500\n",
      "41/41 [==============================] - 0s 513us/step - loss: 0.7567 - accuracy: 0.6975\n",
      "Epoch 112/500\n",
      "41/41 [==============================] - 0s 492us/step - loss: 0.7130 - accuracy: 0.7253\n",
      "Epoch 113/500\n",
      "41/41 [==============================] - 0s 509us/step - loss: 0.7297 - accuracy: 0.6883\n",
      "Epoch 114/500\n",
      "41/41 [==============================] - 0s 593us/step - loss: 0.7402 - accuracy: 0.6883\n",
      "Epoch 115/500\n",
      "41/41 [==============================] - 0s 473us/step - loss: 0.7449 - accuracy: 0.6543\n",
      "Epoch 116/500\n",
      "41/41 [==============================] - 0s 503us/step - loss: 0.7095 - accuracy: 0.7130\n",
      "Epoch 117/500\n",
      "41/41 [==============================] - 0s 516us/step - loss: 0.7674 - accuracy: 0.6698\n",
      "Epoch 118/500\n",
      "41/41 [==============================] - 0s 490us/step - loss: 0.7479 - accuracy: 0.6790\n",
      "Epoch 119/500\n",
      "41/41 [==============================] - 0s 564us/step - loss: 0.6958 - accuracy: 0.7253\n",
      "Epoch 120/500\n",
      "41/41 [==============================] - 0s 535us/step - loss: 0.7018 - accuracy: 0.6944\n",
      "Epoch 121/500\n",
      "41/41 [==============================] - 0s 563us/step - loss: 0.7149 - accuracy: 0.7284\n",
      "Epoch 122/500\n",
      "41/41 [==============================] - 0s 533us/step - loss: 0.7289 - accuracy: 0.7160\n",
      "Epoch 123/500\n",
      "41/41 [==============================] - 0s 533us/step - loss: 0.6910 - accuracy: 0.7160\n",
      "Epoch 124/500\n",
      "41/41 [==============================] - 0s 503us/step - loss: 0.7190 - accuracy: 0.7315\n",
      "Epoch 125/500\n",
      "41/41 [==============================] - 0s 498us/step - loss: 0.7206 - accuracy: 0.7006\n",
      "Epoch 126/500\n",
      "41/41 [==============================] - 0s 477us/step - loss: 0.7260 - accuracy: 0.7006\n",
      "Epoch 127/500\n",
      "41/41 [==============================] - 0s 501us/step - loss: 0.7197 - accuracy: 0.7037\n",
      "Epoch 128/500\n",
      "41/41 [==============================] - 0s 556us/step - loss: 0.7388 - accuracy: 0.7099\n",
      "Epoch 129/500\n",
      "41/41 [==============================] - 0s 536us/step - loss: 0.7205 - accuracy: 0.7160\n",
      "Epoch 130/500\n",
      "41/41 [==============================] - 0s 540us/step - loss: 0.7277 - accuracy: 0.7006\n",
      "Epoch 131/500\n",
      "41/41 [==============================] - 0s 502us/step - loss: 0.7274 - accuracy: 0.6944\n",
      "Epoch 132/500\n",
      "41/41 [==============================] - 0s 458us/step - loss: 0.6955 - accuracy: 0.7099\n",
      "Epoch 133/500\n",
      "41/41 [==============================] - 0s 472us/step - loss: 0.6940 - accuracy: 0.7253\n",
      "Epoch 134/500\n",
      "41/41 [==============================] - 0s 502us/step - loss: 0.7071 - accuracy: 0.7068\n",
      "Epoch 135/500\n",
      "41/41 [==============================] - 0s 498us/step - loss: 0.6935 - accuracy: 0.6975\n",
      "Epoch 136/500\n",
      "41/41 [==============================] - 0s 487us/step - loss: 0.7464 - accuracy: 0.6698\n",
      "Epoch 137/500\n",
      "41/41 [==============================] - 0s 511us/step - loss: 0.6942 - accuracy: 0.7438\n",
      "Epoch 138/500\n",
      "41/41 [==============================] - 0s 466us/step - loss: 0.7355 - accuracy: 0.6759\n",
      "Epoch 139/500\n",
      "41/41 [==============================] - 0s 521us/step - loss: 0.7086 - accuracy: 0.7160\n",
      "Epoch 140/500\n",
      "41/41 [==============================] - 0s 513us/step - loss: 0.6855 - accuracy: 0.7377\n",
      "Epoch 141/500\n",
      "41/41 [==============================] - 0s 487us/step - loss: 0.7014 - accuracy: 0.7130\n",
      "Epoch 142/500\n",
      "41/41 [==============================] - 0s 509us/step - loss: 0.7178 - accuracy: 0.6698\n",
      "Epoch 143/500\n",
      "41/41 [==============================] - 0s 526us/step - loss: 0.6902 - accuracy: 0.7037\n",
      "Epoch 144/500\n",
      "41/41 [==============================] - 0s 541us/step - loss: 0.6754 - accuracy: 0.7346\n",
      "Epoch 145/500\n",
      "41/41 [==============================] - 0s 525us/step - loss: 0.7004 - accuracy: 0.6852\n",
      "Epoch 146/500\n",
      "41/41 [==============================] - 0s 515us/step - loss: 0.6975 - accuracy: 0.7099\n",
      "Epoch 147/500\n",
      "41/41 [==============================] - 0s 514us/step - loss: 0.6826 - accuracy: 0.7191\n",
      "Epoch 148/500\n",
      "41/41 [==============================] - 0s 491us/step - loss: 0.7114 - accuracy: 0.6883\n",
      "Epoch 149/500\n",
      "41/41 [==============================] - 0s 459us/step - loss: 0.6780 - accuracy: 0.7315\n",
      "Epoch 150/500\n",
      "41/41 [==============================] - 0s 551us/step - loss: 0.7107 - accuracy: 0.7438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/500\n",
      "41/41 [==============================] - 0s 507us/step - loss: 0.6965 - accuracy: 0.7377\n",
      "Epoch 152/500\n",
      "41/41 [==============================] - 0s 527us/step - loss: 0.7071 - accuracy: 0.6975\n",
      "Epoch 153/500\n",
      "41/41 [==============================] - 0s 518us/step - loss: 0.7048 - accuracy: 0.6975\n",
      "Epoch 154/500\n",
      "41/41 [==============================] - 0s 498us/step - loss: 0.6796 - accuracy: 0.7346\n",
      "Epoch 155/500\n",
      "41/41 [==============================] - 0s 484us/step - loss: 0.6799 - accuracy: 0.7253\n",
      "Epoch 156/500\n",
      "41/41 [==============================] - 0s 505us/step - loss: 0.6624 - accuracy: 0.7222\n",
      "Epoch 157/500\n",
      "41/41 [==============================] - 0s 533us/step - loss: 0.6628 - accuracy: 0.7346\n",
      "Epoch 158/500\n",
      "41/41 [==============================] - 0s 495us/step - loss: 0.6924 - accuracy: 0.7099\n",
      "Epoch 159/500\n",
      "41/41 [==============================] - 0s 484us/step - loss: 0.6787 - accuracy: 0.7099\n",
      "Epoch 160/500\n",
      "41/41 [==============================] - 0s 465us/step - loss: 0.6996 - accuracy: 0.7160\n",
      "Epoch 161/500\n",
      "41/41 [==============================] - 0s 474us/step - loss: 0.6799 - accuracy: 0.7253\n",
      "Epoch 162/500\n",
      "41/41 [==============================] - 0s 481us/step - loss: 0.6542 - accuracy: 0.7407\n",
      "Epoch 163/500\n",
      "41/41 [==============================] - 0s 502us/step - loss: 0.6851 - accuracy: 0.7099\n",
      "Epoch 164/500\n",
      "41/41 [==============================] - 0s 508us/step - loss: 0.6819 - accuracy: 0.7191\n",
      "Epoch 165/500\n",
      "41/41 [==============================] - 0s 515us/step - loss: 0.6550 - accuracy: 0.7623\n",
      "Epoch 166/500\n",
      "41/41 [==============================] - 0s 511us/step - loss: 0.6696 - accuracy: 0.7315\n",
      "Epoch 167/500\n",
      "41/41 [==============================] - 0s 518us/step - loss: 0.6526 - accuracy: 0.7253\n",
      "Epoch 168/500\n",
      "41/41 [==============================] - 0s 493us/step - loss: 0.6898 - accuracy: 0.7160\n",
      "Epoch 169/500\n",
      "41/41 [==============================] - 0s 491us/step - loss: 0.6981 - accuracy: 0.7284\n",
      "Epoch 170/500\n",
      "41/41 [==============================] - 0s 469us/step - loss: 0.6431 - accuracy: 0.7284\n",
      "Epoch 171/500\n",
      "41/41 [==============================] - 0s 501us/step - loss: 0.6669 - accuracy: 0.7346\n",
      "Epoch 172/500\n",
      "41/41 [==============================] - 0s 439us/step - loss: 0.6958 - accuracy: 0.6975\n",
      "Epoch 173/500\n",
      "41/41 [==============================] - 0s 455us/step - loss: 0.6419 - accuracy: 0.7531\n",
      "Epoch 174/500\n",
      "41/41 [==============================] - 0s 497us/step - loss: 0.6779 - accuracy: 0.7130\n",
      "Epoch 175/500\n",
      "41/41 [==============================] - 0s 453us/step - loss: 0.6624 - accuracy: 0.7284\n",
      "Epoch 176/500\n",
      "41/41 [==============================] - 0s 473us/step - loss: 0.6617 - accuracy: 0.7407\n",
      "Epoch 177/500\n",
      "41/41 [==============================] - 0s 480us/step - loss: 0.6661 - accuracy: 0.7407\n",
      "Epoch 178/500\n",
      "41/41 [==============================] - 0s 493us/step - loss: 0.6573 - accuracy: 0.7438\n",
      "Epoch 179/500\n",
      "41/41 [==============================] - 0s 485us/step - loss: 0.6644 - accuracy: 0.7130\n",
      "Epoch 180/500\n",
      "41/41 [==============================] - 0s 486us/step - loss: 0.6770 - accuracy: 0.7037\n",
      "Epoch 181/500\n",
      "41/41 [==============================] - 0s 504us/step - loss: 0.6840 - accuracy: 0.6975\n",
      "Epoch 182/500\n",
      "41/41 [==============================] - 0s 469us/step - loss: 0.6665 - accuracy: 0.7531\n",
      "Epoch 183/500\n",
      "41/41 [==============================] - 0s 534us/step - loss: 0.6655 - accuracy: 0.7130\n",
      "Epoch 184/500\n",
      "41/41 [==============================] - 0s 517us/step - loss: 0.6668 - accuracy: 0.7346\n",
      "Epoch 185/500\n",
      "41/41 [==============================] - 0s 495us/step - loss: 0.6538 - accuracy: 0.7407\n",
      "Epoch 186/500\n",
      "41/41 [==============================] - 0s 537us/step - loss: 0.6524 - accuracy: 0.7562\n",
      "Epoch 187/500\n",
      "41/41 [==============================] - 0s 506us/step - loss: 0.6338 - accuracy: 0.7469\n",
      "Epoch 188/500\n",
      "41/41 [==============================] - 0s 553us/step - loss: 0.6427 - accuracy: 0.7469\n",
      "Epoch 189/500\n",
      "41/41 [==============================] - 0s 526us/step - loss: 0.6695 - accuracy: 0.7130\n",
      "Epoch 190/500\n",
      "41/41 [==============================] - 0s 513us/step - loss: 0.6499 - accuracy: 0.7346\n",
      "Epoch 191/500\n",
      "41/41 [==============================] - 0s 507us/step - loss: 0.6442 - accuracy: 0.7438\n",
      "Epoch 192/500\n",
      "41/41 [==============================] - 0s 514us/step - loss: 0.6632 - accuracy: 0.7407\n",
      "Epoch 193/500\n",
      "41/41 [==============================] - 0s 495us/step - loss: 0.6327 - accuracy: 0.7284\n",
      "Epoch 194/500\n",
      "41/41 [==============================] - 0s 507us/step - loss: 0.6428 - accuracy: 0.7377\n",
      "Epoch 195/500\n",
      "41/41 [==============================] - 0s 497us/step - loss: 0.6383 - accuracy: 0.7531\n",
      "Epoch 196/500\n",
      "41/41 [==============================] - 0s 503us/step - loss: 0.6383 - accuracy: 0.7469\n",
      "Epoch 197/500\n",
      "41/41 [==============================] - 0s 522us/step - loss: 0.6803 - accuracy: 0.7222\n",
      "Epoch 198/500\n",
      "41/41 [==============================] - 0s 486us/step - loss: 0.6359 - accuracy: 0.7500\n",
      "Epoch 199/500\n",
      "41/41 [==============================] - 0s 515us/step - loss: 0.6554 - accuracy: 0.7253\n",
      "Epoch 200/500\n",
      "41/41 [==============================] - 0s 523us/step - loss: 0.6667 - accuracy: 0.7222\n",
      "Epoch 201/500\n",
      "41/41 [==============================] - 0s 532us/step - loss: 0.6375 - accuracy: 0.7562\n",
      "Epoch 202/500\n",
      "41/41 [==============================] - 0s 537us/step - loss: 0.6605 - accuracy: 0.7407\n",
      "Epoch 203/500\n",
      "41/41 [==============================] - 0s 547us/step - loss: 0.6571 - accuracy: 0.7407\n",
      "Epoch 204/500\n",
      "41/41 [==============================] - 0s 513us/step - loss: 0.6165 - accuracy: 0.7531\n",
      "Epoch 205/500\n",
      "41/41 [==============================] - 0s 520us/step - loss: 0.6630 - accuracy: 0.7346\n",
      "Epoch 206/500\n",
      "41/41 [==============================] - 0s 462us/step - loss: 0.6350 - accuracy: 0.7407\n",
      "Epoch 207/500\n",
      "41/41 [==============================] - 0s 529us/step - loss: 0.6202 - accuracy: 0.7438\n",
      "Epoch 208/500\n",
      "41/41 [==============================] - 0s 487us/step - loss: 0.6444 - accuracy: 0.7531\n",
      "Epoch 209/500\n",
      "41/41 [==============================] - 0s 527us/step - loss: 0.6370 - accuracy: 0.7377\n",
      "Epoch 210/500\n",
      "41/41 [==============================] - 0s 495us/step - loss: 0.6248 - accuracy: 0.7407\n",
      "Epoch 211/500\n",
      "41/41 [==============================] - 0s 523us/step - loss: 0.6543 - accuracy: 0.7438\n",
      "Epoch 212/500\n",
      "41/41 [==============================] - 0s 532us/step - loss: 0.6317 - accuracy: 0.7500\n",
      "Epoch 213/500\n",
      "41/41 [==============================] - 0s 510us/step - loss: 0.6267 - accuracy: 0.7346\n",
      "Epoch 214/500\n",
      "41/41 [==============================] - 0s 543us/step - loss: 0.6120 - accuracy: 0.7623\n",
      "Epoch 215/500\n",
      "41/41 [==============================] - 0s 467us/step - loss: 0.6060 - accuracy: 0.7623\n",
      "Epoch 216/500\n",
      "41/41 [==============================] - 0s 482us/step - loss: 0.6311 - accuracy: 0.7654\n",
      "Epoch 217/500\n",
      "41/41 [==============================] - 0s 489us/step - loss: 0.6156 - accuracy: 0.7407\n",
      "Epoch 218/500\n",
      "41/41 [==============================] - 0s 523us/step - loss: 0.6554 - accuracy: 0.7531\n",
      "Epoch 219/500\n",
      "41/41 [==============================] - 0s 516us/step - loss: 0.6222 - accuracy: 0.7469\n",
      "Epoch 220/500\n",
      "41/41 [==============================] - 0s 483us/step - loss: 0.6199 - accuracy: 0.7500\n",
      "Epoch 221/500\n",
      "41/41 [==============================] - 0s 545us/step - loss: 0.5759 - accuracy: 0.7654\n",
      "Epoch 222/500\n",
      "41/41 [==============================] - 0s 500us/step - loss: 0.6452 - accuracy: 0.7160\n",
      "Epoch 223/500\n",
      "41/41 [==============================] - 0s 535us/step - loss: 0.6372 - accuracy: 0.7284\n",
      "Epoch 224/500\n",
      "41/41 [==============================] - 0s 486us/step - loss: 0.6218 - accuracy: 0.7531\n",
      "Epoch 225/500\n",
      "41/41 [==============================] - 0s 577us/step - loss: 0.6515 - accuracy: 0.7346\n",
      "Epoch 226/500\n",
      "41/41 [==============================] - 0s 569us/step - loss: 0.6152 - accuracy: 0.7469\n",
      "Epoch 227/500\n",
      "41/41 [==============================] - 0s 522us/step - loss: 0.5893 - accuracy: 0.7809\n",
      "Epoch 228/500\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.3867 - accuracy: 0.8750"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-695b788237c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mX_cross_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cross_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cross_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cross_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cross_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpredictions_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "BATCH_SIZE = 8\n",
    "num_folds = 10\n",
    "\n",
    "\n",
    "kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "cv_results = np.array([])\n",
    "for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "    X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "    X_cross_train = scaler.fit_transform(X_cross_train)\n",
    "    X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "    X_cross_test = scaler.transform(X_cross_test)\n",
    "    model = getNetwork()\n",
    "    model.fit(X_cross_train, y_cross_train, epochs=EPOCHS, batch_size=BATCH_SIZE)  \n",
    "    y_pred = model.predict(X_cross_test)\n",
    "    predictions_categorical = np.argmax(y_pred, axis=1)\n",
    "    f1s = f1_score(y_cross_test, predictions_categorical, average=\"weighted\")\n",
    "    cv_results = np.append(cv_results, [f1s])\n",
    "\n",
    "print(f'Average score of Cross Validation: {cv_results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 3,298\n",
      "Trainable params: 3,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7564 - accuracy: 0.5056 - val_loss: 0.6723 - val_accuracy: 0.5000\n",
      "Epoch 2/500\n",
      "23/23 [==============================] - 0s 925us/step - loss: 0.7254 - accuracy: 0.5444 - val_loss: 0.6664 - val_accuracy: 0.5333\n",
      "Epoch 3/500\n",
      "23/23 [==============================] - 0s 949us/step - loss: 0.7399 - accuracy: 0.5389 - val_loss: 0.6627 - val_accuracy: 0.5500\n",
      "Epoch 4/500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.7112 - accuracy: 0.5278 - val_loss: 0.6596 - val_accuracy: 0.5833\n",
      "Epoch 5/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6787 - accuracy: 0.6111 - val_loss: 0.6584 - val_accuracy: 0.5833\n",
      "Epoch 6/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6814 - accuracy: 0.5944 - val_loss: 0.6567 - val_accuracy: 0.5833\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.7042 - accuracy: 0.5389 - val_loss: 0.6552 - val_accuracy: 0.6000\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.6924 - accuracy: 0.5333 - val_loss: 0.6537 - val_accuracy: 0.6000\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.7050 - accuracy: 0.5833 - val_loss: 0.6523 - val_accuracy: 0.5833\n",
      "Epoch 10/500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.6927 - accuracy: 0.5944 - val_loss: 0.6508 - val_accuracy: 0.6167\n",
      "Epoch 11/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6645 - accuracy: 0.5667 - val_loss: 0.6502 - val_accuracy: 0.6500\n",
      "Epoch 12/500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.6560 - accuracy: 0.5944 - val_loss: 0.6494 - val_accuracy: 0.6500\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - 0s 950us/step - loss: 0.6635 - accuracy: 0.6389 - val_loss: 0.6490 - val_accuracy: 0.6500\n",
      "Epoch 14/500\n",
      "23/23 [==============================] - 0s 943us/step - loss: 0.6520 - accuracy: 0.6111 - val_loss: 0.6470 - val_accuracy: 0.6667\n",
      "Epoch 15/500\n",
      "23/23 [==============================] - 0s 946us/step - loss: 0.6543 - accuracy: 0.6833 - val_loss: 0.6461 - val_accuracy: 0.6667\n",
      "Epoch 16/500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.6353 - accuracy: 0.6389 - val_loss: 0.6443 - val_accuracy: 0.6500\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - 0s 935us/step - loss: 0.6533 - accuracy: 0.6389 - val_loss: 0.6454 - val_accuracy: 0.6333\n",
      "Epoch 18/500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.6304 - accuracy: 0.6556 - val_loss: 0.6457 - val_accuracy: 0.6333\n",
      "Epoch 19/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6461 - accuracy: 0.6778 - val_loss: 0.6440 - val_accuracy: 0.6333\n",
      "Epoch 20/500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.6558 - accuracy: 0.6167 - val_loss: 0.6416 - val_accuracy: 0.6333\n",
      "Epoch 21/500\n",
      "23/23 [==============================] - 0s 936us/step - loss: 0.6395 - accuracy: 0.6556 - val_loss: 0.6391 - val_accuracy: 0.6500\n",
      "Epoch 22/500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.6197 - accuracy: 0.6944 - val_loss: 0.6396 - val_accuracy: 0.6333\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6236 - accuracy: 0.6889 - val_loss: 0.6385 - val_accuracy: 0.6333\n",
      "Epoch 24/500\n",
      "23/23 [==============================] - 0s 941us/step - loss: 0.6187 - accuracy: 0.6444 - val_loss: 0.6378 - val_accuracy: 0.6500\n",
      "Epoch 25/500\n",
      "23/23 [==============================] - 0s 951us/step - loss: 0.6302 - accuracy: 0.6500 - val_loss: 0.6366 - val_accuracy: 0.6500\n",
      "Epoch 26/500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.6328 - accuracy: 0.6444 - val_loss: 0.6355 - val_accuracy: 0.6500\n",
      "Epoch 27/500\n",
      "23/23 [==============================] - 0s 913us/step - loss: 0.5976 - accuracy: 0.7222 - val_loss: 0.6342 - val_accuracy: 0.6500\n",
      "Epoch 28/500\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.6102 - accuracy: 0.6889 - val_loss: 0.6337 - val_accuracy: 0.6500\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - 0s 936us/step - loss: 0.6530 - accuracy: 0.6500 - val_loss: 0.6331 - val_accuracy: 0.6500\n",
      "Epoch 30/500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.6079 - accuracy: 0.6944 - val_loss: 0.6319 - val_accuracy: 0.6500\n",
      "Epoch 31/500\n",
      "23/23 [==============================] - 0s 994us/step - loss: 0.5919 - accuracy: 0.7111 - val_loss: 0.6315 - val_accuracy: 0.6500\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - 0s 918us/step - loss: 0.6139 - accuracy: 0.7278 - val_loss: 0.6308 - val_accuracy: 0.6500\n",
      "Epoch 33/500\n",
      "23/23 [==============================] - 0s 992us/step - loss: 0.6284 - accuracy: 0.6667 - val_loss: 0.6292 - val_accuracy: 0.6500\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - 0s 937us/step - loss: 0.6294 - accuracy: 0.6778 - val_loss: 0.6278 - val_accuracy: 0.6500\n",
      "Epoch 35/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6139 - accuracy: 0.6722 - val_loss: 0.6279 - val_accuracy: 0.6500\n",
      "Epoch 36/500\n",
      "23/23 [==============================] - 0s 947us/step - loss: 0.5910 - accuracy: 0.6889 - val_loss: 0.6273 - val_accuracy: 0.6500\n",
      "Epoch 37/500\n",
      "23/23 [==============================] - 0s 942us/step - loss: 0.6093 - accuracy: 0.6556 - val_loss: 0.6254 - val_accuracy: 0.6500\n",
      "Epoch 38/500\n",
      "23/23 [==============================] - 0s 936us/step - loss: 0.6034 - accuracy: 0.6889 - val_loss: 0.6269 - val_accuracy: 0.6500\n",
      "Epoch 39/500\n",
      "23/23 [==============================] - 0s 877us/step - loss: 0.5880 - accuracy: 0.7333 - val_loss: 0.6246 - val_accuracy: 0.6500\n",
      "Epoch 40/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5852 - accuracy: 0.6833 - val_loss: 0.6244 - val_accuracy: 0.6500\n",
      "Epoch 41/500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.5891 - accuracy: 0.7444 - val_loss: 0.6234 - val_accuracy: 0.6500\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - 0s 965us/step - loss: 0.5771 - accuracy: 0.7278 - val_loss: 0.6212 - val_accuracy: 0.6500\n",
      "Epoch 43/500\n",
      "23/23 [==============================] - 0s 897us/step - loss: 0.5708 - accuracy: 0.7333 - val_loss: 0.6218 - val_accuracy: 0.6500\n",
      "Epoch 44/500\n",
      "23/23 [==============================] - 0s 949us/step - loss: 0.5774 - accuracy: 0.7167 - val_loss: 0.6208 - val_accuracy: 0.6500\n",
      "Epoch 45/500\n",
      "23/23 [==============================] - 0s 992us/step - loss: 0.5634 - accuracy: 0.7333 - val_loss: 0.6190 - val_accuracy: 0.6500\n",
      "Epoch 46/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5655 - accuracy: 0.7444 - val_loss: 0.6215 - val_accuracy: 0.6500\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - 0s 947us/step - loss: 0.5593 - accuracy: 0.7611 - val_loss: 0.6202 - val_accuracy: 0.6500\n",
      "Epoch 48/500\n",
      "23/23 [==============================] - 0s 944us/step - loss: 0.5601 - accuracy: 0.7278 - val_loss: 0.6170 - val_accuracy: 0.6500\n",
      "Epoch 49/500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.5573 - accuracy: 0.7111 - val_loss: 0.6167 - val_accuracy: 0.6667\n",
      "Epoch 50/500\n",
      "23/23 [==============================] - 0s 935us/step - loss: 0.5676 - accuracy: 0.7333 - val_loss: 0.6160 - val_accuracy: 0.6500\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 941us/step - loss: 0.5582 - accuracy: 0.7278 - val_loss: 0.6139 - val_accuracy: 0.6833\n",
      "Epoch 52/500\n",
      "23/23 [==============================] - 0s 933us/step - loss: 0.5425 - accuracy: 0.7500 - val_loss: 0.6145 - val_accuracy: 0.6667\n",
      "Epoch 53/500\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.5584 - accuracy: 0.7500 - val_loss: 0.6131 - val_accuracy: 0.6667\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - 0s 918us/step - loss: 0.5709 - accuracy: 0.6944 - val_loss: 0.6113 - val_accuracy: 0.6833\n",
      "Epoch 55/500\n",
      "23/23 [==============================] - 0s 906us/step - loss: 0.5462 - accuracy: 0.7778 - val_loss: 0.6120 - val_accuracy: 0.6833\n",
      "Epoch 56/500\n",
      "23/23 [==============================] - 0s 959us/step - loss: 0.5565 - accuracy: 0.7389 - val_loss: 0.6101 - val_accuracy: 0.6833\n",
      "Epoch 57/500\n",
      "23/23 [==============================] - 0s 938us/step - loss: 0.5379 - accuracy: 0.7722 - val_loss: 0.6062 - val_accuracy: 0.6833\n",
      "Epoch 58/500\n",
      "23/23 [==============================] - 0s 926us/step - loss: 0.5445 - accuracy: 0.7333 - val_loss: 0.6081 - val_accuracy: 0.6833\n",
      "Epoch 59/500\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.5534 - accuracy: 0.7167 - val_loss: 0.6067 - val_accuracy: 0.6833\n",
      "Epoch 60/500\n",
      "23/23 [==============================] - 0s 938us/step - loss: 0.5380 - accuracy: 0.7333 - val_loss: 0.6081 - val_accuracy: 0.6833\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - 0s 940us/step - loss: 0.5189 - accuracy: 0.7667 - val_loss: 0.6073 - val_accuracy: 0.6833\n",
      "Epoch 62/500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.5438 - accuracy: 0.7167 - val_loss: 0.6043 - val_accuracy: 0.6833\n",
      "Epoch 63/500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.5217 - accuracy: 0.7667 - val_loss: 0.6017 - val_accuracy: 0.7000\n",
      "Epoch 64/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5128 - accuracy: 0.7611 - val_loss: 0.5992 - val_accuracy: 0.7167\n",
      "Epoch 65/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5230 - accuracy: 0.7667 - val_loss: 0.6011 - val_accuracy: 0.7000\n",
      "Epoch 66/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5378 - accuracy: 0.7611 - val_loss: 0.5980 - val_accuracy: 0.7000\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.5284 - accuracy: 0.7167 - val_loss: 0.5987 - val_accuracy: 0.7000\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.5582 - accuracy: 0.7667 - val_loss: 0.5968 - val_accuracy: 0.7167\n",
      "Epoch 69/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5385 - accuracy: 0.7722 - val_loss: 0.5964 - val_accuracy: 0.7167\n",
      "Epoch 70/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5176 - accuracy: 0.7722 - val_loss: 0.5977 - val_accuracy: 0.7167\n",
      "Epoch 71/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5018 - accuracy: 0.8111 - val_loss: 0.6000 - val_accuracy: 0.7000\n",
      "Epoch 72/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5068 - accuracy: 0.7722 - val_loss: 0.5988 - val_accuracy: 0.7167\n",
      "Epoch 73/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5424 - accuracy: 0.7611 - val_loss: 0.5942 - val_accuracy: 0.7167\n",
      "Epoch 74/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5284 - accuracy: 0.7333 - val_loss: 0.5934 - val_accuracy: 0.7000\n",
      "Epoch 75/500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.5207 - accuracy: 0.7722 - val_loss: 0.5922 - val_accuracy: 0.7000\n",
      "Epoch 76/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5098 - accuracy: 0.7944 - val_loss: 0.5934 - val_accuracy: 0.7000\n",
      "Epoch 77/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5196 - accuracy: 0.7556 - val_loss: 0.5939 - val_accuracy: 0.7000\n",
      "Epoch 78/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4748 - accuracy: 0.8056 - val_loss: 0.5937 - val_accuracy: 0.7000\n",
      "Epoch 79/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5164 - accuracy: 0.8000 - val_loss: 0.5947 - val_accuracy: 0.7000\n",
      "Epoch 80/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5307 - accuracy: 0.7611 - val_loss: 0.5949 - val_accuracy: 0.7000\n",
      "Epoch 81/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5286 - accuracy: 0.7556 - val_loss: 0.5946 - val_accuracy: 0.7000\n",
      "Epoch 82/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4810 - accuracy: 0.8000 - val_loss: 0.5936 - val_accuracy: 0.7000\n",
      "Epoch 83/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4812 - accuracy: 0.7833 - val_loss: 0.5954 - val_accuracy: 0.6833\n",
      "Epoch 84/500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.5094 - accuracy: 0.7611 - val_loss: 0.5961 - val_accuracy: 0.6833\n",
      "Epoch 85/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5040 - accuracy: 0.7389 - val_loss: 0.5945 - val_accuracy: 0.6833\n",
      "Epoch 86/500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.4806 - accuracy: 0.7667 - val_loss: 0.5918 - val_accuracy: 0.7000\n",
      "Epoch 87/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4815 - accuracy: 0.8222 - val_loss: 0.5936 - val_accuracy: 0.6833\n",
      "Epoch 88/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4812 - accuracy: 0.7611 - val_loss: 0.5960 - val_accuracy: 0.6833\n",
      "Epoch 89/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5018 - accuracy: 0.7722 - val_loss: 0.5956 - val_accuracy: 0.6833\n",
      "Epoch 90/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4594 - accuracy: 0.7944 - val_loss: 0.5948 - val_accuracy: 0.6833\n",
      "Epoch 91/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4577 - accuracy: 0.8056 - val_loss: 0.5928 - val_accuracy: 0.6833\n",
      "Epoch 92/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4956 - accuracy: 0.7389 - val_loss: 0.5939 - val_accuracy: 0.6833\n",
      "Epoch 93/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4746 - accuracy: 0.7778 - val_loss: 0.5909 - val_accuracy: 0.6833\n",
      "Epoch 94/500\n",
      "23/23 [==============================] - 0s 946us/step - loss: 0.4704 - accuracy: 0.7889 - val_loss: 0.5914 - val_accuracy: 0.7000\n",
      "Epoch 95/500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.5259 - accuracy: 0.7611 - val_loss: 0.5893 - val_accuracy: 0.7000\n",
      "Epoch 96/500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.4666 - accuracy: 0.7667 - val_loss: 0.5913 - val_accuracy: 0.6833\n",
      "Epoch 97/500\n",
      "23/23 [==============================] - 0s 941us/step - loss: 0.4604 - accuracy: 0.7611 - val_loss: 0.5915 - val_accuracy: 0.6833\n",
      "Epoch 98/500\n",
      "23/23 [==============================] - 0s 942us/step - loss: 0.4794 - accuracy: 0.8056 - val_loss: 0.5944 - val_accuracy: 0.6833\n",
      "Epoch 99/500\n",
      "23/23 [==============================] - 0s 934us/step - loss: 0.4685 - accuracy: 0.8222 - val_loss: 0.5933 - val_accuracy: 0.6833\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.4621 - accuracy: 0.8111 - val_loss: 0.5905 - val_accuracy: 0.6833\n",
      "Epoch 101/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4806 - accuracy: 0.7778 - val_loss: 0.5882 - val_accuracy: 0.7000\n",
      "Epoch 102/500\n",
      "23/23 [==============================] - 0s 949us/step - loss: 0.4806 - accuracy: 0.8167 - val_loss: 0.5872 - val_accuracy: 0.7000\n",
      "Epoch 103/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4667 - accuracy: 0.7722 - val_loss: 0.5864 - val_accuracy: 0.7000\n",
      "Epoch 104/500\n",
      "23/23 [==============================] - 0s 994us/step - loss: 0.4505 - accuracy: 0.8056 - val_loss: 0.5856 - val_accuracy: 0.7000\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4492 - accuracy: 0.8278 - val_loss: 0.5875 - val_accuracy: 0.7167\n",
      "Epoch 106/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4458 - accuracy: 0.8056 - val_loss: 0.5901 - val_accuracy: 0.7000\n",
      "Epoch 107/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4962 - accuracy: 0.7833 - val_loss: 0.5877 - val_accuracy: 0.7000\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4715 - accuracy: 0.7833 - val_loss: 0.5885 - val_accuracy: 0.7000\n",
      "Epoch 109/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4520 - accuracy: 0.8000 - val_loss: 0.5879 - val_accuracy: 0.7000\n",
      "Epoch 110/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4686 - accuracy: 0.8056 - val_loss: 0.5887 - val_accuracy: 0.7167\n",
      "Epoch 111/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4552 - accuracy: 0.8111 - val_loss: 0.5883 - val_accuracy: 0.7000\n",
      "Epoch 112/500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.4370 - accuracy: 0.7722 - val_loss: 0.5893 - val_accuracy: 0.7000\n",
      "Epoch 113/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4444 - accuracy: 0.7889 - val_loss: 0.5905 - val_accuracy: 0.7000\n",
      "Epoch 114/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4204 - accuracy: 0.8444 - val_loss: 0.5891 - val_accuracy: 0.7000\n",
      "Epoch 115/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4588 - accuracy: 0.7944 - val_loss: 0.5882 - val_accuracy: 0.7000\n",
      "Epoch 116/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4799 - accuracy: 0.7778 - val_loss: 0.5861 - val_accuracy: 0.7167\n",
      "Epoch 117/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4484 - accuracy: 0.8056 - val_loss: 0.5884 - val_accuracy: 0.7167\n",
      "Epoch 118/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4326 - accuracy: 0.8111 - val_loss: 0.5906 - val_accuracy: 0.7167\n",
      "Epoch 119/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4217 - accuracy: 0.8111 - val_loss: 0.5873 - val_accuracy: 0.7167\n",
      "Epoch 120/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4453 - accuracy: 0.8111 - val_loss: 0.5854 - val_accuracy: 0.7000\n",
      "Epoch 121/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4446 - accuracy: 0.7833 - val_loss: 0.5837 - val_accuracy: 0.7000\n",
      "Epoch 122/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4684 - accuracy: 0.7611 - val_loss: 0.5801 - val_accuracy: 0.7000\n",
      "Epoch 123/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4430 - accuracy: 0.7889 - val_loss: 0.5826 - val_accuracy: 0.7167\n",
      "Epoch 124/500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.4266 - accuracy: 0.8167 - val_loss: 0.5848 - val_accuracy: 0.7000\n",
      "Epoch 125/500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.4359 - accuracy: 0.8111 - val_loss: 0.5861 - val_accuracy: 0.7000\n",
      "Epoch 126/500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.4332 - accuracy: 0.8222 - val_loss: 0.5844 - val_accuracy: 0.7000\n",
      "Epoch 127/500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.4176 - accuracy: 0.8167 - val_loss: 0.5798 - val_accuracy: 0.7000\n",
      "Epoch 128/500\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.4186 - accuracy: 0.8222 - val_loss: 0.5777 - val_accuracy: 0.7000\n",
      "Epoch 129/500\n",
      "23/23 [==============================] - 0s 912us/step - loss: 0.4466 - accuracy: 0.8222 - val_loss: 0.5780 - val_accuracy: 0.7000\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - 0s 944us/step - loss: 0.4423 - accuracy: 0.7833 - val_loss: 0.5808 - val_accuracy: 0.7000\n",
      "Epoch 131/500\n",
      "23/23 [==============================] - 0s 914us/step - loss: 0.4255 - accuracy: 0.8278 - val_loss: 0.5767 - val_accuracy: 0.7000\n",
      "Epoch 132/500\n",
      "23/23 [==============================] - 0s 954us/step - loss: 0.4435 - accuracy: 0.7722 - val_loss: 0.5775 - val_accuracy: 0.7000\n",
      "Epoch 133/500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.4381 - accuracy: 0.7944 - val_loss: 0.5779 - val_accuracy: 0.7000\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - 0s 940us/step - loss: 0.4107 - accuracy: 0.8333 - val_loss: 0.5803 - val_accuracy: 0.7000\n",
      "Epoch 135/500\n",
      "23/23 [==============================] - 0s 951us/step - loss: 0.4100 - accuracy: 0.7944 - val_loss: 0.5785 - val_accuracy: 0.7000\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - 0s 940us/step - loss: 0.4380 - accuracy: 0.8111 - val_loss: 0.5741 - val_accuracy: 0.7000\n",
      "Epoch 137/500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.4297 - accuracy: 0.8278 - val_loss: 0.5735 - val_accuracy: 0.7000\n",
      "Epoch 138/500\n",
      "23/23 [==============================] - 0s 954us/step - loss: 0.4145 - accuracy: 0.8056 - val_loss: 0.5723 - val_accuracy: 0.7000\n",
      "Epoch 139/500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.4508 - accuracy: 0.7778 - val_loss: 0.5730 - val_accuracy: 0.7000\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - 0s 947us/step - loss: 0.4118 - accuracy: 0.8000 - val_loss: 0.5757 - val_accuracy: 0.7000\n",
      "Epoch 141/500\n",
      "23/23 [==============================] - 0s 922us/step - loss: 0.4136 - accuracy: 0.8000 - val_loss: 0.5764 - val_accuracy: 0.7000\n",
      "Epoch 142/500\n",
      "23/23 [==============================] - 0s 950us/step - loss: 0.3904 - accuracy: 0.8333 - val_loss: 0.5755 - val_accuracy: 0.7000\n",
      "Epoch 143/500\n",
      "23/23 [==============================] - 0s 908us/step - loss: 0.4023 - accuracy: 0.8333 - val_loss: 0.5762 - val_accuracy: 0.7000\n",
      "Epoch 144/500\n",
      "23/23 [==============================] - 0s 929us/step - loss: 0.3958 - accuracy: 0.8222 - val_loss: 0.5777 - val_accuracy: 0.7000\n",
      "Epoch 145/500\n",
      "23/23 [==============================] - 0s 926us/step - loss: 0.4093 - accuracy: 0.8111 - val_loss: 0.5753 - val_accuracy: 0.7000\n",
      "Epoch 146/500\n",
      "23/23 [==============================] - 0s 932us/step - loss: 0.4094 - accuracy: 0.8222 - val_loss: 0.5750 - val_accuracy: 0.7000\n",
      "Epoch 147/500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.4391 - accuracy: 0.8111 - val_loss: 0.5709 - val_accuracy: 0.7167\n",
      "Epoch 148/500\n",
      "23/23 [==============================] - 0s 945us/step - loss: 0.4328 - accuracy: 0.8056 - val_loss: 0.5747 - val_accuracy: 0.7167\n",
      "Epoch 149/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3899 - accuracy: 0.8667 - val_loss: 0.5761 - val_accuracy: 0.7167\n",
      "Epoch 150/500\n",
      "23/23 [==============================] - 0s 903us/step - loss: 0.4325 - accuracy: 0.8167 - val_loss: 0.5776 - val_accuracy: 0.7167\n",
      "Epoch 151/500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.3955 - accuracy: 0.8389 - val_loss: 0.5766 - val_accuracy: 0.7167\n",
      "Epoch 152/500\n",
      "23/23 [==============================] - 0s 941us/step - loss: 0.3879 - accuracy: 0.8333 - val_loss: 0.5780 - val_accuracy: 0.7167\n",
      "Epoch 153/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.4073 - accuracy: 0.8278 - val_loss: 0.5765 - val_accuracy: 0.7167\n",
      "Epoch 154/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4201 - accuracy: 0.7889 - val_loss: 0.5744 - val_accuracy: 0.7167\n",
      "Epoch 155/500\n",
      "23/23 [==============================] - 0s 919us/step - loss: 0.4394 - accuracy: 0.7556 - val_loss: 0.5780 - val_accuracy: 0.7167\n",
      "Epoch 156/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4111 - accuracy: 0.8056 - val_loss: 0.5777 - val_accuracy: 0.7167\n",
      "Epoch 157/500\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.4207 - accuracy: 0.8333 - val_loss: 0.5751 - val_accuracy: 0.7167\n",
      "Epoch 158/500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.4172 - accuracy: 0.8333 - val_loss: 0.5776 - val_accuracy: 0.7167\n",
      "Epoch 159/500\n",
      "23/23 [==============================] - 0s 942us/step - loss: 0.4736 - accuracy: 0.7833 - val_loss: 0.5781 - val_accuracy: 0.7167\n",
      "Epoch 160/500\n",
      "23/23 [==============================] - 0s 959us/step - loss: 0.4027 - accuracy: 0.7944 - val_loss: 0.5790 - val_accuracy: 0.7167\n",
      "Epoch 161/500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.3782 - accuracy: 0.8333 - val_loss: 0.5810 - val_accuracy: 0.7167\n",
      "Epoch 162/500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.3774 - accuracy: 0.8111 - val_loss: 0.5806 - val_accuracy: 0.7167\n",
      "Epoch 163/500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.4126 - accuracy: 0.8333 - val_loss: 0.5773 - val_accuracy: 0.7167\n",
      "Epoch 164/500\n",
      "23/23 [==============================] - 0s 941us/step - loss: 0.3965 - accuracy: 0.8333 - val_loss: 0.5783 - val_accuracy: 0.7167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/500\n",
      "23/23 [==============================] - 0s 943us/step - loss: 0.3877 - accuracy: 0.8389 - val_loss: 0.5770 - val_accuracy: 0.7167\n",
      "Epoch 166/500\n",
      "23/23 [==============================] - 0s 919us/step - loss: 0.3958 - accuracy: 0.8222 - val_loss: 0.5752 - val_accuracy: 0.7167\n",
      "Epoch 167/500\n",
      "23/23 [==============================] - 0s 947us/step - loss: 0.4147 - accuracy: 0.8333 - val_loss: 0.5761 - val_accuracy: 0.7167\n",
      "Epoch 168/500\n",
      "23/23 [==============================] - 0s 948us/step - loss: 0.3839 - accuracy: 0.8333 - val_loss: 0.5806 - val_accuracy: 0.7167\n",
      "Epoch 169/500\n",
      "23/23 [==============================] - 0s 926us/step - loss: 0.3840 - accuracy: 0.8333 - val_loss: 0.5802 - val_accuracy: 0.7167\n",
      "Epoch 170/500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.3748 - accuracy: 0.8556 - val_loss: 0.5801 - val_accuracy: 0.7167\n",
      "Epoch 171/500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.3824 - accuracy: 0.8222 - val_loss: 0.5797 - val_accuracy: 0.7167\n",
      "Epoch 172/500\n",
      "23/23 [==============================] - 0s 960us/step - loss: 0.4109 - accuracy: 0.8111 - val_loss: 0.5803 - val_accuracy: 0.7167\n",
      "Epoch 173/500\n",
      "23/23 [==============================] - 0s 912us/step - loss: 0.4072 - accuracy: 0.8000 - val_loss: 0.5767 - val_accuracy: 0.7167\n",
      "Epoch 174/500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.3681 - accuracy: 0.8278 - val_loss: 0.5791 - val_accuracy: 0.7167\n",
      "Epoch 175/500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.3497 - accuracy: 0.8444 - val_loss: 0.5779 - val_accuracy: 0.7167\n",
      "Epoch 176/500\n",
      "23/23 [==============================] - 0s 949us/step - loss: 0.3815 - accuracy: 0.8278 - val_loss: 0.5761 - val_accuracy: 0.7167\n",
      "Epoch 177/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3936 - accuracy: 0.8167 - val_loss: 0.5765 - val_accuracy: 0.7167\n",
      "Epoch 178/500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.3825 - accuracy: 0.8167 - val_loss: 0.5859 - val_accuracy: 0.7167\n",
      "Epoch 179/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3913 - accuracy: 0.8222 - val_loss: 0.5832 - val_accuracy: 0.7167\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - 0s 947us/step - loss: 0.4035 - accuracy: 0.8167 - val_loss: 0.5831 - val_accuracy: 0.7167\n",
      "Epoch 181/500\n",
      "23/23 [==============================] - 0s 994us/step - loss: 0.3980 - accuracy: 0.8222 - val_loss: 0.5821 - val_accuracy: 0.7167\n",
      "Epoch 182/500\n",
      "23/23 [==============================] - 0s 916us/step - loss: 0.3877 - accuracy: 0.8500 - val_loss: 0.5853 - val_accuracy: 0.7167\n",
      "Epoch 183/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4059 - accuracy: 0.8000 - val_loss: 0.5879 - val_accuracy: 0.7167\n",
      "Epoch 184/500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.3934 - accuracy: 0.8389 - val_loss: 0.5862 - val_accuracy: 0.7167\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - 0s 944us/step - loss: 0.3570 - accuracy: 0.8667 - val_loss: 0.5885 - val_accuracy: 0.7167\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - 0s 928us/step - loss: 0.3723 - accuracy: 0.8111 - val_loss: 0.5929 - val_accuracy: 0.7000\n",
      "Epoch 187/500\n",
      "23/23 [==============================] - 0s 946us/step - loss: 0.4025 - accuracy: 0.8111 - val_loss: 0.5895 - val_accuracy: 0.7167\n",
      "Epoch 188/500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.3784 - accuracy: 0.8222 - val_loss: 0.5903 - val_accuracy: 0.7000\n",
      "Epoch 189/500\n",
      "23/23 [==============================] - 0s 940us/step - loss: 0.3654 - accuracy: 0.8444 - val_loss: 0.5900 - val_accuracy: 0.7000\n",
      "Epoch 190/500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.3631 - accuracy: 0.8222 - val_loss: 0.5914 - val_accuracy: 0.7000\n",
      "Epoch 191/500\n",
      "23/23 [==============================] - 0s 894us/step - loss: 0.3480 - accuracy: 0.8500 - val_loss: 0.5934 - val_accuracy: 0.7000\n",
      "Epoch 192/500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.3599 - accuracy: 0.8278 - val_loss: 0.5926 - val_accuracy: 0.7000\n",
      "Epoch 193/500\n",
      "23/23 [==============================] - 0s 926us/step - loss: 0.3637 - accuracy: 0.8222 - val_loss: 0.5915 - val_accuracy: 0.7000\n",
      "Epoch 194/500\n",
      "23/23 [==============================] - 0s 992us/step - loss: 0.3747 - accuracy: 0.8389 - val_loss: 0.5973 - val_accuracy: 0.7000\n",
      "Epoch 195/500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.3930 - accuracy: 0.8222 - val_loss: 0.5904 - val_accuracy: 0.7000\n",
      "Epoch 196/500\n",
      "23/23 [==============================] - 0s 919us/step - loss: 0.3554 - accuracy: 0.8278 - val_loss: 0.5887 - val_accuracy: 0.7000\n",
      "Epoch 197/500\n",
      "23/23 [==============================] - 0s 956us/step - loss: 0.3672 - accuracy: 0.8444 - val_loss: 0.5871 - val_accuracy: 0.7000\n",
      "Epoch 198/500\n",
      "23/23 [==============================] - 0s 923us/step - loss: 0.3637 - accuracy: 0.8444 - val_loss: 0.5910 - val_accuracy: 0.7000\n",
      "Epoch 199/500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.4043 - accuracy: 0.8111 - val_loss: 0.5989 - val_accuracy: 0.7000\n",
      "Epoch 200/500\n",
      "23/23 [==============================] - 0s 910us/step - loss: 0.3411 - accuracy: 0.8667 - val_loss: 0.6016 - val_accuracy: 0.7000\n",
      "Epoch 201/500\n",
      "23/23 [==============================] - 0s 945us/step - loss: 0.3396 - accuracy: 0.8778 - val_loss: 0.6018 - val_accuracy: 0.7000\n",
      "Epoch 202/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3779 - accuracy: 0.8333 - val_loss: 0.6025 - val_accuracy: 0.7000\n",
      "Epoch 203/500\n",
      "23/23 [==============================] - 0s 912us/step - loss: 0.3977 - accuracy: 0.8389 - val_loss: 0.5999 - val_accuracy: 0.7000\n",
      "Epoch 204/500\n",
      "23/23 [==============================] - 0s 926us/step - loss: 0.3719 - accuracy: 0.8389 - val_loss: 0.5975 - val_accuracy: 0.7000\n",
      "Epoch 205/500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.3655 - accuracy: 0.8167 - val_loss: 0.5999 - val_accuracy: 0.7000\n",
      "Epoch 206/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3822 - accuracy: 0.8444 - val_loss: 0.5985 - val_accuracy: 0.7000\n",
      "Epoch 207/500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.3804 - accuracy: 0.8278 - val_loss: 0.5983 - val_accuracy: 0.7000\n",
      "Epoch 208/500\n",
      "23/23 [==============================] - 0s 936us/step - loss: 0.3293 - accuracy: 0.8722 - val_loss: 0.5970 - val_accuracy: 0.7000\n",
      "Epoch 209/500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.3901 - accuracy: 0.8444 - val_loss: 0.5912 - val_accuracy: 0.7000\n",
      "Epoch 210/500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.3535 - accuracy: 0.8556 - val_loss: 0.5878 - val_accuracy: 0.7000\n",
      "Epoch 211/500\n",
      "23/23 [==============================] - 0s 992us/step - loss: 0.3435 - accuracy: 0.8667 - val_loss: 0.5831 - val_accuracy: 0.7000\n",
      "Epoch 212/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3797 - accuracy: 0.8278 - val_loss: 0.5858 - val_accuracy: 0.7000\n",
      "Epoch 213/500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.3624 - accuracy: 0.8389 - val_loss: 0.5863 - val_accuracy: 0.7000\n",
      "Epoch 214/500\n",
      "23/23 [==============================] - 0s 925us/step - loss: 0.3424 - accuracy: 0.8500 - val_loss: 0.5904 - val_accuracy: 0.7000\n",
      "Epoch 215/500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.3839 - accuracy: 0.8222 - val_loss: 0.5903 - val_accuracy: 0.7000\n",
      "Epoch 216/500\n",
      "23/23 [==============================] - 0s 937us/step - loss: 0.3535 - accuracy: 0.8278 - val_loss: 0.5893 - val_accuracy: 0.7000\n",
      "Epoch 217/500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.3809 - accuracy: 0.8556 - val_loss: 0.5869 - val_accuracy: 0.7000\n",
      "Epoch 218/500\n",
      "23/23 [==============================] - 0s 914us/step - loss: 0.3754 - accuracy: 0.7889 - val_loss: 0.5850 - val_accuracy: 0.7000\n",
      "Epoch 219/500\n",
      "23/23 [==============================] - 0s 951us/step - loss: 0.3265 - accuracy: 0.8556 - val_loss: 0.5860 - val_accuracy: 0.7000\n",
      "Epoch 220/500\n",
      "23/23 [==============================] - 0s 948us/step - loss: 0.3607 - accuracy: 0.8722 - val_loss: 0.5868 - val_accuracy: 0.7000\n",
      "Epoch 221/500\n",
      "23/23 [==============================] - 0s 943us/step - loss: 0.3437 - accuracy: 0.8444 - val_loss: 0.5895 - val_accuracy: 0.7000\n",
      "Epoch 222/500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.3414 - accuracy: 0.8611 - val_loss: 0.5916 - val_accuracy: 0.7000\n",
      "Epoch 223/500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.3510 - accuracy: 0.8444 - val_loss: 0.5970 - val_accuracy: 0.7000\n",
      "Epoch 224/500\n",
      "23/23 [==============================] - 0s 937us/step - loss: 0.3592 - accuracy: 0.8611 - val_loss: 0.5991 - val_accuracy: 0.7000\n",
      "Epoch 225/500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.3756 - accuracy: 0.8444 - val_loss: 0.6005 - val_accuracy: 0.7000\n",
      "Epoch 226/500\n",
      "23/23 [==============================] - 0s 991us/step - loss: 0.3814 - accuracy: 0.8278 - val_loss: 0.6006 - val_accuracy: 0.7000\n",
      "Epoch 227/500\n",
      "23/23 [==============================] - 0s 924us/step - loss: 0.3431 - accuracy: 0.8333 - val_loss: 0.5971 - val_accuracy: 0.7000\n",
      "Epoch 228/500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.3478 - accuracy: 0.8500 - val_loss: 0.5961 - val_accuracy: 0.7000\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.3064 - accuracy: 0.8722 - val_loss: 0.5962 - val_accuracy: 0.7000\n",
      "Epoch 230/500\n",
      "23/23 [==============================] - 0s 910us/step - loss: 0.3146 - accuracy: 0.8611 - val_loss: 0.5974 - val_accuracy: 0.7000\n",
      "Epoch 231/500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.3213 - accuracy: 0.8611 - val_loss: 0.5919 - val_accuracy: 0.7000\n",
      "Epoch 232/500\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.3413 - accuracy: 0.8611 - val_loss: 0.5897 - val_accuracy: 0.7000\n",
      "Epoch 233/500\n",
      "23/23 [==============================] - 0s 937us/step - loss: 0.3350 - accuracy: 0.8611 - val_loss: 0.5842 - val_accuracy: 0.7167\n",
      "Epoch 234/500\n",
      "23/23 [==============================] - 0s 954us/step - loss: 0.3402 - accuracy: 0.8833 - val_loss: 0.5918 - val_accuracy: 0.7000\n",
      "Epoch 235/500\n",
      "23/23 [==============================] - 0s 906us/step - loss: 0.2933 - accuracy: 0.8778 - val_loss: 0.5922 - val_accuracy: 0.7000\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3799 - accuracy: 0.8500 - val_loss: 0.5949 - val_accuracy: 0.7000\n",
      "Epoch 237/500\n",
      "23/23 [==============================] - 0s 948us/step - loss: 0.3477 - accuracy: 0.8556 - val_loss: 0.5912 - val_accuracy: 0.7000\n",
      "Epoch 238/500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.3560 - accuracy: 0.8333 - val_loss: 0.5889 - val_accuracy: 0.7000\n",
      "Epoch 239/500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.3406 - accuracy: 0.8556 - val_loss: 0.5854 - val_accuracy: 0.7000\n",
      "Epoch 240/500\n",
      "23/23 [==============================] - 0s 968us/step - loss: 0.3482 - accuracy: 0.8556 - val_loss: 0.5860 - val_accuracy: 0.7000\n",
      "Epoch 241/500\n",
      "23/23 [==============================] - 0s 951us/step - loss: 0.3873 - accuracy: 0.8222 - val_loss: 0.5838 - val_accuracy: 0.7000\n",
      "Epoch 242/500\n",
      "23/23 [==============================] - 0s 883us/step - loss: 0.3685 - accuracy: 0.8500 - val_loss: 0.5813 - val_accuracy: 0.7000\n",
      "Epoch 243/500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.3278 - accuracy: 0.8722 - val_loss: 0.5833 - val_accuracy: 0.7000\n",
      "Epoch 244/500\n",
      "23/23 [==============================] - 0s 930us/step - loss: 0.3215 - accuracy: 0.8611 - val_loss: 0.5837 - val_accuracy: 0.7000\n",
      "Epoch 245/500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.3573 - accuracy: 0.8500 - val_loss: 0.5844 - val_accuracy: 0.7000\n",
      "Epoch 246/500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.3168 - accuracy: 0.8611 - val_loss: 0.5848 - val_accuracy: 0.7000\n",
      "Epoch 247/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3531 - accuracy: 0.8556 - val_loss: 0.5848 - val_accuracy: 0.7000\n",
      "Epoch 248/500\n",
      "23/23 [==============================] - 0s 951us/step - loss: 0.3178 - accuracy: 0.8611 - val_loss: 0.5836 - val_accuracy: 0.7000\n",
      "Epoch 249/500\n",
      "23/23 [==============================] - 0s 932us/step - loss: 0.3437 - accuracy: 0.8333 - val_loss: 0.5831 - val_accuracy: 0.7000\n",
      "Epoch 250/500\n",
      "23/23 [==============================] - 0s 975us/step - loss: 0.3381 - accuracy: 0.8556 - val_loss: 0.5844 - val_accuracy: 0.7000\n",
      "Epoch 251/500\n",
      "23/23 [==============================] - 0s 923us/step - loss: 0.3599 - accuracy: 0.8222 - val_loss: 0.5884 - val_accuracy: 0.7000\n",
      "Epoch 252/500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.3122 - accuracy: 0.8778 - val_loss: 0.5840 - val_accuracy: 0.7000\n",
      "Epoch 253/500\n",
      "23/23 [==============================] - 0s 919us/step - loss: 0.3330 - accuracy: 0.8500 - val_loss: 0.5854 - val_accuracy: 0.7000\n",
      "Epoch 254/500\n",
      "23/23 [==============================] - 0s 926us/step - loss: 0.3256 - accuracy: 0.8611 - val_loss: 0.5827 - val_accuracy: 0.7000\n",
      "Epoch 255/500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.3527 - accuracy: 0.8333 - val_loss: 0.5868 - val_accuracy: 0.7000\n",
      "Epoch 256/500\n",
      "23/23 [==============================] - 0s 903us/step - loss: 0.3241 - accuracy: 0.8722 - val_loss: 0.5868 - val_accuracy: 0.7000\n",
      "Epoch 257/500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.3254 - accuracy: 0.8500 - val_loss: 0.5845 - val_accuracy: 0.7000\n",
      "Epoch 258/500\n",
      "23/23 [==============================] - 0s 934us/step - loss: 0.3388 - accuracy: 0.8722 - val_loss: 0.5837 - val_accuracy: 0.7000\n",
      "Epoch 259/500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.3030 - accuracy: 0.8944 - val_loss: 0.5782 - val_accuracy: 0.7000\n",
      "Epoch 260/500\n",
      "23/23 [==============================] - 0s 916us/step - loss: 0.3368 - accuracy: 0.8333 - val_loss: 0.5770 - val_accuracy: 0.7000\n",
      "Epoch 261/500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.3063 - accuracy: 0.8722 - val_loss: 0.5826 - val_accuracy: 0.7000\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3454 - accuracy: 0.8778 - val_loss: 0.5869 - val_accuracy: 0.7000\n",
      "Epoch 263/500\n",
      "23/23 [==============================] - 0s 933us/step - loss: 0.3250 - accuracy: 0.8667 - val_loss: 0.5888 - val_accuracy: 0.7000\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.3489 - accuracy: 0.8556 - val_loss: 0.5879 - val_accuracy: 0.7000\n",
      "Epoch 265/500\n",
      "23/23 [==============================] - 0s 924us/step - loss: 0.3128 - accuracy: 0.8611 - val_loss: 0.5900 - val_accuracy: 0.7000\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3345 - accuracy: 0.8778 - val_loss: 0.5889 - val_accuracy: 0.7000\n",
      "Epoch 267/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3376 - accuracy: 0.8389 - val_loss: 0.5878 - val_accuracy: 0.7000\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.3194 - accuracy: 0.8667 - val_loss: 0.5941 - val_accuracy: 0.7000\n",
      "Epoch 269/500\n",
      "23/23 [==============================] - 0s 916us/step - loss: 0.3475 - accuracy: 0.8444 - val_loss: 0.5957 - val_accuracy: 0.7000\n",
      "Epoch 270/500\n",
      "23/23 [==============================] - 0s 933us/step - loss: 0.2994 - accuracy: 0.8722 - val_loss: 0.5981 - val_accuracy: 0.7000\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - 0s 948us/step - loss: 0.3374 - accuracy: 0.8333 - val_loss: 0.5970 - val_accuracy: 0.7000\n",
      "Epoch 272/500\n",
      "23/23 [==============================] - 0s 939us/step - loss: 0.3581 - accuracy: 0.8722 - val_loss: 0.5981 - val_accuracy: 0.7000\n",
      "Epoch 273/500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.3320 - accuracy: 0.8500 - val_loss: 0.6010 - val_accuracy: 0.7000\n",
      "Epoch 274/500\n",
      "23/23 [==============================] - 0s 912us/step - loss: 0.3040 - accuracy: 0.8611 - val_loss: 0.6026 - val_accuracy: 0.7000\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.3212 - accuracy: 0.8722 - val_loss: 0.5957 - val_accuracy: 0.7000\n",
      "Epoch 276/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3189 - accuracy: 0.8944 - val_loss: 0.5941 - val_accuracy: 0.7000\n",
      "Epoch 277/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 915us/step - loss: 0.3202 - accuracy: 0.8500 - val_loss: 0.5975 - val_accuracy: 0.7000\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3551 - accuracy: 0.8667 - val_loss: 0.5996 - val_accuracy: 0.7000\n",
      "Epoch 279/500\n",
      "23/23 [==============================] - 0s 956us/step - loss: 0.3472 - accuracy: 0.8278 - val_loss: 0.5981 - val_accuracy: 0.7000\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3298 - accuracy: 0.8667 - val_loss: 0.6034 - val_accuracy: 0.7000\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - 0s 922us/step - loss: 0.3082 - accuracy: 0.8722 - val_loss: 0.6032 - val_accuracy: 0.7000\n",
      "Epoch 282/500\n",
      "23/23 [==============================] - 0s 941us/step - loss: 0.2815 - accuracy: 0.8889 - val_loss: 0.6023 - val_accuracy: 0.7000\n",
      "Epoch 283/500\n",
      "23/23 [==============================] - 0s 935us/step - loss: 0.2887 - accuracy: 0.8833 - val_loss: 0.5972 - val_accuracy: 0.7000\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - 0s 926us/step - loss: 0.3146 - accuracy: 0.8889 - val_loss: 0.5997 - val_accuracy: 0.7000\n",
      "Epoch 285/500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.2881 - accuracy: 0.8833 - val_loss: 0.5981 - val_accuracy: 0.7000\n",
      "Epoch 286/500\n",
      "23/23 [==============================] - 0s 925us/step - loss: 0.3184 - accuracy: 0.8667 - val_loss: 0.6022 - val_accuracy: 0.7000\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - 0s 956us/step - loss: 0.2988 - accuracy: 0.8944 - val_loss: 0.6009 - val_accuracy: 0.7000\n",
      "Epoch 288/500\n",
      "23/23 [==============================] - 0s 913us/step - loss: 0.3334 - accuracy: 0.8722 - val_loss: 0.5987 - val_accuracy: 0.7000\n",
      "Epoch 289/500\n",
      "23/23 [==============================] - 0s 923us/step - loss: 0.2798 - accuracy: 0.9056 - val_loss: 0.6033 - val_accuracy: 0.7000\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.3110 - accuracy: 0.8444 - val_loss: 0.6009 - val_accuracy: 0.7000\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - 0s 924us/step - loss: 0.3243 - accuracy: 0.8722 - val_loss: 0.6063 - val_accuracy: 0.7000\n",
      "Epoch 292/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3316 - accuracy: 0.8556 - val_loss: 0.6085 - val_accuracy: 0.7000\n",
      "Epoch 293/500\n",
      "23/23 [==============================] - 0s 947us/step - loss: 0.3089 - accuracy: 0.8889 - val_loss: 0.6149 - val_accuracy: 0.7000\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.3066 - accuracy: 0.8889 - val_loss: 0.6119 - val_accuracy: 0.7000\n",
      "Epoch 295/500\n",
      "23/23 [==============================] - 0s 921us/step - loss: 0.2818 - accuracy: 0.8667 - val_loss: 0.6100 - val_accuracy: 0.7000\n",
      "Epoch 296/500\n",
      "23/23 [==============================] - 0s 914us/step - loss: 0.3353 - accuracy: 0.8722 - val_loss: 0.6053 - val_accuracy: 0.7000\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - 0s 946us/step - loss: 0.3081 - accuracy: 0.8944 - val_loss: 0.6109 - val_accuracy: 0.7000\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - 0s 928us/step - loss: 0.3219 - accuracy: 0.8722 - val_loss: 0.6166 - val_accuracy: 0.7000\n",
      "Epoch 299/500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.3498 - accuracy: 0.8500 - val_loss: 0.6105 - val_accuracy: 0.7000\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - 0s 904us/step - loss: 0.3011 - accuracy: 0.8667 - val_loss: 0.6056 - val_accuracy: 0.7000\n",
      "Epoch 301/500\n",
      "23/23 [==============================] - 0s 918us/step - loss: 0.2999 - accuracy: 0.8889 - val_loss: 0.6064 - val_accuracy: 0.7000\n",
      "Epoch 302/500\n",
      "23/23 [==============================] - 0s 965us/step - loss: 0.3197 - accuracy: 0.8778 - val_loss: 0.6057 - val_accuracy: 0.7000\n",
      "Epoch 303/500\n",
      "23/23 [==============================] - 0s 933us/step - loss: 0.2796 - accuracy: 0.8944 - val_loss: 0.6103 - val_accuracy: 0.7000\n",
      "Epoch 304/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3784 - accuracy: 0.8556 - val_loss: 0.6139 - val_accuracy: 0.7000\n",
      "Epoch 305/500\n",
      "23/23 [==============================] - 0s 919us/step - loss: 0.3055 - accuracy: 0.8833 - val_loss: 0.6137 - val_accuracy: 0.7000\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.3283 - accuracy: 0.8667 - val_loss: 0.6130 - val_accuracy: 0.7000\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - 0s 922us/step - loss: 0.2736 - accuracy: 0.8944 - val_loss: 0.6143 - val_accuracy: 0.7000\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.3180 - accuracy: 0.8667 - val_loss: 0.6154 - val_accuracy: 0.7000\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.3072 - accuracy: 0.8722 - val_loss: 0.6142 - val_accuracy: 0.7000\n",
      "Epoch 310/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3104 - accuracy: 0.8889 - val_loss: 0.6142 - val_accuracy: 0.7000\n",
      "Epoch 311/500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.2677 - accuracy: 0.8944 - val_loss: 0.6135 - val_accuracy: 0.7000\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3464 - accuracy: 0.8278 - val_loss: 0.6086 - val_accuracy: 0.6833\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - 0s 939us/step - loss: 0.3258 - accuracy: 0.8889 - val_loss: 0.6092 - val_accuracy: 0.6833\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.3165 - accuracy: 0.8889 - val_loss: 0.6121 - val_accuracy: 0.6833\n",
      "Epoch 315/500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.2764 - accuracy: 0.8833 - val_loss: 0.6132 - val_accuracy: 0.7000\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.2913 - accuracy: 0.8556 - val_loss: 0.6096 - val_accuracy: 0.7000\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.3238 - accuracy: 0.8611 - val_loss: 0.6123 - val_accuracy: 0.7000\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - 0s 942us/step - loss: 0.3215 - accuracy: 0.8778 - val_loss: 0.6161 - val_accuracy: 0.7000\n",
      "Epoch 319/500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.3063 - accuracy: 0.8778 - val_loss: 0.6192 - val_accuracy: 0.7000\n",
      "Epoch 320/500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.3103 - accuracy: 0.8556 - val_loss: 0.6170 - val_accuracy: 0.7000\n",
      "Epoch 321/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2853 - accuracy: 0.9000 - val_loss: 0.6153 - val_accuracy: 0.7000\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - 0s 923us/step - loss: 0.3296 - accuracy: 0.8611 - val_loss: 0.6029 - val_accuracy: 0.7000\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - 0s 925us/step - loss: 0.2946 - accuracy: 0.8889 - val_loss: 0.6078 - val_accuracy: 0.7000\n",
      "Epoch 324/500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.3442 - accuracy: 0.8500 - val_loss: 0.6057 - val_accuracy: 0.7000\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.2814 - accuracy: 0.8833 - val_loss: 0.6043 - val_accuracy: 0.7000\n",
      "Epoch 326/500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.3007 - accuracy: 0.8556 - val_loss: 0.6044 - val_accuracy: 0.7000\n",
      "Epoch 327/500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.2628 - accuracy: 0.8778 - val_loss: 0.6063 - val_accuracy: 0.7000\n",
      "Epoch 328/500\n",
      "23/23 [==============================] - 0s 994us/step - loss: 0.3088 - accuracy: 0.8778 - val_loss: 0.6064 - val_accuracy: 0.7000\n",
      "Epoch 329/500\n",
      "23/23 [==============================] - 0s 914us/step - loss: 0.3081 - accuracy: 0.8500 - val_loss: 0.6040 - val_accuracy: 0.7000\n",
      "Epoch 330/500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.3143 - accuracy: 0.8611 - val_loss: 0.6039 - val_accuracy: 0.7000\n",
      "Epoch 331/500\n",
      "23/23 [==============================] - 0s 906us/step - loss: 0.3367 - accuracy: 0.8778 - val_loss: 0.6051 - val_accuracy: 0.7000\n",
      "Epoch 332/500\n",
      "23/23 [==============================] - 0s 910us/step - loss: 0.3223 - accuracy: 0.8556 - val_loss: 0.6029 - val_accuracy: 0.7000\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - 0s 945us/step - loss: 0.3052 - accuracy: 0.8722 - val_loss: 0.6074 - val_accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 334/500\n",
      "23/23 [==============================] - 0s 918us/step - loss: 0.2834 - accuracy: 0.9000 - val_loss: 0.6054 - val_accuracy: 0.7000\n",
      "Epoch 335/500\n",
      "23/23 [==============================] - 0s 936us/step - loss: 0.3137 - accuracy: 0.8556 - val_loss: 0.6061 - val_accuracy: 0.7000\n",
      "Epoch 336/500\n",
      "23/23 [==============================] - 0s 911us/step - loss: 0.2809 - accuracy: 0.8722 - val_loss: 0.6026 - val_accuracy: 0.7000\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - 0s 898us/step - loss: 0.2736 - accuracy: 0.9000 - val_loss: 0.6024 - val_accuracy: 0.7000\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.2832 - accuracy: 0.8778 - val_loss: 0.6002 - val_accuracy: 0.7000\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - 0s 945us/step - loss: 0.3160 - accuracy: 0.8556 - val_loss: 0.5992 - val_accuracy: 0.7000\n",
      "Epoch 340/500\n",
      "23/23 [==============================] - 0s 916us/step - loss: 0.2800 - accuracy: 0.9111 - val_loss: 0.5997 - val_accuracy: 0.7000\n",
      "Epoch 341/500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.2932 - accuracy: 0.8722 - val_loss: 0.5990 - val_accuracy: 0.7000\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - 0s 921us/step - loss: 0.2674 - accuracy: 0.8944 - val_loss: 0.5944 - val_accuracy: 0.7000\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.2766 - accuracy: 0.9000 - val_loss: 0.5997 - val_accuracy: 0.7000\n",
      "Epoch 344/500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.3078 - accuracy: 0.8667 - val_loss: 0.5947 - val_accuracy: 0.7000\n",
      "Epoch 345/500\n",
      "23/23 [==============================] - 0s 950us/step - loss: 0.2849 - accuracy: 0.8833 - val_loss: 0.6010 - val_accuracy: 0.7000\n",
      "Epoch 346/500\n",
      "23/23 [==============================] - 0s 941us/step - loss: 0.2457 - accuracy: 0.9222 - val_loss: 0.6036 - val_accuracy: 0.7000\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.2648 - accuracy: 0.8944 - val_loss: 0.6103 - val_accuracy: 0.7000\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - 0s 936us/step - loss: 0.3036 - accuracy: 0.8889 - val_loss: 0.6050 - val_accuracy: 0.7000\n",
      "Epoch 349/500\n",
      "23/23 [==============================] - 0s 951us/step - loss: 0.2589 - accuracy: 0.9000 - val_loss: 0.6088 - val_accuracy: 0.7000\n",
      "Epoch 350/500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.2755 - accuracy: 0.8833 - val_loss: 0.6031 - val_accuracy: 0.7000\n",
      "Epoch 351/500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.2615 - accuracy: 0.8889 - val_loss: 0.6046 - val_accuracy: 0.7000\n",
      "Epoch 352/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3041 - accuracy: 0.8889 - val_loss: 0.6085 - val_accuracy: 0.7000\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.2751 - accuracy: 0.8722 - val_loss: 0.6046 - val_accuracy: 0.7000\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2852 - accuracy: 0.8889 - val_loss: 0.6014 - val_accuracy: 0.7000\n",
      "Epoch 355/500\n",
      "23/23 [==============================] - 0s 929us/step - loss: 0.3192 - accuracy: 0.8667 - val_loss: 0.5981 - val_accuracy: 0.7000\n",
      "Epoch 356/500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.3003 - accuracy: 0.8722 - val_loss: 0.5974 - val_accuracy: 0.7000\n",
      "Epoch 357/500\n",
      "23/23 [==============================] - 0s 960us/step - loss: 0.3001 - accuracy: 0.8778 - val_loss: 0.6081 - val_accuracy: 0.7000\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.3151 - accuracy: 0.8722 - val_loss: 0.6036 - val_accuracy: 0.7000\n",
      "Epoch 359/500\n",
      "23/23 [==============================] - 0s 906us/step - loss: 0.2925 - accuracy: 0.8667 - val_loss: 0.6076 - val_accuracy: 0.7000\n",
      "Epoch 360/500\n",
      "23/23 [==============================] - 0s 918us/step - loss: 0.2985 - accuracy: 0.9056 - val_loss: 0.6082 - val_accuracy: 0.7000\n",
      "Epoch 361/500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.2692 - accuracy: 0.9056 - val_loss: 0.6002 - val_accuracy: 0.7000\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - 0s 899us/step - loss: 0.2958 - accuracy: 0.8778 - val_loss: 0.5960 - val_accuracy: 0.7000\n",
      "Epoch 363/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2775 - accuracy: 0.8944 - val_loss: 0.5995 - val_accuracy: 0.7000\n",
      "Epoch 364/500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.2722 - accuracy: 0.8778 - val_loss: 0.6091 - val_accuracy: 0.7000\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.2710 - accuracy: 0.8889 - val_loss: 0.6117 - val_accuracy: 0.7000\n",
      "Epoch 366/500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.2834 - accuracy: 0.8722 - val_loss: 0.6047 - val_accuracy: 0.7000\n",
      "Epoch 367/500\n",
      "23/23 [==============================] - 0s 946us/step - loss: 0.2847 - accuracy: 0.8667 - val_loss: 0.6112 - val_accuracy: 0.6833\n",
      "Epoch 368/500\n",
      "23/23 [==============================] - 0s 935us/step - loss: 0.2914 - accuracy: 0.8667 - val_loss: 0.6136 - val_accuracy: 0.6833\n",
      "Epoch 369/500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.2689 - accuracy: 0.9222 - val_loss: 0.6097 - val_accuracy: 0.6833\n",
      "Epoch 370/500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.2930 - accuracy: 0.8778 - val_loss: 0.6078 - val_accuracy: 0.6833\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - 0s 940us/step - loss: 0.2896 - accuracy: 0.8556 - val_loss: 0.6022 - val_accuracy: 0.6833\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - 0s 919us/step - loss: 0.2833 - accuracy: 0.8833 - val_loss: 0.6097 - val_accuracy: 0.6833\n",
      "Epoch 373/500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.2841 - accuracy: 0.8833 - val_loss: 0.6072 - val_accuracy: 0.6833\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - 0s 903us/step - loss: 0.2585 - accuracy: 0.8944 - val_loss: 0.5981 - val_accuracy: 0.6833\n",
      "Epoch 375/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2844 - accuracy: 0.8778 - val_loss: 0.5982 - val_accuracy: 0.6833\n",
      "Epoch 376/500\n",
      "23/23 [==============================] - 0s 940us/step - loss: 0.3014 - accuracy: 0.8722 - val_loss: 0.6063 - val_accuracy: 0.6833\n",
      "Epoch 377/500\n",
      "23/23 [==============================] - 0s 938us/step - loss: 0.2651 - accuracy: 0.8889 - val_loss: 0.6042 - val_accuracy: 0.6833\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - 0s 934us/step - loss: 0.2785 - accuracy: 0.8778 - val_loss: 0.6043 - val_accuracy: 0.6833\n",
      "Epoch 379/500\n",
      "23/23 [==============================] - 0s 927us/step - loss: 0.3091 - accuracy: 0.8889 - val_loss: 0.6055 - val_accuracy: 0.7000\n",
      "Epoch 380/500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.2505 - accuracy: 0.9222 - val_loss: 0.6073 - val_accuracy: 0.7000\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2739 - accuracy: 0.8833 - val_loss: 0.6110 - val_accuracy: 0.7000\n",
      "Epoch 382/500\n",
      "23/23 [==============================] - 0s 919us/step - loss: 0.2775 - accuracy: 0.8944 - val_loss: 0.6135 - val_accuracy: 0.7000\n",
      "Epoch 383/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2374 - accuracy: 0.8833 - val_loss: 0.6196 - val_accuracy: 0.7000\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - 0s 943us/step - loss: 0.2900 - accuracy: 0.8889 - val_loss: 0.6165 - val_accuracy: 0.7000\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.3147 - accuracy: 0.8667 - val_loss: 0.6166 - val_accuracy: 0.7000\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - 0s 954us/step - loss: 0.2778 - accuracy: 0.8889 - val_loss: 0.6121 - val_accuracy: 0.7000\n",
      "Epoch 387/500\n",
      "23/23 [==============================] - 0s 936us/step - loss: 0.2638 - accuracy: 0.9056 - val_loss: 0.6130 - val_accuracy: 0.6833\n",
      "Epoch 388/500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.2538 - accuracy: 0.9056 - val_loss: 0.6100 - val_accuracy: 0.7000\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - 0s 921us/step - loss: 0.2597 - accuracy: 0.9000 - val_loss: 0.6137 - val_accuracy: 0.7000\n",
      "Epoch 390/500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.2893 - accuracy: 0.8778 - val_loss: 0.6185 - val_accuracy: 0.7000\n",
      "Epoch 391/500\n",
      "23/23 [==============================] - 0s 940us/step - loss: 0.2521 - accuracy: 0.9056 - val_loss: 0.6145 - val_accuracy: 0.7000\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.2754 - accuracy: 0.8833 - val_loss: 0.6231 - val_accuracy: 0.7000\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - 0s 937us/step - loss: 0.2494 - accuracy: 0.8778 - val_loss: 0.6207 - val_accuracy: 0.7000\n",
      "Epoch 394/500\n",
      "23/23 [==============================] - 0s 921us/step - loss: 0.2919 - accuracy: 0.8833 - val_loss: 0.6156 - val_accuracy: 0.7000\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - 0s 916us/step - loss: 0.2553 - accuracy: 0.9056 - val_loss: 0.6170 - val_accuracy: 0.7000\n",
      "Epoch 396/500\n",
      "23/23 [==============================] - 0s 938us/step - loss: 0.2670 - accuracy: 0.8833 - val_loss: 0.6170 - val_accuracy: 0.7000\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - 0s 877us/step - loss: 0.2899 - accuracy: 0.8722 - val_loss: 0.6158 - val_accuracy: 0.7000\n",
      "Epoch 398/500\n",
      "23/23 [==============================] - 0s 910us/step - loss: 0.2836 - accuracy: 0.9000 - val_loss: 0.6188 - val_accuracy: 0.7000\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.2932 - accuracy: 0.8500 - val_loss: 0.6165 - val_accuracy: 0.7000\n",
      "Epoch 400/500\n",
      "23/23 [==============================] - 0s 944us/step - loss: 0.2838 - accuracy: 0.8889 - val_loss: 0.6193 - val_accuracy: 0.7000\n",
      "Epoch 401/500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.2675 - accuracy: 0.8944 - val_loss: 0.6118 - val_accuracy: 0.7000\n",
      "Epoch 402/500\n",
      "23/23 [==============================] - 0s 906us/step - loss: 0.2879 - accuracy: 0.8611 - val_loss: 0.6112 - val_accuracy: 0.7000\n",
      "Epoch 403/500\n",
      "23/23 [==============================] - 0s 886us/step - loss: 0.2499 - accuracy: 0.9278 - val_loss: 0.6072 - val_accuracy: 0.7000\n",
      "Epoch 404/500\n",
      "23/23 [==============================] - 0s 920us/step - loss: 0.2981 - accuracy: 0.8556 - val_loss: 0.6071 - val_accuracy: 0.7000\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.2424 - accuracy: 0.9222 - val_loss: 0.6167 - val_accuracy: 0.7000\n",
      "Epoch 406/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2787 - accuracy: 0.8667 - val_loss: 0.6194 - val_accuracy: 0.6833\n",
      "Epoch 407/500\n",
      "23/23 [==============================] - 0s 936us/step - loss: 0.2656 - accuracy: 0.9056 - val_loss: 0.6222 - val_accuracy: 0.6833\n",
      "Epoch 408/500\n",
      "23/23 [==============================] - 0s 951us/step - loss: 0.2466 - accuracy: 0.9167 - val_loss: 0.6179 - val_accuracy: 0.6833\n",
      "Epoch 409/500\n",
      "23/23 [==============================] - 0s 970us/step - loss: 0.2623 - accuracy: 0.8889 - val_loss: 0.6199 - val_accuracy: 0.6833\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.2946 - accuracy: 0.8722 - val_loss: 0.6247 - val_accuracy: 0.6833\n",
      "Epoch 411/500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.2517 - accuracy: 0.9167 - val_loss: 0.6282 - val_accuracy: 0.6833\n",
      "Epoch 412/500\n",
      "23/23 [==============================] - 0s 926us/step - loss: 0.2604 - accuracy: 0.8833 - val_loss: 0.6205 - val_accuracy: 0.6833\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - 0s 925us/step - loss: 0.2479 - accuracy: 0.9167 - val_loss: 0.6216 - val_accuracy: 0.6833\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - 0s 934us/step - loss: 0.2715 - accuracy: 0.9000 - val_loss: 0.6212 - val_accuracy: 0.6833\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - 0s 932us/step - loss: 0.3216 - accuracy: 0.8778 - val_loss: 0.6178 - val_accuracy: 0.7000\n",
      "Epoch 416/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2432 - accuracy: 0.9167 - val_loss: 0.6131 - val_accuracy: 0.7000\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - 0s 920us/step - loss: 0.2781 - accuracy: 0.8833 - val_loss: 0.6028 - val_accuracy: 0.7000\n",
      "Epoch 418/500\n",
      "23/23 [==============================] - 0s 922us/step - loss: 0.2825 - accuracy: 0.8722 - val_loss: 0.6013 - val_accuracy: 0.7000\n",
      "Epoch 419/500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.2466 - accuracy: 0.8889 - val_loss: 0.6096 - val_accuracy: 0.7000\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.2948 - accuracy: 0.8722 - val_loss: 0.6195 - val_accuracy: 0.7000\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.2870 - accuracy: 0.8889 - val_loss: 0.6143 - val_accuracy: 0.7000\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.2649 - accuracy: 0.8833 - val_loss: 0.6139 - val_accuracy: 0.7000\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - 0s 919us/step - loss: 0.2826 - accuracy: 0.8833 - val_loss: 0.6155 - val_accuracy: 0.7000\n",
      "Epoch 424/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2431 - accuracy: 0.9111 - val_loss: 0.6173 - val_accuracy: 0.7000\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.2733 - accuracy: 0.9000 - val_loss: 0.6152 - val_accuracy: 0.7000\n",
      "Epoch 426/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2890 - accuracy: 0.8889 - val_loss: 0.6193 - val_accuracy: 0.7000\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.2614 - accuracy: 0.8833 - val_loss: 0.6216 - val_accuracy: 0.7000\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.2723 - accuracy: 0.9000 - val_loss: 0.6182 - val_accuracy: 0.7000\n",
      "Epoch 429/500\n",
      "23/23 [==============================] - 0s 895us/step - loss: 0.2506 - accuracy: 0.9056 - val_loss: 0.6205 - val_accuracy: 0.7000\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2674 - accuracy: 0.9056 - val_loss: 0.6211 - val_accuracy: 0.7000\n",
      "Epoch 431/500\n",
      "23/23 [==============================] - 0s 918us/step - loss: 0.2414 - accuracy: 0.9222 - val_loss: 0.6218 - val_accuracy: 0.6833\n",
      "Epoch 432/500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.2199 - accuracy: 0.9333 - val_loss: 0.6161 - val_accuracy: 0.7000\n",
      "Epoch 433/500\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.3082 - accuracy: 0.8889 - val_loss: 0.6168 - val_accuracy: 0.7000\n",
      "Epoch 434/500\n",
      "23/23 [==============================] - 0s 932us/step - loss: 0.2398 - accuracy: 0.9000 - val_loss: 0.6170 - val_accuracy: 0.7000\n",
      "Epoch 435/500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.2769 - accuracy: 0.9000 - val_loss: 0.6138 - val_accuracy: 0.7000\n",
      "Epoch 436/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.2430 - accuracy: 0.8889 - val_loss: 0.6169 - val_accuracy: 0.7000\n",
      "Epoch 437/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2245 - accuracy: 0.9278 - val_loss: 0.6196 - val_accuracy: 0.7000\n",
      "Epoch 438/500\n",
      "23/23 [==============================] - 0s 942us/step - loss: 0.2656 - accuracy: 0.9111 - val_loss: 0.6194 - val_accuracy: 0.7000\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - 0s 947us/step - loss: 0.2845 - accuracy: 0.8944 - val_loss: 0.6191 - val_accuracy: 0.7000\n",
      "Epoch 440/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2376 - accuracy: 0.9222 - val_loss: 0.6213 - val_accuracy: 0.7000\n",
      "Epoch 441/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2824 - accuracy: 0.9056 - val_loss: 0.6200 - val_accuracy: 0.7000\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - 0s 921us/step - loss: 0.2938 - accuracy: 0.8667 - val_loss: 0.6213 - val_accuracy: 0.7000\n",
      "Epoch 443/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2555 - accuracy: 0.8889 - val_loss: 0.6171 - val_accuracy: 0.7000\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - 0s 944us/step - loss: 0.2629 - accuracy: 0.9056 - val_loss: 0.6200 - val_accuracy: 0.7000\n",
      "Epoch 445/500\n",
      "23/23 [==============================] - 0s 895us/step - loss: 0.2316 - accuracy: 0.9167 - val_loss: 0.6224 - val_accuracy: 0.7000\n",
      "Epoch 446/500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.2481 - accuracy: 0.9111 - val_loss: 0.6206 - val_accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 447/500\n",
      "23/23 [==============================] - 0s 949us/step - loss: 0.2823 - accuracy: 0.8889 - val_loss: 0.6245 - val_accuracy: 0.7000\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - 0s 936us/step - loss: 0.2636 - accuracy: 0.9111 - val_loss: 0.6308 - val_accuracy: 0.7000\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - 0s 921us/step - loss: 0.2413 - accuracy: 0.9000 - val_loss: 0.6308 - val_accuracy: 0.7000\n",
      "Epoch 450/500\n",
      "23/23 [==============================] - 0s 951us/step - loss: 0.2657 - accuracy: 0.8833 - val_loss: 0.6356 - val_accuracy: 0.7000\n",
      "Epoch 451/500\n",
      "23/23 [==============================] - 0s 918us/step - loss: 0.2516 - accuracy: 0.9000 - val_loss: 0.6410 - val_accuracy: 0.6833\n",
      "Epoch 452/500\n",
      "23/23 [==============================] - 0s 944us/step - loss: 0.2270 - accuracy: 0.9333 - val_loss: 0.6513 - val_accuracy: 0.6833\n",
      "Epoch 453/500\n",
      "23/23 [==============================] - 0s 970us/step - loss: 0.2713 - accuracy: 0.8944 - val_loss: 0.6519 - val_accuracy: 0.6833\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - 0s 915us/step - loss: 0.2824 - accuracy: 0.8778 - val_loss: 0.6443 - val_accuracy: 0.6833\n",
      "Epoch 455/500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.2511 - accuracy: 0.9000 - val_loss: 0.6460 - val_accuracy: 0.6833\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - 0s 948us/step - loss: 0.2212 - accuracy: 0.9278 - val_loss: 0.6465 - val_accuracy: 0.6833\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - 0s 914us/step - loss: 0.2470 - accuracy: 0.9056 - val_loss: 0.6405 - val_accuracy: 0.6833\n",
      "Epoch 458/500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.2358 - accuracy: 0.9167 - val_loss: 0.6521 - val_accuracy: 0.6833\n",
      "Epoch 459/500\n",
      "23/23 [==============================] - 0s 937us/step - loss: 0.2506 - accuracy: 0.8722 - val_loss: 0.6527 - val_accuracy: 0.6833\n",
      "Epoch 460/500\n",
      "23/23 [==============================] - 0s 939us/step - loss: 0.2379 - accuracy: 0.9111 - val_loss: 0.6501 - val_accuracy: 0.6833\n",
      "Epoch 461/500\n",
      "23/23 [==============================] - 0s 945us/step - loss: 0.3000 - accuracy: 0.8833 - val_loss: 0.6470 - val_accuracy: 0.6833\n",
      "Epoch 462/500\n",
      "23/23 [==============================] - 0s 956us/step - loss: 0.2827 - accuracy: 0.9056 - val_loss: 0.6412 - val_accuracy: 0.6833\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.2156 - accuracy: 0.9222 - val_loss: 0.6440 - val_accuracy: 0.6833\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - 0s 905us/step - loss: 0.2512 - accuracy: 0.9222 - val_loss: 0.6391 - val_accuracy: 0.6833\n",
      "Epoch 465/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2380 - accuracy: 0.9111 - val_loss: 0.6509 - val_accuracy: 0.6833\n",
      "Epoch 466/500\n",
      "23/23 [==============================] - 0s 946us/step - loss: 0.2228 - accuracy: 0.9111 - val_loss: 0.6555 - val_accuracy: 0.6833\n",
      "Epoch 467/500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.2788 - accuracy: 0.8944 - val_loss: 0.6566 - val_accuracy: 0.6833\n",
      "Epoch 468/500\n",
      "23/23 [==============================] - 0s 995us/step - loss: 0.2301 - accuracy: 0.9222 - val_loss: 0.6592 - val_accuracy: 0.6833\n",
      "Epoch 469/500\n",
      "23/23 [==============================] - 0s 993us/step - loss: 0.2669 - accuracy: 0.9000 - val_loss: 0.6564 - val_accuracy: 0.6833\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.2334 - accuracy: 0.9167 - val_loss: 0.6475 - val_accuracy: 0.6833\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.2524 - accuracy: 0.8889 - val_loss: 0.6546 - val_accuracy: 0.6833\n",
      "Epoch 472/500\n",
      "23/23 [==============================] - 0s 933us/step - loss: 0.2512 - accuracy: 0.9000 - val_loss: 0.6537 - val_accuracy: 0.6833\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - 0s 944us/step - loss: 0.2693 - accuracy: 0.8667 - val_loss: 0.6563 - val_accuracy: 0.6833\n",
      "Epoch 474/500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.2403 - accuracy: 0.9056 - val_loss: 0.6591 - val_accuracy: 0.6833\n",
      "Epoch 475/500\n",
      "23/23 [==============================] - 0s 956us/step - loss: 0.2652 - accuracy: 0.8944 - val_loss: 0.6598 - val_accuracy: 0.6833\n",
      "Epoch 476/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2286 - accuracy: 0.9222 - val_loss: 0.6547 - val_accuracy: 0.6833\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - 0s 1000us/step - loss: 0.2174 - accuracy: 0.9278 - val_loss: 0.6547 - val_accuracy: 0.6833\n",
      "Epoch 478/500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.2563 - accuracy: 0.9167 - val_loss: 0.6542 - val_accuracy: 0.6833\n",
      "Epoch 479/500\n",
      "23/23 [==============================] - 0s 917us/step - loss: 0.2600 - accuracy: 0.9056 - val_loss: 0.6644 - val_accuracy: 0.6833\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2463 - accuracy: 0.8944 - val_loss: 0.6617 - val_accuracy: 0.6833\n",
      "Epoch 481/500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.2784 - accuracy: 0.8722 - val_loss: 0.6563 - val_accuracy: 0.6833\n",
      "Epoch 482/500\n",
      "23/23 [==============================] - 0s 968us/step - loss: 0.2171 - accuracy: 0.9333 - val_loss: 0.6622 - val_accuracy: 0.6833\n",
      "Epoch 483/500\n",
      "23/23 [==============================] - 0s 919us/step - loss: 0.2531 - accuracy: 0.9056 - val_loss: 0.6624 - val_accuracy: 0.6833\n",
      "Epoch 484/500\n",
      "23/23 [==============================] - 0s 946us/step - loss: 0.2710 - accuracy: 0.8833 - val_loss: 0.6509 - val_accuracy: 0.6833\n",
      "Epoch 485/500\n",
      "23/23 [==============================] - 0s 938us/step - loss: 0.2408 - accuracy: 0.9056 - val_loss: 0.6380 - val_accuracy: 0.6833\n",
      "Epoch 486/500\n",
      "23/23 [==============================] - 0s 929us/step - loss: 0.2301 - accuracy: 0.9222 - val_loss: 0.6304 - val_accuracy: 0.7167\n",
      "Epoch 487/500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.2221 - accuracy: 0.9167 - val_loss: 0.6298 - val_accuracy: 0.7333\n",
      "Epoch 488/500\n",
      "23/23 [==============================] - 0s 932us/step - loss: 0.2337 - accuracy: 0.9278 - val_loss: 0.6317 - val_accuracy: 0.7167\n",
      "Epoch 489/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2288 - accuracy: 0.9111 - val_loss: 0.6296 - val_accuracy: 0.7333\n",
      "Epoch 490/500\n",
      "23/23 [==============================] - 0s 915us/step - loss: 0.2173 - accuracy: 0.9222 - val_loss: 0.6336 - val_accuracy: 0.7333\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.2297 - accuracy: 0.9167 - val_loss: 0.6334 - val_accuracy: 0.7333\n",
      "Epoch 492/500\n",
      "23/23 [==============================] - 0s 938us/step - loss: 0.2478 - accuracy: 0.9167 - val_loss: 0.6318 - val_accuracy: 0.7167\n",
      "Epoch 493/500\n",
      "23/23 [==============================] - 0s 965us/step - loss: 0.2503 - accuracy: 0.9111 - val_loss: 0.6354 - val_accuracy: 0.7167\n",
      "Epoch 494/500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.2820 - accuracy: 0.8833 - val_loss: 0.6403 - val_accuracy: 0.7167\n",
      "Epoch 495/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2490 - accuracy: 0.9167 - val_loss: 0.6485 - val_accuracy: 0.7000\n",
      "Epoch 496/500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.2443 - accuracy: 0.9111 - val_loss: 0.6461 - val_accuracy: 0.7000\n",
      "Epoch 497/500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.2397 - accuracy: 0.9167 - val_loss: 0.6355 - val_accuracy: 0.7167\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - 0s 918us/step - loss: 0.2304 - accuracy: 0.9222 - val_loss: 0.6387 - val_accuracy: 0.7167\n",
      "Epoch 499/500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.2821 - accuracy: 0.8778 - val_loss: 0.6503 - val_accuracy: 0.7000\n",
      "Epoch 500/500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.2139 - accuracy: 0.9111 - val_loss: 0.6434 - val_accuracy: 0.7000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "BATCH_SIZE = 8\n",
    "model = getNetwork()\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fea658b8268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fea658b8268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.77        30\n",
      "           1       0.79      0.73      0.76        30\n",
      "\n",
      "    accuracy                           0.77        60\n",
      "   macro avg       0.77      0.77      0.77        60\n",
      "weighted avg       0.77      0.77      0.77        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test = model.predict(X_test)\n",
    "predictions_categorical = np.argmax(pred_test, axis=1)\n",
    "print(classification_report(y_test, predictions_categorical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Models in C code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmppmjwyglg/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmppmjwyglg/assets\n"
     ]
    }
   ],
   "source": [
    "# Neural network with TinyMLGen\n",
    "with open('exportedModels/NNmodel.h', 'w') as f:\n",
    "    f.write(tiny.port(model, optimize=False))\n",
    "\n",
    "# Classifiers with MicroMLGen\n",
    "for name, model in models:\n",
    "    prepath = 'exportedModels/' + str(labels) + \"/\"\n",
    "    path = prepath + name + '.h'\n",
    "    x = port(model, optimize=True)\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(port(model, optimize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
