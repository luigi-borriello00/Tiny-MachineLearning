{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Comparison for TinyML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "import pickle\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,  classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold,StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dense, Input, concatenate, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import tensorflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from micromlgen import port\n",
    "import tinymlgen as tiny\n",
    "\n",
    "import warnings\n",
    "import seaborn as sbs\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "tasks = [\"2Labels\", \"3Labels\", \"4Labels\", \"5Labels\"]\n",
    "choosenIndex = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/X2.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open('data/y2.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 2 + choosenIndex\n",
    "samples = 150\n",
    "X = X[:n_labels*samples]\n",
    "y = y[:n_labels*samples]\n",
    "labels = np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y).tolist()\n",
    "for i in range(len(classes)):\n",
    "    y = np.where(y==classes[i], i, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([int(el) for el in y])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1092.63 1705.31 2309.56 3803.11 5792.62 5792.62 5792.62 5792.62 5434.79\n",
      "  5173.3  4662.33 3211.6  2037.58 1022.57  794.06  898.66  864.78  769.36\n",
      "   747.19  835.31  640.82  410.83  411.76  787.62  380.06  329.46  455.41\n",
      "   404.18  306.83  306.73  471.04  347.55]\n",
      " [1701.47 1604.48 1157.15 1103.53  193.02  332.71  367.69  281.27  295.26\n",
      "   296.19  274.33  281.1   132.73  197.96  201.58  198.8   122.18  153.57\n",
      "   137.97   43.62   63.46   67.99   80.33   59.78   48.34   36.61   31.11\n",
      "    48.69   26.03   43.56   57.2    42.9 ]\n",
      " [1272.09 1428.49 2150.49 1593.85 1828.63 1998.09  757.68  465.55  317.34\n",
      "   351.11  890.19  421.87  300.   1604.99 1506.88 1912.38 1687.22 1819.75\n",
      "  1840.99 1705.02 1281.86 1169.61 1070.86  855.13  518.07  399.11  311.58\n",
      "   262.98  245.87  189.47  210.14  267.5 ]\n",
      " [2839.23 2885.9  3316.22 5013.26 4524.79 4979.87 3519.39 4665.28 2304.54\n",
      "  2951.92 2214.16 2071.49 1641.25 1065.42  959.24 1083.05 1126.97 1095.14\n",
      "   997.5   975.16  948.65  925.67  913.07  890.3   824.29  794.59  741.52\n",
      "   710.75  663.36  700.44  633.27  606.72]\n",
      " [1157.17 1576.82 1116.45 1252.34 1369.68 1178.02 1271.77  940.87 1019.04\n",
      "  1046.5   542.09  724.97  792.3   958.75 1045.16 1081.73 1097.6  1063.26\n",
      "   935.05  773.    554.87  396.02  214.89   63.66   65.92  191.9   208.63\n",
      "   217.17  287.43  341.03  355.89  352.87]\n",
      " [1370.57 1930.87 1178.98 1196.08  486.57  393.83 1027.83 1251.86 1026.55\n",
      "  1023.39  900.18 1027.11  821.48  992.11  547.32  436.31  272.    218.63\n",
      "   194.53  139.63  119.01  118.83   81.25   36.2    43.44   34.45   43.52\n",
      "    51.58  316.78  114.72  181.4   168.89]\n",
      " [2020.39 1934.98 1811.79 1473.21 1809.56  812.53  855.83  628.6   594.84\n",
      "   666.26  595.56  672.    495.62  544.59  366.25  326.28  308.18  132.43\n",
      "    84.18   82.13   28.49   62.6    66.58  143.22  125.97  179.02   51.91\n",
      "    81.08  144.02  272.93  454.68  545.25]\n",
      " [4452.07 3760.89 2768.68 2801.27 2084.34 1570.77 1380.21  811.77  719.93\n",
      "   305.36  182.52  364.44  497.61  861.03 1091.37 1385.22 1474.78 1639.56\n",
      "  1722.31 1738.64 1768.81 1677.32 1677.58 1682.04 1669.21 1631.06 1616.43\n",
      "  1627.5  1573.27 1620.56 1613.7  1646.77]\n",
      " [1190.53  698.37  577.61  398.22  635.05  637.08 1556.98 2566.52 2752.97\n",
      "  1873.47 1157.35 1973.91 1275.33 1106.12 1134.78  684.75  655.45  599.75\n",
      "   503.05  434.68  331.46  211.3   211.37  118.44   87.14   59.98   49.61\n",
      "    55.21   43.13   68.3    64.24   90.75]\n",
      " [1336.74  999.89  965.37  933.35  742.75  932.98  790.3   859.07  952.51\n",
      "  1018.5  1135.87  421.55  220.72  223.91  186.47  165.45  121.55  157.76\n",
      "   158.49  131.99  125.05  201.17  160.12  122.49   75.35   69.27   39.9\n",
      "    27.93   64.73   15.66   96.35  127.21]\n",
      " [1126.43 2458.21 3088.89 1754.39 1596.7   893.3   538.27  299.92  325.43\n",
      "   503.46  695.89  445.92  494.57 1825.46  897.31 2429.64 2566.55 2429.64\n",
      "  2349.17 2134.39 2365.05 1492.96 1488.74 1195.13  527.67  405.32  252.03\n",
      "   229.84  141.56  136.79   99.04   31.63]\n",
      " [1746.25 2267.42 2813.36 2585.33 2727.66 1927.58 2165.79 1282.46  456.62\n",
      "   410.25  321.69  375.17  228.01  155.6   153.15   73.72   40.04  105.78\n",
      "    59.04   76.05  141.73  125.86  172.31   73.08  124.48  123.27  110.75\n",
      "   152.64  134.47  169.47  122.22  106.07]\n",
      " [1473.04 2135.31 1910.89 2114.22 1961.72 2522.15 2194.22 1581.67 2193.98\n",
      "  2572.83 2253.28  680.13  309.38  236.37  120.15  137.79   69.64   63.77\n",
      "    24.43   25.14  101.39   68.47   91.94   92.24   87.96  114.5   130.06\n",
      "   123.63  152.26  102.42  157.31  113.33]\n",
      " [1106.18 1386.72 2259.67 2440.8  2449.16 2031.63 2234.83 2919.23 2554.02\n",
      "  2683.38 3417.36 1928.26 2935.28 1318.27 1520.08  916.07  527.62  339.76\n",
      "   613.02  704.16  677.24  791.79  974.99  960.92  686.92  342.29  270.48\n",
      "   146.     66.11   78.51  117.38   44.59]\n",
      " [1040.32 1039.67 1429.94  592.01  108.56  242.89  225.16  192.67  162.22\n",
      "   115.96  144.87   79.85  136.44  185.62  464.95  678.56  971.29 1042.72\n",
      "   819.16  646.56  239.12   74.8   146.5   271.19  423.33  467.39  527.51\n",
      "   538.42  559.9   607.68  571.61  560.1 ]\n",
      " [ 900.46  746.39 1011.66  859.94  994.34  692.78  804.72 1106.37  562.37\n",
      "   435.    169.87   92.36   51.74   29.09   83.58   64.66  127.8    91.24\n",
      "   180.63  229.71  226.44  279.71  207.55  323.94  321.35  273.28  214.52\n",
      "   213.35  175.24  176.02  139.45  148.63]\n",
      " [1772.07 1684.1  1680.55 1293.09 1428.06 1516.   1152.23  579.54  232.98\n",
      "   249.93  161.65   56.22  125.41  285.57  316.71  344.95  412.25  448.83\n",
      "   394.29  330.67  161.01   72.22  206.84  293.28  457.19  610.38  677.66\n",
      "   643.77  666.45  681.42  655.12  685.59]\n",
      " [1697.65 2710.63 4130.8  4160.69 3536.96 2692.4  3395.88 3730.16 3220.27\n",
      "  2807.   2042.07 1551.33  368.35  374.45  286.03  223.93  111.26   64.99\n",
      "    73.12   72.51   38.04   55.93   46.42   52.69   47.05  103.73  187.1\n",
      "   292.07  226.52  309.43  282.88  360.33]\n",
      " [1203.11  863.69 1204.52 2900.35 4208.86 3707.16 2551.75 2435.23 2834.12\n",
      "  3083.7  2989.82 2519.4  1125.09 1234.6   506.15  907.24  625.56  595.15\n",
      "   576.4   528.6   494.65  466.59  425.61  371.4   352.19  373.32  287.12\n",
      "   338.97  304.77  337.61  336.25  355.33]\n",
      " [1383.7  1383.92  688.62  292.16  446.6   352.37  229.69  830.87 2308.64\n",
      "  1799.48 1940.85 1839.61 1567.81 1620.12 1525.48 1855.76  667.74  431.41\n",
      "   378.86  325.36  511.19  627.26  781.13  739.3   744.33  627.59  485.54\n",
      "   523.42  268.68  193.34   74.07   83.84]\n",
      " [2174.24 1957.3  1929.83 1944.8  2510.42 1408.42  954.84  361.62  188.23\n",
      "   137.1   125.24  124.44  122.78  184.81  155.22  212.59  256.7   354.88\n",
      "   309.17  355.04  296.27  232.51  167.56  118.97  126.48   49.83   31.98\n",
      "    23.96   44.75   66.95  109.7   128.34]\n",
      " [1254.11 1862.83 2096.15 1975.95 1343.04 1705.81 1671.43 2581.53 2362.86\n",
      "  2226.24 1538.17 1299.3  1173.87  625.86  363.8   355.52  318.38  361.31\n",
      "   364.06  398.96  425.59  381.61  473.29  467.33  478.81  393.96  360.8\n",
      "   294.68  234.77  266.11  119.91  184.54]\n",
      " [1381.49 1914.37 1367.61 1276.61 1028.27 1078.   1105.64 1558.86 1801.7\n",
      "  1734.02 1694.12 1294.11  742.21  284.85  279.27  161.25  135.72  115.82\n",
      "   112.11  189.12  131.1   182.88  118.16  113.86   71.8    27.09   24.6\n",
      "    84.47  102.7   102.84  156.28  137.1 ]\n",
      " [1058.47  547.59  538.12  984.75  435.86  486.29  487.34  524.91  569.49\n",
      "   510.85  596.64  576.26  489.65  499.98  394.54  386.89  394.07  301.57\n",
      "   389.85  444.86  512.66  542.01  494.7   455.85  381.05  328.88  193.92\n",
      "   137.25  138.62  382.97  520.02  820.25]\n",
      " [1456.44 1552.69 1612.04 1496.51 1681.22 1342.14  967.14  464.14  489.51\n",
      "   550.01  342.19  285.38 1100.19 2141.43 2094.12 2010.48 2058.94 2494.97\n",
      "  2170.04 1870.41 2133.64 1706.28 1193.17  650.05  642.07  453.2   284.79\n",
      "   165.14  183.05  105.11   52.25   74.82]\n",
      " [ 971.11  548.    866.94 1090.21 1987.67 1336.14 1428.83 1315.12 1811.51\n",
      "  2607.4  1046.03 1190.66 1136.21  650.67  651.72  403.54  281.65  250.65\n",
      "   202.09  183.79  201.02  142.91  129.72  157.37  136.6   125.51   69.27\n",
      "   137.57  212.68  130.42  167.53  230.51]\n",
      " [ 955.2  1518.93 1844.97 2154.18 1969.15 2082.93  412.1   319.69  200.3\n",
      "   439.6   288.26  188.18  280.55 1878.8  2367.31 1806.74 1648.26 1614.76\n",
      "  1751.7  1636.73 1774.94 1520.97  639.47  607.24  306.07  209.95  248.9\n",
      "   324.11  349.21  341.03  358.47  359.6 ]\n",
      " [1225.41 1414.44 1209.9  1125.84  685.32  431.71  280.08  257.32  253.7\n",
      "  1005.42  332.08  913.96 1853.22 2100.52 2107.92 2219.46 2119.82 1942.88\n",
      "  1945.33  956.3   363.7   307.11  275.18  177.97   88.64   86.92   93.71\n",
      "   108.5   152.09  162.29  221.99  194.36]\n",
      " [1309.97  772.03  976.8   864.42  834.7   774.01  733.84  699.06 1210.75\n",
      "   551.35  516.43  367.68  159.96   51.54  110.42  140.75  107.72  197.78\n",
      "   227.01  181.64  134.25  143.2    99.67  140.55   95.56   77.28   28.38\n",
      "    72.5   118.94   88.51  134.15   89.66]\n",
      " [2196.02 1614.72 2014.25 1782.18 1773.88 1407.73  890.5   773.38  640.2\n",
      "   664.87  576.04  596.59  593.6   535.83  524.14  462.2   416.78  361.91\n",
      "   340.29  159.17  110.51   36.59   34.75   36.81   40.38   44.21  114.5\n",
      "   183.15  144.47  100.     29.95   35.05]\n",
      " [3910.08 2839.5  2624.48 2698.08 2016.59 1894.75 1247.53  677.33  819.4\n",
      "   821.63 1162.08  974.76  405.02  262.03  167.9   192.18  157.41  347.42\n",
      "   577.81  506.28  330.31  224.61   96.64  117.99  586.75  581.64  357.21\n",
      "   586.48  779.    929.99 1122.05 1550.88]\n",
      " [1497.1  1899.44 2310.74 3234.93 2657.64 2105.04 2724.92 2773.43 1923.63\n",
      "  1635.4  1221.31  525.15  464.97  441.41  624.21  621.27  662.16  583.35\n",
      "   415.52  460.4   360.69  377.05  290.53  262.76  307.87  248.29  288.74\n",
      "   226.83  299.75  337.08  350.38  427.3 ]\n",
      " [ 946.78 1316.92 1334.38 1486.01 1462.35 1853.13 1984.72 1922.5  2292.35\n",
      "  1507.64 1668.12  897.41  244.69  221.07  131.99  115.24  151.7   119.76\n",
      "   123.    163.61  135.76  197.66  115.77  117.11  106.73  100.9    77.32\n",
      "    39.45   39.34   84.16   26.2    79.25]\n",
      " [2536.21 2000.64 1885.9  1820.49 1297.12  773.23  694.56  612.7   729.69\n",
      "   806.57  723.47  601.56  661.57  630.04  533.52  502.3   490.99  484.33\n",
      "   492.4   323.    384.26  382.91  372.36  395.08  430.64  396.8   421.82\n",
      "   397.49  362.5   282.5   200.83  145.01]\n",
      " [1590.66  852.65  720.14 1089.52 1329.08 1210.19 1160.7  1134.81 1569.64\n",
      "  1749.99 1280.49 1217.51 1836.92 1512.74  318.86  181.5   235.1   223.85\n",
      "   298.69  299.02  261.85  243.97  169.32  146.3    68.11   67.89   69.97\n",
      "    36.82   46.37   81.07   59.99   98.58]\n",
      " [1029.62  881.7   684.41  331.6   324.65  168.38  130.56 1338.27  289.8\n",
      "   319.29 1456.51 1233.44 1039.39 1145.13 1402.4  1433.32 1381.08 1761.93\n",
      "   715.45  395.86  185.57  192.77   93.89   89.34   43.51  102.74   63.53\n",
      "    33.96   77.45   33.12   37.03   28.16]\n",
      " [1273.33 2216.17 2729.05 3070.46 2549.1   822.07  379.33  443.38  943.98\n",
      "   815.4   305.38  285.   2269.3  3724.59 2894.08 2369.44 2786.72 2549.5\n",
      "  2479.45 2533.74 2762.33 2656.43 1950.18 2193.19 1285.67  832.89  354.93\n",
      "   399.84  214.42  195.    249.76  270.79]\n",
      " [1149.81 1410.77 2703.16 2991.58 5092.58 3709.15 1610.31  957.58  456.49\n",
      "   505.18  452.88  477.29  526.07  861.77 2594.23 1995.28 1950.98 1785.51\n",
      "  1798.6  1666.37 1750.78 1730.95 1189.85 1554.02 1000.12  712.69  550.26\n",
      "   340.9   367.17  339.97  285.54  328.92]\n",
      " [1682.99 2819.06 2827.32 3677.63 3436.19 1475.27  799.83  644.23  544.76\n",
      "   538.84  485.67  480.78  461.45  537.95  480.98  358.25  388.59  367.09\n",
      "   387.7   262.26  251.02  153.67  111.51   35.22   52.9    89.64  105.87\n",
      "    86.23   48.43   47.06   58.48  121.11]\n",
      " [1147.79 3776.73 5577.25 2802.51 1355.76  404.21  381.76  808.06  605.88\n",
      "   383.46 1993.18 3430.6  2273.24 3957.9  5339.28 2351.28 2779.4  1298.55\n",
      "   462.9   351.66  376.45  299.02  196.58  144.56  153.94  110.23  105.76\n",
      "    70.39  146.03  130.37  166.    170.83]\n",
      " [1587.04 2052.1  1653.95 1743.73 2086.04 2393.34 1895.58 1685.35 1784.51\n",
      "   808.76  581.46  487.29  344.74  199.22   41.75  322.61  698.5  1379.3\n",
      "  1726.61 2221.08 2283.18 1994.71 1461.49  341.02  415.12 1660.37 2274.91\n",
      "  2810.95 2836.11 2789.96 2565.45 1865.46]\n",
      " [1947.61 1480.86 2014.69 1790.23 1968.18 1934.28 1763.92 1782.94 1714.01\n",
      "  1842.78 1973.17  997.44  442.74  315.84  276.58  284.03  153.86   55.21\n",
      "    41.     78.5    56.23   23.53   40.54   43.99   43.79   45.08   88.38\n",
      "    63.63   35.67   79.05   55.31   63.97]\n",
      " [1379.14 1022.85  555.53  562.75  417.5   767.49  774.54  728.75  829.87\n",
      "   740.42 1318.58 1120.62  673.77  687.33  216.26  108.72   47.82  106.49\n",
      "    92.43  165.5   117.66  105.77  124.51   98.85   74.38   91.98   62.29\n",
      "   103.21  100.97   55.68   54.11   72.19]\n",
      " [1016.76  676.36  742.09  878.85  834.79  697.96  529.96  218.78  155.4\n",
      "    39.47  108.77   36.35   36.28   60.34   38.16   60.2    32.31  110.67\n",
      "   133.28  109.1   169.5   270.52  193.41  338.99  312.77  302.85  274.78\n",
      "   212.95  220.84  357.64  325.22  271.44]\n",
      " [1161.03 1756.71 1675.02 1422.2  1787.37 1636.56 1346.66 1440.76 1259.35\n",
      "   914.99  814.63  514.51  348.56  476.08  528.25  568.05  777.05  883.65\n",
      "   897.78 1059.89 1013.13  982.55  954.29  884.23  787.13  733.41  678.3\n",
      "   598.26  453.37  461.02  319.    273.7 ]\n",
      " [1146.01 1995.76 2296.28 2049.1  1824.41 1645.48 1476.07 1203.49 1152.23\n",
      "   550.19  211.05  243.95  253.25  235.52  245.71  153.87  158.24  123.83\n",
      "   103.55   39.04   46.68   34.29   51.92   31.7   120.94   75.64   96.03\n",
      "    42.45   76.5   174.95  163.42  119.99]\n",
      " [2639.51 1239.41 1757.19 1118.38 1141.75 1163.88 1112.83  892.18  804.58\n",
      "   597.92  459.47  265.68  114.08   92.59  234.76  438.38  574.8   707.55\n",
      "   778.68  886.12  858.49  951.52  985.44 1063.91 1046.76 1047.3  1040.83\n",
      "  1000.63 1012.91  930.66  897.32  872.06]\n",
      " [2125.58 3428.97 2668.43 1679.88 1849.76 1565.09 1482.71 1297.92 1088.31\n",
      "  1011.97  836.72  653.9   507.56  473.94  784.37 1130.69  976.07 1076.41\n",
      "  1327.08 1298.05  585.35  512.63  471.85  269.75  153.38  149.34  142.7\n",
      "   140.54  157.46   83.75  133.96  134.33]\n",
      " [3142.86 1828.87 2438.97 2334.24 2380.31 1832.94 1752.32 1679.69 1922.3\n",
      "  1299.37 1238.42 1179.1   989.32 1355.16  665.85  259.33  115.2   171.25\n",
      "    89.88   82.56   78.11   48.25   46.32   25.49   25.12   23.01   43.21\n",
      "    37.83   60.48   23.3    49.12   31.53]\n",
      " [1425.8  1967.26 1876.71 1664.03 1637.34 1926.64 1879.12 2137.66 2264.91\n",
      "  1913.41 2364.48 1751.52 1115.2   429.7   731.07  290.91  279.76  111.72\n",
      "   141.92   57.68  106.1    87.93   49.59   29.49   27.53   31.86   58.17\n",
      "    50.9    83.28   32.99   87.06   66.76]\n",
      " [1668.07 1578.33 1394.88 1186.03  966.2   814.75  643.52  357.53  343.39\n",
      "   362.36  388.27  401.01  508.91  405.37  418.99  437.81  337.04  264.75\n",
      "   266.96  237.    187.79  272.05  261.38  269.93  233.88  211.83  178.32\n",
      "   195.16  196.67  165.39  149.39  137.24]\n",
      " [1154.21 1384.52 1228.44  718.06  386.54  132.36  190.34  155.16  133.6\n",
      "    86.23  100.03  180.23  137.56  171.56  158.5   238.93  219.29  231.99\n",
      "   230.58  247.56  288.73  310.92  306.84  291.51  323.8   344.81  344.3\n",
      "   364.72  402.99  427.75  435.88  450.26]\n",
      " [1507.49  286.29 1796.27 2026.19 2462.89 2378.04 2417.82 2195.94 3104.65\n",
      "  1985.89 2356.15 1849.93 1540.67 1422.74 1287.44 1204.72  780.19  551.62\n",
      "   475.16  400.27  406.55  392.76  337.7   433.69  408.83  365.14  392.92\n",
      "   438.67  336.05  299.56  302.02  107.81]\n",
      " [1529.05 1187.21  946.9   434.38  315.5   307.14  315.79  349.99  297.23\n",
      "   407.31  443.35  404.18  446.91  497.35  449.53  445.26  427.16  433.55\n",
      "   462.22  467.76  454.07  460.2   438.14  442.69  454.61  384.57  353.13\n",
      "   331.25  372.27  365.45  368.31  372.53]\n",
      " [1053.33 1951.81 1385.09 2069.59 1710.22 1733.97 1808.31 1446.17 2148.94\n",
      "  1830.25 2308.91 1517.57 2538.1  1811.62 2391.06 1736.18  430.56  340.94\n",
      "   369.76  424.57  399.3   447.02  364.48  445.27  324.72  364.55  360.6\n",
      "   300.31  305.26  285.93  254.44  288.47]\n",
      " [1181.53 1562.56 2331.63 1884.42 1472.77  973.11  720.3   314.92   96.14\n",
      "   137.94  874.53  701.61  385.51  275.69 1538.43 1684.68 2295.18 2173.01\n",
      "  2499.15 1898.26 2033.91 1503.55  737.08  693.48  637.56  404.93  301.69\n",
      "   251.52  139.78  159.32  113.83   35.63]\n",
      " [1092.69 1188.97 1006.34 1286.09 1039.97  436.96  769.04 1042.24  638.98\n",
      "   524.52  347.07  416.25  420.72  255.58  332.15  198.88  245.57  141.46\n",
      "   141.92  205.68  175.54  240.27  244.37  333.81  323.37  350.39  393.62\n",
      "   328.73  403.24  449.47  544.67  615.29]\n",
      " [1398.58 1598.64 3130.16 4341.44 3707.34 4104.54 2616.32 2078.89 1744.17\n",
      "  1684.   1602.77 1338.    491.86  447.15  441.73  590.41  628.18  374.11\n",
      "   494.31  387.42  400.84  339.39  298.64  298.84  265.6   262.73  197.33\n",
      "    79.92   44.99   90.87   67.65   67.36]\n",
      " [1649.02 1464.62  913.96  928.75  401.29  260.13  285.68  253.72  314.22\n",
      "   385.07  386.46  462.89  524.19  468.13  402.8   323.76  295.94  197.64\n",
      "    98.44   62.97  229.19  165.9   269.29  215.75  240.68  226.4   204.56\n",
      "   159.46  128.22  240.74  240.03  272.48]\n",
      " [3545.91 2880.23 1764.21 1654.36 1831.8  1582.65 1273.07 1601.51 1247.63\n",
      "  1045.11  509.59  319.5   304.3   277.16  382.86  518.79  429.94  464.61\n",
      "   683.88  656.65  542.26  484.58  527.96  484.89  473.98  330.4   349.9\n",
      "   378.37  375.36  442.96  346.99  360.02]\n",
      " [1170.65 1569.12 1278.54 1234.71 1222.01 1373.05 1151.02 1053.77  692.9\n",
      "   276.57  258.77  202.17  148.52  126.7   116.39  145.04  148.55  166.53\n",
      "   156.58   83.82  138.88   28.81   97.71  118.39   60.53  154.34   60.41\n",
      "    37.78  111.43   47.32   77.58   74.13]\n",
      " [1634.01 1802.79 1972.71 3114.11 5792.62 4405.89 5792.62 4662.61 3941.35\n",
      "  3832.2  3873.12 3116.74 2059.34 1699.37 1408.84 1105.99 1017.68 1049.38\n",
      "   976.27  825.78  627.41  655.15  313.46  252.57  190.25  142.49  116.59\n",
      "    97.78  117.17  222.4   200.14  100.01]\n",
      " [1156.17 1175.65 1282.99 1250.42 1557.85 1625.95 1701.15 1691.69 1730.17\n",
      "  1549.94 1988.96 1816.63 1729.78 1879.03 1473.02 1825.67  891.57  823.41\n",
      "   403.37  198.48  180.77   89.78  165.24  172.22  228.59  252.59  187.21\n",
      "   182.94  185.59  206.7   297.45  417.52]\n",
      " [3759.69 3560.12 3173.46 3431.46 2676.15 2049.18 2081.37 1541.08 1074.15\n",
      "   671.35  552.79  529.34  508.46  493.78  358.74  441.1   450.06  410.35\n",
      "   433.6   408.97  379.06  397.4   386.25  423.35  401.36  428.83  378.37\n",
      "   383.84  341.94  357.98  325.55  292.92]\n",
      " [1362.27  788.    624.98  762.2   776.45  834.05  745.19  762.14  826.66\n",
      "   733.38  503.3   531.76  402.65  254.    190.83  297.52  420.83  550.\n",
      "   324.94  219.28  235.98  288.51  490.83  733.92  770.64  468.67  502.01\n",
      "   447.67  627.23  767.05  548.81  662.47]\n",
      " [ 957.2   534.03  729.72  692.63  853.72  679.58  736.84  595.64  341.71\n",
      "   399.23  358.21  324.15  322.23  334.82  336.38  211.33  237.04  194.78\n",
      "   241.83  234.33  147.47   49.05   52.29   56.54   50.93   49.26   54.58\n",
      "    66.94   76.26   52.34   74.26  116.72]\n",
      " [1588.98  411.23 2129.31 2623.   3005.98 2274.6  2022.07 2317.92 2535.52\n",
      "  2858.05 2057.35 1754.01 1874.25 1051.41  674.39  297.68  152.1   125.94\n",
      "    70.39   69.71   97.01  360.68   98.67   78.77   68.66   75.47   49.29\n",
      "   120.63   66.92   68.64   46.75   54.36]\n",
      " [1159.18  623.    379.17  502.08  373.96  221.03  233.61   71.76  170.89\n",
      "   130.56   95.16   40.02   99.76  105.95  119.35   68.76  123.    198.98\n",
      "   192.17  139.9    67.92  148.05  132.56  166.07  242.11  272.71  348.28\n",
      "   398.64  475.85  458.94  605.4   659.83]\n",
      " [1681.61 1793.95 1284.47  688.31  831.26  326.18  198.29  185.82   80.8\n",
      "   120.12   73.22  118.04  192.28  194.27  315.88  310.18  335.88  414.11\n",
      "   434.93  338.98  266.97  308.11  283.55  275.53  331.38  402.95  195.61\n",
      "   377.56  174.6   100.59   93.3   162.98]\n",
      " [2157.01 1900.63 1841.95 1669.87  573.81  522.12  383.12  371.85  375.23\n",
      "   195.78  198.83  133.42  132.83   42.03   38.31   31.65   22.59   30.59\n",
      "    68.55  137.07   96.33  139.05   61.61   71.85   73.7    77.48   27.99\n",
      "    81.05   45.09   44.48   28.12   41.69]\n",
      " [ 961.16 1500.53 1648.11 1306.06  684.13  424.84  453.69  399.   1557.4\n",
      "   493.59 2007.78 1870.8  1742.81 1382.57 1669.01 1336.56 1617.   1932.43\n",
      "  1632.68  679.47  494.63  406.66  409.73  318.57  203.17  148.04  188.58\n",
      "   180.18  210.49   76.3   139.32   77.4 ]\n",
      " [1245.1  1740.58 1063.22 1478.88  951.08 1731.05 1584.1  1699.44  643.71\n",
      "   941.47  519.61  558.83  516.74  485.51  482.49  538.11  585.46  561.13\n",
      "   578.99  547.18  591.06  516.02  441.57  366.21  297.74  294.4   133.44\n",
      "    37.86   65.18  138.38  235.35  428.43]\n",
      " [1310.22  700.07  621.88 2597.61 2384.14 2145.32 1998.82 1589.24 2369.79\n",
      "  2008.17 1881.86  872.85  505.14  415.22  282.8   313.49  291.94  272.86\n",
      "   254.69  221.69  267.75  276.94  257.25  287.08  311.48  243.71  232.84\n",
      "   237.63  210.5   189.93  201.69  130.67]\n",
      " [ 922.28 1460.13 2429.23 1931.23 2257.21 1844.03 1033.45  662.27  286.21\n",
      "   178.5  1573.64  653.02  357.43  741.17 2516.44 1995.8  2764.1  2952.01\n",
      "  2158.43 2023.61 1753.27 1841.23 1638.75 1587.67 1203.04  639.33  462.61\n",
      "   316.67  313.84  281.21  252.79  186.06]\n",
      " [1983.56 1507.67 1800.98 1340.65  708.68  628.28  432.44  410.45  407.58\n",
      "   380.48  310.46  282.98  347.51  259.82  216.89  140.48  146.27   75.1\n",
      "   113.15  117.24   62.72   85.5    44.42  171.38  211.35  254.52  237.91\n",
      "   477.81  356.82  252.43  299.39  190.09]\n",
      " [2330.41 2758.58 1550.98 1642.51 1392.73  878.39  706.66  552.98  511.96\n",
      "   602.86  545.95  639.37  539.09  475.2   408.11  452.36  230.5   250.33\n",
      "   101.57   43.09   82.19   67.89  119.57   53.28   86.42  282.89  316.43\n",
      "   479.42  529.43  652.71  601.63  608.1 ]\n",
      " [ 970.42 1347.88 1604.85 1692.95 2126.52 1706.02 1840.21 1411.25 1180.59\n",
      "   480.41  423.58  382.21  313.15  408.17  266.84  310.27  177.97  282.45\n",
      "   147.68  105.29  127.17   47.07   57.27   72.49   74.17   49.32   88.21\n",
      "   157.44  131.62  134.97  142.65  211.99]\n",
      " [1502.82 1379.8  1180.59  928.76  687.11  716.78  204.07  102.1   200.27\n",
      "   287.47  368.37  444.76  404.9   349.54  294.66  186.8   100.72  134.21\n",
      "   169.96  149.33  213.35  212.35  194.38  220.81  151.17  124.85   77.28\n",
      "    34.4    42.64   39.76   22.19   26.09]\n",
      " [1611.62 2476.26 4008.82 3914.41 2657.83 2263.76 1739.06 2446.64 1547.32\n",
      "  1428.38  967.11  834.27  424.25  337.18  331.82  471.41  538.77  628.98\n",
      "   735.48  761.16  808.73  794.81  721.37  589.59  541.88  389.88  340.38\n",
      "   255.5   197.73  108.67   83.51   50.92]\n",
      " [1779.89 1791.35 1494.59  710.52  355.31  385.89  228.2   163.7   214.51\n",
      "   258.1   294.98  277.49  290.84  259.43  325.53  298.6   293.89  228.14\n",
      "   153.42   86.63  130.9   110.5    89.41   92.85  116.69   66.83   60.72\n",
      "    55.11   86.3    42.49   64.93   48.33]\n",
      " [ 992.46  619.03  611.55  698.45  530.76  368.28  425.24  374.29   81.4\n",
      "    63.62   26.31   43.05   52.25   37.99   68.4    17.81   48.28   68.\n",
      "    84.44   98.93  133.35  127.66  172.49  145.4   179.1   172.35  160.66\n",
      "   143.45   45.22   90.11   64.85   51.88]\n",
      " [1189.34 1430.91 1472.26 1462.33 1230.77 1544.84 1608.64 1328.68  948.52\n",
      "   881.85  661.5   618.82  647.99  569.41  508.    407.04  283.44  217.69\n",
      "   122.92   34.91  112.49  136.36  204.74  309.    464.48  581.21  655.63\n",
      "   731.96  695.7   665.08  644.18  708.09]\n",
      " [1850.58 1830.83 1088.57  932.79  905.61  743.19  426.77  417.78  264.74\n",
      "   260.3   353.97  387.05  364.78  352.    448.64  389.15  731.43  412.83\n",
      "   413.25  431.7   312.07  328.54  328.77  370.69  358.04  238.51  106.44\n",
      "   113.12   44.1    33.77  131.91  186.51]\n",
      " [1140.16 1431.42 1232.82 1323.84 1527.77 1672.49 1960.84 1558.77  987.3\n",
      "   420.21  293.99  388.24  253.54  145.35  164.36  198.24  157.28   79.14\n",
      "    72.71   77.94   76.16   67.24  102.9   109.09  132.84   92.51   90.87\n",
      "    70.63   61.4    41.56   69.23   30.52]\n",
      " [1091.81 1392.28 1674.51 2352.35 1944.87 2283.48 2229.04 2231.1  2023.52\n",
      "  2927.07 2462.96 1781.6   903.02  468.96  269.09  121.57  180.66  114.31\n",
      "    63.22   50.05  106.17  104.06  154.24  206.14  108.56  142.22  212.08\n",
      "   208.66  243.16  360.43  387.76  490.73]\n",
      " [1135.86 1803.2  1824.75 1946.16 2328.1  1958.5  1250.09  780.39  343.95\n",
      "   300.3   219.78  249.39  104.48   96.     63.81   63.05   45.9    59.17\n",
      "    77.08   51.34  152.78   81.42  106.27  171.49  252.16  156.79  204.62\n",
      "   164.39  132.74   95.88   78.16   69.8 ]\n",
      " [2071.12 2024.   1847.67 1861.79 2082.8  1632.09 1617.32  974.8   480.78\n",
      "   288.22  424.97  297.99  305.07  258.8   322.48  368.89  325.04  358.51\n",
      "   375.98  418.38  414.76  421.43  462.35  460.42  498.89  487.21  464.8\n",
      "   394.31  367.86  296.2   253.96  217.95]\n",
      " [1316.75 2332.31 4383.06 4273.09 3310.93 2674.73 2462.37 2718.98 2777.68\n",
      "  1618.11  770.5   551.36  621.33  736.94  754.65  711.61  511.85  336.05\n",
      "   381.88  265.13  123.31   53.46   79.52   53.64   26.5    41.86   79.63\n",
      "    40.03   80.21  106.23  121.2   142.93]\n",
      " [1166.48  631.56  644.22  461.22  402.78  365.2   181.08  153.92  113.11\n",
      "   114.56  169.97  317.17  307.25  436.88  472.36  523.69  439.04  389.97\n",
      "   370.43  241.68  197.81  118.52   41.68  115.92  160.76  269.39  325.97\n",
      "   369.57  378.1   356.87  335.29  316.35]\n",
      " [2047.3  2615.58 1882.01 1934.87 1876.42 1181.83  431.09  213.33  188.26\n",
      "  1561.06  502.04 3085.54 3213.28 2719.   2677.33 2275.87 2393.48 2424.63\n",
      "  2713.05 2151.1  1017.25  477.33  306.5   141.13   81.94   64.3    94.25\n",
      "    48.66   31.77  167.03  237.94  308.53]]\n",
      "(90,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)\n",
    "print(X_test)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 2, 1, 2, 2, 1, 1, 0, 1, 0, 0, 2, 2, 2, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 2, 1, 1, 2, 2, 1, 2, 0, 2, 0, 1, 1, 1, 0, 1, 0, 0, 2, 2,\n",
       "       2, 0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 1, 1, 2, 0, 2, 1, 2, 0, 2, 1, 2,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 0, 2, 0, 2, 2, 0, 2, 1, 2, 0, 0, 0, 1, 2,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1092.63, 1705.31, 2309.56, 3803.11, 5792.62, 5792.62, 5792.62,\n",
       "        5792.62, 5434.79, 5173.3 , 4662.33, 3211.6 , 2037.58, 1022.57,\n",
       "         794.06,  898.66,  864.78,  769.36,  747.19,  835.31,  640.82,\n",
       "         410.83,  411.76,  787.62,  380.06,  329.46,  455.41,  404.18,\n",
       "         306.83,  306.73,  471.04,  347.55],\n",
       "       [1701.47, 1604.48, 1157.15, 1103.53,  193.02,  332.71,  367.69,\n",
       "         281.27,  295.26,  296.19,  274.33,  281.1 ,  132.73,  197.96,\n",
       "         201.58,  198.8 ,  122.18,  153.57,  137.97,   43.62,   63.46,\n",
       "          67.99,   80.33,   59.78,   48.34,   36.61,   31.11,   48.69,\n",
       "          26.03,   43.56,   57.2 ,   42.9 ],\n",
       "       [1272.09, 1428.49, 2150.49, 1593.85, 1828.63, 1998.09,  757.68,\n",
       "         465.55,  317.34,  351.11,  890.19,  421.87,  300.  , 1604.99,\n",
       "        1506.88, 1912.38, 1687.22, 1819.75, 1840.99, 1705.02, 1281.86,\n",
       "        1169.61, 1070.86,  855.13,  518.07,  399.11,  311.58,  262.98,\n",
       "         245.87,  189.47,  210.14,  267.5 ],\n",
       "       [2839.23, 2885.9 , 3316.22, 5013.26, 4524.79, 4979.87, 3519.39,\n",
       "        4665.28, 2304.54, 2951.92, 2214.16, 2071.49, 1641.25, 1065.42,\n",
       "         959.24, 1083.05, 1126.97, 1095.14,  997.5 ,  975.16,  948.65,\n",
       "         925.67,  913.07,  890.3 ,  824.29,  794.59,  741.52,  710.75,\n",
       "         663.36,  700.44,  633.27,  606.72],\n",
       "       [1157.17, 1576.82, 1116.45, 1252.34, 1369.68, 1178.02, 1271.77,\n",
       "         940.87, 1019.04, 1046.5 ,  542.09,  724.97,  792.3 ,  958.75,\n",
       "        1045.16, 1081.73, 1097.6 , 1063.26,  935.05,  773.  ,  554.87,\n",
       "         396.02,  214.89,   63.66,   65.92,  191.9 ,  208.63,  217.17,\n",
       "         287.43,  341.03,  355.89,  352.87],\n",
       "       [1370.57, 1930.87, 1178.98, 1196.08,  486.57,  393.83, 1027.83,\n",
       "        1251.86, 1026.55, 1023.39,  900.18, 1027.11,  821.48,  992.11,\n",
       "         547.32,  436.31,  272.  ,  218.63,  194.53,  139.63,  119.01,\n",
       "         118.83,   81.25,   36.2 ,   43.44,   34.45,   43.52,   51.58,\n",
       "         316.78,  114.72,  181.4 ,  168.89],\n",
       "       [2020.39, 1934.98, 1811.79, 1473.21, 1809.56,  812.53,  855.83,\n",
       "         628.6 ,  594.84,  666.26,  595.56,  672.  ,  495.62,  544.59,\n",
       "         366.25,  326.28,  308.18,  132.43,   84.18,   82.13,   28.49,\n",
       "          62.6 ,   66.58,  143.22,  125.97,  179.02,   51.91,   81.08,\n",
       "         144.02,  272.93,  454.68,  545.25],\n",
       "       [4452.07, 3760.89, 2768.68, 2801.27, 2084.34, 1570.77, 1380.21,\n",
       "         811.77,  719.93,  305.36,  182.52,  364.44,  497.61,  861.03,\n",
       "        1091.37, 1385.22, 1474.78, 1639.56, 1722.31, 1738.64, 1768.81,\n",
       "        1677.32, 1677.58, 1682.04, 1669.21, 1631.06, 1616.43, 1627.5 ,\n",
       "        1573.27, 1620.56, 1613.7 , 1646.77],\n",
       "       [1190.53,  698.37,  577.61,  398.22,  635.05,  637.08, 1556.98,\n",
       "        2566.52, 2752.97, 1873.47, 1157.35, 1973.91, 1275.33, 1106.12,\n",
       "        1134.78,  684.75,  655.45,  599.75,  503.05,  434.68,  331.46,\n",
       "         211.3 ,  211.37,  118.44,   87.14,   59.98,   49.61,   55.21,\n",
       "          43.13,   68.3 ,   64.24,   90.75],\n",
       "       [1336.74,  999.89,  965.37,  933.35,  742.75,  932.98,  790.3 ,\n",
       "         859.07,  952.51, 1018.5 , 1135.87,  421.55,  220.72,  223.91,\n",
       "         186.47,  165.45,  121.55,  157.76,  158.49,  131.99,  125.05,\n",
       "         201.17,  160.12,  122.49,   75.35,   69.27,   39.9 ,   27.93,\n",
       "          64.73,   15.66,   96.35,  127.21],\n",
       "       [1126.43, 2458.21, 3088.89, 1754.39, 1596.7 ,  893.3 ,  538.27,\n",
       "         299.92,  325.43,  503.46,  695.89,  445.92,  494.57, 1825.46,\n",
       "         897.31, 2429.64, 2566.55, 2429.64, 2349.17, 2134.39, 2365.05,\n",
       "        1492.96, 1488.74, 1195.13,  527.67,  405.32,  252.03,  229.84,\n",
       "         141.56,  136.79,   99.04,   31.63],\n",
       "       [1746.25, 2267.42, 2813.36, 2585.33, 2727.66, 1927.58, 2165.79,\n",
       "        1282.46,  456.62,  410.25,  321.69,  375.17,  228.01,  155.6 ,\n",
       "         153.15,   73.72,   40.04,  105.78,   59.04,   76.05,  141.73,\n",
       "         125.86,  172.31,   73.08,  124.48,  123.27,  110.75,  152.64,\n",
       "         134.47,  169.47,  122.22,  106.07],\n",
       "       [1473.04, 2135.31, 1910.89, 2114.22, 1961.72, 2522.15, 2194.22,\n",
       "        1581.67, 2193.98, 2572.83, 2253.28,  680.13,  309.38,  236.37,\n",
       "         120.15,  137.79,   69.64,   63.77,   24.43,   25.14,  101.39,\n",
       "          68.47,   91.94,   92.24,   87.96,  114.5 ,  130.06,  123.63,\n",
       "         152.26,  102.42,  157.31,  113.33],\n",
       "       [1106.18, 1386.72, 2259.67, 2440.8 , 2449.16, 2031.63, 2234.83,\n",
       "        2919.23, 2554.02, 2683.38, 3417.36, 1928.26, 2935.28, 1318.27,\n",
       "        1520.08,  916.07,  527.62,  339.76,  613.02,  704.16,  677.24,\n",
       "         791.79,  974.99,  960.92,  686.92,  342.29,  270.48,  146.  ,\n",
       "          66.11,   78.51,  117.38,   44.59],\n",
       "       [1040.32, 1039.67, 1429.94,  592.01,  108.56,  242.89,  225.16,\n",
       "         192.67,  162.22,  115.96,  144.87,   79.85,  136.44,  185.62,\n",
       "         464.95,  678.56,  971.29, 1042.72,  819.16,  646.56,  239.12,\n",
       "          74.8 ,  146.5 ,  271.19,  423.33,  467.39,  527.51,  538.42,\n",
       "         559.9 ,  607.68,  571.61,  560.1 ],\n",
       "       [ 900.46,  746.39, 1011.66,  859.94,  994.34,  692.78,  804.72,\n",
       "        1106.37,  562.37,  435.  ,  169.87,   92.36,   51.74,   29.09,\n",
       "          83.58,   64.66,  127.8 ,   91.24,  180.63,  229.71,  226.44,\n",
       "         279.71,  207.55,  323.94,  321.35,  273.28,  214.52,  213.35,\n",
       "         175.24,  176.02,  139.45,  148.63],\n",
       "       [1772.07, 1684.1 , 1680.55, 1293.09, 1428.06, 1516.  , 1152.23,\n",
       "         579.54,  232.98,  249.93,  161.65,   56.22,  125.41,  285.57,\n",
       "         316.71,  344.95,  412.25,  448.83,  394.29,  330.67,  161.01,\n",
       "          72.22,  206.84,  293.28,  457.19,  610.38,  677.66,  643.77,\n",
       "         666.45,  681.42,  655.12,  685.59],\n",
       "       [1697.65, 2710.63, 4130.8 , 4160.69, 3536.96, 2692.4 , 3395.88,\n",
       "        3730.16, 3220.27, 2807.  , 2042.07, 1551.33,  368.35,  374.45,\n",
       "         286.03,  223.93,  111.26,   64.99,   73.12,   72.51,   38.04,\n",
       "          55.93,   46.42,   52.69,   47.05,  103.73,  187.1 ,  292.07,\n",
       "         226.52,  309.43,  282.88,  360.33],\n",
       "       [1203.11,  863.69, 1204.52, 2900.35, 4208.86, 3707.16, 2551.75,\n",
       "        2435.23, 2834.12, 3083.7 , 2989.82, 2519.4 , 1125.09, 1234.6 ,\n",
       "         506.15,  907.24,  625.56,  595.15,  576.4 ,  528.6 ,  494.65,\n",
       "         466.59,  425.61,  371.4 ,  352.19,  373.32,  287.12,  338.97,\n",
       "         304.77,  337.61,  336.25,  355.33],\n",
       "       [1383.7 , 1383.92,  688.62,  292.16,  446.6 ,  352.37,  229.69,\n",
       "         830.87, 2308.64, 1799.48, 1940.85, 1839.61, 1567.81, 1620.12,\n",
       "        1525.48, 1855.76,  667.74,  431.41,  378.86,  325.36,  511.19,\n",
       "         627.26,  781.13,  739.3 ,  744.33,  627.59,  485.54,  523.42,\n",
       "         268.68,  193.34,   74.07,   83.84],\n",
       "       [2174.24, 1957.3 , 1929.83, 1944.8 , 2510.42, 1408.42,  954.84,\n",
       "         361.62,  188.23,  137.1 ,  125.24,  124.44,  122.78,  184.81,\n",
       "         155.22,  212.59,  256.7 ,  354.88,  309.17,  355.04,  296.27,\n",
       "         232.51,  167.56,  118.97,  126.48,   49.83,   31.98,   23.96,\n",
       "          44.75,   66.95,  109.7 ,  128.34],\n",
       "       [1254.11, 1862.83, 2096.15, 1975.95, 1343.04, 1705.81, 1671.43,\n",
       "        2581.53, 2362.86, 2226.24, 1538.17, 1299.3 , 1173.87,  625.86,\n",
       "         363.8 ,  355.52,  318.38,  361.31,  364.06,  398.96,  425.59,\n",
       "         381.61,  473.29,  467.33,  478.81,  393.96,  360.8 ,  294.68,\n",
       "         234.77,  266.11,  119.91,  184.54],\n",
       "       [1381.49, 1914.37, 1367.61, 1276.61, 1028.27, 1078.  , 1105.64,\n",
       "        1558.86, 1801.7 , 1734.02, 1694.12, 1294.11,  742.21,  284.85,\n",
       "         279.27,  161.25,  135.72,  115.82,  112.11,  189.12,  131.1 ,\n",
       "         182.88,  118.16,  113.86,   71.8 ,   27.09,   24.6 ,   84.47,\n",
       "         102.7 ,  102.84,  156.28,  137.1 ],\n",
       "       [1058.47,  547.59,  538.12,  984.75,  435.86,  486.29,  487.34,\n",
       "         524.91,  569.49,  510.85,  596.64,  576.26,  489.65,  499.98,\n",
       "         394.54,  386.89,  394.07,  301.57,  389.85,  444.86,  512.66,\n",
       "         542.01,  494.7 ,  455.85,  381.05,  328.88,  193.92,  137.25,\n",
       "         138.62,  382.97,  520.02,  820.25],\n",
       "       [1456.44, 1552.69, 1612.04, 1496.51, 1681.22, 1342.14,  967.14,\n",
       "         464.14,  489.51,  550.01,  342.19,  285.38, 1100.19, 2141.43,\n",
       "        2094.12, 2010.48, 2058.94, 2494.97, 2170.04, 1870.41, 2133.64,\n",
       "        1706.28, 1193.17,  650.05,  642.07,  453.2 ,  284.79,  165.14,\n",
       "         183.05,  105.11,   52.25,   74.82],\n",
       "       [ 971.11,  548.  ,  866.94, 1090.21, 1987.67, 1336.14, 1428.83,\n",
       "        1315.12, 1811.51, 2607.4 , 1046.03, 1190.66, 1136.21,  650.67,\n",
       "         651.72,  403.54,  281.65,  250.65,  202.09,  183.79,  201.02,\n",
       "         142.91,  129.72,  157.37,  136.6 ,  125.51,   69.27,  137.57,\n",
       "         212.68,  130.42,  167.53,  230.51],\n",
       "       [ 955.2 , 1518.93, 1844.97, 2154.18, 1969.15, 2082.93,  412.1 ,\n",
       "         319.69,  200.3 ,  439.6 ,  288.26,  188.18,  280.55, 1878.8 ,\n",
       "        2367.31, 1806.74, 1648.26, 1614.76, 1751.7 , 1636.73, 1774.94,\n",
       "        1520.97,  639.47,  607.24,  306.07,  209.95,  248.9 ,  324.11,\n",
       "         349.21,  341.03,  358.47,  359.6 ],\n",
       "       [1225.41, 1414.44, 1209.9 , 1125.84,  685.32,  431.71,  280.08,\n",
       "         257.32,  253.7 , 1005.42,  332.08,  913.96, 1853.22, 2100.52,\n",
       "        2107.92, 2219.46, 2119.82, 1942.88, 1945.33,  956.3 ,  363.7 ,\n",
       "         307.11,  275.18,  177.97,   88.64,   86.92,   93.71,  108.5 ,\n",
       "         152.09,  162.29,  221.99,  194.36],\n",
       "       [1309.97,  772.03,  976.8 ,  864.42,  834.7 ,  774.01,  733.84,\n",
       "         699.06, 1210.75,  551.35,  516.43,  367.68,  159.96,   51.54,\n",
       "         110.42,  140.75,  107.72,  197.78,  227.01,  181.64,  134.25,\n",
       "         143.2 ,   99.67,  140.55,   95.56,   77.28,   28.38,   72.5 ,\n",
       "         118.94,   88.51,  134.15,   89.66],\n",
       "       [2196.02, 1614.72, 2014.25, 1782.18, 1773.88, 1407.73,  890.5 ,\n",
       "         773.38,  640.2 ,  664.87,  576.04,  596.59,  593.6 ,  535.83,\n",
       "         524.14,  462.2 ,  416.78,  361.91,  340.29,  159.17,  110.51,\n",
       "          36.59,   34.75,   36.81,   40.38,   44.21,  114.5 ,  183.15,\n",
       "         144.47,  100.  ,   29.95,   35.05],\n",
       "       [3910.08, 2839.5 , 2624.48, 2698.08, 2016.59, 1894.75, 1247.53,\n",
       "         677.33,  819.4 ,  821.63, 1162.08,  974.76,  405.02,  262.03,\n",
       "         167.9 ,  192.18,  157.41,  347.42,  577.81,  506.28,  330.31,\n",
       "         224.61,   96.64,  117.99,  586.75,  581.64,  357.21,  586.48,\n",
       "         779.  ,  929.99, 1122.05, 1550.88],\n",
       "       [1497.1 , 1899.44, 2310.74, 3234.93, 2657.64, 2105.04, 2724.92,\n",
       "        2773.43, 1923.63, 1635.4 , 1221.31,  525.15,  464.97,  441.41,\n",
       "         624.21,  621.27,  662.16,  583.35,  415.52,  460.4 ,  360.69,\n",
       "         377.05,  290.53,  262.76,  307.87,  248.29,  288.74,  226.83,\n",
       "         299.75,  337.08,  350.38,  427.3 ],\n",
       "       [ 946.78, 1316.92, 1334.38, 1486.01, 1462.35, 1853.13, 1984.72,\n",
       "        1922.5 , 2292.35, 1507.64, 1668.12,  897.41,  244.69,  221.07,\n",
       "         131.99,  115.24,  151.7 ,  119.76,  123.  ,  163.61,  135.76,\n",
       "         197.66,  115.77,  117.11,  106.73,  100.9 ,   77.32,   39.45,\n",
       "          39.34,   84.16,   26.2 ,   79.25],\n",
       "       [2536.21, 2000.64, 1885.9 , 1820.49, 1297.12,  773.23,  694.56,\n",
       "         612.7 ,  729.69,  806.57,  723.47,  601.56,  661.57,  630.04,\n",
       "         533.52,  502.3 ,  490.99,  484.33,  492.4 ,  323.  ,  384.26,\n",
       "         382.91,  372.36,  395.08,  430.64,  396.8 ,  421.82,  397.49,\n",
       "         362.5 ,  282.5 ,  200.83,  145.01],\n",
       "       [1590.66,  852.65,  720.14, 1089.52, 1329.08, 1210.19, 1160.7 ,\n",
       "        1134.81, 1569.64, 1749.99, 1280.49, 1217.51, 1836.92, 1512.74,\n",
       "         318.86,  181.5 ,  235.1 ,  223.85,  298.69,  299.02,  261.85,\n",
       "         243.97,  169.32,  146.3 ,   68.11,   67.89,   69.97,   36.82,\n",
       "          46.37,   81.07,   59.99,   98.58],\n",
       "       [1029.62,  881.7 ,  684.41,  331.6 ,  324.65,  168.38,  130.56,\n",
       "        1338.27,  289.8 ,  319.29, 1456.51, 1233.44, 1039.39, 1145.13,\n",
       "        1402.4 , 1433.32, 1381.08, 1761.93,  715.45,  395.86,  185.57,\n",
       "         192.77,   93.89,   89.34,   43.51,  102.74,   63.53,   33.96,\n",
       "          77.45,   33.12,   37.03,   28.16],\n",
       "       [1273.33, 2216.17, 2729.05, 3070.46, 2549.1 ,  822.07,  379.33,\n",
       "         443.38,  943.98,  815.4 ,  305.38,  285.  , 2269.3 , 3724.59,\n",
       "        2894.08, 2369.44, 2786.72, 2549.5 , 2479.45, 2533.74, 2762.33,\n",
       "        2656.43, 1950.18, 2193.19, 1285.67,  832.89,  354.93,  399.84,\n",
       "         214.42,  195.  ,  249.76,  270.79],\n",
       "       [1149.81, 1410.77, 2703.16, 2991.58, 5092.58, 3709.15, 1610.31,\n",
       "         957.58,  456.49,  505.18,  452.88,  477.29,  526.07,  861.77,\n",
       "        2594.23, 1995.28, 1950.98, 1785.51, 1798.6 , 1666.37, 1750.78,\n",
       "        1730.95, 1189.85, 1554.02, 1000.12,  712.69,  550.26,  340.9 ,\n",
       "         367.17,  339.97,  285.54,  328.92],\n",
       "       [1682.99, 2819.06, 2827.32, 3677.63, 3436.19, 1475.27,  799.83,\n",
       "         644.23,  544.76,  538.84,  485.67,  480.78,  461.45,  537.95,\n",
       "         480.98,  358.25,  388.59,  367.09,  387.7 ,  262.26,  251.02,\n",
       "         153.67,  111.51,   35.22,   52.9 ,   89.64,  105.87,   86.23,\n",
       "          48.43,   47.06,   58.48,  121.11],\n",
       "       [1147.79, 3776.73, 5577.25, 2802.51, 1355.76,  404.21,  381.76,\n",
       "         808.06,  605.88,  383.46, 1993.18, 3430.6 , 2273.24, 3957.9 ,\n",
       "        5339.28, 2351.28, 2779.4 , 1298.55,  462.9 ,  351.66,  376.45,\n",
       "         299.02,  196.58,  144.56,  153.94,  110.23,  105.76,   70.39,\n",
       "         146.03,  130.37,  166.  ,  170.83],\n",
       "       [1587.04, 2052.1 , 1653.95, 1743.73, 2086.04, 2393.34, 1895.58,\n",
       "        1685.35, 1784.51,  808.76,  581.46,  487.29,  344.74,  199.22,\n",
       "          41.75,  322.61,  698.5 , 1379.3 , 1726.61, 2221.08, 2283.18,\n",
       "        1994.71, 1461.49,  341.02,  415.12, 1660.37, 2274.91, 2810.95,\n",
       "        2836.11, 2789.96, 2565.45, 1865.46],\n",
       "       [1947.61, 1480.86, 2014.69, 1790.23, 1968.18, 1934.28, 1763.92,\n",
       "        1782.94, 1714.01, 1842.78, 1973.17,  997.44,  442.74,  315.84,\n",
       "         276.58,  284.03,  153.86,   55.21,   41.  ,   78.5 ,   56.23,\n",
       "          23.53,   40.54,   43.99,   43.79,   45.08,   88.38,   63.63,\n",
       "          35.67,   79.05,   55.31,   63.97],\n",
       "       [1379.14, 1022.85,  555.53,  562.75,  417.5 ,  767.49,  774.54,\n",
       "         728.75,  829.87,  740.42, 1318.58, 1120.62,  673.77,  687.33,\n",
       "         216.26,  108.72,   47.82,  106.49,   92.43,  165.5 ,  117.66,\n",
       "         105.77,  124.51,   98.85,   74.38,   91.98,   62.29,  103.21,\n",
       "         100.97,   55.68,   54.11,   72.19],\n",
       "       [1016.76,  676.36,  742.09,  878.85,  834.79,  697.96,  529.96,\n",
       "         218.78,  155.4 ,   39.47,  108.77,   36.35,   36.28,   60.34,\n",
       "          38.16,   60.2 ,   32.31,  110.67,  133.28,  109.1 ,  169.5 ,\n",
       "         270.52,  193.41,  338.99,  312.77,  302.85,  274.78,  212.95,\n",
       "         220.84,  357.64,  325.22,  271.44],\n",
       "       [1161.03, 1756.71, 1675.02, 1422.2 , 1787.37, 1636.56, 1346.66,\n",
       "        1440.76, 1259.35,  914.99,  814.63,  514.51,  348.56,  476.08,\n",
       "         528.25,  568.05,  777.05,  883.65,  897.78, 1059.89, 1013.13,\n",
       "         982.55,  954.29,  884.23,  787.13,  733.41,  678.3 ,  598.26,\n",
       "         453.37,  461.02,  319.  ,  273.7 ],\n",
       "       [1146.01, 1995.76, 2296.28, 2049.1 , 1824.41, 1645.48, 1476.07,\n",
       "        1203.49, 1152.23,  550.19,  211.05,  243.95,  253.25,  235.52,\n",
       "         245.71,  153.87,  158.24,  123.83,  103.55,   39.04,   46.68,\n",
       "          34.29,   51.92,   31.7 ,  120.94,   75.64,   96.03,   42.45,\n",
       "          76.5 ,  174.95,  163.42,  119.99],\n",
       "       [2639.51, 1239.41, 1757.19, 1118.38, 1141.75, 1163.88, 1112.83,\n",
       "         892.18,  804.58,  597.92,  459.47,  265.68,  114.08,   92.59,\n",
       "         234.76,  438.38,  574.8 ,  707.55,  778.68,  886.12,  858.49,\n",
       "         951.52,  985.44, 1063.91, 1046.76, 1047.3 , 1040.83, 1000.63,\n",
       "        1012.91,  930.66,  897.32,  872.06],\n",
       "       [2125.58, 3428.97, 2668.43, 1679.88, 1849.76, 1565.09, 1482.71,\n",
       "        1297.92, 1088.31, 1011.97,  836.72,  653.9 ,  507.56,  473.94,\n",
       "         784.37, 1130.69,  976.07, 1076.41, 1327.08, 1298.05,  585.35,\n",
       "         512.63,  471.85,  269.75,  153.38,  149.34,  142.7 ,  140.54,\n",
       "         157.46,   83.75,  133.96,  134.33],\n",
       "       [3142.86, 1828.87, 2438.97, 2334.24, 2380.31, 1832.94, 1752.32,\n",
       "        1679.69, 1922.3 , 1299.37, 1238.42, 1179.1 ,  989.32, 1355.16,\n",
       "         665.85,  259.33,  115.2 ,  171.25,   89.88,   82.56,   78.11,\n",
       "          48.25,   46.32,   25.49,   25.12,   23.01,   43.21,   37.83,\n",
       "          60.48,   23.3 ,   49.12,   31.53],\n",
       "       [1425.8 , 1967.26, 1876.71, 1664.03, 1637.34, 1926.64, 1879.12,\n",
       "        2137.66, 2264.91, 1913.41, 2364.48, 1751.52, 1115.2 ,  429.7 ,\n",
       "         731.07,  290.91,  279.76,  111.72,  141.92,   57.68,  106.1 ,\n",
       "          87.93,   49.59,   29.49,   27.53,   31.86,   58.17,   50.9 ,\n",
       "          83.28,   32.99,   87.06,   66.76],\n",
       "       [1668.07, 1578.33, 1394.88, 1186.03,  966.2 ,  814.75,  643.52,\n",
       "         357.53,  343.39,  362.36,  388.27,  401.01,  508.91,  405.37,\n",
       "         418.99,  437.81,  337.04,  264.75,  266.96,  237.  ,  187.79,\n",
       "         272.05,  261.38,  269.93,  233.88,  211.83,  178.32,  195.16,\n",
       "         196.67,  165.39,  149.39,  137.24],\n",
       "       [1154.21, 1384.52, 1228.44,  718.06,  386.54,  132.36,  190.34,\n",
       "         155.16,  133.6 ,   86.23,  100.03,  180.23,  137.56,  171.56,\n",
       "         158.5 ,  238.93,  219.29,  231.99,  230.58,  247.56,  288.73,\n",
       "         310.92,  306.84,  291.51,  323.8 ,  344.81,  344.3 ,  364.72,\n",
       "         402.99,  427.75,  435.88,  450.26],\n",
       "       [1507.49,  286.29, 1796.27, 2026.19, 2462.89, 2378.04, 2417.82,\n",
       "        2195.94, 3104.65, 1985.89, 2356.15, 1849.93, 1540.67, 1422.74,\n",
       "        1287.44, 1204.72,  780.19,  551.62,  475.16,  400.27,  406.55,\n",
       "         392.76,  337.7 ,  433.69,  408.83,  365.14,  392.92,  438.67,\n",
       "         336.05,  299.56,  302.02,  107.81],\n",
       "       [1529.05, 1187.21,  946.9 ,  434.38,  315.5 ,  307.14,  315.79,\n",
       "         349.99,  297.23,  407.31,  443.35,  404.18,  446.91,  497.35,\n",
       "         449.53,  445.26,  427.16,  433.55,  462.22,  467.76,  454.07,\n",
       "         460.2 ,  438.14,  442.69,  454.61,  384.57,  353.13,  331.25,\n",
       "         372.27,  365.45,  368.31,  372.53],\n",
       "       [1053.33, 1951.81, 1385.09, 2069.59, 1710.22, 1733.97, 1808.31,\n",
       "        1446.17, 2148.94, 1830.25, 2308.91, 1517.57, 2538.1 , 1811.62,\n",
       "        2391.06, 1736.18,  430.56,  340.94,  369.76,  424.57,  399.3 ,\n",
       "         447.02,  364.48,  445.27,  324.72,  364.55,  360.6 ,  300.31,\n",
       "         305.26,  285.93,  254.44,  288.47],\n",
       "       [1181.53, 1562.56, 2331.63, 1884.42, 1472.77,  973.11,  720.3 ,\n",
       "         314.92,   96.14,  137.94,  874.53,  701.61,  385.51,  275.69,\n",
       "        1538.43, 1684.68, 2295.18, 2173.01, 2499.15, 1898.26, 2033.91,\n",
       "        1503.55,  737.08,  693.48,  637.56,  404.93,  301.69,  251.52,\n",
       "         139.78,  159.32,  113.83,   35.63],\n",
       "       [1092.69, 1188.97, 1006.34, 1286.09, 1039.97,  436.96,  769.04,\n",
       "        1042.24,  638.98,  524.52,  347.07,  416.25,  420.72,  255.58,\n",
       "         332.15,  198.88,  245.57,  141.46,  141.92,  205.68,  175.54,\n",
       "         240.27,  244.37,  333.81,  323.37,  350.39,  393.62,  328.73,\n",
       "         403.24,  449.47,  544.67,  615.29],\n",
       "       [1398.58, 1598.64, 3130.16, 4341.44, 3707.34, 4104.54, 2616.32,\n",
       "        2078.89, 1744.17, 1684.  , 1602.77, 1338.  ,  491.86,  447.15,\n",
       "         441.73,  590.41,  628.18,  374.11,  494.31,  387.42,  400.84,\n",
       "         339.39,  298.64,  298.84,  265.6 ,  262.73,  197.33,   79.92,\n",
       "          44.99,   90.87,   67.65,   67.36],\n",
       "       [1649.02, 1464.62,  913.96,  928.75,  401.29,  260.13,  285.68,\n",
       "         253.72,  314.22,  385.07,  386.46,  462.89,  524.19,  468.13,\n",
       "         402.8 ,  323.76,  295.94,  197.64,   98.44,   62.97,  229.19,\n",
       "         165.9 ,  269.29,  215.75,  240.68,  226.4 ,  204.56,  159.46,\n",
       "         128.22,  240.74,  240.03,  272.48],\n",
       "       [3545.91, 2880.23, 1764.21, 1654.36, 1831.8 , 1582.65, 1273.07,\n",
       "        1601.51, 1247.63, 1045.11,  509.59,  319.5 ,  304.3 ,  277.16,\n",
       "         382.86,  518.79,  429.94,  464.61,  683.88,  656.65,  542.26,\n",
       "         484.58,  527.96,  484.89,  473.98,  330.4 ,  349.9 ,  378.37,\n",
       "         375.36,  442.96,  346.99,  360.02],\n",
       "       [1170.65, 1569.12, 1278.54, 1234.71, 1222.01, 1373.05, 1151.02,\n",
       "        1053.77,  692.9 ,  276.57,  258.77,  202.17,  148.52,  126.7 ,\n",
       "         116.39,  145.04,  148.55,  166.53,  156.58,   83.82,  138.88,\n",
       "          28.81,   97.71,  118.39,   60.53,  154.34,   60.41,   37.78,\n",
       "         111.43,   47.32,   77.58,   74.13],\n",
       "       [1634.01, 1802.79, 1972.71, 3114.11, 5792.62, 4405.89, 5792.62,\n",
       "        4662.61, 3941.35, 3832.2 , 3873.12, 3116.74, 2059.34, 1699.37,\n",
       "        1408.84, 1105.99, 1017.68, 1049.38,  976.27,  825.78,  627.41,\n",
       "         655.15,  313.46,  252.57,  190.25,  142.49,  116.59,   97.78,\n",
       "         117.17,  222.4 ,  200.14,  100.01],\n",
       "       [1156.17, 1175.65, 1282.99, 1250.42, 1557.85, 1625.95, 1701.15,\n",
       "        1691.69, 1730.17, 1549.94, 1988.96, 1816.63, 1729.78, 1879.03,\n",
       "        1473.02, 1825.67,  891.57,  823.41,  403.37,  198.48,  180.77,\n",
       "          89.78,  165.24,  172.22,  228.59,  252.59,  187.21,  182.94,\n",
       "         185.59,  206.7 ,  297.45,  417.52],\n",
       "       [3759.69, 3560.12, 3173.46, 3431.46, 2676.15, 2049.18, 2081.37,\n",
       "        1541.08, 1074.15,  671.35,  552.79,  529.34,  508.46,  493.78,\n",
       "         358.74,  441.1 ,  450.06,  410.35,  433.6 ,  408.97,  379.06,\n",
       "         397.4 ,  386.25,  423.35,  401.36,  428.83,  378.37,  383.84,\n",
       "         341.94,  357.98,  325.55,  292.92],\n",
       "       [1362.27,  788.  ,  624.98,  762.2 ,  776.45,  834.05,  745.19,\n",
       "         762.14,  826.66,  733.38,  503.3 ,  531.76,  402.65,  254.  ,\n",
       "         190.83,  297.52,  420.83,  550.  ,  324.94,  219.28,  235.98,\n",
       "         288.51,  490.83,  733.92,  770.64,  468.67,  502.01,  447.67,\n",
       "         627.23,  767.05,  548.81,  662.47],\n",
       "       [ 957.2 ,  534.03,  729.72,  692.63,  853.72,  679.58,  736.84,\n",
       "         595.64,  341.71,  399.23,  358.21,  324.15,  322.23,  334.82,\n",
       "         336.38,  211.33,  237.04,  194.78,  241.83,  234.33,  147.47,\n",
       "          49.05,   52.29,   56.54,   50.93,   49.26,   54.58,   66.94,\n",
       "          76.26,   52.34,   74.26,  116.72],\n",
       "       [1588.98,  411.23, 2129.31, 2623.  , 3005.98, 2274.6 , 2022.07,\n",
       "        2317.92, 2535.52, 2858.05, 2057.35, 1754.01, 1874.25, 1051.41,\n",
       "         674.39,  297.68,  152.1 ,  125.94,   70.39,   69.71,   97.01,\n",
       "         360.68,   98.67,   78.77,   68.66,   75.47,   49.29,  120.63,\n",
       "          66.92,   68.64,   46.75,   54.36],\n",
       "       [1159.18,  623.  ,  379.17,  502.08,  373.96,  221.03,  233.61,\n",
       "          71.76,  170.89,  130.56,   95.16,   40.02,   99.76,  105.95,\n",
       "         119.35,   68.76,  123.  ,  198.98,  192.17,  139.9 ,   67.92,\n",
       "         148.05,  132.56,  166.07,  242.11,  272.71,  348.28,  398.64,\n",
       "         475.85,  458.94,  605.4 ,  659.83],\n",
       "       [1681.61, 1793.95, 1284.47,  688.31,  831.26,  326.18,  198.29,\n",
       "         185.82,   80.8 ,  120.12,   73.22,  118.04,  192.28,  194.27,\n",
       "         315.88,  310.18,  335.88,  414.11,  434.93,  338.98,  266.97,\n",
       "         308.11,  283.55,  275.53,  331.38,  402.95,  195.61,  377.56,\n",
       "         174.6 ,  100.59,   93.3 ,  162.98],\n",
       "       [2157.01, 1900.63, 1841.95, 1669.87,  573.81,  522.12,  383.12,\n",
       "         371.85,  375.23,  195.78,  198.83,  133.42,  132.83,   42.03,\n",
       "          38.31,   31.65,   22.59,   30.59,   68.55,  137.07,   96.33,\n",
       "         139.05,   61.61,   71.85,   73.7 ,   77.48,   27.99,   81.05,\n",
       "          45.09,   44.48,   28.12,   41.69],\n",
       "       [ 961.16, 1500.53, 1648.11, 1306.06,  684.13,  424.84,  453.69,\n",
       "         399.  , 1557.4 ,  493.59, 2007.78, 1870.8 , 1742.81, 1382.57,\n",
       "        1669.01, 1336.56, 1617.  , 1932.43, 1632.68,  679.47,  494.63,\n",
       "         406.66,  409.73,  318.57,  203.17,  148.04,  188.58,  180.18,\n",
       "         210.49,   76.3 ,  139.32,   77.4 ],\n",
       "       [1245.1 , 1740.58, 1063.22, 1478.88,  951.08, 1731.05, 1584.1 ,\n",
       "        1699.44,  643.71,  941.47,  519.61,  558.83,  516.74,  485.51,\n",
       "         482.49,  538.11,  585.46,  561.13,  578.99,  547.18,  591.06,\n",
       "         516.02,  441.57,  366.21,  297.74,  294.4 ,  133.44,   37.86,\n",
       "          65.18,  138.38,  235.35,  428.43],\n",
       "       [1310.22,  700.07,  621.88, 2597.61, 2384.14, 2145.32, 1998.82,\n",
       "        1589.24, 2369.79, 2008.17, 1881.86,  872.85,  505.14,  415.22,\n",
       "         282.8 ,  313.49,  291.94,  272.86,  254.69,  221.69,  267.75,\n",
       "         276.94,  257.25,  287.08,  311.48,  243.71,  232.84,  237.63,\n",
       "         210.5 ,  189.93,  201.69,  130.67],\n",
       "       [ 922.28, 1460.13, 2429.23, 1931.23, 2257.21, 1844.03, 1033.45,\n",
       "         662.27,  286.21,  178.5 , 1573.64,  653.02,  357.43,  741.17,\n",
       "        2516.44, 1995.8 , 2764.1 , 2952.01, 2158.43, 2023.61, 1753.27,\n",
       "        1841.23, 1638.75, 1587.67, 1203.04,  639.33,  462.61,  316.67,\n",
       "         313.84,  281.21,  252.79,  186.06],\n",
       "       [1983.56, 1507.67, 1800.98, 1340.65,  708.68,  628.28,  432.44,\n",
       "         410.45,  407.58,  380.48,  310.46,  282.98,  347.51,  259.82,\n",
       "         216.89,  140.48,  146.27,   75.1 ,  113.15,  117.24,   62.72,\n",
       "          85.5 ,   44.42,  171.38,  211.35,  254.52,  237.91,  477.81,\n",
       "         356.82,  252.43,  299.39,  190.09],\n",
       "       [2330.41, 2758.58, 1550.98, 1642.51, 1392.73,  878.39,  706.66,\n",
       "         552.98,  511.96,  602.86,  545.95,  639.37,  539.09,  475.2 ,\n",
       "         408.11,  452.36,  230.5 ,  250.33,  101.57,   43.09,   82.19,\n",
       "          67.89,  119.57,   53.28,   86.42,  282.89,  316.43,  479.42,\n",
       "         529.43,  652.71,  601.63,  608.1 ],\n",
       "       [ 970.42, 1347.88, 1604.85, 1692.95, 2126.52, 1706.02, 1840.21,\n",
       "        1411.25, 1180.59,  480.41,  423.58,  382.21,  313.15,  408.17,\n",
       "         266.84,  310.27,  177.97,  282.45,  147.68,  105.29,  127.17,\n",
       "          47.07,   57.27,   72.49,   74.17,   49.32,   88.21,  157.44,\n",
       "         131.62,  134.97,  142.65,  211.99],\n",
       "       [1502.82, 1379.8 , 1180.59,  928.76,  687.11,  716.78,  204.07,\n",
       "         102.1 ,  200.27,  287.47,  368.37,  444.76,  404.9 ,  349.54,\n",
       "         294.66,  186.8 ,  100.72,  134.21,  169.96,  149.33,  213.35,\n",
       "         212.35,  194.38,  220.81,  151.17,  124.85,   77.28,   34.4 ,\n",
       "          42.64,   39.76,   22.19,   26.09],\n",
       "       [1611.62, 2476.26, 4008.82, 3914.41, 2657.83, 2263.76, 1739.06,\n",
       "        2446.64, 1547.32, 1428.38,  967.11,  834.27,  424.25,  337.18,\n",
       "         331.82,  471.41,  538.77,  628.98,  735.48,  761.16,  808.73,\n",
       "         794.81,  721.37,  589.59,  541.88,  389.88,  340.38,  255.5 ,\n",
       "         197.73,  108.67,   83.51,   50.92],\n",
       "       [1779.89, 1791.35, 1494.59,  710.52,  355.31,  385.89,  228.2 ,\n",
       "         163.7 ,  214.51,  258.1 ,  294.98,  277.49,  290.84,  259.43,\n",
       "         325.53,  298.6 ,  293.89,  228.14,  153.42,   86.63,  130.9 ,\n",
       "         110.5 ,   89.41,   92.85,  116.69,   66.83,   60.72,   55.11,\n",
       "          86.3 ,   42.49,   64.93,   48.33],\n",
       "       [ 992.46,  619.03,  611.55,  698.45,  530.76,  368.28,  425.24,\n",
       "         374.29,   81.4 ,   63.62,   26.31,   43.05,   52.25,   37.99,\n",
       "          68.4 ,   17.81,   48.28,   68.  ,   84.44,   98.93,  133.35,\n",
       "         127.66,  172.49,  145.4 ,  179.1 ,  172.35,  160.66,  143.45,\n",
       "          45.22,   90.11,   64.85,   51.88],\n",
       "       [1189.34, 1430.91, 1472.26, 1462.33, 1230.77, 1544.84, 1608.64,\n",
       "        1328.68,  948.52,  881.85,  661.5 ,  618.82,  647.99,  569.41,\n",
       "         508.  ,  407.04,  283.44,  217.69,  122.92,   34.91,  112.49,\n",
       "         136.36,  204.74,  309.  ,  464.48,  581.21,  655.63,  731.96,\n",
       "         695.7 ,  665.08,  644.18,  708.09],\n",
       "       [1850.58, 1830.83, 1088.57,  932.79,  905.61,  743.19,  426.77,\n",
       "         417.78,  264.74,  260.3 ,  353.97,  387.05,  364.78,  352.  ,\n",
       "         448.64,  389.15,  731.43,  412.83,  413.25,  431.7 ,  312.07,\n",
       "         328.54,  328.77,  370.69,  358.04,  238.51,  106.44,  113.12,\n",
       "          44.1 ,   33.77,  131.91,  186.51],\n",
       "       [1140.16, 1431.42, 1232.82, 1323.84, 1527.77, 1672.49, 1960.84,\n",
       "        1558.77,  987.3 ,  420.21,  293.99,  388.24,  253.54,  145.35,\n",
       "         164.36,  198.24,  157.28,   79.14,   72.71,   77.94,   76.16,\n",
       "          67.24,  102.9 ,  109.09,  132.84,   92.51,   90.87,   70.63,\n",
       "          61.4 ,   41.56,   69.23,   30.52],\n",
       "       [1091.81, 1392.28, 1674.51, 2352.35, 1944.87, 2283.48, 2229.04,\n",
       "        2231.1 , 2023.52, 2927.07, 2462.96, 1781.6 ,  903.02,  468.96,\n",
       "         269.09,  121.57,  180.66,  114.31,   63.22,   50.05,  106.17,\n",
       "         104.06,  154.24,  206.14,  108.56,  142.22,  212.08,  208.66,\n",
       "         243.16,  360.43,  387.76,  490.73],\n",
       "       [1135.86, 1803.2 , 1824.75, 1946.16, 2328.1 , 1958.5 , 1250.09,\n",
       "         780.39,  343.95,  300.3 ,  219.78,  249.39,  104.48,   96.  ,\n",
       "          63.81,   63.05,   45.9 ,   59.17,   77.08,   51.34,  152.78,\n",
       "          81.42,  106.27,  171.49,  252.16,  156.79,  204.62,  164.39,\n",
       "         132.74,   95.88,   78.16,   69.8 ],\n",
       "       [2071.12, 2024.  , 1847.67, 1861.79, 2082.8 , 1632.09, 1617.32,\n",
       "         974.8 ,  480.78,  288.22,  424.97,  297.99,  305.07,  258.8 ,\n",
       "         322.48,  368.89,  325.04,  358.51,  375.98,  418.38,  414.76,\n",
       "         421.43,  462.35,  460.42,  498.89,  487.21,  464.8 ,  394.31,\n",
       "         367.86,  296.2 ,  253.96,  217.95],\n",
       "       [1316.75, 2332.31, 4383.06, 4273.09, 3310.93, 2674.73, 2462.37,\n",
       "        2718.98, 2777.68, 1618.11,  770.5 ,  551.36,  621.33,  736.94,\n",
       "         754.65,  711.61,  511.85,  336.05,  381.88,  265.13,  123.31,\n",
       "          53.46,   79.52,   53.64,   26.5 ,   41.86,   79.63,   40.03,\n",
       "          80.21,  106.23,  121.2 ,  142.93],\n",
       "       [1166.48,  631.56,  644.22,  461.22,  402.78,  365.2 ,  181.08,\n",
       "         153.92,  113.11,  114.56,  169.97,  317.17,  307.25,  436.88,\n",
       "         472.36,  523.69,  439.04,  389.97,  370.43,  241.68,  197.81,\n",
       "         118.52,   41.68,  115.92,  160.76,  269.39,  325.97,  369.57,\n",
       "         378.1 ,  356.87,  335.29,  316.35],\n",
       "       [2047.3 , 2615.58, 1882.01, 1934.87, 1876.42, 1181.83,  431.09,\n",
       "         213.33,  188.26, 1561.06,  502.04, 3085.54, 3213.28, 2719.  ,\n",
       "        2677.33, 2275.87, 2393.48, 2424.63, 2713.05, 2151.1 , 1017.25,\n",
       "         477.33,  306.5 ,  141.13,   81.94,   64.3 ,   94.25,   48.66,\n",
       "          31.77,  167.03,  237.94,  308.53]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Spotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 42\n",
    "scoring = 'f1_macro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "\n",
    "#models.append(('XGB', XGBClassifier(random_state=seed)))\n",
    "models.append(('GNB', GaussianNB(var_smoothing=2e-9)))\n",
    "models.append(('LR', LogisticRegression(random_state=seed)))\n",
    "models.append(('CART' , DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('SVC' , SVC(gamma=0.05, random_state=seed)))\n",
    "if n_labels == 5:\n",
    "    models.append(('RF', RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=250, \n",
    "                                                           max_features=9, criterion='entropy', max_depth=None,\n",
    "                                                           min_samples_split=4, min_samples_leaf=1\n",
    "                                                          )))\n",
    "\n",
    "else:\n",
    "    models.append(('RF', RandomForestClassifier(random_state=RANDOM_SEED, n_estimators=50, \n",
    "                                                           max_features=5, criterion='gini', max_depth=None,\n",
    "                                                           min_samples_split=4, min_samples_leaf=1\n",
    "                                                          )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB - 0,60 0,07\n",
      "LR - 0,52 0,08\n",
      "CART - 0,56 0,05\n",
      "SVC - 0,17 0,00\n",
      "RF - 0,68 0,08\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    # Dividere dati in n = num_folds\n",
    "    kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = np.array([])\n",
    "    for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "        X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "        X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "        model.fit(X_cross_train, y_cross_train)  \n",
    "        y_pred = model.predict(X_cross_test)\n",
    "        f1s = f1_score(y_cross_test, y_pred, average=\"weighted\")\n",
    "        cv_results = np.append(cv_results, [f1s])\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #msg = \"%s - %f - %f\" % (name, cv_results.mean(), cv_results.std())\n",
    "    msg = \"{} - {:.2f} {:.2f}\".format(name, cv_results.mean(), cv_results.std()).replace('.', ',')\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "fig.suptitle('Algorithms Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valutazione dei migliori algoritmi su test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione modelli sul Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model GNB: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.83      0.62        30\n",
      "           1       0.79      0.50      0.61        30\n",
      "           2       0.62      0.43      0.51        30\n",
      "\n",
      "    accuracy                           0.59        90\n",
      "   macro avg       0.64      0.59      0.58        90\n",
      "weighted avg       0.64      0.59      0.58        90\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model LR: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.57      0.56        30\n",
      "           1       0.74      0.57      0.64        30\n",
      "           2       0.53      0.63      0.58        30\n",
      "\n",
      "    accuracy                           0.59        90\n",
      "   macro avg       0.61      0.59      0.59        90\n",
      "weighted avg       0.61      0.59      0.59        90\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model CART: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.47      0.51        30\n",
      "           1       0.54      0.67      0.60        30\n",
      "           2       0.68      0.63      0.66        30\n",
      "\n",
      "    accuracy                           0.59        90\n",
      "   macro avg       0.59      0.59      0.59        90\n",
      "weighted avg       0.59      0.59      0.59        90\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model SVC: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        30\n",
      "           1       0.00      0.00      0.00        30\n",
      "           2       0.33      1.00      0.50        30\n",
      "\n",
      "    accuracy                           0.33        90\n",
      "   macro avg       0.11      0.33      0.17        90\n",
      "weighted avg       0.11      0.33      0.17        90\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model RF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.70      0.71        30\n",
      "           1       0.69      0.60      0.64        30\n",
      "           2       0.69      0.80      0.74        30\n",
      "\n",
      "    accuracy                           0.70        90\n",
      "   macro avg       0.70      0.70      0.70        90\n",
      "weighted avg       0.70      0.70      0.70        90\n",
      "\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def classification_report_csv(report, model_name):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    index = 0\n",
    "    row = lines[-4].split('    ')\n",
    "    accuracy = row[-2] if choosenIndex > 1 else row[-3]\n",
    "    for line in lines[2:-5]:\n",
    "        row = {}\n",
    "        row_data = line.split('      ')\n",
    "        row['class'] = labels[index]\n",
    "        row['precision'] = float(row_data[2]) \n",
    "        row['recall'] = float(row_data[3]) \n",
    "        row['f1_score'] = float(row_data[4])\n",
    "        row['accuracy'] = accuracy\n",
    "        report_data.append(row)\n",
    "        index += 1\n",
    "    dataframe = pd.DataFrame.from_dict(report_data)\n",
    "    dataframe.to_csv(tasks[choosenIndex] + \"/classificationReports/\" +'report' + model_name +  '.csv', index = False)\n",
    "    \n",
    "for name, model in models:\n",
    "    model.fit(X_train,  y_train)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    print(f\"Model {name}: \")\n",
    "    report = classification_report(y_test, pred_test)\n",
    "    print(report)\n",
    "    classification_report_csv(report, name)\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione Inferance Rate medio (|X_test| = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = read_csv(\"InfTimeReport.csv\")\n",
    "g = sbs.barplot(x=csv['Algoritmo'], y=csv['InfTime'])\n",
    "g.set_yscale(\"log\")\n",
    "plt.ylabel(\"Inference Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memoria occupata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = read_csv(\"MemOccupationReport.csv\")\n",
    "sbs.barplot(x=csv['Algoritmo'], y=csv['MemOccupata2'])\n",
    "plt.ylabel(\"MemOccupata in Byte\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1000 #if labels == 3 else 500\n",
    "BATCH_SIZE = 4\n",
    "learn_rate = 0.0001\n",
    "def getNetwork():\n",
    "    model = Sequential(name=\"Sequential-NN\")\n",
    "    model.add(layers.Dense(X.shape[1], activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Dense(np.unique(y).size, activation='softmax'))\n",
    "    opt = Adam(learning_rate=learn_rate)\n",
    "    # SGB\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 10,789\n",
      "Trainable params: 10,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "135/135 [==============================] - 0s 467us/step - loss: 1.6556 - accuracy: 0.1796\n",
      "Epoch 2/1000\n",
      "135/135 [==============================] - 0s 466us/step - loss: 1.5816 - accuracy: 0.2889\n",
      "Epoch 3/1000\n",
      "135/135 [==============================] - 0s 449us/step - loss: 1.5596 - accuracy: 0.2778\n",
      "Epoch 4/1000\n",
      "135/135 [==============================] - 0s 450us/step - loss: 1.5162 - accuracy: 0.3759\n",
      "Epoch 5/1000\n",
      "135/135 [==============================] - 0s 467us/step - loss: 1.4925 - accuracy: 0.3685\n",
      "Epoch 6/1000\n",
      "135/135 [==============================] - 0s 459us/step - loss: 1.4808 - accuracy: 0.3833\n",
      "Epoch 7/1000\n",
      "135/135 [==============================] - 0s 446us/step - loss: 1.4651 - accuracy: 0.3852\n",
      "Epoch 8/1000\n",
      "135/135 [==============================] - 0s 456us/step - loss: 1.4463 - accuracy: 0.3944\n",
      "Epoch 9/1000\n",
      "135/135 [==============================] - 0s 438us/step - loss: 1.4325 - accuracy: 0.4241\n",
      "Epoch 10/1000\n",
      "135/135 [==============================] - 0s 440us/step - loss: 1.4240 - accuracy: 0.3778\n",
      "Epoch 11/1000\n",
      "135/135 [==============================] - 0s 459us/step - loss: 1.4130 - accuracy: 0.4185\n",
      "Epoch 12/1000\n",
      "135/135 [==============================] - 0s 495us/step - loss: 1.3971 - accuracy: 0.4167\n",
      "Epoch 13/1000\n",
      "135/135 [==============================] - 0s 506us/step - loss: 1.3696 - accuracy: 0.4185\n",
      "Epoch 14/1000\n",
      "135/135 [==============================] - 0s 502us/step - loss: 1.3762 - accuracy: 0.4333\n",
      "Epoch 15/1000\n",
      "135/135 [==============================] - 0s 485us/step - loss: 1.3656 - accuracy: 0.4667\n",
      "Epoch 16/1000\n",
      "135/135 [==============================] - 0s 449us/step - loss: 1.3648 - accuracy: 0.4426\n",
      "Epoch 17/1000\n",
      "135/135 [==============================] - 0s 493us/step - loss: 1.3815 - accuracy: 0.4630\n",
      "Epoch 18/1000\n",
      "135/135 [==============================] - 0s 506us/step - loss: 1.3362 - accuracy: 0.4648\n",
      "Epoch 19/1000\n",
      "135/135 [==============================] - 0s 461us/step - loss: 1.3370 - accuracy: 0.4852\n",
      "Epoch 20/1000\n",
      "135/135 [==============================] - 0s 436us/step - loss: 1.3444 - accuracy: 0.4759\n",
      "Epoch 21/1000\n",
      "135/135 [==============================] - 0s 472us/step - loss: 1.3251 - accuracy: 0.4722\n",
      "Epoch 22/1000\n",
      "135/135 [==============================] - 0s 486us/step - loss: 1.3074 - accuracy: 0.4852\n",
      "Epoch 23/1000\n",
      "135/135 [==============================] - 0s 496us/step - loss: 1.3091 - accuracy: 0.4759\n",
      "Epoch 24/1000\n",
      "135/135 [==============================] - 0s 514us/step - loss: 1.3273 - accuracy: 0.4667\n",
      "Epoch 25/1000\n",
      "135/135 [==============================] - 0s 500us/step - loss: 1.3189 - accuracy: 0.4704\n",
      "Epoch 26/1000\n",
      "135/135 [==============================] - 0s 505us/step - loss: 1.3131 - accuracy: 0.4574\n",
      "Epoch 27/1000\n",
      "135/135 [==============================] - 0s 490us/step - loss: 1.2936 - accuracy: 0.5000\n",
      "Epoch 28/1000\n",
      "135/135 [==============================] - 0s 507us/step - loss: 1.3031 - accuracy: 0.4630\n",
      "Epoch 29/1000\n",
      "135/135 [==============================] - 0s 500us/step - loss: 1.2949 - accuracy: 0.4815\n",
      "Epoch 30/1000\n",
      "135/135 [==============================] - 0s 553us/step - loss: 1.2822 - accuracy: 0.4722\n",
      "Epoch 31/1000\n",
      "135/135 [==============================] - 0s 520us/step - loss: 1.2713 - accuracy: 0.5111\n",
      "Epoch 32/1000\n",
      "135/135 [==============================] - 0s 504us/step - loss: 1.3046 - accuracy: 0.4685\n",
      "Epoch 33/1000\n",
      "135/135 [==============================] - 0s 454us/step - loss: 1.2718 - accuracy: 0.4722\n",
      "Epoch 34/1000\n",
      "135/135 [==============================] - 0s 488us/step - loss: 1.2459 - accuracy: 0.5296\n",
      "Epoch 35/1000\n",
      "135/135 [==============================] - 0s 492us/step - loss: 1.2558 - accuracy: 0.5148\n",
      "Epoch 36/1000\n",
      "135/135 [==============================] - 0s 452us/step - loss: 1.2619 - accuracy: 0.4889\n",
      "Epoch 37/1000\n",
      "135/135 [==============================] - 0s 460us/step - loss: 1.2654 - accuracy: 0.4907\n",
      "Epoch 38/1000\n",
      "  1/135 [..............................] - ETA: 0s - loss: 1.3930 - accuracy: 0.5000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-2806a537579f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mX_cross_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cross_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cross_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cross_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cross_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpredictions_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0mepoch_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m                 \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 batch_size=batch_size):\n\u001b[0m\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0mtraining\u001b[0m \u001b[0mstep\u001b[0m \u001b[0mbeing\u001b[0m \u001b[0mtraced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \"\"\"\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m       \u001b[0;31m# Creating _pywrap_traceme.TraceMe starts the clock.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traceme\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pywrap_traceme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTraceMe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "\n",
    "kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "cv_results = np.array([])\n",
    "for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "    X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "    X_cross_train = scaler.fit_transform(X_cross_train)\n",
    "    X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "    X_cross_test = scaler.transform(X_cross_test)\n",
    "    model = getNetwork()\n",
    "    model.fit(X_cross_train, y_cross_train, epochs=EPOCHS, batch_size=BATCH_SIZE)  \n",
    "    y_pred = model.predict(X_cross_test)\n",
    "    predictions_categorical = np.argmax(y_pred, axis=1)\n",
    "    f1s = f1_score(y_cross_test, predictions_categorical, average=\"weighted\")\n",
    "    cv_results = np.append(cv_results, [f1s])\n",
    "\n",
    "print(f'Average score of Cross Validation: {cv_results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 10,275\n",
      "Trainable params: 10,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "68/68 [==============================] - 0s 4ms/step - loss: 308.0091 - accuracy: 0.3296 - val_loss: 97.0383 - val_accuracy: 0.4222\n",
      "Epoch 2/1000\n",
      "68/68 [==============================] - 0s 768us/step - loss: 219.1084 - accuracy: 0.3667 - val_loss: 74.3499 - val_accuracy: 0.5000\n",
      "Epoch 3/1000\n",
      "68/68 [==============================] - 0s 778us/step - loss: 219.1879 - accuracy: 0.3481 - val_loss: 64.4671 - val_accuracy: 0.5444\n",
      "Epoch 4/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 194.7016 - accuracy: 0.3963 - val_loss: 66.5045 - val_accuracy: 0.5667\n",
      "Epoch 5/1000\n",
      "68/68 [==============================] - 0s 758us/step - loss: 182.8297 - accuracy: 0.4037 - val_loss: 52.2275 - val_accuracy: 0.5778\n",
      "Epoch 6/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 153.4498 - accuracy: 0.4481 - val_loss: 41.6399 - val_accuracy: 0.5778\n",
      "Epoch 7/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 128.2272 - accuracy: 0.5000 - val_loss: 40.4114 - val_accuracy: 0.5778\n",
      "Epoch 8/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 142.2481 - accuracy: 0.4222 - val_loss: 45.0494 - val_accuracy: 0.5778\n",
      "Epoch 9/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 151.8981 - accuracy: 0.4222 - val_loss: 40.1536 - val_accuracy: 0.5667\n",
      "Epoch 10/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 146.6378 - accuracy: 0.4074 - val_loss: 38.9679 - val_accuracy: 0.5667\n",
      "Epoch 11/1000\n",
      "68/68 [==============================] - 0s 716us/step - loss: 134.8494 - accuracy: 0.4333 - val_loss: 36.9060 - val_accuracy: 0.5889\n",
      "Epoch 12/1000\n",
      "68/68 [==============================] - 0s 755us/step - loss: 110.4312 - accuracy: 0.4852 - val_loss: 30.0425 - val_accuracy: 0.6444\n",
      "Epoch 13/1000\n",
      "68/68 [==============================] - 0s 857us/step - loss: 126.0267 - accuracy: 0.4148 - val_loss: 34.8077 - val_accuracy: 0.5778\n",
      "Epoch 14/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 116.2366 - accuracy: 0.4370 - val_loss: 31.6750 - val_accuracy: 0.6333\n",
      "Epoch 15/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 105.2015 - accuracy: 0.4370 - val_loss: 30.5022 - val_accuracy: 0.7000\n",
      "Epoch 16/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 100.4576 - accuracy: 0.4889 - val_loss: 34.0373 - val_accuracy: 0.6444\n",
      "Epoch 17/1000\n",
      "68/68 [==============================] - 0s 722us/step - loss: 102.5646 - accuracy: 0.4889 - val_loss: 32.9514 - val_accuracy: 0.6333\n",
      "Epoch 18/1000\n",
      "68/68 [==============================] - 0s 758us/step - loss: 84.3271 - accuracy: 0.5111 - val_loss: 27.4428 - val_accuracy: 0.6556\n",
      "Epoch 19/1000\n",
      "68/68 [==============================] - 0s 783us/step - loss: 96.0533 - accuracy: 0.3963 - val_loss: 28.1658 - val_accuracy: 0.6667\n",
      "Epoch 20/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 92.4011 - accuracy: 0.4444 - val_loss: 27.3475 - val_accuracy: 0.6889\n",
      "Epoch 21/1000\n",
      "68/68 [==============================] - 0s 744us/step - loss: 79.5938 - accuracy: 0.5074 - val_loss: 26.2746 - val_accuracy: 0.6444\n",
      "Epoch 22/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 93.1028 - accuracy: 0.4630 - val_loss: 22.8419 - val_accuracy: 0.7222\n",
      "Epoch 23/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 68.6736 - accuracy: 0.5111 - val_loss: 23.3511 - val_accuracy: 0.7000\n",
      "Epoch 24/1000\n",
      "68/68 [==============================] - 0s 753us/step - loss: 70.8185 - accuracy: 0.5222 - val_loss: 22.0254 - val_accuracy: 0.6889\n",
      "Epoch 25/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 65.3692 - accuracy: 0.5259 - val_loss: 23.0585 - val_accuracy: 0.7111\n",
      "Epoch 26/1000\n",
      "68/68 [==============================] - 0s 760us/step - loss: 68.1426 - accuracy: 0.4741 - val_loss: 20.3594 - val_accuracy: 0.6889\n",
      "Epoch 27/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 59.1952 - accuracy: 0.5185 - val_loss: 19.9596 - val_accuracy: 0.6667\n",
      "Epoch 28/1000\n",
      "68/68 [==============================] - 0s 711us/step - loss: 54.5990 - accuracy: 0.5333 - val_loss: 24.5802 - val_accuracy: 0.6778\n",
      "Epoch 29/1000\n",
      "68/68 [==============================] - 0s 727us/step - loss: 55.6669 - accuracy: 0.5185 - val_loss: 23.1707 - val_accuracy: 0.7000\n",
      "Epoch 30/1000\n",
      "68/68 [==============================] - 0s 748us/step - loss: 52.4818 - accuracy: 0.5222 - val_loss: 22.1776 - val_accuracy: 0.6889\n",
      "Epoch 31/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 54.1575 - accuracy: 0.5185 - val_loss: 23.6357 - val_accuracy: 0.6556\n",
      "Epoch 32/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 47.5237 - accuracy: 0.5333 - val_loss: 22.0084 - val_accuracy: 0.6778\n",
      "Epoch 33/1000\n",
      "68/68 [==============================] - 0s 835us/step - loss: 58.2677 - accuracy: 0.4815 - val_loss: 22.2342 - val_accuracy: 0.6778\n",
      "Epoch 34/1000\n",
      "68/68 [==============================] - 0s 776us/step - loss: 38.6133 - accuracy: 0.5667 - val_loss: 22.1798 - val_accuracy: 0.6667\n",
      "Epoch 35/1000\n",
      "68/68 [==============================] - 0s 860us/step - loss: 51.1179 - accuracy: 0.5037 - val_loss: 21.3559 - val_accuracy: 0.6778\n",
      "Epoch 36/1000\n",
      "68/68 [==============================] - 0s 778us/step - loss: 42.9593 - accuracy: 0.5593 - val_loss: 20.4576 - val_accuracy: 0.6667\n",
      "Epoch 37/1000\n",
      "68/68 [==============================] - 0s 709us/step - loss: 46.6879 - accuracy: 0.5222 - val_loss: 20.3928 - val_accuracy: 0.6667\n",
      "Epoch 38/1000\n",
      "68/68 [==============================] - 0s 763us/step - loss: 48.3831 - accuracy: 0.4963 - val_loss: 21.4262 - val_accuracy: 0.6889\n",
      "Epoch 39/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 42.5353 - accuracy: 0.5148 - val_loss: 20.1178 - val_accuracy: 0.6556\n",
      "Epoch 40/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 42.1265 - accuracy: 0.5148 - val_loss: 20.7982 - val_accuracy: 0.6667\n",
      "Epoch 41/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 36.9066 - accuracy: 0.5296 - val_loss: 19.1645 - val_accuracy: 0.7000\n",
      "Epoch 42/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 43.7197 - accuracy: 0.4852 - val_loss: 19.4827 - val_accuracy: 0.7000\n",
      "Epoch 43/1000\n",
      "68/68 [==============================] - 0s 748us/step - loss: 39.0442 - accuracy: 0.5074 - val_loss: 19.9235 - val_accuracy: 0.6556\n",
      "Epoch 44/1000\n",
      "68/68 [==============================] - 0s 748us/step - loss: 37.1079 - accuracy: 0.5185 - val_loss: 20.3858 - val_accuracy: 0.6556\n",
      "Epoch 45/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 33.0687 - accuracy: 0.5037 - val_loss: 18.7576 - val_accuracy: 0.6778\n",
      "Epoch 46/1000\n",
      "68/68 [==============================] - 0s 736us/step - loss: 34.6788 - accuracy: 0.4852 - val_loss: 18.6429 - val_accuracy: 0.6889\n",
      "Epoch 47/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 32.3804 - accuracy: 0.5333 - val_loss: 18.2467 - val_accuracy: 0.6778\n",
      "Epoch 48/1000\n",
      "68/68 [==============================] - 0s 752us/step - loss: 26.5883 - accuracy: 0.5037 - val_loss: 17.4419 - val_accuracy: 0.6889\n",
      "Epoch 49/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 22.0860 - accuracy: 0.5815 - val_loss: 17.4588 - val_accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 28.0298 - accuracy: 0.5037 - val_loss: 17.8622 - val_accuracy: 0.6778\n",
      "Epoch 51/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 29.2722 - accuracy: 0.5296 - val_loss: 16.5419 - val_accuracy: 0.6778\n",
      "Epoch 52/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 25.5834 - accuracy: 0.5444 - val_loss: 14.7746 - val_accuracy: 0.6778\n",
      "Epoch 53/1000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 26.6618 - accuracy: 0.5222 - val_loss: 14.5382 - val_accuracy: 0.6778\n",
      "Epoch 54/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 20.4913 - accuracy: 0.5593 - val_loss: 15.7615 - val_accuracy: 0.6667\n",
      "Epoch 55/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 26.4294 - accuracy: 0.5074 - val_loss: 17.1543 - val_accuracy: 0.6444\n",
      "Epoch 56/1000\n",
      "68/68 [==============================] - 0s 762us/step - loss: 19.4758 - accuracy: 0.5519 - val_loss: 15.8692 - val_accuracy: 0.6667\n",
      "Epoch 57/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 19.2618 - accuracy: 0.5481 - val_loss: 17.4502 - val_accuracy: 0.6444\n",
      "Epoch 58/1000\n",
      "68/68 [==============================] - 0s 767us/step - loss: 17.9280 - accuracy: 0.5667 - val_loss: 16.5169 - val_accuracy: 0.6444\n",
      "Epoch 59/1000\n",
      "68/68 [==============================] - 0s 736us/step - loss: 20.0421 - accuracy: 0.5704 - val_loss: 13.8118 - val_accuracy: 0.6778\n",
      "Epoch 60/1000\n",
      "68/68 [==============================] - 0s 746us/step - loss: 22.1533 - accuracy: 0.5741 - val_loss: 14.2846 - val_accuracy: 0.6778\n",
      "Epoch 61/1000\n",
      "68/68 [==============================] - 0s 739us/step - loss: 15.2470 - accuracy: 0.5481 - val_loss: 15.0820 - val_accuracy: 0.6556\n",
      "Epoch 62/1000\n",
      "68/68 [==============================] - 0s 750us/step - loss: 16.8607 - accuracy: 0.5667 - val_loss: 14.8857 - val_accuracy: 0.6444\n",
      "Epoch 63/1000\n",
      "68/68 [==============================] - 0s 705us/step - loss: 13.9252 - accuracy: 0.6222 - val_loss: 13.9932 - val_accuracy: 0.6556\n",
      "Epoch 64/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 18.4888 - accuracy: 0.5259 - val_loss: 12.6989 - val_accuracy: 0.7000\n",
      "Epoch 65/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 17.8851 - accuracy: 0.5370 - val_loss: 12.6157 - val_accuracy: 0.6889\n",
      "Epoch 66/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 15.1208 - accuracy: 0.5815 - val_loss: 11.2659 - val_accuracy: 0.7111\n",
      "Epoch 67/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 13.2839 - accuracy: 0.5741 - val_loss: 11.7688 - val_accuracy: 0.6778\n",
      "Epoch 68/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 15.5860 - accuracy: 0.5000 - val_loss: 12.4434 - val_accuracy: 0.6889\n",
      "Epoch 69/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 10.9048 - accuracy: 0.5926 - val_loss: 12.4596 - val_accuracy: 0.6889\n",
      "Epoch 70/1000\n",
      "68/68 [==============================] - 0s 712us/step - loss: 9.0693 - accuracy: 0.5926 - val_loss: 12.6295 - val_accuracy: 0.6778\n",
      "Epoch 71/1000\n",
      "68/68 [==============================] - 0s 722us/step - loss: 13.3389 - accuracy: 0.5556 - val_loss: 11.8322 - val_accuracy: 0.6667\n",
      "Epoch 72/1000\n",
      "68/68 [==============================] - 0s 841us/step - loss: 14.7846 - accuracy: 0.5259 - val_loss: 11.1624 - val_accuracy: 0.6778\n",
      "Epoch 73/1000\n",
      "68/68 [==============================] - 0s 795us/step - loss: 13.8784 - accuracy: 0.5667 - val_loss: 11.1040 - val_accuracy: 0.7000\n",
      "Epoch 74/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 10.9797 - accuracy: 0.5519 - val_loss: 11.4403 - val_accuracy: 0.6889\n",
      "Epoch 75/1000\n",
      "68/68 [==============================] - 0s 746us/step - loss: 10.0978 - accuracy: 0.5667 - val_loss: 10.5723 - val_accuracy: 0.6889\n",
      "Epoch 76/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 10.9857 - accuracy: 0.5963 - val_loss: 10.6959 - val_accuracy: 0.6889\n",
      "Epoch 77/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 9.9529 - accuracy: 0.5593 - val_loss: 10.9121 - val_accuracy: 0.7000\n",
      "Epoch 78/1000\n",
      "68/68 [==============================] - 0s 746us/step - loss: 7.6885 - accuracy: 0.6148 - val_loss: 10.8780 - val_accuracy: 0.6889\n",
      "Epoch 79/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 9.9832 - accuracy: 0.5741 - val_loss: 10.1801 - val_accuracy: 0.7000\n",
      "Epoch 80/1000\n",
      "68/68 [==============================] - 0s 711us/step - loss: 9.3831 - accuracy: 0.6000 - val_loss: 10.0549 - val_accuracy: 0.6778\n",
      "Epoch 81/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 8.6792 - accuracy: 0.5704 - val_loss: 9.9119 - val_accuracy: 0.6889\n",
      "Epoch 82/1000\n",
      "68/68 [==============================] - 0s 763us/step - loss: 8.1165 - accuracy: 0.6111 - val_loss: 9.8199 - val_accuracy: 0.6778\n",
      "Epoch 83/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 11.7611 - accuracy: 0.5963 - val_loss: 9.7616 - val_accuracy: 0.6778\n",
      "Epoch 84/1000\n",
      "68/68 [==============================] - 0s 842us/step - loss: 8.8268 - accuracy: 0.5667 - val_loss: 10.0392 - val_accuracy: 0.6889\n",
      "Epoch 85/1000\n",
      "68/68 [==============================] - 0s 761us/step - loss: 7.8555 - accuracy: 0.6111 - val_loss: 9.5635 - val_accuracy: 0.6556\n",
      "Epoch 86/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 8.1462 - accuracy: 0.6000 - val_loss: 9.5807 - val_accuracy: 0.6778\n",
      "Epoch 87/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 7.0272 - accuracy: 0.6000 - val_loss: 9.2026 - val_accuracy: 0.6778\n",
      "Epoch 88/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 6.4472 - accuracy: 0.6370 - val_loss: 8.8662 - val_accuracy: 0.6778\n",
      "Epoch 89/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 4.6704 - accuracy: 0.6259 - val_loss: 8.0927 - val_accuracy: 0.6667\n",
      "Epoch 90/1000\n",
      "68/68 [==============================] - 0s 743us/step - loss: 9.1104 - accuracy: 0.5704 - val_loss: 8.5526 - val_accuracy: 0.6444\n",
      "Epoch 91/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 4.7167 - accuracy: 0.6222 - val_loss: 8.3642 - val_accuracy: 0.6222\n",
      "Epoch 92/1000\n",
      "68/68 [==============================] - 0s 809us/step - loss: 5.8888 - accuracy: 0.5741 - val_loss: 8.6512 - val_accuracy: 0.6444\n",
      "Epoch 93/1000\n",
      "68/68 [==============================] - 0s 758us/step - loss: 4.5606 - accuracy: 0.6222 - val_loss: 8.5572 - val_accuracy: 0.6222\n",
      "Epoch 94/1000\n",
      "68/68 [==============================] - 0s 769us/step - loss: 6.0725 - accuracy: 0.5519 - val_loss: 8.0506 - val_accuracy: 0.6333\n",
      "Epoch 95/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 4.7006 - accuracy: 0.6222 - val_loss: 7.8804 - val_accuracy: 0.6556\n",
      "Epoch 96/1000\n",
      "68/68 [==============================] - 0s 757us/step - loss: 6.2085 - accuracy: 0.5630 - val_loss: 7.7279 - val_accuracy: 0.6889\n",
      "Epoch 97/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 4.4837 - accuracy: 0.6148 - val_loss: 7.7632 - val_accuracy: 0.6778\n",
      "Epoch 98/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 6.0027 - accuracy: 0.5259 - val_loss: 7.5472 - val_accuracy: 0.6444\n",
      "Epoch 99/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 3.9751 - accuracy: 0.6185 - val_loss: 7.4519 - val_accuracy: 0.6778\n",
      "Epoch 100/1000\n",
      "68/68 [==============================] - 0s 714us/step - loss: 3.8951 - accuracy: 0.6000 - val_loss: 7.7428 - val_accuracy: 0.6667\n",
      "Epoch 101/1000\n",
      "68/68 [==============================] - 0s 774us/step - loss: 6.4166 - accuracy: 0.5889 - val_loss: 7.7140 - val_accuracy: 0.6667\n",
      "Epoch 102/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 3.8196 - accuracy: 0.5778 - val_loss: 7.4513 - val_accuracy: 0.6667\n",
      "Epoch 103/1000\n",
      "68/68 [==============================] - 0s 712us/step - loss: 3.5713 - accuracy: 0.6185 - val_loss: 6.9761 - val_accuracy: 0.6778\n",
      "Epoch 104/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 4.7475 - accuracy: 0.5889 - val_loss: 6.8804 - val_accuracy: 0.6889\n",
      "Epoch 105/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 4.2100 - accuracy: 0.5963 - val_loss: 6.6785 - val_accuracy: 0.6667\n",
      "Epoch 106/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 719us/step - loss: 4.7580 - accuracy: 0.6037 - val_loss: 6.6152 - val_accuracy: 0.6889\n",
      "Epoch 107/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 3.7953 - accuracy: 0.6074 - val_loss: 6.8394 - val_accuracy: 0.7000\n",
      "Epoch 108/1000\n",
      "68/68 [==============================] - 0s 702us/step - loss: 2.4808 - accuracy: 0.6148 - val_loss: 6.5397 - val_accuracy: 0.6889\n",
      "Epoch 109/1000\n",
      "68/68 [==============================] - 0s 727us/step - loss: 3.0610 - accuracy: 0.6185 - val_loss: 6.5863 - val_accuracy: 0.6889\n",
      "Epoch 110/1000\n",
      "68/68 [==============================] - 0s 710us/step - loss: 4.4719 - accuracy: 0.6296 - val_loss: 6.4971 - val_accuracy: 0.6778\n",
      "Epoch 111/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 3.7400 - accuracy: 0.6074 - val_loss: 6.7095 - val_accuracy: 0.6778\n",
      "Epoch 112/1000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.9608 - accuracy: 0.6148 - val_loss: 6.7807 - val_accuracy: 0.6889\n",
      "Epoch 113/1000\n",
      "68/68 [==============================] - 0s 829us/step - loss: 3.3480 - accuracy: 0.6074 - val_loss: 6.5786 - val_accuracy: 0.6778\n",
      "Epoch 114/1000\n",
      "68/68 [==============================] - 0s 716us/step - loss: 3.2789 - accuracy: 0.5963 - val_loss: 6.3050 - val_accuracy: 0.6667\n",
      "Epoch 115/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 2.4636 - accuracy: 0.5926 - val_loss: 6.3790 - val_accuracy: 0.6889\n",
      "Epoch 116/1000\n",
      "68/68 [==============================] - 0s 739us/step - loss: 2.8684 - accuracy: 0.6185 - val_loss: 6.5037 - val_accuracy: 0.6667\n",
      "Epoch 117/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 3.3269 - accuracy: 0.5741 - val_loss: 6.8814 - val_accuracy: 0.6556\n",
      "Epoch 118/1000\n",
      "68/68 [==============================] - 0s 745us/step - loss: 3.2791 - accuracy: 0.5667 - val_loss: 6.7060 - val_accuracy: 0.6778\n",
      "Epoch 119/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 2.8598 - accuracy: 0.6222 - val_loss: 6.5779 - val_accuracy: 0.6889\n",
      "Epoch 120/1000\n",
      "68/68 [==============================] - 0s 716us/step - loss: 2.7513 - accuracy: 0.6037 - val_loss: 6.3055 - val_accuracy: 0.6889\n",
      "Epoch 121/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 2.3216 - accuracy: 0.6037 - val_loss: 6.4105 - val_accuracy: 0.7000\n",
      "Epoch 122/1000\n",
      "68/68 [==============================] - 0s 746us/step - loss: 2.6405 - accuracy: 0.6185 - val_loss: 6.2714 - val_accuracy: 0.7111\n",
      "Epoch 123/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 3.1031 - accuracy: 0.6000 - val_loss: 6.1822 - val_accuracy: 0.7333\n",
      "Epoch 124/1000\n",
      "68/68 [==============================] - 0s 750us/step - loss: 2.6441 - accuracy: 0.5778 - val_loss: 6.4868 - val_accuracy: 0.7000\n",
      "Epoch 125/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 2.3123 - accuracy: 0.6222 - val_loss: 6.3202 - val_accuracy: 0.7222\n",
      "Epoch 126/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 2.1167 - accuracy: 0.6185 - val_loss: 6.7239 - val_accuracy: 0.7222\n",
      "Epoch 127/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 2.8543 - accuracy: 0.5963 - val_loss: 6.7070 - val_accuracy: 0.7000\n",
      "Epoch 128/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 1.6569 - accuracy: 0.6630 - val_loss: 6.3497 - val_accuracy: 0.7222\n",
      "Epoch 129/1000\n",
      "68/68 [==============================] - 0s 739us/step - loss: 2.7566 - accuracy: 0.5667 - val_loss: 6.4889 - val_accuracy: 0.7000\n",
      "Epoch 130/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 3.3723 - accuracy: 0.6111 - val_loss: 6.4538 - val_accuracy: 0.6889\n",
      "Epoch 131/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 2.3388 - accuracy: 0.6037 - val_loss: 6.4328 - val_accuracy: 0.6778\n",
      "Epoch 132/1000\n",
      "68/68 [==============================] - 0s 855us/step - loss: 2.0640 - accuracy: 0.6444 - val_loss: 5.9538 - val_accuracy: 0.7000\n",
      "Epoch 133/1000\n",
      "68/68 [==============================] - 0s 755us/step - loss: 2.4645 - accuracy: 0.6000 - val_loss: 6.2099 - val_accuracy: 0.7000\n",
      "Epoch 134/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 2.0609 - accuracy: 0.5926 - val_loss: 6.2707 - val_accuracy: 0.7000\n",
      "Epoch 135/1000\n",
      "68/68 [==============================] - 0s 744us/step - loss: 2.2299 - accuracy: 0.5963 - val_loss: 5.9606 - val_accuracy: 0.7000\n",
      "Epoch 136/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 2.6219 - accuracy: 0.6148 - val_loss: 5.8321 - val_accuracy: 0.7111\n",
      "Epoch 137/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 1.6782 - accuracy: 0.6259 - val_loss: 5.7176 - val_accuracy: 0.7222\n",
      "Epoch 138/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 1.4886 - accuracy: 0.6000 - val_loss: 5.6176 - val_accuracy: 0.7111\n",
      "Epoch 139/1000\n",
      "68/68 [==============================] - 0s 722us/step - loss: 2.2764 - accuracy: 0.6185 - val_loss: 5.5595 - val_accuracy: 0.7000\n",
      "Epoch 140/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 2.5951 - accuracy: 0.5889 - val_loss: 5.6486 - val_accuracy: 0.6778\n",
      "Epoch 141/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 2.1048 - accuracy: 0.5852 - val_loss: 5.7452 - val_accuracy: 0.7222\n",
      "Epoch 142/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 1.2849 - accuracy: 0.6333 - val_loss: 5.4805 - val_accuracy: 0.7111\n",
      "Epoch 143/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 1.5363 - accuracy: 0.6370 - val_loss: 5.2536 - val_accuracy: 0.7222\n",
      "Epoch 144/1000\n",
      "68/68 [==============================] - 0s 727us/step - loss: 1.3590 - accuracy: 0.6111 - val_loss: 5.3470 - val_accuracy: 0.7111\n",
      "Epoch 145/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 1.5583 - accuracy: 0.6259 - val_loss: 5.0478 - val_accuracy: 0.7222\n",
      "Epoch 146/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 1.9265 - accuracy: 0.6185 - val_loss: 4.8522 - val_accuracy: 0.7111\n",
      "Epoch 147/1000\n",
      "68/68 [==============================] - 0s 743us/step - loss: 1.5490 - accuracy: 0.6037 - val_loss: 4.5703 - val_accuracy: 0.7333\n",
      "Epoch 148/1000\n",
      "68/68 [==============================] - 0s 746us/step - loss: 1.4970 - accuracy: 0.6185 - val_loss: 4.8627 - val_accuracy: 0.7000\n",
      "Epoch 149/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 1.6788 - accuracy: 0.6185 - val_loss: 4.7998 - val_accuracy: 0.6889\n",
      "Epoch 150/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 1.5087 - accuracy: 0.6148 - val_loss: 4.6467 - val_accuracy: 0.6889\n",
      "Epoch 151/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 1.4743 - accuracy: 0.6074 - val_loss: 4.5628 - val_accuracy: 0.7000\n",
      "Epoch 152/1000\n",
      "68/68 [==============================] - 0s 860us/step - loss: 1.3888 - accuracy: 0.6222 - val_loss: 4.5701 - val_accuracy: 0.7222\n",
      "Epoch 153/1000\n",
      "68/68 [==============================] - 0s 793us/step - loss: 1.4741 - accuracy: 0.6222 - val_loss: 4.5712 - val_accuracy: 0.7111\n",
      "Epoch 154/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 1.6497 - accuracy: 0.5889 - val_loss: 4.3350 - val_accuracy: 0.7222\n",
      "Epoch 155/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 1.3670 - accuracy: 0.5926 - val_loss: 3.9202 - val_accuracy: 0.7333\n",
      "Epoch 156/1000\n",
      "68/68 [==============================] - 0s 755us/step - loss: 1.8696 - accuracy: 0.5815 - val_loss: 4.0224 - val_accuracy: 0.7222\n",
      "Epoch 157/1000\n",
      "68/68 [==============================] - 0s 760us/step - loss: 1.6601 - accuracy: 0.5963 - val_loss: 3.9998 - val_accuracy: 0.7333\n",
      "Epoch 158/1000\n",
      "68/68 [==============================] - 0s 745us/step - loss: 1.7794 - accuracy: 0.5852 - val_loss: 4.2726 - val_accuracy: 0.7111\n",
      "Epoch 159/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 1.3012 - accuracy: 0.6222 - val_loss: 4.2281 - val_accuracy: 0.7000\n",
      "Epoch 160/1000\n",
      "68/68 [==============================] - 0s 744us/step - loss: 0.8257 - accuracy: 0.6296 - val_loss: 4.1179 - val_accuracy: 0.6889\n",
      "Epoch 161/1000\n",
      "68/68 [==============================] - 0s 712us/step - loss: 1.3546 - accuracy: 0.5778 - val_loss: 4.2361 - val_accuracy: 0.6778\n",
      "Epoch 162/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 758us/step - loss: 1.6753 - accuracy: 0.6000 - val_loss: 4.4597 - val_accuracy: 0.6667\n",
      "Epoch 163/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 1.7314 - accuracy: 0.5778 - val_loss: 4.2347 - val_accuracy: 0.6444\n",
      "Epoch 164/1000\n",
      "68/68 [==============================] - 0s 713us/step - loss: 1.1584 - accuracy: 0.6370 - val_loss: 4.1900 - val_accuracy: 0.6667\n",
      "Epoch 165/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 1.5569 - accuracy: 0.6000 - val_loss: 3.8967 - val_accuracy: 0.6667\n",
      "Epoch 166/1000\n",
      "68/68 [==============================] - 0s 714us/step - loss: 1.1010 - accuracy: 0.6148 - val_loss: 3.8977 - val_accuracy: 0.6778\n",
      "Epoch 167/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 1.1482 - accuracy: 0.6259 - val_loss: 3.6865 - val_accuracy: 0.6667\n",
      "Epoch 168/1000\n",
      "68/68 [==============================] - 0s 746us/step - loss: 1.1410 - accuracy: 0.6333 - val_loss: 3.8139 - val_accuracy: 0.6778\n",
      "Epoch 169/1000\n",
      "68/68 [==============================] - 0s 756us/step - loss: 1.4658 - accuracy: 0.5926 - val_loss: 3.5384 - val_accuracy: 0.6556\n",
      "Epoch 170/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.9813 - accuracy: 0.6037 - val_loss: 3.5089 - val_accuracy: 0.6667\n",
      "Epoch 171/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 1.2179 - accuracy: 0.6111 - val_loss: 3.6059 - val_accuracy: 0.6667\n",
      "Epoch 172/1000\n",
      "68/68 [==============================] - 0s 867us/step - loss: 0.9425 - accuracy: 0.6037 - val_loss: 3.5247 - val_accuracy: 0.6778\n",
      "Epoch 173/1000\n",
      "68/68 [==============================] - 0s 748us/step - loss: 1.4682 - accuracy: 0.5852 - val_loss: 3.4245 - val_accuracy: 0.6889\n",
      "Epoch 174/1000\n",
      "68/68 [==============================] - 0s 710us/step - loss: 1.2899 - accuracy: 0.6148 - val_loss: 3.3222 - val_accuracy: 0.6889\n",
      "Epoch 175/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 1.3433 - accuracy: 0.6037 - val_loss: 3.3181 - val_accuracy: 0.6889\n",
      "Epoch 176/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 1.0236 - accuracy: 0.6074 - val_loss: 3.2961 - val_accuracy: 0.6667\n",
      "Epoch 177/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 1.1482 - accuracy: 0.5815 - val_loss: 3.2531 - val_accuracy: 0.6667\n",
      "Epoch 178/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.9300 - accuracy: 0.6111 - val_loss: 3.1254 - val_accuracy: 0.6889\n",
      "Epoch 179/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.9295 - accuracy: 0.6037 - val_loss: 3.1709 - val_accuracy: 0.6667\n",
      "Epoch 180/1000\n",
      "68/68 [==============================] - 0s 755us/step - loss: 1.3237 - accuracy: 0.6111 - val_loss: 3.1071 - val_accuracy: 0.6556\n",
      "Epoch 181/1000\n",
      "68/68 [==============================] - 0s 709us/step - loss: 1.0924 - accuracy: 0.6148 - val_loss: 2.9605 - val_accuracy: 0.6667\n",
      "Epoch 182/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 0.9550 - accuracy: 0.6037 - val_loss: 3.0394 - val_accuracy: 0.6778\n",
      "Epoch 183/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 1.3129 - accuracy: 0.5963 - val_loss: 3.1553 - val_accuracy: 0.6667\n",
      "Epoch 184/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.8415 - accuracy: 0.6037 - val_loss: 3.0900 - val_accuracy: 0.6667\n",
      "Epoch 185/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 1.1259 - accuracy: 0.6111 - val_loss: 2.7346 - val_accuracy: 0.6778\n",
      "Epoch 186/1000\n",
      "68/68 [==============================] - 0s 746us/step - loss: 1.1015 - accuracy: 0.5852 - val_loss: 2.9208 - val_accuracy: 0.6444\n",
      "Epoch 187/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 1.2453 - accuracy: 0.6148 - val_loss: 2.4989 - val_accuracy: 0.6667\n",
      "Epoch 188/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 1.1492 - accuracy: 0.5963 - val_loss: 2.5600 - val_accuracy: 0.6778\n",
      "Epoch 189/1000\n",
      "68/68 [==============================] - 0s 744us/step - loss: 0.9924 - accuracy: 0.5741 - val_loss: 2.7208 - val_accuracy: 0.6556\n",
      "Epoch 190/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 1.0724 - accuracy: 0.5926 - val_loss: 2.7522 - val_accuracy: 0.6556\n",
      "Epoch 191/1000\n",
      "68/68 [==============================] - 0s 713us/step - loss: 1.3350 - accuracy: 0.6000 - val_loss: 2.8768 - val_accuracy: 0.6667\n",
      "Epoch 192/1000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.2636 - accuracy: 0.5963 - val_loss: 2.8513 - val_accuracy: 0.6667\n",
      "Epoch 193/1000\n",
      "68/68 [==============================] - 0s 749us/step - loss: 1.0165 - accuracy: 0.6111 - val_loss: 2.8550 - val_accuracy: 0.6778\n",
      "Epoch 194/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 1.0344 - accuracy: 0.6000 - val_loss: 2.6903 - val_accuracy: 0.6778\n",
      "Epoch 195/1000\n",
      "68/68 [==============================] - 0s 788us/step - loss: 1.1354 - accuracy: 0.6000 - val_loss: 2.7624 - val_accuracy: 0.6667\n",
      "Epoch 196/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 0.8064 - accuracy: 0.6148 - val_loss: 2.6933 - val_accuracy: 0.6778\n",
      "Epoch 197/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 1.2422 - accuracy: 0.5963 - val_loss: 2.7921 - val_accuracy: 0.6444\n",
      "Epoch 198/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 0.9869 - accuracy: 0.6111 - val_loss: 2.8232 - val_accuracy: 0.6667\n",
      "Epoch 199/1000\n",
      "68/68 [==============================] - 0s 739us/step - loss: 0.9765 - accuracy: 0.5963 - val_loss: 2.7724 - val_accuracy: 0.6444\n",
      "Epoch 200/1000\n",
      "68/68 [==============================] - 0s 771us/step - loss: 0.9615 - accuracy: 0.6222 - val_loss: 2.7325 - val_accuracy: 0.6444\n",
      "Epoch 201/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 0.7886 - accuracy: 0.6074 - val_loss: 2.6691 - val_accuracy: 0.6556\n",
      "Epoch 202/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 1.1096 - accuracy: 0.5630 - val_loss: 2.7249 - val_accuracy: 0.6556\n",
      "Epoch 203/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 0.8553 - accuracy: 0.6000 - val_loss: 2.7578 - val_accuracy: 0.6667\n",
      "Epoch 204/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.9012 - accuracy: 0.6185 - val_loss: 2.8547 - val_accuracy: 0.6333\n",
      "Epoch 205/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 1.0030 - accuracy: 0.6037 - val_loss: 2.8702 - val_accuracy: 0.6444\n",
      "Epoch 206/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.8932 - accuracy: 0.6000 - val_loss: 2.9948 - val_accuracy: 0.6444\n",
      "Epoch 207/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 1.1502 - accuracy: 0.5741 - val_loss: 2.7742 - val_accuracy: 0.6556\n",
      "Epoch 208/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 0.8025 - accuracy: 0.6185 - val_loss: 2.8299 - val_accuracy: 0.6556\n",
      "Epoch 209/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.8540 - accuracy: 0.5963 - val_loss: 2.8035 - val_accuracy: 0.6667\n",
      "Epoch 210/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 0.9455 - accuracy: 0.5630 - val_loss: 2.8078 - val_accuracy: 0.6556\n",
      "Epoch 211/1000\n",
      "68/68 [==============================] - 0s 838us/step - loss: 0.8983 - accuracy: 0.5963 - val_loss: 2.9498 - val_accuracy: 0.6333\n",
      "Epoch 212/1000\n",
      "68/68 [==============================] - 0s 750us/step - loss: 1.0112 - accuracy: 0.6111 - val_loss: 2.7563 - val_accuracy: 0.6667\n",
      "Epoch 213/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 0.9127 - accuracy: 0.5778 - val_loss: 2.7354 - val_accuracy: 0.6556\n",
      "Epoch 214/1000\n",
      "68/68 [==============================] - 0s 743us/step - loss: 0.9029 - accuracy: 0.5963 - val_loss: 2.9136 - val_accuracy: 0.6333\n",
      "Epoch 215/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.9315 - accuracy: 0.5926 - val_loss: 2.6652 - val_accuracy: 0.6556\n",
      "Epoch 216/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 1.0586 - accuracy: 0.5926 - val_loss: 2.6256 - val_accuracy: 0.6444\n",
      "Epoch 217/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.7977 - accuracy: 0.6037 - val_loss: 2.4697 - val_accuracy: 0.6556\n",
      "Epoch 218/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 741us/step - loss: 0.9473 - accuracy: 0.6148 - val_loss: 2.3782 - val_accuracy: 0.6556\n",
      "Epoch 219/1000\n",
      "68/68 [==============================] - 0s 752us/step - loss: 0.8344 - accuracy: 0.5926 - val_loss: 2.5640 - val_accuracy: 0.6667\n",
      "Epoch 220/1000\n",
      "68/68 [==============================] - 0s 746us/step - loss: 0.9941 - accuracy: 0.5815 - val_loss: 2.2017 - val_accuracy: 0.6778\n",
      "Epoch 221/1000\n",
      "68/68 [==============================] - 0s 739us/step - loss: 0.8315 - accuracy: 0.5889 - val_loss: 2.2059 - val_accuracy: 0.6667\n",
      "Epoch 222/1000\n",
      "68/68 [==============================] - 0s 736us/step - loss: 0.7390 - accuracy: 0.6185 - val_loss: 2.2530 - val_accuracy: 0.6667\n",
      "Epoch 223/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.8708 - accuracy: 0.5963 - val_loss: 2.2713 - val_accuracy: 0.6667\n",
      "Epoch 224/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.9353 - accuracy: 0.5741 - val_loss: 2.3993 - val_accuracy: 0.6667\n",
      "Epoch 225/1000\n",
      "68/68 [==============================] - 0s 744us/step - loss: 0.7923 - accuracy: 0.6074 - val_loss: 2.5244 - val_accuracy: 0.6556\n",
      "Epoch 226/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 0.8532 - accuracy: 0.5741 - val_loss: 2.4877 - val_accuracy: 0.6667\n",
      "Epoch 227/1000\n",
      "68/68 [==============================] - 0s 754us/step - loss: 1.0419 - accuracy: 0.5667 - val_loss: 2.3557 - val_accuracy: 0.6667\n",
      "Epoch 228/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.8159 - accuracy: 0.6074 - val_loss: 2.3593 - val_accuracy: 0.6667\n",
      "Epoch 229/1000\n",
      "68/68 [==============================] - 0s 727us/step - loss: 0.7953 - accuracy: 0.6037 - val_loss: 2.3888 - val_accuracy: 0.6556\n",
      "Epoch 230/1000\n",
      "68/68 [==============================] - 0s 748us/step - loss: 0.8387 - accuracy: 0.6000 - val_loss: 2.4063 - val_accuracy: 0.6667\n",
      "Epoch 231/1000\n",
      "68/68 [==============================] - 0s 807us/step - loss: 0.9323 - accuracy: 0.5815 - val_loss: 2.5029 - val_accuracy: 0.6667\n",
      "Epoch 232/1000\n",
      "68/68 [==============================] - 0s 759us/step - loss: 0.9169 - accuracy: 0.5815 - val_loss: 2.5800 - val_accuracy: 0.6556\n",
      "Epoch 233/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.9185 - accuracy: 0.5667 - val_loss: 2.6029 - val_accuracy: 0.6556\n",
      "Epoch 234/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 1.0911 - accuracy: 0.5556 - val_loss: 2.7429 - val_accuracy: 0.6444\n",
      "Epoch 235/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 1.0618 - accuracy: 0.5593 - val_loss: 2.6923 - val_accuracy: 0.6444\n",
      "Epoch 236/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.8522 - accuracy: 0.6074 - val_loss: 2.8001 - val_accuracy: 0.6556\n",
      "Epoch 237/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 0.8142 - accuracy: 0.5889 - val_loss: 2.8003 - val_accuracy: 0.6556\n",
      "Epoch 238/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 0.8073 - accuracy: 0.5778 - val_loss: 2.8419 - val_accuracy: 0.6444\n",
      "Epoch 239/1000\n",
      "68/68 [==============================] - 0s 699us/step - loss: 0.8548 - accuracy: 0.5815 - val_loss: 2.8118 - val_accuracy: 0.6556\n",
      "Epoch 240/1000\n",
      "68/68 [==============================] - 0s 727us/step - loss: 0.8355 - accuracy: 0.5926 - val_loss: 2.8174 - val_accuracy: 0.6667\n",
      "Epoch 241/1000\n",
      "68/68 [==============================] - 0s 748us/step - loss: 0.9378 - accuracy: 0.5778 - val_loss: 2.9047 - val_accuracy: 0.6667\n",
      "Epoch 242/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.9313 - accuracy: 0.5630 - val_loss: 2.8390 - val_accuracy: 0.6556\n",
      "Epoch 243/1000\n",
      "68/68 [==============================] - 0s 739us/step - loss: 0.8529 - accuracy: 0.5741 - val_loss: 2.6531 - val_accuracy: 0.6778\n",
      "Epoch 244/1000\n",
      "68/68 [==============================] - 0s 745us/step - loss: 0.8263 - accuracy: 0.5926 - val_loss: 2.6580 - val_accuracy: 0.6556\n",
      "Epoch 245/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 1.4923 - accuracy: 0.5667 - val_loss: 2.7118 - val_accuracy: 0.6556\n",
      "Epoch 246/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.8567 - accuracy: 0.5815 - val_loss: 2.7641 - val_accuracy: 0.6556\n",
      "Epoch 247/1000\n",
      "68/68 [==============================] - 0s 747us/step - loss: 0.8555 - accuracy: 0.6000 - val_loss: 2.7322 - val_accuracy: 0.6444\n",
      "Epoch 248/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.7735 - accuracy: 0.6148 - val_loss: 2.7149 - val_accuracy: 0.6444\n",
      "Epoch 249/1000\n",
      "68/68 [==============================] - 0s 743us/step - loss: 0.8626 - accuracy: 0.5963 - val_loss: 2.7327 - val_accuracy: 0.6222\n",
      "Epoch 250/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.7797 - accuracy: 0.6037 - val_loss: 2.6432 - val_accuracy: 0.6333\n",
      "Epoch 251/1000\n",
      "68/68 [==============================] - 0s 994us/step - loss: 0.7592 - accuracy: 0.6074 - val_loss: 2.6413 - val_accuracy: 0.6333\n",
      "Epoch 252/1000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.8178 - accuracy: 0.6074 - val_loss: 2.6861 - val_accuracy: 0.6333\n",
      "Epoch 253/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 0.9304 - accuracy: 0.6000 - val_loss: 3.0321 - val_accuracy: 0.6444\n",
      "Epoch 254/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.8613 - accuracy: 0.5778 - val_loss: 3.0792 - val_accuracy: 0.6444\n",
      "Epoch 255/1000\n",
      "68/68 [==============================] - 0s 748us/step - loss: 1.0928 - accuracy: 0.6037 - val_loss: 2.9335 - val_accuracy: 0.6556\n",
      "Epoch 256/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.9771 - accuracy: 0.5852 - val_loss: 2.8461 - val_accuracy: 0.6444\n",
      "Epoch 257/1000\n",
      "68/68 [==============================] - 0s 767us/step - loss: 0.8137 - accuracy: 0.6000 - val_loss: 2.7710 - val_accuracy: 0.6444\n",
      "Epoch 258/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.7891 - accuracy: 0.6000 - val_loss: 2.8722 - val_accuracy: 0.6444\n",
      "Epoch 259/1000\n",
      "68/68 [==============================] - 0s 748us/step - loss: 0.7416 - accuracy: 0.6185 - val_loss: 2.8531 - val_accuracy: 0.6444\n",
      "Epoch 260/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.8477 - accuracy: 0.5889 - val_loss: 2.6412 - val_accuracy: 0.6556\n",
      "Epoch 261/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.7722 - accuracy: 0.6074 - val_loss: 2.6482 - val_accuracy: 0.6778\n",
      "Epoch 262/1000\n",
      "68/68 [==============================] - 0s 800us/step - loss: 0.9261 - accuracy: 0.5963 - val_loss: 2.6123 - val_accuracy: 0.6667\n",
      "Epoch 263/1000\n",
      "68/68 [==============================] - 0s 722us/step - loss: 0.7757 - accuracy: 0.6185 - val_loss: 2.6641 - val_accuracy: 0.6556\n",
      "Epoch 264/1000\n",
      "68/68 [==============================] - 0s 757us/step - loss: 0.9834 - accuracy: 0.5852 - val_loss: 2.7326 - val_accuracy: 0.6667\n",
      "Epoch 265/1000\n",
      "68/68 [==============================] - 0s 736us/step - loss: 0.7722 - accuracy: 0.5963 - val_loss: 2.7748 - val_accuracy: 0.6667\n",
      "Epoch 266/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 0.8851 - accuracy: 0.5889 - val_loss: 2.7939 - val_accuracy: 0.6444\n",
      "Epoch 267/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.8065 - accuracy: 0.5889 - val_loss: 2.7371 - val_accuracy: 0.6444\n",
      "Epoch 268/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 0.7961 - accuracy: 0.5963 - val_loss: 2.7601 - val_accuracy: 0.6556\n",
      "Epoch 269/1000\n",
      "68/68 [==============================] - 0s 745us/step - loss: 0.8361 - accuracy: 0.5704 - val_loss: 2.8915 - val_accuracy: 0.6444\n",
      "Epoch 270/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 0.8397 - accuracy: 0.6148 - val_loss: 2.7815 - val_accuracy: 0.6444\n",
      "Epoch 271/1000\n",
      "68/68 [==============================] - 0s 826us/step - loss: 0.7887 - accuracy: 0.6037 - val_loss: 2.6989 - val_accuracy: 0.6333\n",
      "Epoch 272/1000\n",
      "68/68 [==============================] - 0s 759us/step - loss: 0.8295 - accuracy: 0.6074 - val_loss: 2.7375 - val_accuracy: 0.6444\n",
      "Epoch 273/1000\n",
      "68/68 [==============================] - 0s 716us/step - loss: 0.7963 - accuracy: 0.5963 - val_loss: 2.7103 - val_accuracy: 0.6444\n",
      "Epoch 274/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 741us/step - loss: 0.7365 - accuracy: 0.6185 - val_loss: 2.6457 - val_accuracy: 0.6444\n",
      "Epoch 275/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.9933 - accuracy: 0.5778 - val_loss: 2.7896 - val_accuracy: 0.6556\n",
      "Epoch 276/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 0.8129 - accuracy: 0.6000 - val_loss: 2.7928 - val_accuracy: 0.6444\n",
      "Epoch 277/1000\n",
      "68/68 [==============================] - 0s 762us/step - loss: 0.8035 - accuracy: 0.6037 - val_loss: 2.8240 - val_accuracy: 0.6444\n",
      "Epoch 278/1000\n",
      "68/68 [==============================] - 0s 722us/step - loss: 0.7808 - accuracy: 0.5926 - val_loss: 2.9246 - val_accuracy: 0.6667\n",
      "Epoch 279/1000\n",
      "68/68 [==============================] - 0s 744us/step - loss: 0.8854 - accuracy: 0.5963 - val_loss: 2.8563 - val_accuracy: 0.6667\n",
      "Epoch 280/1000\n",
      "68/68 [==============================] - 0s 700us/step - loss: 0.7716 - accuracy: 0.6148 - val_loss: 2.8398 - val_accuracy: 0.6556\n",
      "Epoch 281/1000\n",
      "68/68 [==============================] - 0s 752us/step - loss: 0.7579 - accuracy: 0.6148 - val_loss: 2.8425 - val_accuracy: 0.6667\n",
      "Epoch 282/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 0.8455 - accuracy: 0.5889 - val_loss: 2.7899 - val_accuracy: 0.6444\n",
      "Epoch 283/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.8246 - accuracy: 0.6074 - val_loss: 2.5269 - val_accuracy: 0.6778\n",
      "Epoch 284/1000\n",
      "68/68 [==============================] - 0s 743us/step - loss: 0.7705 - accuracy: 0.6037 - val_loss: 2.5747 - val_accuracy: 0.6556\n",
      "Epoch 285/1000\n",
      "68/68 [==============================] - 0s 747us/step - loss: 0.7938 - accuracy: 0.5926 - val_loss: 2.6408 - val_accuracy: 0.6556\n",
      "Epoch 286/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 0.7983 - accuracy: 0.5926 - val_loss: 2.6381 - val_accuracy: 0.6444\n",
      "Epoch 287/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 0.7911 - accuracy: 0.5852 - val_loss: 2.6623 - val_accuracy: 0.6556\n",
      "Epoch 288/1000\n",
      "68/68 [==============================] - 0s 702us/step - loss: 0.7839 - accuracy: 0.6037 - val_loss: 2.7473 - val_accuracy: 0.6667\n",
      "Epoch 289/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 0.7831 - accuracy: 0.6000 - val_loss: 2.7649 - val_accuracy: 0.6556\n",
      "Epoch 290/1000\n",
      "68/68 [==============================] - 0s 749us/step - loss: 0.7435 - accuracy: 0.6222 - val_loss: 2.8222 - val_accuracy: 0.6556\n",
      "Epoch 291/1000\n",
      "68/68 [==============================] - 0s 820us/step - loss: 0.9149 - accuracy: 0.5852 - val_loss: 2.7076 - val_accuracy: 0.6556\n",
      "Epoch 292/1000\n",
      "68/68 [==============================] - 0s 753us/step - loss: 0.7682 - accuracy: 0.5889 - val_loss: 2.7759 - val_accuracy: 0.6556\n",
      "Epoch 293/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 0.8557 - accuracy: 0.5963 - val_loss: 2.8254 - val_accuracy: 0.6556\n",
      "Epoch 294/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.7458 - accuracy: 0.6037 - val_loss: 2.8437 - val_accuracy: 0.6444\n",
      "Epoch 295/1000\n",
      "68/68 [==============================] - 0s 747us/step - loss: 0.8849 - accuracy: 0.5889 - val_loss: 2.8585 - val_accuracy: 0.6444\n",
      "Epoch 296/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.8764 - accuracy: 0.5630 - val_loss: 2.7694 - val_accuracy: 0.6556\n",
      "Epoch 297/1000\n",
      "68/68 [==============================] - 0s 709us/step - loss: 0.9181 - accuracy: 0.5926 - val_loss: 2.5666 - val_accuracy: 0.6444\n",
      "Epoch 298/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.8645 - accuracy: 0.5926 - val_loss: 2.5796 - val_accuracy: 0.6444\n",
      "Epoch 299/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.8254 - accuracy: 0.5963 - val_loss: 2.5969 - val_accuracy: 0.6222\n",
      "Epoch 300/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.7630 - accuracy: 0.6111 - val_loss: 2.6035 - val_accuracy: 0.6444\n",
      "Epoch 301/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 0.8518 - accuracy: 0.5741 - val_loss: 2.6221 - val_accuracy: 0.6556\n",
      "Epoch 302/1000\n",
      "68/68 [==============================] - 0s 760us/step - loss: 0.7972 - accuracy: 0.5926 - val_loss: 2.5874 - val_accuracy: 0.6667\n",
      "Epoch 303/1000\n",
      "68/68 [==============================] - 0s 745us/step - loss: 0.8152 - accuracy: 0.5778 - val_loss: 2.5854 - val_accuracy: 0.6556\n",
      "Epoch 304/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 0.8457 - accuracy: 0.5963 - val_loss: 2.4822 - val_accuracy: 0.6667\n",
      "Epoch 305/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 0.9451 - accuracy: 0.5963 - val_loss: 2.5050 - val_accuracy: 0.6667\n",
      "Epoch 306/1000\n",
      "68/68 [==============================] - 0s 802us/step - loss: 0.7315 - accuracy: 0.6222 - val_loss: 2.4991 - val_accuracy: 0.6667\n",
      "Epoch 307/1000\n",
      "68/68 [==============================] - 0s 713us/step - loss: 0.8340 - accuracy: 0.5926 - val_loss: 2.5397 - val_accuracy: 0.6556\n",
      "Epoch 308/1000\n",
      "68/68 [==============================] - 0s 708us/step - loss: 0.9074 - accuracy: 0.5741 - val_loss: 2.7193 - val_accuracy: 0.6667\n",
      "Epoch 309/1000\n",
      "68/68 [==============================] - 0s 714us/step - loss: 0.8245 - accuracy: 0.6185 - val_loss: 2.6386 - val_accuracy: 0.6667\n",
      "Epoch 310/1000\n",
      "68/68 [==============================] - 0s 727us/step - loss: 0.7605 - accuracy: 0.6185 - val_loss: 2.3354 - val_accuracy: 0.6778\n",
      "Epoch 311/1000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.8184 - accuracy: 0.6148 - val_loss: 2.5471 - val_accuracy: 0.6556\n",
      "Epoch 312/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 0.7613 - accuracy: 0.6148 - val_loss: 2.5812 - val_accuracy: 0.6444\n",
      "Epoch 313/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 0.8228 - accuracy: 0.6148 - val_loss: 2.4626 - val_accuracy: 0.6444\n",
      "Epoch 314/1000\n",
      "68/68 [==============================] - 0s 750us/step - loss: 0.7782 - accuracy: 0.6000 - val_loss: 2.3364 - val_accuracy: 0.6556\n",
      "Epoch 315/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.7325 - accuracy: 0.6296 - val_loss: 2.3245 - val_accuracy: 0.6556\n",
      "Epoch 316/1000\n",
      "68/68 [==============================] - 0s 746us/step - loss: 0.7221 - accuracy: 0.6370 - val_loss: 2.3611 - val_accuracy: 0.6556\n",
      "Epoch 317/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.7382 - accuracy: 0.6259 - val_loss: 2.4080 - val_accuracy: 0.6667\n",
      "Epoch 318/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 0.9212 - accuracy: 0.6148 - val_loss: 2.4083 - val_accuracy: 0.6333\n",
      "Epoch 319/1000\n",
      "68/68 [==============================] - 0s 785us/step - loss: 0.8398 - accuracy: 0.5926 - val_loss: 2.3837 - val_accuracy: 0.6444\n",
      "Epoch 320/1000\n",
      "68/68 [==============================] - 0s 711us/step - loss: 0.8223 - accuracy: 0.5889 - val_loss: 2.2475 - val_accuracy: 0.6667\n",
      "Epoch 321/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.7561 - accuracy: 0.5963 - val_loss: 2.2904 - val_accuracy: 0.6667\n",
      "Epoch 322/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.7589 - accuracy: 0.6185 - val_loss: 2.3107 - val_accuracy: 0.6556\n",
      "Epoch 323/1000\n",
      "68/68 [==============================] - 0s 745us/step - loss: 0.8506 - accuracy: 0.6037 - val_loss: 2.2982 - val_accuracy: 0.6556\n",
      "Epoch 324/1000\n",
      "68/68 [==============================] - 0s 747us/step - loss: 0.8086 - accuracy: 0.6037 - val_loss: 2.3144 - val_accuracy: 0.6556\n",
      "Epoch 325/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.9475 - accuracy: 0.6111 - val_loss: 2.3944 - val_accuracy: 0.6444\n",
      "Epoch 326/1000\n",
      "68/68 [==============================] - 0s 755us/step - loss: 1.0891 - accuracy: 0.6185 - val_loss: 2.4540 - val_accuracy: 0.6333\n",
      "Epoch 327/1000\n",
      "68/68 [==============================] - 0s 746us/step - loss: 0.8895 - accuracy: 0.5852 - val_loss: 2.5257 - val_accuracy: 0.6556\n",
      "Epoch 328/1000\n",
      "68/68 [==============================] - 0s 749us/step - loss: 0.7766 - accuracy: 0.6074 - val_loss: 2.5093 - val_accuracy: 0.6556\n",
      "Epoch 329/1000\n",
      "68/68 [==============================] - 0s 770us/step - loss: 0.7655 - accuracy: 0.6148 - val_loss: 2.5276 - val_accuracy: 0.6556\n",
      "Epoch 330/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 826us/step - loss: 0.8366 - accuracy: 0.6074 - val_loss: 2.4998 - val_accuracy: 0.6556\n",
      "Epoch 331/1000\n",
      "68/68 [==============================] - 0s 736us/step - loss: 0.8217 - accuracy: 0.6074 - val_loss: 2.5363 - val_accuracy: 0.6444\n",
      "Epoch 332/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.7817 - accuracy: 0.6000 - val_loss: 2.3796 - val_accuracy: 0.6556\n",
      "Epoch 333/1000\n",
      "68/68 [==============================] - 0s 736us/step - loss: 0.7475 - accuracy: 0.6111 - val_loss: 2.4308 - val_accuracy: 0.6667\n",
      "Epoch 334/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.7826 - accuracy: 0.5889 - val_loss: 2.4222 - val_accuracy: 0.6667\n",
      "Epoch 335/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 0.8455 - accuracy: 0.6037 - val_loss: 2.4540 - val_accuracy: 0.6667\n",
      "Epoch 336/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 0.8105 - accuracy: 0.5889 - val_loss: 2.4768 - val_accuracy: 0.6667\n",
      "Epoch 337/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 0.7419 - accuracy: 0.6333 - val_loss: 2.5548 - val_accuracy: 0.6556\n",
      "Epoch 338/1000\n",
      "68/68 [==============================] - 0s 753us/step - loss: 0.7721 - accuracy: 0.5963 - val_loss: 2.6534 - val_accuracy: 0.6556\n",
      "Epoch 339/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.9051 - accuracy: 0.6185 - val_loss: 2.7352 - val_accuracy: 0.6667\n",
      "Epoch 340/1000\n",
      "68/68 [==============================] - 0s 761us/step - loss: 0.7769 - accuracy: 0.6037 - val_loss: 2.5657 - val_accuracy: 0.6667\n",
      "Epoch 341/1000\n",
      "68/68 [==============================] - 0s 940us/step - loss: 0.8131 - accuracy: 0.6222 - val_loss: 2.6072 - val_accuracy: 0.6667\n",
      "Epoch 342/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 0.7640 - accuracy: 0.6148 - val_loss: 2.5710 - val_accuracy: 0.6556\n",
      "Epoch 343/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.7767 - accuracy: 0.5889 - val_loss: 2.4339 - val_accuracy: 0.6556\n",
      "Epoch 344/1000\n",
      "68/68 [==============================] - 0s 727us/step - loss: 0.8041 - accuracy: 0.6222 - val_loss: 2.5055 - val_accuracy: 0.6556\n",
      "Epoch 345/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.7535 - accuracy: 0.6185 - val_loss: 2.5439 - val_accuracy: 0.6444\n",
      "Epoch 346/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 0.7813 - accuracy: 0.6148 - val_loss: 2.5542 - val_accuracy: 0.6444\n",
      "Epoch 347/1000\n",
      "68/68 [==============================] - 0s 704us/step - loss: 0.8235 - accuracy: 0.6148 - val_loss: 2.5899 - val_accuracy: 0.6444\n",
      "Epoch 348/1000\n",
      "68/68 [==============================] - 0s 753us/step - loss: 0.7985 - accuracy: 0.6074 - val_loss: 2.5097 - val_accuracy: 0.6444\n",
      "Epoch 349/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 0.7225 - accuracy: 0.6222 - val_loss: 2.4948 - val_accuracy: 0.6444\n",
      "Epoch 350/1000\n",
      "68/68 [==============================] - 0s 838us/step - loss: 0.7551 - accuracy: 0.6111 - val_loss: 2.5148 - val_accuracy: 0.6444\n",
      "Epoch 351/1000\n",
      "68/68 [==============================] - 0s 764us/step - loss: 0.7590 - accuracy: 0.6111 - val_loss: 2.4899 - val_accuracy: 0.6667\n",
      "Epoch 352/1000\n",
      "68/68 [==============================] - 0s 706us/step - loss: 0.7430 - accuracy: 0.6111 - val_loss: 2.5428 - val_accuracy: 0.6667\n",
      "Epoch 353/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.7722 - accuracy: 0.6037 - val_loss: 2.5690 - val_accuracy: 0.6778\n",
      "Epoch 354/1000\n",
      "68/68 [==============================] - 0s 762us/step - loss: 0.7382 - accuracy: 0.6148 - val_loss: 2.5940 - val_accuracy: 0.6667\n",
      "Epoch 355/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 0.7363 - accuracy: 0.6185 - val_loss: 2.3993 - val_accuracy: 0.6667\n",
      "Epoch 356/1000\n",
      "68/68 [==============================] - 0s 773us/step - loss: 0.7996 - accuracy: 0.6037 - val_loss: 2.4506 - val_accuracy: 0.6444\n",
      "Epoch 357/1000\n",
      "68/68 [==============================] - 0s 747us/step - loss: 0.8420 - accuracy: 0.6148 - val_loss: 2.2998 - val_accuracy: 0.6333\n",
      "Epoch 358/1000\n",
      "68/68 [==============================] - 0s 743us/step - loss: 0.7861 - accuracy: 0.5926 - val_loss: 2.3666 - val_accuracy: 0.6333\n",
      "Epoch 359/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.7577 - accuracy: 0.6037 - val_loss: 2.4206 - val_accuracy: 0.6333\n",
      "Epoch 360/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.7718 - accuracy: 0.5926 - val_loss: 2.3718 - val_accuracy: 0.6444\n",
      "Epoch 361/1000\n",
      "68/68 [==============================] - 0s 755us/step - loss: 0.7468 - accuracy: 0.6111 - val_loss: 2.3656 - val_accuracy: 0.6444\n",
      "Epoch 362/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.7832 - accuracy: 0.5926 - val_loss: 2.4265 - val_accuracy: 0.6444\n",
      "Epoch 363/1000\n",
      "68/68 [==============================] - 0s 771us/step - loss: 0.7798 - accuracy: 0.6185 - val_loss: 2.3737 - val_accuracy: 0.6444\n",
      "Epoch 364/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.7422 - accuracy: 0.6111 - val_loss: 2.4489 - val_accuracy: 0.6444\n",
      "Epoch 365/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 0.7672 - accuracy: 0.6000 - val_loss: 2.4869 - val_accuracy: 0.6444\n",
      "Epoch 366/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.7717 - accuracy: 0.5815 - val_loss: 2.5172 - val_accuracy: 0.6444\n",
      "Epoch 367/1000\n",
      "68/68 [==============================] - 0s 743us/step - loss: 0.8007 - accuracy: 0.5926 - val_loss: 2.6709 - val_accuracy: 0.6333\n",
      "Epoch 368/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.7436 - accuracy: 0.6111 - val_loss: 2.4363 - val_accuracy: 0.6444\n",
      "Epoch 369/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.7471 - accuracy: 0.6222 - val_loss: 2.4436 - val_accuracy: 0.6444\n",
      "Epoch 370/1000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.8314 - accuracy: 0.5889 - val_loss: 2.5102 - val_accuracy: 0.6667\n",
      "Epoch 371/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.7672 - accuracy: 0.6185 - val_loss: 2.4703 - val_accuracy: 0.6556\n",
      "Epoch 372/1000\n",
      "68/68 [==============================] - 0s 727us/step - loss: 0.7816 - accuracy: 0.5778 - val_loss: 2.4950 - val_accuracy: 0.6444\n",
      "Epoch 373/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.7623 - accuracy: 0.6037 - val_loss: 2.5179 - val_accuracy: 0.6444\n",
      "Epoch 374/1000\n",
      "68/68 [==============================] - 0s 736us/step - loss: 0.7908 - accuracy: 0.6074 - val_loss: 2.6331 - val_accuracy: 0.6556\n",
      "Epoch 375/1000\n",
      "68/68 [==============================] - 0s 711us/step - loss: 0.7345 - accuracy: 0.6222 - val_loss: 2.6113 - val_accuracy: 0.6333\n",
      "Epoch 376/1000\n",
      "68/68 [==============================] - 0s 747us/step - loss: 0.7869 - accuracy: 0.5963 - val_loss: 2.6556 - val_accuracy: 0.6556\n",
      "Epoch 377/1000\n",
      "68/68 [==============================] - 0s 759us/step - loss: 0.7314 - accuracy: 0.6148 - val_loss: 2.6846 - val_accuracy: 0.6556\n",
      "Epoch 378/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 0.7537 - accuracy: 0.6222 - val_loss: 2.6888 - val_accuracy: 0.6333\n",
      "Epoch 379/1000\n",
      "68/68 [==============================] - 0s 747us/step - loss: 0.7190 - accuracy: 0.6296 - val_loss: 2.7245 - val_accuracy: 0.6333\n",
      "Epoch 380/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 0.7326 - accuracy: 0.6185 - val_loss: 2.7740 - val_accuracy: 0.6333\n",
      "Epoch 381/1000\n",
      "68/68 [==============================] - 0s 749us/step - loss: 0.7488 - accuracy: 0.6074 - val_loss: 2.7851 - val_accuracy: 0.6333\n",
      "Epoch 382/1000\n",
      "68/68 [==============================] - 0s 756us/step - loss: 0.7675 - accuracy: 0.6000 - val_loss: 2.7230 - val_accuracy: 0.6333\n",
      "Epoch 383/1000\n",
      "68/68 [==============================] - 0s 706us/step - loss: 0.7682 - accuracy: 0.6037 - val_loss: 2.6812 - val_accuracy: 0.6333\n",
      "Epoch 384/1000\n",
      "68/68 [==============================] - 0s 722us/step - loss: 0.7516 - accuracy: 0.6037 - val_loss: 2.7153 - val_accuracy: 0.6333\n",
      "Epoch 385/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.7613 - accuracy: 0.6111 - val_loss: 2.7389 - val_accuracy: 0.6333\n",
      "Epoch 386/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 763us/step - loss: 0.7434 - accuracy: 0.6037 - val_loss: 2.7286 - val_accuracy: 0.6333\n",
      "Epoch 387/1000\n",
      "68/68 [==============================] - 0s 716us/step - loss: 0.7635 - accuracy: 0.6074 - val_loss: 2.7742 - val_accuracy: 0.6333\n",
      "Epoch 388/1000\n",
      "68/68 [==============================] - 0s 777us/step - loss: 0.7122 - accuracy: 0.6222 - val_loss: 2.7847 - val_accuracy: 0.6333\n",
      "Epoch 389/1000\n",
      "68/68 [==============================] - 0s 841us/step - loss: 0.7618 - accuracy: 0.6148 - val_loss: 2.7709 - val_accuracy: 0.6333\n",
      "Epoch 390/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.7365 - accuracy: 0.6222 - val_loss: 2.7510 - val_accuracy: 0.6333\n",
      "Epoch 391/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.8026 - accuracy: 0.6000 - val_loss: 2.7745 - val_accuracy: 0.6444\n",
      "Epoch 392/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.9085 - accuracy: 0.5778 - val_loss: 2.9522 - val_accuracy: 0.6222\n",
      "Epoch 393/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 0.7681 - accuracy: 0.6148 - val_loss: 2.8437 - val_accuracy: 0.6333\n",
      "Epoch 394/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.7369 - accuracy: 0.6074 - val_loss: 2.7870 - val_accuracy: 0.6333\n",
      "Epoch 395/1000\n",
      "68/68 [==============================] - 0s 745us/step - loss: 0.8191 - accuracy: 0.6111 - val_loss: 2.7565 - val_accuracy: 0.6333\n",
      "Epoch 396/1000\n",
      "68/68 [==============================] - 0s 771us/step - loss: 0.7519 - accuracy: 0.6111 - val_loss: 2.8150 - val_accuracy: 0.6333\n",
      "Epoch 397/1000\n",
      "68/68 [==============================] - 0s 747us/step - loss: 0.7388 - accuracy: 0.6148 - val_loss: 2.8640 - val_accuracy: 0.6333\n",
      "Epoch 398/1000\n",
      "68/68 [==============================] - 0s 744us/step - loss: 0.7994 - accuracy: 0.6000 - val_loss: 2.8836 - val_accuracy: 0.6444\n",
      "Epoch 399/1000\n",
      "68/68 [==============================] - 0s 812us/step - loss: 0.7819 - accuracy: 0.6037 - val_loss: 2.7311 - val_accuracy: 0.6333\n",
      "Epoch 400/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.7634 - accuracy: 0.6037 - val_loss: 2.8251 - val_accuracy: 0.6444\n",
      "Epoch 401/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.7620 - accuracy: 0.6037 - val_loss: 2.6215 - val_accuracy: 0.6556\n",
      "Epoch 402/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.7316 - accuracy: 0.6222 - val_loss: 2.6778 - val_accuracy: 0.6556\n",
      "Epoch 403/1000\n",
      "68/68 [==============================] - 0s 736us/step - loss: 0.7844 - accuracy: 0.6074 - val_loss: 2.7200 - val_accuracy: 0.6556\n",
      "Epoch 404/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.7224 - accuracy: 0.6259 - val_loss: 2.7786 - val_accuracy: 0.6556\n",
      "Epoch 405/1000\n",
      "68/68 [==============================] - 0s 712us/step - loss: 0.7852 - accuracy: 0.5889 - val_loss: 2.8226 - val_accuracy: 0.6556\n",
      "Epoch 406/1000\n",
      "68/68 [==============================] - 0s 727us/step - loss: 0.7473 - accuracy: 0.6000 - val_loss: 2.8369 - val_accuracy: 0.6556\n",
      "Epoch 407/1000\n",
      "68/68 [==============================] - 0s 762us/step - loss: 0.8148 - accuracy: 0.5778 - val_loss: 2.9341 - val_accuracy: 0.6444\n",
      "Epoch 408/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 0.7248 - accuracy: 0.6222 - val_loss: 2.8825 - val_accuracy: 0.6444\n",
      "Epoch 409/1000\n",
      "68/68 [==============================] - 0s 852us/step - loss: 0.7581 - accuracy: 0.5963 - val_loss: 2.8677 - val_accuracy: 0.6333\n",
      "Epoch 410/1000\n",
      "68/68 [==============================] - 0s 764us/step - loss: 0.8784 - accuracy: 0.6037 - val_loss: 2.8524 - val_accuracy: 0.6333\n",
      "Epoch 411/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.7282 - accuracy: 0.6148 - val_loss: 2.8980 - val_accuracy: 0.6556\n",
      "Epoch 412/1000\n",
      "68/68 [==============================] - 0s 806us/step - loss: 0.7384 - accuracy: 0.6074 - val_loss: 2.9026 - val_accuracy: 0.6444\n",
      "Epoch 413/1000\n",
      "68/68 [==============================] - 0s 749us/step - loss: 0.7933 - accuracy: 0.6185 - val_loss: 2.7559 - val_accuracy: 0.6444\n",
      "Epoch 414/1000\n",
      "68/68 [==============================] - 0s 761us/step - loss: 0.6957 - accuracy: 0.6481 - val_loss: 2.8242 - val_accuracy: 0.6444\n",
      "Epoch 415/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.7266 - accuracy: 0.6185 - val_loss: 2.8378 - val_accuracy: 0.6444\n",
      "Epoch 416/1000\n",
      "68/68 [==============================] - 0s 756us/step - loss: 0.7063 - accuracy: 0.6296 - val_loss: 2.8992 - val_accuracy: 0.6444\n",
      "Epoch 417/1000\n",
      "68/68 [==============================] - 0s 710us/step - loss: 0.7559 - accuracy: 0.5963 - val_loss: 2.9601 - val_accuracy: 0.6444\n",
      "Epoch 418/1000\n",
      "68/68 [==============================] - 0s 708us/step - loss: 0.7527 - accuracy: 0.6259 - val_loss: 2.9590 - val_accuracy: 0.6444\n",
      "Epoch 419/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.7822 - accuracy: 0.6111 - val_loss: 2.9601 - val_accuracy: 0.6444\n",
      "Epoch 420/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.7244 - accuracy: 0.6222 - val_loss: 2.9675 - val_accuracy: 0.6556\n",
      "Epoch 421/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.7918 - accuracy: 0.6074 - val_loss: 2.9339 - val_accuracy: 0.6556\n",
      "Epoch 422/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 0.7554 - accuracy: 0.6000 - val_loss: 2.9477 - val_accuracy: 0.6444\n",
      "Epoch 423/1000\n",
      "68/68 [==============================] - 0s 727us/step - loss: 0.7664 - accuracy: 0.5963 - val_loss: 3.1052 - val_accuracy: 0.6556\n",
      "Epoch 424/1000\n",
      "68/68 [==============================] - 0s 743us/step - loss: 0.7446 - accuracy: 0.6222 - val_loss: 3.0773 - val_accuracy: 0.6333\n",
      "Epoch 425/1000\n",
      "68/68 [==============================] - 0s 749us/step - loss: 0.7361 - accuracy: 0.6111 - val_loss: 3.1385 - val_accuracy: 0.6444\n",
      "Epoch 426/1000\n",
      "68/68 [==============================] - 0s 787us/step - loss: 0.7777 - accuracy: 0.5963 - val_loss: 3.0931 - val_accuracy: 0.6444\n",
      "Epoch 427/1000\n",
      "68/68 [==============================] - 0s 810us/step - loss: 0.7071 - accuracy: 0.6296 - val_loss: 3.1027 - val_accuracy: 0.6444\n",
      "Epoch 428/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.7416 - accuracy: 0.6259 - val_loss: 3.0713 - val_accuracy: 0.6444\n",
      "Epoch 429/1000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7095 - accuracy: 0.6296 - val_loss: 3.0800 - val_accuracy: 0.6444\n",
      "Epoch 430/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.7316 - accuracy: 0.6185 - val_loss: 3.1131 - val_accuracy: 0.6444\n",
      "Epoch 431/1000\n",
      "68/68 [==============================] - 0s 706us/step - loss: 0.7313 - accuracy: 0.6185 - val_loss: 3.1120 - val_accuracy: 0.6444\n",
      "Epoch 432/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 0.7635 - accuracy: 0.5963 - val_loss: 3.1117 - val_accuracy: 0.6444\n",
      "Epoch 433/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 0.8140 - accuracy: 0.6148 - val_loss: 3.1737 - val_accuracy: 0.6333\n",
      "Epoch 434/1000\n",
      "68/68 [==============================] - 0s 750us/step - loss: 0.7489 - accuracy: 0.6222 - val_loss: 3.0164 - val_accuracy: 0.6444\n",
      "Epoch 435/1000\n",
      "68/68 [==============================] - 0s 708us/step - loss: 0.8170 - accuracy: 0.5963 - val_loss: 2.8215 - val_accuracy: 0.6444\n",
      "Epoch 436/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 0.7298 - accuracy: 0.6111 - val_loss: 2.9594 - val_accuracy: 0.6333\n",
      "Epoch 437/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 0.7879 - accuracy: 0.6000 - val_loss: 2.8496 - val_accuracy: 0.6222\n",
      "Epoch 438/1000\n",
      "68/68 [==============================] - 0s 764us/step - loss: 0.7542 - accuracy: 0.6074 - val_loss: 2.7796 - val_accuracy: 0.6556\n",
      "Epoch 439/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 0.7726 - accuracy: 0.5963 - val_loss: 2.7765 - val_accuracy: 0.6556\n",
      "Epoch 440/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.7863 - accuracy: 0.6074 - val_loss: 2.8125 - val_accuracy: 0.6444\n",
      "Epoch 441/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.7997 - accuracy: 0.6259 - val_loss: 2.9005 - val_accuracy: 0.6556\n",
      "Epoch 442/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 750us/step - loss: 0.7345 - accuracy: 0.6185 - val_loss: 2.9615 - val_accuracy: 0.6444\n",
      "Epoch 443/1000\n",
      "68/68 [==============================] - 0s 727us/step - loss: 0.7994 - accuracy: 0.6074 - val_loss: 2.9604 - val_accuracy: 0.6444\n",
      "Epoch 444/1000\n",
      "68/68 [==============================] - 0s 722us/step - loss: 0.7897 - accuracy: 0.5815 - val_loss: 2.7742 - val_accuracy: 0.6444\n",
      "Epoch 445/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 0.7772 - accuracy: 0.5926 - val_loss: 2.8114 - val_accuracy: 0.6444\n",
      "Epoch 446/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.8079 - accuracy: 0.6148 - val_loss: 2.8220 - val_accuracy: 0.6222\n",
      "Epoch 447/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 0.7576 - accuracy: 0.5926 - val_loss: 2.8656 - val_accuracy: 0.6444\n",
      "Epoch 448/1000\n",
      "68/68 [==============================] - 0s 805us/step - loss: 0.8051 - accuracy: 0.5926 - val_loss: 2.8827 - val_accuracy: 0.6333\n",
      "Epoch 449/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.7407 - accuracy: 0.6037 - val_loss: 2.9058 - val_accuracy: 0.6333\n",
      "Epoch 450/1000\n",
      "68/68 [==============================] - 0s 768us/step - loss: 0.7557 - accuracy: 0.6037 - val_loss: 2.9824 - val_accuracy: 0.6333\n",
      "Epoch 451/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 0.8830 - accuracy: 0.6000 - val_loss: 2.9123 - val_accuracy: 0.6444\n",
      "Epoch 452/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.7182 - accuracy: 0.6222 - val_loss: 2.9978 - val_accuracy: 0.6444\n",
      "Epoch 453/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 0.7574 - accuracy: 0.6037 - val_loss: 3.0223 - val_accuracy: 0.6333\n",
      "Epoch 454/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.7641 - accuracy: 0.6185 - val_loss: 2.8378 - val_accuracy: 0.6444\n",
      "Epoch 455/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 0.7701 - accuracy: 0.6074 - val_loss: 2.8222 - val_accuracy: 0.6444\n",
      "Epoch 456/1000\n",
      "68/68 [==============================] - 0s 714us/step - loss: 0.7266 - accuracy: 0.6148 - val_loss: 2.8671 - val_accuracy: 0.6444\n",
      "Epoch 457/1000\n",
      "68/68 [==============================] - 0s 702us/step - loss: 0.7489 - accuracy: 0.6074 - val_loss: 2.8717 - val_accuracy: 0.6444\n",
      "Epoch 458/1000\n",
      "68/68 [==============================] - 0s 711us/step - loss: 0.7412 - accuracy: 0.6111 - val_loss: 2.9055 - val_accuracy: 0.6444\n",
      "Epoch 459/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.7292 - accuracy: 0.6148 - val_loss: 3.0139 - val_accuracy: 0.6444\n",
      "Epoch 460/1000\n",
      "68/68 [==============================] - 0s 716us/step - loss: 0.7217 - accuracy: 0.6222 - val_loss: 3.0103 - val_accuracy: 0.6556\n",
      "Epoch 461/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.8443 - accuracy: 0.5926 - val_loss: 2.9127 - val_accuracy: 0.6444\n",
      "Epoch 462/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.7104 - accuracy: 0.6222 - val_loss: 2.9144 - val_accuracy: 0.6444\n",
      "Epoch 463/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 0.7964 - accuracy: 0.6000 - val_loss: 2.9594 - val_accuracy: 0.6444\n",
      "Epoch 464/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 0.7502 - accuracy: 0.6111 - val_loss: 2.9006 - val_accuracy: 0.6444\n",
      "Epoch 465/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.7777 - accuracy: 0.6074 - val_loss: 2.8298 - val_accuracy: 0.6333\n",
      "Epoch 466/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.7501 - accuracy: 0.6037 - val_loss: 2.8216 - val_accuracy: 0.6444\n",
      "Epoch 467/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.7270 - accuracy: 0.6296 - val_loss: 2.7279 - val_accuracy: 0.6333\n",
      "Epoch 468/1000\n",
      "68/68 [==============================] - 0s 792us/step - loss: 0.7047 - accuracy: 0.6370 - val_loss: 2.7434 - val_accuracy: 0.6333\n",
      "Epoch 469/1000\n",
      "68/68 [==============================] - 0s 798us/step - loss: 0.7479 - accuracy: 0.6037 - val_loss: 2.6816 - val_accuracy: 0.6444\n",
      "Epoch 470/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.7299 - accuracy: 0.6148 - val_loss: 2.6848 - val_accuracy: 0.6556\n",
      "Epoch 471/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.8245 - accuracy: 0.6000 - val_loss: 2.7235 - val_accuracy: 0.6556\n",
      "Epoch 472/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.7216 - accuracy: 0.6222 - val_loss: 2.7554 - val_accuracy: 0.6556\n",
      "Epoch 473/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.7314 - accuracy: 0.6259 - val_loss: 2.8046 - val_accuracy: 0.6556\n",
      "Epoch 474/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.7828 - accuracy: 0.6000 - val_loss: 2.8257 - val_accuracy: 0.6444\n",
      "Epoch 475/1000\n",
      "68/68 [==============================] - 0s 712us/step - loss: 0.7357 - accuracy: 0.6037 - val_loss: 2.7887 - val_accuracy: 0.6556\n",
      "Epoch 476/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.7116 - accuracy: 0.6222 - val_loss: 2.8053 - val_accuracy: 0.6556\n",
      "Epoch 477/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 0.7351 - accuracy: 0.6148 - val_loss: 2.7106 - val_accuracy: 0.6556\n",
      "Epoch 478/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.7714 - accuracy: 0.5963 - val_loss: 2.7616 - val_accuracy: 0.6556\n",
      "Epoch 479/1000\n",
      "68/68 [==============================] - 0s 764us/step - loss: 0.7535 - accuracy: 0.5963 - val_loss: 2.8005 - val_accuracy: 0.6444\n",
      "Epoch 480/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.7587 - accuracy: 0.6037 - val_loss: 2.8251 - val_accuracy: 0.6556\n",
      "Epoch 481/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.7337 - accuracy: 0.6148 - val_loss: 2.8421 - val_accuracy: 0.6556\n",
      "Epoch 482/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.8185 - accuracy: 0.5963 - val_loss: 2.8379 - val_accuracy: 0.6556\n",
      "Epoch 483/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 0.7569 - accuracy: 0.6111 - val_loss: 2.8100 - val_accuracy: 0.6556\n",
      "Epoch 484/1000\n",
      "68/68 [==============================] - 0s 739us/step - loss: 0.8606 - accuracy: 0.5889 - val_loss: 2.8345 - val_accuracy: 0.6556\n",
      "Epoch 485/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.7213 - accuracy: 0.6259 - val_loss: 2.8627 - val_accuracy: 0.6556\n",
      "Epoch 486/1000\n",
      "68/68 [==============================] - 0s 761us/step - loss: 0.7263 - accuracy: 0.6259 - val_loss: 2.8831 - val_accuracy: 0.6444\n",
      "Epoch 487/1000\n",
      "68/68 [==============================] - 0s 714us/step - loss: 0.7523 - accuracy: 0.6000 - val_loss: 2.8380 - val_accuracy: 0.6444\n",
      "Epoch 488/1000\n",
      "68/68 [==============================] - 0s 765us/step - loss: 0.7243 - accuracy: 0.6259 - val_loss: 2.8435 - val_accuracy: 0.6556\n",
      "Epoch 489/1000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.8494 - accuracy: 0.6148 - val_loss: 2.8872 - val_accuracy: 0.6556\n",
      "Epoch 490/1000\n",
      "68/68 [==============================] - 0s 764us/step - loss: 0.8066 - accuracy: 0.5815 - val_loss: 2.8696 - val_accuracy: 0.6556\n",
      "Epoch 491/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.7599 - accuracy: 0.6111 - val_loss: 2.9310 - val_accuracy: 0.6556\n",
      "Epoch 492/1000\n",
      "68/68 [==============================] - 0s 753us/step - loss: 0.7337 - accuracy: 0.6148 - val_loss: 2.8374 - val_accuracy: 0.6556\n",
      "Epoch 493/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.7142 - accuracy: 0.6333 - val_loss: 3.0753 - val_accuracy: 0.6556\n",
      "Epoch 494/1000\n",
      "68/68 [==============================] - 0s 755us/step - loss: 0.7145 - accuracy: 0.6259 - val_loss: 3.1087 - val_accuracy: 0.6556\n",
      "Epoch 495/1000\n",
      "68/68 [==============================] - 0s 748us/step - loss: 0.7541 - accuracy: 0.6037 - val_loss: 3.0746 - val_accuracy: 0.6444\n",
      "Epoch 496/1000\n",
      "68/68 [==============================] - 0s 746us/step - loss: 0.7878 - accuracy: 0.6000 - val_loss: 3.0796 - val_accuracy: 0.6333\n",
      "Epoch 497/1000\n",
      "68/68 [==============================] - 0s 716us/step - loss: 0.6958 - accuracy: 0.6370 - val_loss: 2.9890 - val_accuracy: 0.6333\n",
      "Epoch 498/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 749us/step - loss: 0.7648 - accuracy: 0.5852 - val_loss: 3.0475 - val_accuracy: 0.6333\n",
      "Epoch 499/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 0.7189 - accuracy: 0.6185 - val_loss: 3.0199 - val_accuracy: 0.6333\n",
      "Epoch 500/1000\n",
      "68/68 [==============================] - 0s 727us/step - loss: 0.7345 - accuracy: 0.6111 - val_loss: 3.0227 - val_accuracy: 0.6333\n",
      "Epoch 501/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.7251 - accuracy: 0.6148 - val_loss: 3.0311 - val_accuracy: 0.6444\n",
      "Epoch 502/1000\n",
      "68/68 [==============================] - 0s 750us/step - loss: 0.7780 - accuracy: 0.6222 - val_loss: 3.1053 - val_accuracy: 0.6444\n",
      "Epoch 503/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 0.7200 - accuracy: 0.6259 - val_loss: 3.0859 - val_accuracy: 0.6444\n",
      "Epoch 504/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.7308 - accuracy: 0.6148 - val_loss: 3.0885 - val_accuracy: 0.6444\n",
      "Epoch 505/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.7112 - accuracy: 0.6222 - val_loss: 3.1054 - val_accuracy: 0.6556\n",
      "Epoch 506/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.7531 - accuracy: 0.6000 - val_loss: 3.1166 - val_accuracy: 0.6556\n",
      "Epoch 507/1000\n",
      "68/68 [==============================] - 0s 806us/step - loss: 0.6923 - accuracy: 0.6407 - val_loss: 3.1364 - val_accuracy: 0.6667\n",
      "Epoch 508/1000\n",
      "68/68 [==============================] - 0s 832us/step - loss: 0.7299 - accuracy: 0.6333 - val_loss: 3.1690 - val_accuracy: 0.6556\n",
      "Epoch 509/1000\n",
      "68/68 [==============================] - 0s 767us/step - loss: 0.7704 - accuracy: 0.6111 - val_loss: 3.2007 - val_accuracy: 0.6556\n",
      "Epoch 510/1000\n",
      "68/68 [==============================] - 0s 746us/step - loss: 0.7681 - accuracy: 0.6037 - val_loss: 3.1799 - val_accuracy: 0.6444\n",
      "Epoch 511/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 0.7283 - accuracy: 0.6185 - val_loss: 3.1691 - val_accuracy: 0.6444\n",
      "Epoch 512/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.7283 - accuracy: 0.6185 - val_loss: 3.1869 - val_accuracy: 0.6444\n",
      "Epoch 513/1000\n",
      "68/68 [==============================] - 0s 722us/step - loss: 0.7248 - accuracy: 0.6185 - val_loss: 3.1936 - val_accuracy: 0.6444\n",
      "Epoch 514/1000\n",
      "68/68 [==============================] - 0s 722us/step - loss: 0.7064 - accuracy: 0.6259 - val_loss: 3.1960 - val_accuracy: 0.6444\n",
      "Epoch 515/1000\n",
      "68/68 [==============================] - 0s 756us/step - loss: 0.7084 - accuracy: 0.6296 - val_loss: 3.1907 - val_accuracy: 0.6444\n",
      "Epoch 516/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 0.8184 - accuracy: 0.5963 - val_loss: 3.3404 - val_accuracy: 0.6444\n",
      "Epoch 517/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 0.6978 - accuracy: 0.6370 - val_loss: 3.2988 - val_accuracy: 0.6556\n",
      "Epoch 518/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.7242 - accuracy: 0.6185 - val_loss: 3.2963 - val_accuracy: 0.6556\n",
      "Epoch 519/1000\n",
      "68/68 [==============================] - 0s 739us/step - loss: 0.7278 - accuracy: 0.6222 - val_loss: 3.3035 - val_accuracy: 0.6556\n",
      "Epoch 520/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.7142 - accuracy: 0.6296 - val_loss: 3.2925 - val_accuracy: 0.6556\n",
      "Epoch 521/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 0.7311 - accuracy: 0.6111 - val_loss: 3.2268 - val_accuracy: 0.6444\n",
      "Epoch 522/1000\n",
      "68/68 [==============================] - 0s 753us/step - loss: 0.7167 - accuracy: 0.6185 - val_loss: 3.2322 - val_accuracy: 0.6556\n",
      "Epoch 523/1000\n",
      "68/68 [==============================] - 0s 755us/step - loss: 0.7628 - accuracy: 0.6259 - val_loss: 3.2749 - val_accuracy: 0.6556\n",
      "Epoch 524/1000\n",
      "68/68 [==============================] - 0s 711us/step - loss: 0.7501 - accuracy: 0.6000 - val_loss: 3.1739 - val_accuracy: 0.6556\n",
      "Epoch 525/1000\n",
      "68/68 [==============================] - 0s 711us/step - loss: 0.7223 - accuracy: 0.6185 - val_loss: 3.0413 - val_accuracy: 0.6556\n",
      "Epoch 526/1000\n",
      "68/68 [==============================] - 0s 707us/step - loss: 0.7199 - accuracy: 0.6222 - val_loss: 3.0282 - val_accuracy: 0.6556\n",
      "Epoch 527/1000\n",
      "68/68 [==============================] - 0s 707us/step - loss: 0.7145 - accuracy: 0.6185 - val_loss: 3.0571 - val_accuracy: 0.6556\n",
      "Epoch 528/1000\n",
      "68/68 [==============================] - 0s 802us/step - loss: 0.7208 - accuracy: 0.6185 - val_loss: 3.0371 - val_accuracy: 0.6556\n",
      "Epoch 529/1000\n",
      "68/68 [==============================] - 0s 707us/step - loss: 0.7496 - accuracy: 0.5926 - val_loss: 2.9369 - val_accuracy: 0.6667\n",
      "Epoch 530/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.6958 - accuracy: 0.6444 - val_loss: 2.9784 - val_accuracy: 0.6778\n",
      "Epoch 531/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 0.7423 - accuracy: 0.6074 - val_loss: 2.9266 - val_accuracy: 0.6889\n",
      "Epoch 532/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.8637 - accuracy: 0.6111 - val_loss: 2.9770 - val_accuracy: 0.6667\n",
      "Epoch 533/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.7637 - accuracy: 0.6296 - val_loss: 3.0984 - val_accuracy: 0.6667\n",
      "Epoch 534/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 0.7194 - accuracy: 0.6222 - val_loss: 3.1025 - val_accuracy: 0.6667\n",
      "Epoch 535/1000\n",
      "68/68 [==============================] - 0s 781us/step - loss: 0.7268 - accuracy: 0.6333 - val_loss: 3.0064 - val_accuracy: 0.6889\n",
      "Epoch 536/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.7204 - accuracy: 0.6370 - val_loss: 2.9204 - val_accuracy: 0.7111\n",
      "Epoch 537/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 0.7523 - accuracy: 0.6111 - val_loss: 2.9611 - val_accuracy: 0.7000\n",
      "Epoch 538/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.7381 - accuracy: 0.6296 - val_loss: 2.8111 - val_accuracy: 0.6778\n",
      "Epoch 539/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 0.7869 - accuracy: 0.6111 - val_loss: 2.7724 - val_accuracy: 0.6778\n",
      "Epoch 540/1000\n",
      "68/68 [==============================] - 0s 739us/step - loss: 0.7683 - accuracy: 0.6185 - val_loss: 2.9218 - val_accuracy: 0.6556\n",
      "Epoch 541/1000\n",
      "68/68 [==============================] - 0s 749us/step - loss: 0.7521 - accuracy: 0.6148 - val_loss: 2.8758 - val_accuracy: 0.6667\n",
      "Epoch 542/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.8383 - accuracy: 0.6296 - val_loss: 3.0460 - val_accuracy: 0.6667\n",
      "Epoch 543/1000\n",
      "68/68 [==============================] - 0s 710us/step - loss: 0.7352 - accuracy: 0.6259 - val_loss: 3.3871 - val_accuracy: 0.6556\n",
      "Epoch 544/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.7757 - accuracy: 0.6370 - val_loss: 2.9690 - val_accuracy: 0.6556\n",
      "Epoch 545/1000\n",
      "68/68 [==============================] - 0s 781us/step - loss: 0.9153 - accuracy: 0.6074 - val_loss: 3.0106 - val_accuracy: 0.6667\n",
      "Epoch 546/1000\n",
      "68/68 [==============================] - 0s 754us/step - loss: 0.7658 - accuracy: 0.5926 - val_loss: 3.0804 - val_accuracy: 0.6667\n",
      "Epoch 547/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.7032 - accuracy: 0.6333 - val_loss: 3.0969 - val_accuracy: 0.6556\n",
      "Epoch 548/1000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7244 - accuracy: 0.6148 - val_loss: 3.1731 - val_accuracy: 0.6556\n",
      "Epoch 549/1000\n",
      "68/68 [==============================] - 0s 779us/step - loss: 0.7411 - accuracy: 0.6074 - val_loss: 3.1892 - val_accuracy: 0.6556\n",
      "Epoch 550/1000\n",
      "68/68 [==============================] - 0s 736us/step - loss: 0.7794 - accuracy: 0.6333 - val_loss: 3.0969 - val_accuracy: 0.6556\n",
      "Epoch 551/1000\n",
      "68/68 [==============================] - 0s 722us/step - loss: 0.7660 - accuracy: 0.5889 - val_loss: 2.9806 - val_accuracy: 0.6556\n",
      "Epoch 552/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.7039 - accuracy: 0.6222 - val_loss: 2.9742 - val_accuracy: 0.6556\n",
      "Epoch 553/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 0.7550 - accuracy: 0.6074 - val_loss: 2.9834 - val_accuracy: 0.6556\n",
      "Epoch 554/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 714us/step - loss: 0.7175 - accuracy: 0.6185 - val_loss: 2.9871 - val_accuracy: 0.6556\n",
      "Epoch 555/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 0.7233 - accuracy: 0.6185 - val_loss: 3.0131 - val_accuracy: 0.6556\n",
      "Epoch 556/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.7335 - accuracy: 0.6074 - val_loss: 3.0125 - val_accuracy: 0.6556\n",
      "Epoch 557/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.7784 - accuracy: 0.6185 - val_loss: 3.0655 - val_accuracy: 0.6556\n",
      "Epoch 558/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.7095 - accuracy: 0.6185 - val_loss: 3.1457 - val_accuracy: 0.6444\n",
      "Epoch 559/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.7287 - accuracy: 0.6074 - val_loss: 3.0529 - val_accuracy: 0.6556\n",
      "Epoch 560/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.7559 - accuracy: 0.6148 - val_loss: 3.0644 - val_accuracy: 0.6556\n",
      "Epoch 561/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 0.7194 - accuracy: 0.6370 - val_loss: 3.0823 - val_accuracy: 0.6556\n",
      "Epoch 562/1000\n",
      "68/68 [==============================] - 0s 747us/step - loss: 0.7692 - accuracy: 0.6111 - val_loss: 3.1435 - val_accuracy: 0.6556\n",
      "Epoch 563/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.7435 - accuracy: 0.6074 - val_loss: 2.9304 - val_accuracy: 0.6667\n",
      "Epoch 564/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.7237 - accuracy: 0.6185 - val_loss: 2.9517 - val_accuracy: 0.6667\n",
      "Epoch 565/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.7827 - accuracy: 0.5889 - val_loss: 2.9976 - val_accuracy: 0.6556\n",
      "Epoch 566/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 0.7032 - accuracy: 0.6259 - val_loss: 3.0067 - val_accuracy: 0.6556\n",
      "Epoch 567/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 0.7205 - accuracy: 0.6185 - val_loss: 2.9903 - val_accuracy: 0.6556\n",
      "Epoch 568/1000\n",
      "68/68 [==============================] - 0s 822us/step - loss: 0.7031 - accuracy: 0.6296 - val_loss: 2.9963 - val_accuracy: 0.6556\n",
      "Epoch 569/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.7815 - accuracy: 0.6037 - val_loss: 2.9245 - val_accuracy: 0.6444\n",
      "Epoch 570/1000\n",
      "68/68 [==============================] - 0s 755us/step - loss: 0.7249 - accuracy: 0.6259 - val_loss: 3.0016 - val_accuracy: 0.6556\n",
      "Epoch 571/1000\n",
      "68/68 [==============================] - 0s 714us/step - loss: 0.7614 - accuracy: 0.6185 - val_loss: 2.8762 - val_accuracy: 0.6556\n",
      "Epoch 572/1000\n",
      "68/68 [==============================] - 0s 748us/step - loss: 0.7319 - accuracy: 0.6074 - val_loss: 2.8639 - val_accuracy: 0.6556\n",
      "Epoch 573/1000\n",
      "68/68 [==============================] - 0s 705us/step - loss: 0.7485 - accuracy: 0.6000 - val_loss: 2.9252 - val_accuracy: 0.6556\n",
      "Epoch 574/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 0.6898 - accuracy: 0.6370 - val_loss: 2.8081 - val_accuracy: 0.6556\n",
      "Epoch 575/1000\n",
      "68/68 [==============================] - 0s 708us/step - loss: 0.7040 - accuracy: 0.6222 - val_loss: 2.6911 - val_accuracy: 0.6556\n",
      "Epoch 576/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.7077 - accuracy: 0.6222 - val_loss: 2.6782 - val_accuracy: 0.6556\n",
      "Epoch 577/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.7244 - accuracy: 0.6111 - val_loss: 2.6866 - val_accuracy: 0.6556\n",
      "Epoch 578/1000\n",
      "68/68 [==============================] - 0s 747us/step - loss: 0.7758 - accuracy: 0.6074 - val_loss: 2.7461 - val_accuracy: 0.6556\n",
      "Epoch 579/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 0.7829 - accuracy: 0.6111 - val_loss: 2.5097 - val_accuracy: 0.6667\n",
      "Epoch 580/1000\n",
      "68/68 [==============================] - 0s 757us/step - loss: 0.7740 - accuracy: 0.6074 - val_loss: 2.6019 - val_accuracy: 0.6778\n",
      "Epoch 581/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 0.7467 - accuracy: 0.6000 - val_loss: 2.5989 - val_accuracy: 0.6556\n",
      "Epoch 582/1000\n",
      "68/68 [==============================] - 0s 743us/step - loss: 0.7306 - accuracy: 0.6111 - val_loss: 2.6135 - val_accuracy: 0.6444\n",
      "Epoch 583/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.7042 - accuracy: 0.6296 - val_loss: 2.6263 - val_accuracy: 0.6444\n",
      "Epoch 584/1000\n",
      "68/68 [==============================] - 0s 727us/step - loss: 0.7335 - accuracy: 0.6037 - val_loss: 2.6483 - val_accuracy: 0.6556\n",
      "Epoch 585/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.7997 - accuracy: 0.6222 - val_loss: 2.6874 - val_accuracy: 0.6556\n",
      "Epoch 586/1000\n",
      "68/68 [==============================] - 0s 758us/step - loss: 0.7140 - accuracy: 0.6259 - val_loss: 2.7290 - val_accuracy: 0.6556\n",
      "Epoch 587/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.8854 - accuracy: 0.6259 - val_loss: 2.3263 - val_accuracy: 0.6444\n",
      "Epoch 588/1000\n",
      "68/68 [==============================] - 0s 821us/step - loss: 0.7144 - accuracy: 0.6185 - val_loss: 2.3322 - val_accuracy: 0.6444\n",
      "Epoch 589/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.7395 - accuracy: 0.6185 - val_loss: 2.3577 - val_accuracy: 0.6556\n",
      "Epoch 590/1000\n",
      "68/68 [==============================] - 0s 757us/step - loss: 0.7224 - accuracy: 0.6111 - val_loss: 2.3961 - val_accuracy: 0.6444\n",
      "Epoch 591/1000\n",
      "68/68 [==============================] - 0s 763us/step - loss: 0.7637 - accuracy: 0.6185 - val_loss: 2.2707 - val_accuracy: 0.6556\n",
      "Epoch 592/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 0.7121 - accuracy: 0.6370 - val_loss: 2.3382 - val_accuracy: 0.6444\n",
      "Epoch 593/1000\n",
      "68/68 [==============================] - 0s 754us/step - loss: 0.7082 - accuracy: 0.6222 - val_loss: 2.3618 - val_accuracy: 0.6444\n",
      "Epoch 594/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.7622 - accuracy: 0.6185 - val_loss: 2.1956 - val_accuracy: 0.6556\n",
      "Epoch 595/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 0.6974 - accuracy: 0.6296 - val_loss: 2.2084 - val_accuracy: 0.6556\n",
      "Epoch 596/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.7254 - accuracy: 0.6185 - val_loss: 2.2266 - val_accuracy: 0.6556\n",
      "Epoch 597/1000\n",
      "68/68 [==============================] - 0s 727us/step - loss: 0.7603 - accuracy: 0.5852 - val_loss: 2.2554 - val_accuracy: 0.6556\n",
      "Epoch 598/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.7020 - accuracy: 0.6370 - val_loss: 2.2766 - val_accuracy: 0.6556\n",
      "Epoch 599/1000\n",
      "68/68 [==============================] - 0s 760us/step - loss: 0.7699 - accuracy: 0.6000 - val_loss: 2.1867 - val_accuracy: 0.6556\n",
      "Epoch 600/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 0.7201 - accuracy: 0.6259 - val_loss: 2.1803 - val_accuracy: 0.6444\n",
      "Epoch 601/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 1.0817 - accuracy: 0.6148 - val_loss: 2.2387 - val_accuracy: 0.6556\n",
      "Epoch 602/1000\n",
      "68/68 [==============================] - 0s 756us/step - loss: 0.7192 - accuracy: 0.6296 - val_loss: 2.2921 - val_accuracy: 0.6556\n",
      "Epoch 603/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 0.7151 - accuracy: 0.6148 - val_loss: 2.2886 - val_accuracy: 0.6556\n",
      "Epoch 604/1000\n",
      "68/68 [==============================] - 0s 716us/step - loss: 0.7466 - accuracy: 0.6037 - val_loss: 2.2892 - val_accuracy: 0.6556\n",
      "Epoch 605/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.7292 - accuracy: 0.6148 - val_loss: 2.2907 - val_accuracy: 0.6556\n",
      "Epoch 606/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.7141 - accuracy: 0.6185 - val_loss: 2.3040 - val_accuracy: 0.6556\n",
      "Epoch 607/1000\n",
      "68/68 [==============================] - 0s 744us/step - loss: 0.7230 - accuracy: 0.6148 - val_loss: 2.3589 - val_accuracy: 0.6556\n",
      "Epoch 608/1000\n",
      "68/68 [==============================] - 0s 830us/step - loss: 0.7191 - accuracy: 0.6148 - val_loss: 2.3817 - val_accuracy: 0.6556\n",
      "Epoch 609/1000\n",
      "68/68 [==============================] - 0s 743us/step - loss: 0.6982 - accuracy: 0.6370 - val_loss: 2.3278 - val_accuracy: 0.6556\n",
      "Epoch 610/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 710us/step - loss: 0.7211 - accuracy: 0.6148 - val_loss: 2.3372 - val_accuracy: 0.6556\n",
      "Epoch 611/1000\n",
      "68/68 [==============================] - 0s 711us/step - loss: 0.7629 - accuracy: 0.6296 - val_loss: 2.3415 - val_accuracy: 0.6556\n",
      "Epoch 612/1000\n",
      "68/68 [==============================] - 0s 745us/step - loss: 0.7219 - accuracy: 0.6148 - val_loss: 2.3700 - val_accuracy: 0.6556\n",
      "Epoch 613/1000\n",
      "68/68 [==============================] - 0s 716us/step - loss: 0.7547 - accuracy: 0.6296 - val_loss: 2.5286 - val_accuracy: 0.6556\n",
      "Epoch 614/1000\n",
      "68/68 [==============================] - 0s 759us/step - loss: 0.7295 - accuracy: 0.6185 - val_loss: 2.5594 - val_accuracy: 0.6556\n",
      "Epoch 615/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.8255 - accuracy: 0.5889 - val_loss: 2.5301 - val_accuracy: 0.6556\n",
      "Epoch 616/1000\n",
      "68/68 [==============================] - 0s 714us/step - loss: 0.7733 - accuracy: 0.6370 - val_loss: 2.5841 - val_accuracy: 0.6556\n",
      "Epoch 617/1000\n",
      "68/68 [==============================] - 0s 745us/step - loss: 0.7183 - accuracy: 0.6222 - val_loss: 2.5445 - val_accuracy: 0.6556\n",
      "Epoch 618/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.7606 - accuracy: 0.5926 - val_loss: 2.5548 - val_accuracy: 0.6556\n",
      "Epoch 619/1000\n",
      "68/68 [==============================] - 0s 727us/step - loss: 0.7080 - accuracy: 0.6296 - val_loss: 2.5432 - val_accuracy: 0.6556\n",
      "Epoch 620/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 0.7352 - accuracy: 0.6074 - val_loss: 2.5365 - val_accuracy: 0.6556\n",
      "Epoch 621/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.7493 - accuracy: 0.5926 - val_loss: 2.5429 - val_accuracy: 0.6556\n",
      "Epoch 622/1000\n",
      "68/68 [==============================] - 0s 758us/step - loss: 0.8105 - accuracy: 0.6222 - val_loss: 2.5171 - val_accuracy: 0.6556\n",
      "Epoch 623/1000\n",
      "68/68 [==============================] - 0s 712us/step - loss: 0.7699 - accuracy: 0.6519 - val_loss: 2.5838 - val_accuracy: 0.6444\n",
      "Epoch 624/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 0.7390 - accuracy: 0.6407 - val_loss: 2.5004 - val_accuracy: 0.6444\n",
      "Epoch 625/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.7897 - accuracy: 0.6407 - val_loss: 2.5533 - val_accuracy: 0.6667\n",
      "Epoch 626/1000\n",
      "68/68 [==============================] - 0s 805us/step - loss: 0.7708 - accuracy: 0.6296 - val_loss: 2.5735 - val_accuracy: 0.6778\n",
      "Epoch 627/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.6944 - accuracy: 0.6333 - val_loss: 2.5770 - val_accuracy: 0.6778\n",
      "Epoch 628/1000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7168 - accuracy: 0.6222 - val_loss: 2.4793 - val_accuracy: 0.7111\n",
      "Epoch 629/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.7605 - accuracy: 0.6407 - val_loss: 2.4906 - val_accuracy: 0.7111\n",
      "Epoch 630/1000\n",
      "68/68 [==============================] - 0s 711us/step - loss: 0.7502 - accuracy: 0.6296 - val_loss: 2.5934 - val_accuracy: 0.6778\n",
      "Epoch 631/1000\n",
      "68/68 [==============================] - 0s 709us/step - loss: 0.8352 - accuracy: 0.6556 - val_loss: 2.3722 - val_accuracy: 0.7111\n",
      "Epoch 632/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.7049 - accuracy: 0.6407 - val_loss: 2.3878 - val_accuracy: 0.7111\n",
      "Epoch 633/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.7369 - accuracy: 0.6481 - val_loss: 2.4219 - val_accuracy: 0.7000\n",
      "Epoch 634/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.7159 - accuracy: 0.6519 - val_loss: 2.4566 - val_accuracy: 0.6778\n",
      "Epoch 635/1000\n",
      "68/68 [==============================] - 0s 736us/step - loss: 0.7207 - accuracy: 0.6222 - val_loss: 2.4876 - val_accuracy: 0.6667\n",
      "Epoch 636/1000\n",
      "68/68 [==============================] - 0s 748us/step - loss: 0.7264 - accuracy: 0.6556 - val_loss: 2.5063 - val_accuracy: 0.6667\n",
      "Epoch 637/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.7408 - accuracy: 0.6333 - val_loss: 2.4992 - val_accuracy: 0.6667\n",
      "Epoch 638/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.6973 - accuracy: 0.6407 - val_loss: 2.5161 - val_accuracy: 0.6667\n",
      "Epoch 639/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.6865 - accuracy: 0.6444 - val_loss: 2.5019 - val_accuracy: 0.6667\n",
      "Epoch 640/1000\n",
      "68/68 [==============================] - 0s 778us/step - loss: 0.6959 - accuracy: 0.6630 - val_loss: 2.5196 - val_accuracy: 0.6667\n",
      "Epoch 641/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.7294 - accuracy: 0.6259 - val_loss: 2.5320 - val_accuracy: 0.6778\n",
      "Epoch 642/1000\n",
      "68/68 [==============================] - 0s 752us/step - loss: 0.6804 - accuracy: 0.6481 - val_loss: 2.5616 - val_accuracy: 0.7000\n",
      "Epoch 643/1000\n",
      "68/68 [==============================] - 0s 748us/step - loss: 0.8118 - accuracy: 0.6481 - val_loss: 2.6040 - val_accuracy: 0.7000\n",
      "Epoch 644/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.6832 - accuracy: 0.6407 - val_loss: 2.6223 - val_accuracy: 0.6778\n",
      "Epoch 645/1000\n",
      "68/68 [==============================] - 0s 756us/step - loss: 0.7275 - accuracy: 0.6259 - val_loss: 2.6282 - val_accuracy: 0.6778\n",
      "Epoch 646/1000\n",
      "68/68 [==============================] - 0s 716us/step - loss: 0.7218 - accuracy: 0.6259 - val_loss: 2.6188 - val_accuracy: 0.6778\n",
      "Epoch 647/1000\n",
      "68/68 [==============================] - 0s 872us/step - loss: 0.6734 - accuracy: 0.6556 - val_loss: 2.5985 - val_accuracy: 0.7000\n",
      "Epoch 648/1000\n",
      "68/68 [==============================] - 0s 749us/step - loss: 0.6925 - accuracy: 0.6444 - val_loss: 2.5888 - val_accuracy: 0.6778\n",
      "Epoch 649/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 0.6951 - accuracy: 0.6444 - val_loss: 2.6239 - val_accuracy: 0.6778\n",
      "Epoch 650/1000\n",
      "68/68 [==============================] - 0s 727us/step - loss: 0.7021 - accuracy: 0.6333 - val_loss: 2.6325 - val_accuracy: 0.6778\n",
      "Epoch 651/1000\n",
      "68/68 [==============================] - 0s 744us/step - loss: 0.7289 - accuracy: 0.6259 - val_loss: 2.5829 - val_accuracy: 0.6667\n",
      "Epoch 652/1000\n",
      "68/68 [==============================] - 0s 745us/step - loss: 0.6879 - accuracy: 0.6519 - val_loss: 2.6110 - val_accuracy: 0.6889\n",
      "Epoch 653/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.7213 - accuracy: 0.6407 - val_loss: 2.5740 - val_accuracy: 0.7000\n",
      "Epoch 654/1000\n",
      "68/68 [==============================] - 0s 774us/step - loss: 0.7054 - accuracy: 0.6444 - val_loss: 2.7703 - val_accuracy: 0.6778\n",
      "Epoch 655/1000\n",
      "68/68 [==============================] - 0s 758us/step - loss: 0.7389 - accuracy: 0.6370 - val_loss: 2.7326 - val_accuracy: 0.6778\n",
      "Epoch 656/1000\n",
      "68/68 [==============================] - 0s 743us/step - loss: 0.6784 - accuracy: 0.6556 - val_loss: 2.4966 - val_accuracy: 0.6778\n",
      "Epoch 657/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.7151 - accuracy: 0.6481 - val_loss: 2.3377 - val_accuracy: 0.6778\n",
      "Epoch 658/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.6980 - accuracy: 0.6407 - val_loss: 2.4370 - val_accuracy: 0.6778\n",
      "Epoch 659/1000\n",
      "68/68 [==============================] - 0s 769us/step - loss: 0.7173 - accuracy: 0.6333 - val_loss: 2.4950 - val_accuracy: 0.6778\n",
      "Epoch 660/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.7501 - accuracy: 0.6259 - val_loss: 2.6062 - val_accuracy: 0.6778\n",
      "Epoch 661/1000\n",
      "68/68 [==============================] - 0s 765us/step - loss: 0.7090 - accuracy: 0.6296 - val_loss: 2.6335 - val_accuracy: 0.6778\n",
      "Epoch 662/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.6980 - accuracy: 0.6519 - val_loss: 2.6308 - val_accuracy: 0.6889\n",
      "Epoch 663/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.7100 - accuracy: 0.6519 - val_loss: 2.5249 - val_accuracy: 0.6778\n",
      "Epoch 664/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.7103 - accuracy: 0.6370 - val_loss: 2.5858 - val_accuracy: 0.6778\n",
      "Epoch 665/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.6760 - accuracy: 0.6556 - val_loss: 2.5908 - val_accuracy: 0.6778\n",
      "Epoch 666/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 727us/step - loss: 0.7426 - accuracy: 0.6259 - val_loss: 2.6142 - val_accuracy: 0.6778\n",
      "Epoch 667/1000\n",
      "68/68 [==============================] - 0s 825us/step - loss: 0.6817 - accuracy: 0.6556 - val_loss: 2.6106 - val_accuracy: 0.6778\n",
      "Epoch 668/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.7061 - accuracy: 0.6333 - val_loss: 2.6242 - val_accuracy: 0.6778\n",
      "Epoch 669/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.7069 - accuracy: 0.6296 - val_loss: 2.5945 - val_accuracy: 0.6778\n",
      "Epoch 670/1000\n",
      "68/68 [==============================] - 0s 713us/step - loss: 0.7040 - accuracy: 0.6333 - val_loss: 2.6173 - val_accuracy: 0.6778\n",
      "Epoch 671/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.7717 - accuracy: 0.6407 - val_loss: 2.5372 - val_accuracy: 0.6778\n",
      "Epoch 672/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.7234 - accuracy: 0.6407 - val_loss: 2.5215 - val_accuracy: 0.6778\n",
      "Epoch 673/1000\n",
      "68/68 [==============================] - 0s 713us/step - loss: 0.7133 - accuracy: 0.6259 - val_loss: 2.5304 - val_accuracy: 0.6889\n",
      "Epoch 674/1000\n",
      "68/68 [==============================] - 0s 736us/step - loss: 0.6930 - accuracy: 0.6333 - val_loss: 2.5413 - val_accuracy: 0.6889\n",
      "Epoch 675/1000\n",
      "68/68 [==============================] - 0s 708us/step - loss: 0.7062 - accuracy: 0.6296 - val_loss: 2.5533 - val_accuracy: 0.6889\n",
      "Epoch 676/1000\n",
      "68/68 [==============================] - 0s 744us/step - loss: 0.7874 - accuracy: 0.6444 - val_loss: 2.6430 - val_accuracy: 0.6889\n",
      "Epoch 677/1000\n",
      "68/68 [==============================] - 0s 739us/step - loss: 0.6888 - accuracy: 0.6481 - val_loss: 2.6727 - val_accuracy: 0.7000\n",
      "Epoch 678/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 0.7221 - accuracy: 0.6481 - val_loss: 2.9019 - val_accuracy: 0.7000\n",
      "Epoch 679/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 0.7377 - accuracy: 0.6333 - val_loss: 2.8682 - val_accuracy: 0.7000\n",
      "Epoch 680/1000\n",
      "68/68 [==============================] - 0s 760us/step - loss: 0.7358 - accuracy: 0.6407 - val_loss: 2.6621 - val_accuracy: 0.7000\n",
      "Epoch 681/1000\n",
      "68/68 [==============================] - 0s 753us/step - loss: 0.9321 - accuracy: 0.6259 - val_loss: 2.8714 - val_accuracy: 0.6778\n",
      "Epoch 682/1000\n",
      "68/68 [==============================] - 0s 760us/step - loss: 0.7735 - accuracy: 0.6556 - val_loss: 2.4501 - val_accuracy: 0.7000\n",
      "Epoch 683/1000\n",
      "68/68 [==============================] - 0s 752us/step - loss: 0.7093 - accuracy: 0.6481 - val_loss: 2.4563 - val_accuracy: 0.7000\n",
      "Epoch 684/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.6884 - accuracy: 0.6630 - val_loss: 2.4550 - val_accuracy: 0.7111\n",
      "Epoch 685/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.8094 - accuracy: 0.6481 - val_loss: 2.5580 - val_accuracy: 0.6889\n",
      "Epoch 686/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.7143 - accuracy: 0.6407 - val_loss: 2.5295 - val_accuracy: 0.6889\n",
      "Epoch 687/1000\n",
      "68/68 [==============================] - 0s 863us/step - loss: 0.6565 - accuracy: 0.6704 - val_loss: 2.5136 - val_accuracy: 0.6889\n",
      "Epoch 688/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 0.6618 - accuracy: 0.6667 - val_loss: 2.5287 - val_accuracy: 0.6889\n",
      "Epoch 689/1000\n",
      "68/68 [==============================] - 0s 739us/step - loss: 0.7437 - accuracy: 0.6222 - val_loss: 2.5431 - val_accuracy: 0.6889\n",
      "Epoch 690/1000\n",
      "68/68 [==============================] - 0s 722us/step - loss: 0.6618 - accuracy: 0.6630 - val_loss: 2.5484 - val_accuracy: 0.6889\n",
      "Epoch 691/1000\n",
      "68/68 [==============================] - 0s 759us/step - loss: 0.6770 - accuracy: 0.6593 - val_loss: 2.5090 - val_accuracy: 0.6889\n",
      "Epoch 692/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.7005 - accuracy: 0.6444 - val_loss: 2.5741 - val_accuracy: 0.6778\n",
      "Epoch 693/1000\n",
      "68/68 [==============================] - 0s 748us/step - loss: 0.6911 - accuracy: 0.6481 - val_loss: 2.6092 - val_accuracy: 0.6889\n",
      "Epoch 694/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.6994 - accuracy: 0.6333 - val_loss: 2.5784 - val_accuracy: 0.6889\n",
      "Epoch 695/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 0.7025 - accuracy: 0.6333 - val_loss: 2.5516 - val_accuracy: 0.6889\n",
      "Epoch 696/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.7139 - accuracy: 0.6222 - val_loss: 2.5676 - val_accuracy: 0.6889\n",
      "Epoch 697/1000\n",
      "68/68 [==============================] - 0s 713us/step - loss: 0.6791 - accuracy: 0.6481 - val_loss: 2.5470 - val_accuracy: 0.6889\n",
      "Epoch 698/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.6830 - accuracy: 0.6481 - val_loss: 2.5745 - val_accuracy: 0.6889\n",
      "Epoch 699/1000\n",
      "68/68 [==============================] - 0s 763us/step - loss: 0.7221 - accuracy: 0.6185 - val_loss: 2.5733 - val_accuracy: 0.6778\n",
      "Epoch 700/1000\n",
      "68/68 [==============================] - 0s 743us/step - loss: 0.6863 - accuracy: 0.6481 - val_loss: 2.5932 - val_accuracy: 0.6889\n",
      "Epoch 701/1000\n",
      "68/68 [==============================] - 0s 756us/step - loss: 0.6816 - accuracy: 0.6556 - val_loss: 2.5708 - val_accuracy: 0.6778\n",
      "Epoch 702/1000\n",
      "68/68 [==============================] - 0s 767us/step - loss: 0.6832 - accuracy: 0.6444 - val_loss: 2.5676 - val_accuracy: 0.6778\n",
      "Epoch 703/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.6896 - accuracy: 0.6407 - val_loss: 2.5716 - val_accuracy: 0.6778\n",
      "Epoch 704/1000\n",
      "68/68 [==============================] - 0s 739us/step - loss: 0.6803 - accuracy: 0.6444 - val_loss: 2.5622 - val_accuracy: 0.6778\n",
      "Epoch 705/1000\n",
      "68/68 [==============================] - 0s 736us/step - loss: 0.6803 - accuracy: 0.6556 - val_loss: 2.5886 - val_accuracy: 0.6778\n",
      "Epoch 706/1000\n",
      "68/68 [==============================] - 0s 739us/step - loss: 0.6658 - accuracy: 0.6556 - val_loss: 2.5935 - val_accuracy: 0.6778\n",
      "Epoch 707/1000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7052 - accuracy: 0.6296 - val_loss: 2.5922 - val_accuracy: 0.6889\n",
      "Epoch 708/1000\n",
      "68/68 [==============================] - 0s 762us/step - loss: 0.7205 - accuracy: 0.6444 - val_loss: 2.6038 - val_accuracy: 0.6889\n",
      "Epoch 709/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 0.6761 - accuracy: 0.6556 - val_loss: 2.6213 - val_accuracy: 0.6889\n",
      "Epoch 710/1000\n",
      "68/68 [==============================] - 0s 769us/step - loss: 0.6573 - accuracy: 0.6630 - val_loss: 2.6441 - val_accuracy: 0.6889\n",
      "Epoch 711/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 0.7033 - accuracy: 0.6407 - val_loss: 2.5669 - val_accuracy: 0.6889\n",
      "Epoch 712/1000\n",
      "68/68 [==============================] - 0s 783us/step - loss: 0.6588 - accuracy: 0.6593 - val_loss: 2.5790 - val_accuracy: 0.6889\n",
      "Epoch 713/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 0.8349 - accuracy: 0.6593 - val_loss: 2.6497 - val_accuracy: 0.6889\n",
      "Epoch 714/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 0.6788 - accuracy: 0.6481 - val_loss: 2.6521 - val_accuracy: 0.6889\n",
      "Epoch 715/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 0.6642 - accuracy: 0.6593 - val_loss: 2.6543 - val_accuracy: 0.6889\n",
      "Epoch 716/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.6805 - accuracy: 0.6481 - val_loss: 2.6899 - val_accuracy: 0.6889\n",
      "Epoch 717/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 0.6799 - accuracy: 0.6556 - val_loss: 2.6568 - val_accuracy: 0.6889\n",
      "Epoch 718/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.7065 - accuracy: 0.6481 - val_loss: 2.7480 - val_accuracy: 0.6889\n",
      "Epoch 719/1000\n",
      "68/68 [==============================] - 0s 712us/step - loss: 0.7101 - accuracy: 0.6407 - val_loss: 2.6528 - val_accuracy: 0.6778\n",
      "Epoch 720/1000\n",
      "68/68 [==============================] - 0s 747us/step - loss: 0.6781 - accuracy: 0.6519 - val_loss: 2.6162 - val_accuracy: 0.6778\n",
      "Epoch 721/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.6835 - accuracy: 0.6556 - val_loss: 2.6247 - val_accuracy: 0.6778\n",
      "Epoch 722/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 721us/step - loss: 0.6928 - accuracy: 0.6407 - val_loss: 2.6108 - val_accuracy: 0.6778\n",
      "Epoch 723/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.7035 - accuracy: 0.6296 - val_loss: 2.6096 - val_accuracy: 0.6778\n",
      "Epoch 724/1000\n",
      "68/68 [==============================] - 0s 705us/step - loss: 0.6925 - accuracy: 0.6481 - val_loss: 2.6367 - val_accuracy: 0.6778\n",
      "Epoch 725/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.6934 - accuracy: 0.6407 - val_loss: 2.6554 - val_accuracy: 0.6778\n",
      "Epoch 726/1000\n",
      "68/68 [==============================] - 0s 709us/step - loss: 0.6678 - accuracy: 0.6556 - val_loss: 2.5879 - val_accuracy: 0.6889\n",
      "Epoch 727/1000\n",
      "68/68 [==============================] - 0s 830us/step - loss: 0.6727 - accuracy: 0.6481 - val_loss: 2.5799 - val_accuracy: 0.6889\n",
      "Epoch 728/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 0.7381 - accuracy: 0.6556 - val_loss: 2.7075 - val_accuracy: 0.6778\n",
      "Epoch 729/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.6992 - accuracy: 0.6407 - val_loss: 2.7546 - val_accuracy: 0.6778\n",
      "Epoch 730/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 0.6757 - accuracy: 0.6556 - val_loss: 2.7808 - val_accuracy: 0.6778\n",
      "Epoch 731/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.7110 - accuracy: 0.6407 - val_loss: 2.7443 - val_accuracy: 0.6778\n",
      "Epoch 732/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.7064 - accuracy: 0.6222 - val_loss: 2.7298 - val_accuracy: 0.6778\n",
      "Epoch 733/1000\n",
      "68/68 [==============================] - 0s 749us/step - loss: 0.7250 - accuracy: 0.6259 - val_loss: 2.7435 - val_accuracy: 0.6778\n",
      "Epoch 734/1000\n",
      "68/68 [==============================] - 0s 767us/step - loss: 0.6844 - accuracy: 0.6519 - val_loss: 2.7306 - val_accuracy: 0.6778\n",
      "Epoch 735/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.6777 - accuracy: 0.6630 - val_loss: 2.7073 - val_accuracy: 0.6778\n",
      "Epoch 736/1000\n",
      "68/68 [==============================] - 0s 808us/step - loss: 0.7197 - accuracy: 0.6333 - val_loss: 2.6447 - val_accuracy: 0.6778\n",
      "Epoch 737/1000\n",
      "68/68 [==============================] - 0s 763us/step - loss: 0.6632 - accuracy: 0.6593 - val_loss: 2.6670 - val_accuracy: 0.6778\n",
      "Epoch 738/1000\n",
      "68/68 [==============================] - 0s 705us/step - loss: 0.6865 - accuracy: 0.6407 - val_loss: 2.6766 - val_accuracy: 0.6778\n",
      "Epoch 739/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 0.6641 - accuracy: 0.6593 - val_loss: 2.6890 - val_accuracy: 0.6778\n",
      "Epoch 740/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.6851 - accuracy: 0.6481 - val_loss: 2.7193 - val_accuracy: 0.6778\n",
      "Epoch 741/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.6759 - accuracy: 0.6519 - val_loss: 2.7091 - val_accuracy: 0.6778\n",
      "Epoch 742/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 0.7237 - accuracy: 0.6259 - val_loss: 2.6530 - val_accuracy: 0.6778\n",
      "Epoch 743/1000\n",
      "68/68 [==============================] - 0s 754us/step - loss: 0.6871 - accuracy: 0.6481 - val_loss: 2.7113 - val_accuracy: 0.6778\n",
      "Epoch 744/1000\n",
      "68/68 [==============================] - 0s 709us/step - loss: 0.6592 - accuracy: 0.6667 - val_loss: 2.7101 - val_accuracy: 0.6778\n",
      "Epoch 745/1000\n",
      "68/68 [==============================] - 0s 702us/step - loss: 0.8273 - accuracy: 0.6185 - val_loss: 2.6989 - val_accuracy: 0.6667\n",
      "Epoch 746/1000\n",
      "68/68 [==============================] - 0s 713us/step - loss: 0.7290 - accuracy: 0.6185 - val_loss: 2.6420 - val_accuracy: 0.6778\n",
      "Epoch 747/1000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 0.6875 - accuracy: 0.6444 - val_loss: 2.6636 - val_accuracy: 0.6778\n",
      "Epoch 748/1000\n",
      "68/68 [==============================] - 0s 763us/step - loss: 0.7521 - accuracy: 0.6074 - val_loss: 2.7375 - val_accuracy: 0.6778\n",
      "Epoch 749/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 0.6788 - accuracy: 0.6481 - val_loss: 2.7480 - val_accuracy: 0.6778\n",
      "Epoch 750/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 0.7346 - accuracy: 0.6074 - val_loss: 2.7474 - val_accuracy: 0.6778\n",
      "Epoch 751/1000\n",
      "68/68 [==============================] - 0s 756us/step - loss: 0.6761 - accuracy: 0.6741 - val_loss: 2.7658 - val_accuracy: 0.7000\n",
      "Epoch 752/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.6642 - accuracy: 0.6593 - val_loss: 2.7160 - val_accuracy: 0.6889\n",
      "Epoch 753/1000\n",
      "68/68 [==============================] - 0s 745us/step - loss: 0.7779 - accuracy: 0.6630 - val_loss: 2.2866 - val_accuracy: 0.7222\n",
      "Epoch 754/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.7098 - accuracy: 0.6407 - val_loss: 2.3481 - val_accuracy: 0.7111\n",
      "Epoch 755/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.7342 - accuracy: 0.6148 - val_loss: 2.3807 - val_accuracy: 0.7222\n",
      "Epoch 756/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 0.7186 - accuracy: 0.6259 - val_loss: 2.5213 - val_accuracy: 0.7111\n",
      "Epoch 757/1000\n",
      "68/68 [==============================] - 0s 712us/step - loss: 0.6840 - accuracy: 0.6444 - val_loss: 2.5748 - val_accuracy: 0.7111\n",
      "Epoch 758/1000\n",
      "68/68 [==============================] - 0s 757us/step - loss: 0.7202 - accuracy: 0.6444 - val_loss: 2.6398 - val_accuracy: 0.7111\n",
      "Epoch 759/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.8056 - accuracy: 0.6630 - val_loss: 2.5370 - val_accuracy: 0.7111\n",
      "Epoch 760/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 0.6810 - accuracy: 0.6519 - val_loss: 2.5595 - val_accuracy: 0.7222\n",
      "Epoch 761/1000\n",
      "68/68 [==============================] - 0s 718us/step - loss: 0.8087 - accuracy: 0.6370 - val_loss: 2.6304 - val_accuracy: 0.7222\n",
      "Epoch 762/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.7113 - accuracy: 0.6407 - val_loss: 2.6274 - val_accuracy: 0.7111\n",
      "Epoch 763/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.6958 - accuracy: 0.6407 - val_loss: 2.6387 - val_accuracy: 0.7111\n",
      "Epoch 764/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 0.7112 - accuracy: 0.6259 - val_loss: 2.8216 - val_accuracy: 0.7111\n",
      "Epoch 765/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.7155 - accuracy: 0.6370 - val_loss: 2.7027 - val_accuracy: 0.7111\n",
      "Epoch 766/1000\n",
      "68/68 [==============================] - 0s 759us/step - loss: 0.7457 - accuracy: 0.6444 - val_loss: 2.7640 - val_accuracy: 0.7000\n",
      "Epoch 767/1000\n",
      "68/68 [==============================] - 0s 831us/step - loss: 0.6851 - accuracy: 0.6444 - val_loss: 2.8279 - val_accuracy: 0.7000\n",
      "Epoch 768/1000\n",
      "68/68 [==============================] - 0s 746us/step - loss: 0.6917 - accuracy: 0.6407 - val_loss: 2.9012 - val_accuracy: 0.7000\n",
      "Epoch 769/1000\n",
      "68/68 [==============================] - 0s 745us/step - loss: 0.8123 - accuracy: 0.6444 - val_loss: 2.9368 - val_accuracy: 0.7000\n",
      "Epoch 770/1000\n",
      "68/68 [==============================] - 0s 744us/step - loss: 0.7108 - accuracy: 0.6296 - val_loss: 2.9119 - val_accuracy: 0.7000\n",
      "Epoch 771/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 0.6817 - accuracy: 0.6407 - val_loss: 2.9269 - val_accuracy: 0.7000\n",
      "Epoch 772/1000\n",
      "68/68 [==============================] - 0s 716us/step - loss: 0.9577 - accuracy: 0.6407 - val_loss: 2.9955 - val_accuracy: 0.7000\n",
      "Epoch 773/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 0.6777 - accuracy: 0.6444 - val_loss: 3.0071 - val_accuracy: 0.7000\n",
      "Epoch 774/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 0.6813 - accuracy: 0.6407 - val_loss: 3.0125 - val_accuracy: 0.7000\n",
      "Epoch 775/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.6787 - accuracy: 0.6556 - val_loss: 2.9556 - val_accuracy: 0.7000\n",
      "Epoch 776/1000\n",
      "68/68 [==============================] - 0s 744us/step - loss: 0.6745 - accuracy: 0.6556 - val_loss: 3.0050 - val_accuracy: 0.7000\n",
      "Epoch 777/1000\n",
      "68/68 [==============================] - 0s 712us/step - loss: 0.7044 - accuracy: 0.6593 - val_loss: 3.0371 - val_accuracy: 0.7000\n",
      "Epoch 778/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 732us/step - loss: 0.6651 - accuracy: 0.6630 - val_loss: 3.0399 - val_accuracy: 0.7000\n",
      "Epoch 779/1000\n",
      "68/68 [==============================] - 0s 745us/step - loss: 0.7025 - accuracy: 0.6407 - val_loss: 3.0415 - val_accuracy: 0.7000\n",
      "Epoch 780/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.6637 - accuracy: 0.6630 - val_loss: 3.0443 - val_accuracy: 0.7000\n",
      "Epoch 781/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.7001 - accuracy: 0.6407 - val_loss: 3.0525 - val_accuracy: 0.6889\n",
      "Epoch 782/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 0.6413 - accuracy: 0.6704 - val_loss: 3.0454 - val_accuracy: 0.6889\n",
      "Epoch 783/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.7011 - accuracy: 0.6333 - val_loss: 2.9517 - val_accuracy: 0.6889\n",
      "Epoch 784/1000\n",
      "68/68 [==============================] - 0s 702us/step - loss: 0.7207 - accuracy: 0.6296 - val_loss: 2.7298 - val_accuracy: 0.7000\n",
      "Epoch 785/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.6553 - accuracy: 0.6593 - val_loss: 2.7356 - val_accuracy: 0.7000\n",
      "Epoch 786/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.6922 - accuracy: 0.6519 - val_loss: 2.6259 - val_accuracy: 0.7000\n",
      "Epoch 787/1000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6979 - accuracy: 0.6370 - val_loss: 2.5883 - val_accuracy: 0.7000\n",
      "Epoch 788/1000\n",
      "68/68 [==============================] - 0s 753us/step - loss: 0.7281 - accuracy: 0.6444 - val_loss: 2.6093 - val_accuracy: 0.7000\n",
      "Epoch 789/1000\n",
      "68/68 [==============================] - 0s 757us/step - loss: 0.6844 - accuracy: 0.6519 - val_loss: 2.6000 - val_accuracy: 0.7000\n",
      "Epoch 790/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.6618 - accuracy: 0.6593 - val_loss: 2.6703 - val_accuracy: 0.7000\n",
      "Epoch 791/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.6542 - accuracy: 0.6630 - val_loss: 2.6810 - val_accuracy: 0.7000\n",
      "Epoch 792/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.6834 - accuracy: 0.6593 - val_loss: 2.7051 - val_accuracy: 0.7111\n",
      "Epoch 793/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.6792 - accuracy: 0.6519 - val_loss: 2.7329 - val_accuracy: 0.7000\n",
      "Epoch 794/1000\n",
      "68/68 [==============================] - 0s 744us/step - loss: 0.6587 - accuracy: 0.6630 - val_loss: 2.7338 - val_accuracy: 0.7000\n",
      "Epoch 795/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.6785 - accuracy: 0.6519 - val_loss: 2.7202 - val_accuracy: 0.7000\n",
      "Epoch 796/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 0.6747 - accuracy: 0.6519 - val_loss: 2.7123 - val_accuracy: 0.7000\n",
      "Epoch 797/1000\n",
      "68/68 [==============================] - 0s 753us/step - loss: 0.6880 - accuracy: 0.6407 - val_loss: 2.7244 - val_accuracy: 0.7000\n",
      "Epoch 798/1000\n",
      "68/68 [==============================] - 0s 765us/step - loss: 0.7382 - accuracy: 0.6259 - val_loss: 2.7201 - val_accuracy: 0.6889\n",
      "Epoch 799/1000\n",
      "68/68 [==============================] - 0s 745us/step - loss: 0.6554 - accuracy: 0.6630 - val_loss: 2.6071 - val_accuracy: 0.7111\n",
      "Epoch 800/1000\n",
      "68/68 [==============================] - 0s 746us/step - loss: 0.7218 - accuracy: 0.6556 - val_loss: 2.7129 - val_accuracy: 0.6889\n",
      "Epoch 801/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.6728 - accuracy: 0.6481 - val_loss: 2.6797 - val_accuracy: 0.6889\n",
      "Epoch 802/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 0.6928 - accuracy: 0.6370 - val_loss: 2.6869 - val_accuracy: 0.6889\n",
      "Epoch 803/1000\n",
      "68/68 [==============================] - 0s 712us/step - loss: 0.7109 - accuracy: 0.6556 - val_loss: 2.7257 - val_accuracy: 0.6889\n",
      "Epoch 804/1000\n",
      "68/68 [==============================] - 0s 714us/step - loss: 0.6840 - accuracy: 0.6370 - val_loss: 2.7422 - val_accuracy: 0.6889\n",
      "Epoch 805/1000\n",
      "68/68 [==============================] - 0s 739us/step - loss: 0.6675 - accuracy: 0.6519 - val_loss: 2.7469 - val_accuracy: 0.6889\n",
      "Epoch 806/1000\n",
      "68/68 [==============================] - 0s 842us/step - loss: 0.6771 - accuracy: 0.6481 - val_loss: 2.7558 - val_accuracy: 0.6889\n",
      "Epoch 807/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 0.6814 - accuracy: 0.6519 - val_loss: 3.0790 - val_accuracy: 0.6889\n",
      "Epoch 808/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 0.6552 - accuracy: 0.6630 - val_loss: 3.0352 - val_accuracy: 0.7000\n",
      "Epoch 809/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.7203 - accuracy: 0.6259 - val_loss: 3.1191 - val_accuracy: 0.7000\n",
      "Epoch 810/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.7990 - accuracy: 0.6407 - val_loss: 3.1514 - val_accuracy: 0.7000\n",
      "Epoch 811/1000\n",
      "68/68 [==============================] - 0s 712us/step - loss: 0.7004 - accuracy: 0.6481 - val_loss: 3.0402 - val_accuracy: 0.7000\n",
      "Epoch 812/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.7260 - accuracy: 0.6296 - val_loss: 2.8876 - val_accuracy: 0.6889\n",
      "Epoch 813/1000\n",
      "68/68 [==============================] - 0s 722us/step - loss: 0.6892 - accuracy: 0.6444 - val_loss: 2.8261 - val_accuracy: 0.6889\n",
      "Epoch 814/1000\n",
      "68/68 [==============================] - 0s 772us/step - loss: 0.7133 - accuracy: 0.6407 - val_loss: 2.8500 - val_accuracy: 0.6889\n",
      "Epoch 815/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.6482 - accuracy: 0.6778 - val_loss: 2.8815 - val_accuracy: 0.6889\n",
      "Epoch 816/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.7042 - accuracy: 0.6296 - val_loss: 2.8958 - val_accuracy: 0.6889\n",
      "Epoch 817/1000\n",
      "68/68 [==============================] - 0s 755us/step - loss: 0.7021 - accuracy: 0.6407 - val_loss: 2.9027 - val_accuracy: 0.6889\n",
      "Epoch 818/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.6829 - accuracy: 0.6593 - val_loss: 3.0066 - val_accuracy: 0.6889\n",
      "Epoch 819/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.6667 - accuracy: 0.6667 - val_loss: 2.8564 - val_accuracy: 0.7000\n",
      "Epoch 820/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.6587 - accuracy: 0.6593 - val_loss: 2.8807 - val_accuracy: 0.7000\n",
      "Epoch 821/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.6906 - accuracy: 0.6481 - val_loss: 2.8365 - val_accuracy: 0.7000\n",
      "Epoch 822/1000\n",
      "68/68 [==============================] - 0s 750us/step - loss: 0.6754 - accuracy: 0.6556 - val_loss: 2.8787 - val_accuracy: 0.7000\n",
      "Epoch 823/1000\n",
      "68/68 [==============================] - 0s 756us/step - loss: 0.6584 - accuracy: 0.6630 - val_loss: 2.9277 - val_accuracy: 0.6889\n",
      "Epoch 824/1000\n",
      "68/68 [==============================] - 0s 743us/step - loss: 0.6697 - accuracy: 0.6481 - val_loss: 2.9768 - val_accuracy: 0.6889\n",
      "Epoch 825/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.6787 - accuracy: 0.6481 - val_loss: 2.9700 - val_accuracy: 0.6889\n",
      "Epoch 826/1000\n",
      "68/68 [==============================] - 0s 843us/step - loss: 0.6632 - accuracy: 0.6556 - val_loss: 3.0044 - val_accuracy: 0.6889\n",
      "Epoch 827/1000\n",
      "68/68 [==============================] - 0s 755us/step - loss: 0.6483 - accuracy: 0.6667 - val_loss: 3.0076 - val_accuracy: 0.6889\n",
      "Epoch 828/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 0.7902 - accuracy: 0.6296 - val_loss: 2.9933 - val_accuracy: 0.6889\n",
      "Epoch 829/1000\n",
      "68/68 [==============================] - 0s 783us/step - loss: 0.7023 - accuracy: 0.6370 - val_loss: 3.0622 - val_accuracy: 0.6889\n",
      "Epoch 830/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 0.6826 - accuracy: 0.6519 - val_loss: 3.0716 - val_accuracy: 0.6889\n",
      "Epoch 831/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.7003 - accuracy: 0.6370 - val_loss: 3.0717 - val_accuracy: 0.6889\n",
      "Epoch 832/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.6926 - accuracy: 0.6407 - val_loss: 3.0397 - val_accuracy: 0.6778\n",
      "Epoch 833/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 0.7240 - accuracy: 0.6407 - val_loss: 2.9784 - val_accuracy: 0.6778\n",
      "Epoch 834/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 722us/step - loss: 0.6733 - accuracy: 0.6556 - val_loss: 2.9799 - val_accuracy: 0.6778\n",
      "Epoch 835/1000\n",
      "68/68 [==============================] - 0s 744us/step - loss: 0.7010 - accuracy: 0.6519 - val_loss: 2.9150 - val_accuracy: 0.6667\n",
      "Epoch 836/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.6961 - accuracy: 0.6296 - val_loss: 2.8446 - val_accuracy: 0.6889\n",
      "Epoch 837/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.6935 - accuracy: 0.6370 - val_loss: 2.8298 - val_accuracy: 0.7111\n",
      "Epoch 838/1000\n",
      "68/68 [==============================] - 0s 759us/step - loss: 0.6887 - accuracy: 0.6407 - val_loss: 2.8686 - val_accuracy: 0.7000\n",
      "Epoch 839/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 0.6831 - accuracy: 0.6519 - val_loss: 2.8927 - val_accuracy: 0.7000\n",
      "Epoch 840/1000\n",
      "68/68 [==============================] - 0s 736us/step - loss: 0.6862 - accuracy: 0.6519 - val_loss: 2.8894 - val_accuracy: 0.7000\n",
      "Epoch 841/1000\n",
      "68/68 [==============================] - 0s 756us/step - loss: 0.6819 - accuracy: 0.6444 - val_loss: 2.8828 - val_accuracy: 0.7000\n",
      "Epoch 842/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 0.6645 - accuracy: 0.6593 - val_loss: 2.8827 - val_accuracy: 0.7000\n",
      "Epoch 843/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.6789 - accuracy: 0.6481 - val_loss: 2.8238 - val_accuracy: 0.7000\n",
      "Epoch 844/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.6746 - accuracy: 0.6481 - val_loss: 2.8266 - val_accuracy: 0.7000\n",
      "Epoch 845/1000\n",
      "68/68 [==============================] - 0s 711us/step - loss: 0.6613 - accuracy: 0.6519 - val_loss: 2.8623 - val_accuracy: 0.7000\n",
      "Epoch 846/1000\n",
      "68/68 [==============================] - 0s 843us/step - loss: 0.6766 - accuracy: 0.6519 - val_loss: 2.8766 - val_accuracy: 0.7000\n",
      "Epoch 847/1000\n",
      "68/68 [==============================] - 0s 764us/step - loss: 0.7836 - accuracy: 0.6370 - val_loss: 2.8177 - val_accuracy: 0.7111\n",
      "Epoch 848/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 0.7146 - accuracy: 0.6407 - val_loss: 2.6174 - val_accuracy: 0.7111\n",
      "Epoch 849/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 0.6515 - accuracy: 0.6667 - val_loss: 2.5938 - val_accuracy: 0.7111\n",
      "Epoch 850/1000\n",
      "68/68 [==============================] - 0s 751us/step - loss: 0.6611 - accuracy: 0.6593 - val_loss: 2.6084 - val_accuracy: 0.7111\n",
      "Epoch 851/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.6714 - accuracy: 0.6593 - val_loss: 2.6896 - val_accuracy: 0.6889\n",
      "Epoch 852/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.6910 - accuracy: 0.6333 - val_loss: 2.6216 - val_accuracy: 0.7111\n",
      "Epoch 853/1000\n",
      "68/68 [==============================] - 0s 742us/step - loss: 0.6834 - accuracy: 0.6519 - val_loss: 2.5337 - val_accuracy: 0.7111\n",
      "Epoch 854/1000\n",
      "68/68 [==============================] - 0s 842us/step - loss: 0.6775 - accuracy: 0.6519 - val_loss: 2.5494 - val_accuracy: 0.7111\n",
      "Epoch 855/1000\n",
      "68/68 [==============================] - 0s 744us/step - loss: 0.6741 - accuracy: 0.6519 - val_loss: 2.5112 - val_accuracy: 0.7111\n",
      "Epoch 856/1000\n",
      "68/68 [==============================] - 0s 766us/step - loss: 0.6716 - accuracy: 0.6519 - val_loss: 2.5450 - val_accuracy: 0.7111\n",
      "Epoch 857/1000\n",
      "68/68 [==============================] - 0s 749us/step - loss: 0.6514 - accuracy: 0.6704 - val_loss: 2.5971 - val_accuracy: 0.7111\n",
      "Epoch 858/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.6939 - accuracy: 0.6556 - val_loss: 2.6499 - val_accuracy: 0.7111\n",
      "Epoch 859/1000\n",
      "68/68 [==============================] - 0s 745us/step - loss: 0.6499 - accuracy: 0.6630 - val_loss: 2.6893 - val_accuracy: 0.7111\n",
      "Epoch 860/1000\n",
      "68/68 [==============================] - 0s 691us/step - loss: 0.6562 - accuracy: 0.6667 - val_loss: 2.7109 - val_accuracy: 0.7111\n",
      "Epoch 861/1000\n",
      "68/68 [==============================] - 0s 706us/step - loss: 0.6449 - accuracy: 0.6704 - val_loss: 2.7266 - val_accuracy: 0.7111\n",
      "Epoch 862/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.6936 - accuracy: 0.6333 - val_loss: 2.7343 - val_accuracy: 0.7111\n",
      "Epoch 863/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.6472 - accuracy: 0.6630 - val_loss: 2.7416 - val_accuracy: 0.7111\n",
      "Epoch 864/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.6951 - accuracy: 0.6370 - val_loss: 2.7481 - val_accuracy: 0.7111\n",
      "Epoch 865/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.6832 - accuracy: 0.6407 - val_loss: 2.7415 - val_accuracy: 0.7111\n",
      "Epoch 866/1000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6588 - accuracy: 0.6667 - val_loss: 2.6268 - val_accuracy: 0.7000\n",
      "Epoch 867/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.7094 - accuracy: 0.6259 - val_loss: 2.6390 - val_accuracy: 0.7000\n",
      "Epoch 868/1000\n",
      "68/68 [==============================] - 0s 728us/step - loss: 0.6939 - accuracy: 0.6407 - val_loss: 2.5986 - val_accuracy: 0.7000\n",
      "Epoch 869/1000\n",
      "68/68 [==============================] - 0s 757us/step - loss: 0.7080 - accuracy: 0.6444 - val_loss: 2.6396 - val_accuracy: 0.7000\n",
      "Epoch 870/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.6918 - accuracy: 0.6333 - val_loss: 2.6515 - val_accuracy: 0.7000\n",
      "Epoch 871/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.6796 - accuracy: 0.6481 - val_loss: 2.6625 - val_accuracy: 0.7000\n",
      "Epoch 872/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.6758 - accuracy: 0.6519 - val_loss: 2.6881 - val_accuracy: 0.6889\n",
      "Epoch 873/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.7477 - accuracy: 0.6333 - val_loss: 2.6628 - val_accuracy: 0.6889\n",
      "Epoch 874/1000\n",
      "68/68 [==============================] - 0s 758us/step - loss: 0.7200 - accuracy: 0.6296 - val_loss: 2.8385 - val_accuracy: 0.7000\n",
      "Epoch 875/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 0.6566 - accuracy: 0.6630 - val_loss: 2.8976 - val_accuracy: 0.7000\n",
      "Epoch 876/1000\n",
      "68/68 [==============================] - 0s 739us/step - loss: 0.7159 - accuracy: 0.6333 - val_loss: 2.7520 - val_accuracy: 0.6889\n",
      "Epoch 877/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 0.6705 - accuracy: 0.6556 - val_loss: 2.7620 - val_accuracy: 0.6889\n",
      "Epoch 878/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.6921 - accuracy: 0.6370 - val_loss: 2.7559 - val_accuracy: 0.6889\n",
      "Epoch 879/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.6645 - accuracy: 0.6593 - val_loss: 2.7643 - val_accuracy: 0.6889\n",
      "Epoch 880/1000\n",
      "68/68 [==============================] - 0s 750us/step - loss: 0.6899 - accuracy: 0.6593 - val_loss: 2.7978 - val_accuracy: 0.7000\n",
      "Epoch 881/1000\n",
      "68/68 [==============================] - 0s 729us/step - loss: 0.7138 - accuracy: 0.6185 - val_loss: 2.8186 - val_accuracy: 0.7000\n",
      "Epoch 882/1000\n",
      "68/68 [==============================] - 0s 724us/step - loss: 0.7569 - accuracy: 0.6259 - val_loss: 2.7642 - val_accuracy: 0.7000\n",
      "Epoch 883/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.6758 - accuracy: 0.6519 - val_loss: 2.7694 - val_accuracy: 0.7000\n",
      "Epoch 884/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 0.6779 - accuracy: 0.6444 - val_loss: 2.7166 - val_accuracy: 0.7000\n",
      "Epoch 885/1000\n",
      "68/68 [==============================] - 0s 805us/step - loss: 0.7785 - accuracy: 0.6519 - val_loss: 2.6963 - val_accuracy: 0.7000\n",
      "Epoch 886/1000\n",
      "68/68 [==============================] - 0s 788us/step - loss: 0.6764 - accuracy: 0.6519 - val_loss: 2.7380 - val_accuracy: 0.7000\n",
      "Epoch 887/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 0.7083 - accuracy: 0.6296 - val_loss: 2.6609 - val_accuracy: 0.7111\n",
      "Epoch 888/1000\n",
      "68/68 [==============================] - 0s 752us/step - loss: 0.7226 - accuracy: 0.6370 - val_loss: 2.6490 - val_accuracy: 0.6889\n",
      "Epoch 889/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.6703 - accuracy: 0.6556 - val_loss: 2.7688 - val_accuracy: 0.6889\n",
      "Epoch 890/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 722us/step - loss: 0.6939 - accuracy: 0.6407 - val_loss: 2.8285 - val_accuracy: 0.7000\n",
      "Epoch 891/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 0.6461 - accuracy: 0.6704 - val_loss: 3.5557 - val_accuracy: 0.7000\n",
      "Epoch 892/1000\n",
      "68/68 [==============================] - 0s 743us/step - loss: 0.7074 - accuracy: 0.6481 - val_loss: 3.0699 - val_accuracy: 0.6889\n",
      "Epoch 893/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.6785 - accuracy: 0.6519 - val_loss: 2.9189 - val_accuracy: 0.7000\n",
      "Epoch 894/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 0.6894 - accuracy: 0.6519 - val_loss: 3.2576 - val_accuracy: 0.6778\n",
      "Epoch 895/1000\n",
      "68/68 [==============================] - 0s 710us/step - loss: 0.7189 - accuracy: 0.6444 - val_loss: 3.6849 - val_accuracy: 0.6889\n",
      "Epoch 896/1000\n",
      "68/68 [==============================] - 0s 708us/step - loss: 0.6840 - accuracy: 0.6519 - val_loss: 3.3762 - val_accuracy: 0.6889\n",
      "Epoch 897/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.6687 - accuracy: 0.6630 - val_loss: 3.4558 - val_accuracy: 0.6889\n",
      "Epoch 898/1000\n",
      "68/68 [==============================] - 0s 693us/step - loss: 0.6827 - accuracy: 0.6481 - val_loss: 3.0235 - val_accuracy: 0.6889\n",
      "Epoch 899/1000\n",
      "68/68 [==============================] - 0s 713us/step - loss: 0.6756 - accuracy: 0.6481 - val_loss: 3.0322 - val_accuracy: 0.6889\n",
      "Epoch 900/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 0.7443 - accuracy: 0.6519 - val_loss: 2.6976 - val_accuracy: 0.6889\n",
      "Epoch 901/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.6728 - accuracy: 0.6556 - val_loss: 2.8044 - val_accuracy: 0.6889\n",
      "Epoch 902/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.6949 - accuracy: 0.6481 - val_loss: 3.4299 - val_accuracy: 0.7000\n",
      "Epoch 903/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 0.6752 - accuracy: 0.6704 - val_loss: 2.8098 - val_accuracy: 0.7000\n",
      "Epoch 904/1000\n",
      "68/68 [==============================] - 0s 705us/step - loss: 0.6630 - accuracy: 0.6741 - val_loss: 2.9263 - val_accuracy: 0.7000\n",
      "Epoch 905/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 0.6853 - accuracy: 0.6481 - val_loss: 3.0322 - val_accuracy: 0.7000\n",
      "Epoch 906/1000\n",
      "68/68 [==============================] - 0s 825us/step - loss: 0.6912 - accuracy: 0.6407 - val_loss: 3.2761 - val_accuracy: 0.6889\n",
      "Epoch 907/1000\n",
      "68/68 [==============================] - 0s 762us/step - loss: 0.7165 - accuracy: 0.6519 - val_loss: 3.0998 - val_accuracy: 0.6889\n",
      "Epoch 908/1000\n",
      "68/68 [==============================] - 0s 779us/step - loss: 0.7429 - accuracy: 0.6370 - val_loss: 2.8871 - val_accuracy: 0.6889\n",
      "Epoch 909/1000\n",
      "68/68 [==============================] - 0s 736us/step - loss: 0.6853 - accuracy: 0.6444 - val_loss: 2.8976 - val_accuracy: 0.7000\n",
      "Epoch 910/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.6521 - accuracy: 0.6630 - val_loss: 2.9078 - val_accuracy: 0.7000\n",
      "Epoch 911/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.6909 - accuracy: 0.6407 - val_loss: 2.9150 - val_accuracy: 0.7000\n",
      "Epoch 912/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 0.6550 - accuracy: 0.6630 - val_loss: 2.9666 - val_accuracy: 0.7000\n",
      "Epoch 913/1000\n",
      "68/68 [==============================] - 0s 705us/step - loss: 0.6562 - accuracy: 0.6667 - val_loss: 2.9773 - val_accuracy: 0.7000\n",
      "Epoch 914/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.6738 - accuracy: 0.6519 - val_loss: 2.9924 - val_accuracy: 0.7000\n",
      "Epoch 915/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.6564 - accuracy: 0.6667 - val_loss: 2.9951 - val_accuracy: 0.7000\n",
      "Epoch 916/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.6798 - accuracy: 0.6481 - val_loss: 3.0173 - val_accuracy: 0.7000\n",
      "Epoch 917/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.6739 - accuracy: 0.6481 - val_loss: 3.0226 - val_accuracy: 0.7000\n",
      "Epoch 918/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.6537 - accuracy: 0.6630 - val_loss: 3.0253 - val_accuracy: 0.7000\n",
      "Epoch 919/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.6713 - accuracy: 0.6519 - val_loss: 3.0694 - val_accuracy: 0.7000\n",
      "Epoch 920/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.7100 - accuracy: 0.6667 - val_loss: 3.0288 - val_accuracy: 0.7000\n",
      "Epoch 921/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.7420 - accuracy: 0.6481 - val_loss: 2.8245 - val_accuracy: 0.6778\n",
      "Epoch 922/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 0.6487 - accuracy: 0.6778 - val_loss: 2.8217 - val_accuracy: 0.6889\n",
      "Epoch 923/1000\n",
      "68/68 [==============================] - 0s 747us/step - loss: 0.6440 - accuracy: 0.6704 - val_loss: 2.8609 - val_accuracy: 0.6778\n",
      "Epoch 924/1000\n",
      "68/68 [==============================] - 0s 745us/step - loss: 0.6730 - accuracy: 0.6556 - val_loss: 2.8498 - val_accuracy: 0.6889\n",
      "Epoch 925/1000\n",
      "68/68 [==============================] - 0s 716us/step - loss: 0.7336 - accuracy: 0.6333 - val_loss: 2.7440 - val_accuracy: 0.6889\n",
      "Epoch 926/1000\n",
      "68/68 [==============================] - 0s 844us/step - loss: 0.7183 - accuracy: 0.6222 - val_loss: 2.7687 - val_accuracy: 0.6889\n",
      "Epoch 927/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.6796 - accuracy: 0.6481 - val_loss: 2.7493 - val_accuracy: 0.7000\n",
      "Epoch 928/1000\n",
      "68/68 [==============================] - 0s 722us/step - loss: 0.6840 - accuracy: 0.6444 - val_loss: 2.7650 - val_accuracy: 0.7111\n",
      "Epoch 929/1000\n",
      "68/68 [==============================] - 0s 756us/step - loss: 0.6964 - accuracy: 0.6296 - val_loss: 2.7697 - val_accuracy: 0.7111\n",
      "Epoch 930/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.6690 - accuracy: 0.6556 - val_loss: 2.7769 - val_accuracy: 0.7000\n",
      "Epoch 931/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 0.6750 - accuracy: 0.6556 - val_loss: 2.9089 - val_accuracy: 0.7000\n",
      "Epoch 932/1000\n",
      "68/68 [==============================] - 0s 755us/step - loss: 0.7029 - accuracy: 0.6444 - val_loss: 2.8647 - val_accuracy: 0.6889\n",
      "Epoch 933/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 0.6888 - accuracy: 0.6370 - val_loss: 2.9088 - val_accuracy: 0.7000\n",
      "Epoch 934/1000\n",
      "68/68 [==============================] - 0s 743us/step - loss: 0.7252 - accuracy: 0.6370 - val_loss: 2.8843 - val_accuracy: 0.7111\n",
      "Epoch 935/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.7131 - accuracy: 0.6185 - val_loss: 2.8719 - val_accuracy: 0.7111\n",
      "Epoch 936/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.6587 - accuracy: 0.6667 - val_loss: 2.9241 - val_accuracy: 0.7111\n",
      "Epoch 937/1000\n",
      "68/68 [==============================] - 0s 706us/step - loss: 0.6487 - accuracy: 0.6704 - val_loss: 2.9333 - val_accuracy: 0.7111\n",
      "Epoch 938/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 0.6812 - accuracy: 0.6481 - val_loss: 2.9269 - val_accuracy: 0.7111\n",
      "Epoch 939/1000\n",
      "68/68 [==============================] - 0s 710us/step - loss: 0.6678 - accuracy: 0.6519 - val_loss: 2.8851 - val_accuracy: 0.7111\n",
      "Epoch 940/1000\n",
      "68/68 [==============================] - 0s 733us/step - loss: 0.7187 - accuracy: 0.6370 - val_loss: 2.7493 - val_accuracy: 0.7111\n",
      "Epoch 941/1000\n",
      "68/68 [==============================] - 0s 714us/step - loss: 0.6994 - accuracy: 0.6481 - val_loss: 2.6929 - val_accuracy: 0.7111\n",
      "Epoch 942/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.6866 - accuracy: 0.6370 - val_loss: 2.7075 - val_accuracy: 0.7111\n",
      "Epoch 943/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.6887 - accuracy: 0.6407 - val_loss: 2.7785 - val_accuracy: 0.7111\n",
      "Epoch 944/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.6715 - accuracy: 0.6519 - val_loss: 2.8222 - val_accuracy: 0.7111\n",
      "Epoch 945/1000\n",
      "68/68 [==============================] - 0s 737us/step - loss: 0.6749 - accuracy: 0.6519 - val_loss: 2.8251 - val_accuracy: 0.7111\n",
      "Epoch 946/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6742 - accuracy: 0.6519 - val_loss: 2.8345 - val_accuracy: 0.7111\n",
      "Epoch 947/1000\n",
      "68/68 [==============================] - 0s 753us/step - loss: 0.6519 - accuracy: 0.6667 - val_loss: 2.8497 - val_accuracy: 0.7111\n",
      "Epoch 948/1000\n",
      "68/68 [==============================] - 0s 758us/step - loss: 0.6769 - accuracy: 0.6519 - val_loss: 2.8727 - val_accuracy: 0.7111\n",
      "Epoch 949/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 0.6916 - accuracy: 0.6481 - val_loss: 2.9592 - val_accuracy: 0.7111\n",
      "Epoch 950/1000\n",
      "68/68 [==============================] - 0s 762us/step - loss: 0.6467 - accuracy: 0.6704 - val_loss: 2.9750 - val_accuracy: 0.7111\n",
      "Epoch 951/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.6799 - accuracy: 0.6444 - val_loss: 2.9643 - val_accuracy: 0.7111\n",
      "Epoch 952/1000\n",
      "68/68 [==============================] - 0s 703us/step - loss: 0.6846 - accuracy: 0.6481 - val_loss: 2.9698 - val_accuracy: 0.7111\n",
      "Epoch 953/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 0.6718 - accuracy: 0.6630 - val_loss: 2.9685 - val_accuracy: 0.7111\n",
      "Epoch 954/1000\n",
      "68/68 [==============================] - 0s 732us/step - loss: 0.6982 - accuracy: 0.6370 - val_loss: 2.9891 - val_accuracy: 0.7111\n",
      "Epoch 955/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.8159 - accuracy: 0.6370 - val_loss: 3.0628 - val_accuracy: 0.7111\n",
      "Epoch 956/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 0.7186 - accuracy: 0.6296 - val_loss: 2.9355 - val_accuracy: 0.7111\n",
      "Epoch 957/1000\n",
      "68/68 [==============================] - 0s 748us/step - loss: 0.6605 - accuracy: 0.6556 - val_loss: 2.9014 - val_accuracy: 0.7111\n",
      "Epoch 958/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.6652 - accuracy: 0.6630 - val_loss: 2.9123 - val_accuracy: 0.7111\n",
      "Epoch 959/1000\n",
      "68/68 [==============================] - 0s 749us/step - loss: 0.6821 - accuracy: 0.6407 - val_loss: 2.9206 - val_accuracy: 0.7111\n",
      "Epoch 960/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.6589 - accuracy: 0.6630 - val_loss: 2.9240 - val_accuracy: 0.7111\n",
      "Epoch 961/1000\n",
      "68/68 [==============================] - 0s 754us/step - loss: 0.6681 - accuracy: 0.6593 - val_loss: 2.9336 - val_accuracy: 0.7111\n",
      "Epoch 962/1000\n",
      "68/68 [==============================] - 0s 763us/step - loss: 0.6769 - accuracy: 0.6556 - val_loss: 2.8606 - val_accuracy: 0.7111\n",
      "Epoch 963/1000\n",
      "68/68 [==============================] - 0s 740us/step - loss: 0.6769 - accuracy: 0.6444 - val_loss: 2.8621 - val_accuracy: 0.7111\n",
      "Epoch 964/1000\n",
      "68/68 [==============================] - 0s 815us/step - loss: 0.7288 - accuracy: 0.6444 - val_loss: 3.0337 - val_accuracy: 0.7111\n",
      "Epoch 965/1000\n",
      "68/68 [==============================] - 0s 846us/step - loss: 0.7341 - accuracy: 0.6333 - val_loss: 2.8433 - val_accuracy: 0.7111\n",
      "Epoch 966/1000\n",
      "68/68 [==============================] - 0s 804us/step - loss: 0.6513 - accuracy: 0.6667 - val_loss: 2.8987 - val_accuracy: 0.7000\n",
      "Epoch 967/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 0.6905 - accuracy: 0.6444 - val_loss: 2.9202 - val_accuracy: 0.7000\n",
      "Epoch 968/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.6880 - accuracy: 0.6444 - val_loss: 3.0098 - val_accuracy: 0.7000\n",
      "Epoch 969/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.6993 - accuracy: 0.6370 - val_loss: 3.0306 - val_accuracy: 0.7000\n",
      "Epoch 970/1000\n",
      "68/68 [==============================] - 0s 715us/step - loss: 0.7017 - accuracy: 0.6407 - val_loss: 3.0095 - val_accuracy: 0.6889\n",
      "Epoch 971/1000\n",
      "68/68 [==============================] - 0s 716us/step - loss: 0.6935 - accuracy: 0.6481 - val_loss: 2.8956 - val_accuracy: 0.7000\n",
      "Epoch 972/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.6683 - accuracy: 0.6519 - val_loss: 2.8825 - val_accuracy: 0.7000\n",
      "Epoch 973/1000\n",
      "68/68 [==============================] - 0s 720us/step - loss: 0.6484 - accuracy: 0.6778 - val_loss: 2.8958 - val_accuracy: 0.7000\n",
      "Epoch 974/1000\n",
      "68/68 [==============================] - 0s 726us/step - loss: 0.6899 - accuracy: 0.6370 - val_loss: 2.8716 - val_accuracy: 0.7111\n",
      "Epoch 975/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.6895 - accuracy: 0.6407 - val_loss: 2.8621 - val_accuracy: 0.7111\n",
      "Epoch 976/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.6792 - accuracy: 0.6481 - val_loss: 2.8420 - val_accuracy: 0.7111\n",
      "Epoch 977/1000\n",
      "68/68 [==============================] - 0s 761us/step - loss: 0.6696 - accuracy: 0.6556 - val_loss: 2.8489 - val_accuracy: 0.7111\n",
      "Epoch 978/1000\n",
      "68/68 [==============================] - 0s 748us/step - loss: 0.6804 - accuracy: 0.6444 - val_loss: 2.9158 - val_accuracy: 0.7111\n",
      "Epoch 979/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.6776 - accuracy: 0.6444 - val_loss: 2.9229 - val_accuracy: 0.7111\n",
      "Epoch 980/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.6617 - accuracy: 0.6630 - val_loss: 2.9277 - val_accuracy: 0.7111\n",
      "Epoch 981/1000\n",
      "68/68 [==============================] - 0s 731us/step - loss: 0.6869 - accuracy: 0.6593 - val_loss: 2.9443 - val_accuracy: 0.7111\n",
      "Epoch 982/1000\n",
      "68/68 [==============================] - 0s 749us/step - loss: 0.6836 - accuracy: 0.6481 - val_loss: 2.9552 - val_accuracy: 0.7111\n",
      "Epoch 983/1000\n",
      "68/68 [==============================] - 0s 725us/step - loss: 0.7127 - accuracy: 0.6407 - val_loss: 2.9262 - val_accuracy: 0.7000\n",
      "Epoch 984/1000\n",
      "68/68 [==============================] - 0s 711us/step - loss: 0.7090 - accuracy: 0.6481 - val_loss: 2.9543 - val_accuracy: 0.7111\n",
      "Epoch 985/1000\n",
      "68/68 [==============================] - 0s 841us/step - loss: 0.6654 - accuracy: 0.6593 - val_loss: 2.9921 - val_accuracy: 0.7000\n",
      "Epoch 986/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.6588 - accuracy: 0.6667 - val_loss: 3.0859 - val_accuracy: 0.7000\n",
      "Epoch 987/1000\n",
      "68/68 [==============================] - 0s 734us/step - loss: 0.6719 - accuracy: 0.6630 - val_loss: 3.0608 - val_accuracy: 0.7000\n",
      "Epoch 988/1000\n",
      "68/68 [==============================] - 0s 709us/step - loss: 0.6886 - accuracy: 0.6593 - val_loss: 2.9836 - val_accuracy: 0.7111\n",
      "Epoch 989/1000\n",
      "68/68 [==============================] - 0s 696us/step - loss: 0.7081 - accuracy: 0.6370 - val_loss: 3.1442 - val_accuracy: 0.6889\n",
      "Epoch 990/1000\n",
      "68/68 [==============================] - 0s 708us/step - loss: 0.6690 - accuracy: 0.6593 - val_loss: 3.1492 - val_accuracy: 0.7000\n",
      "Epoch 991/1000\n",
      "68/68 [==============================] - 0s 723us/step - loss: 0.7515 - accuracy: 0.6444 - val_loss: 3.2376 - val_accuracy: 0.6889\n",
      "Epoch 992/1000\n",
      "68/68 [==============================] - 0s 735us/step - loss: 0.6771 - accuracy: 0.6519 - val_loss: 3.2101 - val_accuracy: 0.6889\n",
      "Epoch 993/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.6849 - accuracy: 0.6481 - val_loss: 3.1713 - val_accuracy: 0.7000\n",
      "Epoch 994/1000\n",
      "68/68 [==============================] - 0s 717us/step - loss: 0.6730 - accuracy: 0.6519 - val_loss: 3.1028 - val_accuracy: 0.7111\n",
      "Epoch 995/1000\n",
      "68/68 [==============================] - 0s 721us/step - loss: 0.6916 - accuracy: 0.6333 - val_loss: 3.1091 - val_accuracy: 0.7000\n",
      "Epoch 996/1000\n",
      "68/68 [==============================] - 0s 719us/step - loss: 0.6631 - accuracy: 0.6593 - val_loss: 3.1153 - val_accuracy: 0.7000\n",
      "Epoch 997/1000\n",
      "68/68 [==============================] - 0s 738us/step - loss: 0.6978 - accuracy: 0.6407 - val_loss: 3.1432 - val_accuracy: 0.7000\n",
      "Epoch 998/1000\n",
      "68/68 [==============================] - 0s 730us/step - loss: 0.7336 - accuracy: 0.6407 - val_loss: 3.1341 - val_accuracy: 0.6889\n",
      "Epoch 999/1000\n",
      "68/68 [==============================] - 0s 741us/step - loss: 0.6848 - accuracy: 0.6519 - val_loss: 3.1055 - val_accuracy: 0.7000\n",
      "Epoch 1000/1000\n",
      "68/68 [==============================] - 0s 766us/step - loss: 0.6987 - accuracy: 0.6370 - val_loss: 3.1134 - val_accuracy: 0.7000\n"
     ]
    }
   ],
   "source": [
    "model = getNetwork()\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f395677d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f395677d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.53      0.59        30\n",
      "           1       0.87      0.43      0.58        30\n",
      "           2       0.45      0.77      0.57        30\n",
      "\n",
      "    accuracy                           0.58        90\n",
      "   macro avg       0.66      0.58      0.58        90\n",
      "weighted avg       0.66      0.58      0.58        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test = model.predict(X_test)\n",
    "predictions_categorical = np.argmax(pred_test, axis=1)\n",
    "report = classification_report(y_test, predictions_categorical)\n",
    "print(report)\n",
    "classification_report_csv(report, \"NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Models in C code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpx5bgf0yx/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpx5bgf0yx/assets\n"
     ]
    }
   ],
   "source": [
    "# Neural network with TinyMLGen\n",
    "with open(tasks[choosenIndex] + '/exportedModels/NNmodel.h', 'w') as f:\n",
    "    f.write(tiny.port(model, optimize=False))\n",
    "\n",
    "# Classifiers with MicroMLGen\n",
    "for name, model in models:\n",
    "    prepath = tasks[choosenIndex] + '/exportedModels/'\n",
    "    path = prepath + name + '.h'\n",
    "    x = port(model, optimize=True)\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(port(model, optimize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
