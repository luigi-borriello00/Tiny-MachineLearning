{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Comparison for TinyML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "import pickle\n",
    "from pandas import read_csv\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,  classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold,StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
    "\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dense, Input, concatenate, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import tensorflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from micromlgen import port\n",
    "import tinymlgen as tiny\n",
    "\n",
    "import warnings\n",
    "import seaborn as sbs\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/X2.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open('data/y2.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 5\n",
    "samples = 150\n",
    "X = X[:labels*samples]\n",
    "y = y[:labels*samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y).tolist()\n",
    "for i in range(len(classes)):\n",
    "    y = np.where(y==classes[i], i, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 32)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([int(el) for el in y])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Spotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 42\n",
    "scoring = 'f1_macro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "\n",
    "#models.append(('XGB', XGBClassifier(random_state=seed)))\n",
    "models.append(('GNB', GaussianNB(var_smoothing=2e-9)))\n",
    "models.append(('LR', LogisticRegression(random_state=seed)))\n",
    "models.append(('CART' , DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('SVC' , SVC(gamma=0.05, random_state=seed)))\n",
    "models.append(('RF', RandomForestClassifier(random_state=seed, n_estimators = 50)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB - 0,36 0,06\n",
      "LR - 0,42 0,06\n",
      "CART - 0,43 0,06\n",
      "SVC - 0,55 0,04\n",
      "RF - 0,59 0,04\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    # Dividere dati in n = num_folds\n",
    "    kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = np.array([])\n",
    "    for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "        X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "        X_cross_train = scaler.fit_transform(X_cross_train)\n",
    "        X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "        X_cross_test = scaler.transform(X_cross_test)\n",
    "        model.fit(X_cross_train, y_cross_train)  \n",
    "        y_pred = model.predict(X_cross_test)\n",
    "        f1s = f1_score(y_cross_test, y_pred, average=\"weighted\")\n",
    "        cv_results = np.append(cv_results, [f1s])\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #msg = \"%s - %f - %f\" % (name, cv_results.mean(), cv_results.std())\n",
    "    msg = \"{} - {:.2f} {:.2f}\".format(name, cv_results.mean(), cv_results.std()).replace('.', ',')\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "fig.suptitle('Algorithms Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valutazione dei migliori algoritmi su test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione modelli sul Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model GNB: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.77      0.49        30\n",
      "           1       0.52      0.43      0.47        30\n",
      "           2       0.32      0.23      0.27        30\n",
      "           3       0.33      0.10      0.15        30\n",
      "           4       0.35      0.37      0.36        30\n",
      "\n",
      "    accuracy                           0.38       150\n",
      "   macro avg       0.38      0.38      0.35       150\n",
      "weighted avg       0.38      0.38      0.35       150\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model LR: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.43      0.40        30\n",
      "           1       0.50      0.57      0.53        30\n",
      "           2       0.35      0.30      0.32        30\n",
      "           3       0.45      0.43      0.44        30\n",
      "           4       0.31      0.27      0.29        30\n",
      "\n",
      "    accuracy                           0.40       150\n",
      "   macro avg       0.39      0.40      0.40       150\n",
      "weighted avg       0.39      0.40      0.40       150\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model CART: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.53      0.47        30\n",
      "           1       0.50      0.47      0.48        30\n",
      "           2       0.45      0.43      0.44        30\n",
      "           3       0.67      0.60      0.63        30\n",
      "           4       0.54      0.50      0.52        30\n",
      "\n",
      "    accuracy                           0.51       150\n",
      "   macro avg       0.51      0.51      0.51       150\n",
      "weighted avg       0.51      0.51      0.51       150\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model SVC: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.43      0.51        30\n",
      "           1       0.49      0.60      0.54        30\n",
      "           2       0.47      0.23      0.31        30\n",
      "           3       0.52      0.57      0.54        30\n",
      "           4       0.45      0.67      0.54        30\n",
      "\n",
      "    accuracy                           0.50       150\n",
      "   macro avg       0.51      0.50      0.49       150\n",
      "weighted avg       0.51      0.50      0.49       150\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model RF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.50      0.52        30\n",
      "           1       0.59      0.53      0.56        30\n",
      "           2       0.61      0.47      0.53        30\n",
      "           3       0.59      0.63      0.61        30\n",
      "           4       0.57      0.77      0.66        30\n",
      "\n",
      "    accuracy                           0.58       150\n",
      "   macro avg       0.58      0.58      0.58       150\n",
      "weighted avg       0.58      0.58      0.58       150\n",
      "\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "for name, model in models:\n",
    "    model.fit(X_train,  y_train)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    print(f\"Model {name}: \")\n",
    "    print(classification_report(y_test, pred_test))\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione Inferance Rate medio (|X_test| = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = read_csv(\"InfTimeReport.csv\")\n",
    "g = sbs.barplot(x=csv['Algoritmo'], y=csv['InfTime'])\n",
    "g.set_yscale(\"log\")\n",
    "plt.ylabel(\"Inference Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memoria occupata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = read_csv(\"MemOccupationReport.csv\")\n",
    "sbs.barplot(x=csv['Algoritmo'], y=csv['MemOccupata2'])\n",
    "plt.ylabel(\"MemOccupata in Byte\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getNetwork():\n",
    "    model = Sequential(name=\"Sequential-NN\")\n",
    "    model.add(layers.Dense(X.shape[1], activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Dense(np.unique(y).size, activation='softmax'))\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "    # SGB\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 3,363\n",
      "Trainable params: 3,363\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "41/41 [==============================] - 0s 496us/step - loss: 1.1479 - accuracy: 0.3241\n",
      "Epoch 2/500\n",
      "41/41 [==============================] - 0s 486us/step - loss: 1.1312 - accuracy: 0.3457\n",
      "Epoch 3/500\n",
      "41/41 [==============================] - 0s 513us/step - loss: 1.0842 - accuracy: 0.3549\n",
      "Epoch 4/500\n",
      "41/41 [==============================] - 0s 446us/step - loss: 1.0609 - accuracy: 0.3704\n",
      "Epoch 5/500\n",
      "41/41 [==============================] - 0s 491us/step - loss: 1.0395 - accuracy: 0.4043\n",
      "Epoch 6/500\n",
      "41/41 [==============================] - 0s 465us/step - loss: 1.0436 - accuracy: 0.4259\n",
      "Epoch 7/500\n",
      "41/41 [==============================] - 0s 492us/step - loss: 1.0204 - accuracy: 0.4228\n",
      "Epoch 8/500\n",
      "41/41 [==============================] - 0s 481us/step - loss: 1.0124 - accuracy: 0.4043\n",
      "Epoch 9/500\n",
      "41/41 [==============================] - 0s 503us/step - loss: 1.0068 - accuracy: 0.4414\n",
      "Epoch 10/500\n",
      "41/41 [==============================] - 0s 454us/step - loss: 1.0074 - accuracy: 0.4506\n",
      "Epoch 11/500\n",
      "41/41 [==============================] - 0s 466us/step - loss: 0.9773 - accuracy: 0.5154\n",
      "Epoch 12/500\n",
      "41/41 [==============================] - 0s 485us/step - loss: 0.9992 - accuracy: 0.4938\n",
      "Epoch 13/500\n",
      "41/41 [==============================] - 0s 444us/step - loss: 0.9471 - accuracy: 0.5154\n",
      "Epoch 14/500\n",
      "41/41 [==============================] - 0s 462us/step - loss: 0.9922 - accuracy: 0.5000\n",
      "Epoch 15/500\n",
      "41/41 [==============================] - 0s 471us/step - loss: 0.9601 - accuracy: 0.4938\n",
      "Epoch 16/500\n",
      "41/41 [==============================] - 0s 522us/step - loss: 0.9210 - accuracy: 0.5833\n",
      "Epoch 17/500\n",
      "41/41 [==============================] - 0s 491us/step - loss: 0.9566 - accuracy: 0.4753\n",
      "Epoch 18/500\n",
      "41/41 [==============================] - 0s 471us/step - loss: 0.9402 - accuracy: 0.5216\n",
      "Epoch 19/500\n",
      "41/41 [==============================] - 0s 486us/step - loss: 0.9296 - accuracy: 0.5247\n",
      "Epoch 20/500\n",
      "41/41 [==============================] - 0s 463us/step - loss: 0.9338 - accuracy: 0.5370\n",
      "Epoch 21/500\n",
      "41/41 [==============================] - 0s 490us/step - loss: 0.9247 - accuracy: 0.5586\n",
      "Epoch 22/500\n",
      "41/41 [==============================] - 0s 494us/step - loss: 0.9098 - accuracy: 0.5772\n",
      "Epoch 23/500\n",
      "41/41 [==============================] - 0s 479us/step - loss: 0.9183 - accuracy: 0.5525\n",
      "Epoch 24/500\n",
      "41/41 [==============================] - 0s 496us/step - loss: 0.9178 - accuracy: 0.5741\n",
      "Epoch 25/500\n",
      "41/41 [==============================] - 0s 494us/step - loss: 0.9017 - accuracy: 0.6173\n",
      "Epoch 26/500\n",
      "41/41 [==============================] - 0s 501us/step - loss: 0.9122 - accuracy: 0.5833\n",
      "Epoch 27/500\n",
      "41/41 [==============================] - 0s 464us/step - loss: 0.9188 - accuracy: 0.5617\n",
      "Epoch 28/500\n",
      "41/41 [==============================] - 0s 481us/step - loss: 0.8968 - accuracy: 0.5988\n",
      "Epoch 29/500\n",
      "41/41 [==============================] - 0s 491us/step - loss: 0.8886 - accuracy: 0.5586\n",
      "Epoch 30/500\n",
      "41/41 [==============================] - 0s 498us/step - loss: 0.8913 - accuracy: 0.6296\n",
      "Epoch 31/500\n",
      "41/41 [==============================] - 0s 498us/step - loss: 0.8710 - accuracy: 0.5864\n",
      "Epoch 32/500\n",
      "41/41 [==============================] - 0s 479us/step - loss: 0.8839 - accuracy: 0.6049\n",
      "Epoch 33/500\n",
      "41/41 [==============================] - 0s 456us/step - loss: 0.8965 - accuracy: 0.5802\n",
      "Epoch 34/500\n",
      "41/41 [==============================] - 0s 476us/step - loss: 0.8874 - accuracy: 0.5833\n",
      "Epoch 35/500\n",
      "41/41 [==============================] - 0s 470us/step - loss: 0.8851 - accuracy: 0.5741\n",
      "Epoch 36/500\n",
      "41/41 [==============================] - 0s 500us/step - loss: 0.8599 - accuracy: 0.6204\n",
      "Epoch 37/500\n",
      "41/41 [==============================] - 0s 459us/step - loss: 0.8474 - accuracy: 0.6296\n",
      "Epoch 38/500\n",
      "41/41 [==============================] - 0s 500us/step - loss: 0.8790 - accuracy: 0.5988\n",
      "Epoch 39/500\n",
      "41/41 [==============================] - 0s 460us/step - loss: 0.8551 - accuracy: 0.6080\n",
      "Epoch 40/500\n",
      "41/41 [==============================] - 0s 482us/step - loss: 0.8714 - accuracy: 0.5988\n",
      "Epoch 41/500\n",
      "41/41 [==============================] - 0s 491us/step - loss: 0.8615 - accuracy: 0.6235\n",
      "Epoch 42/500\n",
      "41/41 [==============================] - 0s 467us/step - loss: 0.8605 - accuracy: 0.6420\n",
      "Epoch 43/500\n",
      "41/41 [==============================] - 0s 465us/step - loss: 0.8304 - accuracy: 0.6481\n",
      "Epoch 44/500\n",
      "41/41 [==============================] - 0s 465us/step - loss: 0.8184 - accuracy: 0.6512\n",
      "Epoch 45/500\n",
      "41/41 [==============================] - 0s 477us/step - loss: 0.8420 - accuracy: 0.6327\n",
      "Epoch 46/500\n",
      "41/41 [==============================] - 0s 502us/step - loss: 0.8442 - accuracy: 0.6512\n",
      "Epoch 47/500\n",
      "41/41 [==============================] - 0s 471us/step - loss: 0.8393 - accuracy: 0.5988\n",
      "Epoch 48/500\n",
      "41/41 [==============================] - 0s 497us/step - loss: 0.8472 - accuracy: 0.6296\n",
      "Epoch 49/500\n",
      "41/41 [==============================] - 0s 475us/step - loss: 0.8208 - accuracy: 0.6636\n",
      "Epoch 50/500\n",
      "41/41 [==============================] - 0s 486us/step - loss: 0.8427 - accuracy: 0.6019\n",
      "Epoch 51/500\n",
      "41/41 [==============================] - 0s 494us/step - loss: 0.8508 - accuracy: 0.6265\n",
      "Epoch 52/500\n",
      "41/41 [==============================] - 0s 486us/step - loss: 0.8455 - accuracy: 0.6173\n",
      "Epoch 53/500\n",
      "41/41 [==============================] - 0s 492us/step - loss: 0.8215 - accuracy: 0.6451\n",
      "Epoch 54/500\n",
      "41/41 [==============================] - 0s 535us/step - loss: 0.7937 - accuracy: 0.6728\n",
      "Epoch 55/500\n",
      "41/41 [==============================] - 0s 508us/step - loss: 0.8116 - accuracy: 0.6605\n",
      "Epoch 56/500\n",
      "41/41 [==============================] - 0s 507us/step - loss: 0.8328 - accuracy: 0.6111\n",
      "Epoch 57/500\n",
      "41/41 [==============================] - 0s 518us/step - loss: 0.8310 - accuracy: 0.6543\n",
      "Epoch 58/500\n",
      "41/41 [==============================] - 0s 509us/step - loss: 0.7920 - accuracy: 0.6420\n",
      "Epoch 59/500\n",
      "41/41 [==============================] - 0s 528us/step - loss: 0.8327 - accuracy: 0.6451\n",
      "Epoch 60/500\n",
      "41/41 [==============================] - 0s 511us/step - loss: 0.8034 - accuracy: 0.6481\n",
      "Epoch 61/500\n",
      "41/41 [==============================] - 0s 499us/step - loss: 0.8271 - accuracy: 0.6728\n",
      "Epoch 62/500\n",
      "41/41 [==============================] - 0s 515us/step - loss: 0.8105 - accuracy: 0.6821\n",
      "Epoch 63/500\n",
      "41/41 [==============================] - 0s 508us/step - loss: 0.7931 - accuracy: 0.6543\n",
      "Epoch 64/500\n",
      "41/41 [==============================] - 0s 488us/step - loss: 0.8133 - accuracy: 0.6574\n",
      "Epoch 65/500\n",
      "41/41 [==============================] - 0s 509us/step - loss: 0.8053 - accuracy: 0.6636\n",
      "Epoch 66/500\n",
      "41/41 [==============================] - 0s 520us/step - loss: 0.7893 - accuracy: 0.6667\n",
      "Epoch 67/500\n",
      "41/41 [==============================] - 0s 555us/step - loss: 0.7716 - accuracy: 0.6852\n",
      "Epoch 68/500\n",
      "41/41 [==============================] - 0s 509us/step - loss: 0.8262 - accuracy: 0.6420\n",
      "Epoch 69/500\n",
      "41/41 [==============================] - 0s 601us/step - loss: 0.8094 - accuracy: 0.6728\n",
      "Epoch 70/500\n",
      "41/41 [==============================] - 0s 504us/step - loss: 0.8176 - accuracy: 0.6265\n",
      "Epoch 71/500\n",
      "41/41 [==============================] - 0s 528us/step - loss: 0.7801 - accuracy: 0.6636\n",
      "Epoch 72/500\n",
      "41/41 [==============================] - 0s 520us/step - loss: 0.8054 - accuracy: 0.6728\n",
      "Epoch 73/500\n",
      "41/41 [==============================] - 0s 472us/step - loss: 0.7891 - accuracy: 0.6512\n",
      "Epoch 74/500\n",
      "41/41 [==============================] - 0s 513us/step - loss: 0.7749 - accuracy: 0.6667\n",
      "Epoch 75/500\n",
      "41/41 [==============================] - 0s 568us/step - loss: 0.7996 - accuracy: 0.6698\n",
      "Epoch 76/500\n",
      "41/41 [==============================] - 0s 494us/step - loss: 0.8167 - accuracy: 0.6451\n",
      "Epoch 77/500\n",
      "41/41 [==============================] - 0s 515us/step - loss: 0.7868 - accuracy: 0.6636\n",
      "Epoch 78/500\n",
      "41/41 [==============================] - 0s 525us/step - loss: 0.7951 - accuracy: 0.6543\n",
      "Epoch 79/500\n",
      "41/41 [==============================] - 0s 636us/step - loss: 0.7709 - accuracy: 0.6883\n",
      "Epoch 80/500\n",
      "41/41 [==============================] - 0s 480us/step - loss: 0.7560 - accuracy: 0.6636\n",
      "Epoch 81/500\n",
      "41/41 [==============================] - 0s 496us/step - loss: 0.7655 - accuracy: 0.6790\n",
      "Epoch 82/500\n",
      "41/41 [==============================] - 0s 470us/step - loss: 0.8017 - accuracy: 0.6512\n",
      "Epoch 83/500\n",
      "41/41 [==============================] - 0s 507us/step - loss: 0.7789 - accuracy: 0.6605\n",
      "Epoch 84/500\n",
      "41/41 [==============================] - 0s 512us/step - loss: 0.7792 - accuracy: 0.6636\n",
      "Epoch 85/500\n",
      "41/41 [==============================] - 0s 500us/step - loss: 0.7635 - accuracy: 0.6883\n",
      "Epoch 86/500\n",
      "41/41 [==============================] - 0s 488us/step - loss: 0.7972 - accuracy: 0.6821\n",
      "Epoch 87/500\n",
      "41/41 [==============================] - 0s 498us/step - loss: 0.7712 - accuracy: 0.6698\n",
      "Epoch 88/500\n",
      "41/41 [==============================] - 0s 510us/step - loss: 0.7764 - accuracy: 0.6512\n",
      "Epoch 89/500\n",
      "41/41 [==============================] - 0s 463us/step - loss: 0.7553 - accuracy: 0.6636\n",
      "Epoch 90/500\n",
      "41/41 [==============================] - 0s 505us/step - loss: 0.7739 - accuracy: 0.6512\n",
      "Epoch 91/500\n",
      "41/41 [==============================] - 0s 506us/step - loss: 0.7564 - accuracy: 0.6914\n",
      "Epoch 92/500\n",
      "41/41 [==============================] - 0s 467us/step - loss: 0.7595 - accuracy: 0.7037\n",
      "Epoch 93/500\n",
      "41/41 [==============================] - 0s 511us/step - loss: 0.7461 - accuracy: 0.7006\n",
      "Epoch 94/500\n",
      "41/41 [==============================] - 0s 492us/step - loss: 0.7735 - accuracy: 0.6944\n",
      "Epoch 95/500\n",
      "41/41 [==============================] - 0s 518us/step - loss: 0.7801 - accuracy: 0.6605\n",
      "Epoch 96/500\n",
      "41/41 [==============================] - 0s 464us/step - loss: 0.7528 - accuracy: 0.6914\n",
      "Epoch 97/500\n",
      "41/41 [==============================] - 0s 472us/step - loss: 0.7694 - accuracy: 0.6883\n",
      "Epoch 98/500\n",
      "41/41 [==============================] - 0s 441us/step - loss: 0.7275 - accuracy: 0.7006\n",
      "Epoch 99/500\n",
      "41/41 [==============================] - 0s 485us/step - loss: 0.7522 - accuracy: 0.6790\n",
      "Epoch 100/500\n",
      "41/41 [==============================] - 0s 495us/step - loss: 0.7405 - accuracy: 0.6944\n",
      "Epoch 101/500\n",
      "41/41 [==============================] - 0s 474us/step - loss: 0.7556 - accuracy: 0.6944\n",
      "Epoch 102/500\n",
      "41/41 [==============================] - 0s 477us/step - loss: 0.7339 - accuracy: 0.7191\n",
      "Epoch 103/500\n",
      "41/41 [==============================] - 0s 498us/step - loss: 0.7324 - accuracy: 0.6944\n",
      "Epoch 104/500\n",
      "41/41 [==============================] - 0s 478us/step - loss: 0.7751 - accuracy: 0.6852\n",
      "Epoch 105/500\n",
      "41/41 [==============================] - 0s 483us/step - loss: 0.7442 - accuracy: 0.7068\n",
      "Epoch 106/500\n",
      "41/41 [==============================] - 0s 498us/step - loss: 0.7660 - accuracy: 0.6975\n",
      "Epoch 107/500\n",
      "41/41 [==============================] - 0s 503us/step - loss: 0.7347 - accuracy: 0.6821\n",
      "Epoch 108/500\n",
      "41/41 [==============================] - 0s 511us/step - loss: 0.7364 - accuracy: 0.7130\n",
      "Epoch 109/500\n",
      "41/41 [==============================] - 0s 496us/step - loss: 0.7464 - accuracy: 0.7006\n",
      "Epoch 110/500\n",
      "41/41 [==============================] - 0s 518us/step - loss: 0.7540 - accuracy: 0.6821\n",
      "Epoch 111/500\n",
      "41/41 [==============================] - 0s 513us/step - loss: 0.7567 - accuracy: 0.6975\n",
      "Epoch 112/500\n",
      "41/41 [==============================] - 0s 492us/step - loss: 0.7130 - accuracy: 0.7253\n",
      "Epoch 113/500\n",
      "41/41 [==============================] - 0s 509us/step - loss: 0.7297 - accuracy: 0.6883\n",
      "Epoch 114/500\n",
      "41/41 [==============================] - 0s 593us/step - loss: 0.7402 - accuracy: 0.6883\n",
      "Epoch 115/500\n",
      "41/41 [==============================] - 0s 473us/step - loss: 0.7449 - accuracy: 0.6543\n",
      "Epoch 116/500\n",
      "41/41 [==============================] - 0s 503us/step - loss: 0.7095 - accuracy: 0.7130\n",
      "Epoch 117/500\n",
      "41/41 [==============================] - 0s 516us/step - loss: 0.7674 - accuracy: 0.6698\n",
      "Epoch 118/500\n",
      "41/41 [==============================] - 0s 490us/step - loss: 0.7479 - accuracy: 0.6790\n",
      "Epoch 119/500\n",
      "41/41 [==============================] - 0s 564us/step - loss: 0.6958 - accuracy: 0.7253\n",
      "Epoch 120/500\n",
      "41/41 [==============================] - 0s 535us/step - loss: 0.7018 - accuracy: 0.6944\n",
      "Epoch 121/500\n",
      "41/41 [==============================] - 0s 563us/step - loss: 0.7149 - accuracy: 0.7284\n",
      "Epoch 122/500\n",
      "41/41 [==============================] - 0s 533us/step - loss: 0.7289 - accuracy: 0.7160\n",
      "Epoch 123/500\n",
      "41/41 [==============================] - 0s 533us/step - loss: 0.6910 - accuracy: 0.7160\n",
      "Epoch 124/500\n",
      "41/41 [==============================] - 0s 503us/step - loss: 0.7190 - accuracy: 0.7315\n",
      "Epoch 125/500\n",
      "41/41 [==============================] - 0s 498us/step - loss: 0.7206 - accuracy: 0.7006\n",
      "Epoch 126/500\n",
      "41/41 [==============================] - 0s 477us/step - loss: 0.7260 - accuracy: 0.7006\n",
      "Epoch 127/500\n",
      "41/41 [==============================] - 0s 501us/step - loss: 0.7197 - accuracy: 0.7037\n",
      "Epoch 128/500\n",
      "41/41 [==============================] - 0s 556us/step - loss: 0.7388 - accuracy: 0.7099\n",
      "Epoch 129/500\n",
      "41/41 [==============================] - 0s 536us/step - loss: 0.7205 - accuracy: 0.7160\n",
      "Epoch 130/500\n",
      "41/41 [==============================] - 0s 540us/step - loss: 0.7277 - accuracy: 0.7006\n",
      "Epoch 131/500\n",
      "41/41 [==============================] - 0s 502us/step - loss: 0.7274 - accuracy: 0.6944\n",
      "Epoch 132/500\n",
      "41/41 [==============================] - 0s 458us/step - loss: 0.6955 - accuracy: 0.7099\n",
      "Epoch 133/500\n",
      "41/41 [==============================] - 0s 472us/step - loss: 0.6940 - accuracy: 0.7253\n",
      "Epoch 134/500\n",
      "41/41 [==============================] - 0s 502us/step - loss: 0.7071 - accuracy: 0.7068\n",
      "Epoch 135/500\n",
      "41/41 [==============================] - 0s 498us/step - loss: 0.6935 - accuracy: 0.6975\n",
      "Epoch 136/500\n",
      "41/41 [==============================] - 0s 487us/step - loss: 0.7464 - accuracy: 0.6698\n",
      "Epoch 137/500\n",
      "41/41 [==============================] - 0s 511us/step - loss: 0.6942 - accuracy: 0.7438\n",
      "Epoch 138/500\n",
      "41/41 [==============================] - 0s 466us/step - loss: 0.7355 - accuracy: 0.6759\n",
      "Epoch 139/500\n",
      "41/41 [==============================] - 0s 521us/step - loss: 0.7086 - accuracy: 0.7160\n",
      "Epoch 140/500\n",
      "41/41 [==============================] - 0s 513us/step - loss: 0.6855 - accuracy: 0.7377\n",
      "Epoch 141/500\n",
      "41/41 [==============================] - 0s 487us/step - loss: 0.7014 - accuracy: 0.7130\n",
      "Epoch 142/500\n",
      "41/41 [==============================] - 0s 509us/step - loss: 0.7178 - accuracy: 0.6698\n",
      "Epoch 143/500\n",
      "41/41 [==============================] - 0s 526us/step - loss: 0.6902 - accuracy: 0.7037\n",
      "Epoch 144/500\n",
      "41/41 [==============================] - 0s 541us/step - loss: 0.6754 - accuracy: 0.7346\n",
      "Epoch 145/500\n",
      "41/41 [==============================] - 0s 525us/step - loss: 0.7004 - accuracy: 0.6852\n",
      "Epoch 146/500\n",
      "41/41 [==============================] - 0s 515us/step - loss: 0.6975 - accuracy: 0.7099\n",
      "Epoch 147/500\n",
      "41/41 [==============================] - 0s 514us/step - loss: 0.6826 - accuracy: 0.7191\n",
      "Epoch 148/500\n",
      "41/41 [==============================] - 0s 491us/step - loss: 0.7114 - accuracy: 0.6883\n",
      "Epoch 149/500\n",
      "41/41 [==============================] - 0s 459us/step - loss: 0.6780 - accuracy: 0.7315\n",
      "Epoch 150/500\n",
      "41/41 [==============================] - 0s 551us/step - loss: 0.7107 - accuracy: 0.7438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/500\n",
      "41/41 [==============================] - 0s 507us/step - loss: 0.6965 - accuracy: 0.7377\n",
      "Epoch 152/500\n",
      "41/41 [==============================] - 0s 527us/step - loss: 0.7071 - accuracy: 0.6975\n",
      "Epoch 153/500\n",
      "41/41 [==============================] - 0s 518us/step - loss: 0.7048 - accuracy: 0.6975\n",
      "Epoch 154/500\n",
      "41/41 [==============================] - 0s 498us/step - loss: 0.6796 - accuracy: 0.7346\n",
      "Epoch 155/500\n",
      "41/41 [==============================] - 0s 484us/step - loss: 0.6799 - accuracy: 0.7253\n",
      "Epoch 156/500\n",
      "41/41 [==============================] - 0s 505us/step - loss: 0.6624 - accuracy: 0.7222\n",
      "Epoch 157/500\n",
      "41/41 [==============================] - 0s 533us/step - loss: 0.6628 - accuracy: 0.7346\n",
      "Epoch 158/500\n",
      "41/41 [==============================] - 0s 495us/step - loss: 0.6924 - accuracy: 0.7099\n",
      "Epoch 159/500\n",
      "41/41 [==============================] - 0s 484us/step - loss: 0.6787 - accuracy: 0.7099\n",
      "Epoch 160/500\n",
      "41/41 [==============================] - 0s 465us/step - loss: 0.6996 - accuracy: 0.7160\n",
      "Epoch 161/500\n",
      "41/41 [==============================] - 0s 474us/step - loss: 0.6799 - accuracy: 0.7253\n",
      "Epoch 162/500\n",
      "41/41 [==============================] - 0s 481us/step - loss: 0.6542 - accuracy: 0.7407\n",
      "Epoch 163/500\n",
      "41/41 [==============================] - 0s 502us/step - loss: 0.6851 - accuracy: 0.7099\n",
      "Epoch 164/500\n",
      "41/41 [==============================] - 0s 508us/step - loss: 0.6819 - accuracy: 0.7191\n",
      "Epoch 165/500\n",
      "41/41 [==============================] - 0s 515us/step - loss: 0.6550 - accuracy: 0.7623\n",
      "Epoch 166/500\n",
      "41/41 [==============================] - 0s 511us/step - loss: 0.6696 - accuracy: 0.7315\n",
      "Epoch 167/500\n",
      "41/41 [==============================] - 0s 518us/step - loss: 0.6526 - accuracy: 0.7253\n",
      "Epoch 168/500\n",
      "41/41 [==============================] - 0s 493us/step - loss: 0.6898 - accuracy: 0.7160\n",
      "Epoch 169/500\n",
      "41/41 [==============================] - 0s 491us/step - loss: 0.6981 - accuracy: 0.7284\n",
      "Epoch 170/500\n",
      "41/41 [==============================] - 0s 469us/step - loss: 0.6431 - accuracy: 0.7284\n",
      "Epoch 171/500\n",
      "41/41 [==============================] - 0s 501us/step - loss: 0.6669 - accuracy: 0.7346\n",
      "Epoch 172/500\n",
      "41/41 [==============================] - 0s 439us/step - loss: 0.6958 - accuracy: 0.6975\n",
      "Epoch 173/500\n",
      "41/41 [==============================] - 0s 455us/step - loss: 0.6419 - accuracy: 0.7531\n",
      "Epoch 174/500\n",
      "41/41 [==============================] - 0s 497us/step - loss: 0.6779 - accuracy: 0.7130\n",
      "Epoch 175/500\n",
      "41/41 [==============================] - 0s 453us/step - loss: 0.6624 - accuracy: 0.7284\n",
      "Epoch 176/500\n",
      "41/41 [==============================] - 0s 473us/step - loss: 0.6617 - accuracy: 0.7407\n",
      "Epoch 177/500\n",
      "41/41 [==============================] - 0s 480us/step - loss: 0.6661 - accuracy: 0.7407\n",
      "Epoch 178/500\n",
      "41/41 [==============================] - 0s 493us/step - loss: 0.6573 - accuracy: 0.7438\n",
      "Epoch 179/500\n",
      "41/41 [==============================] - 0s 485us/step - loss: 0.6644 - accuracy: 0.7130\n",
      "Epoch 180/500\n",
      "41/41 [==============================] - 0s 486us/step - loss: 0.6770 - accuracy: 0.7037\n",
      "Epoch 181/500\n",
      "41/41 [==============================] - 0s 504us/step - loss: 0.6840 - accuracy: 0.6975\n",
      "Epoch 182/500\n",
      "41/41 [==============================] - 0s 469us/step - loss: 0.6665 - accuracy: 0.7531\n",
      "Epoch 183/500\n",
      "41/41 [==============================] - 0s 534us/step - loss: 0.6655 - accuracy: 0.7130\n",
      "Epoch 184/500\n",
      "41/41 [==============================] - 0s 517us/step - loss: 0.6668 - accuracy: 0.7346\n",
      "Epoch 185/500\n",
      "41/41 [==============================] - 0s 495us/step - loss: 0.6538 - accuracy: 0.7407\n",
      "Epoch 186/500\n",
      "41/41 [==============================] - 0s 537us/step - loss: 0.6524 - accuracy: 0.7562\n",
      "Epoch 187/500\n",
      "41/41 [==============================] - 0s 506us/step - loss: 0.6338 - accuracy: 0.7469\n",
      "Epoch 188/500\n",
      "41/41 [==============================] - 0s 553us/step - loss: 0.6427 - accuracy: 0.7469\n",
      "Epoch 189/500\n",
      "41/41 [==============================] - 0s 526us/step - loss: 0.6695 - accuracy: 0.7130\n",
      "Epoch 190/500\n",
      "41/41 [==============================] - 0s 513us/step - loss: 0.6499 - accuracy: 0.7346\n",
      "Epoch 191/500\n",
      "41/41 [==============================] - 0s 507us/step - loss: 0.6442 - accuracy: 0.7438\n",
      "Epoch 192/500\n",
      "41/41 [==============================] - 0s 514us/step - loss: 0.6632 - accuracy: 0.7407\n",
      "Epoch 193/500\n",
      "41/41 [==============================] - 0s 495us/step - loss: 0.6327 - accuracy: 0.7284\n",
      "Epoch 194/500\n",
      "41/41 [==============================] - 0s 507us/step - loss: 0.6428 - accuracy: 0.7377\n",
      "Epoch 195/500\n",
      "41/41 [==============================] - 0s 497us/step - loss: 0.6383 - accuracy: 0.7531\n",
      "Epoch 196/500\n",
      "41/41 [==============================] - 0s 503us/step - loss: 0.6383 - accuracy: 0.7469\n",
      "Epoch 197/500\n",
      "41/41 [==============================] - 0s 522us/step - loss: 0.6803 - accuracy: 0.7222\n",
      "Epoch 198/500\n",
      "41/41 [==============================] - 0s 486us/step - loss: 0.6359 - accuracy: 0.7500\n",
      "Epoch 199/500\n",
      "41/41 [==============================] - 0s 515us/step - loss: 0.6554 - accuracy: 0.7253\n",
      "Epoch 200/500\n",
      "41/41 [==============================] - 0s 523us/step - loss: 0.6667 - accuracy: 0.7222\n",
      "Epoch 201/500\n",
      "41/41 [==============================] - 0s 532us/step - loss: 0.6375 - accuracy: 0.7562\n",
      "Epoch 202/500\n",
      "41/41 [==============================] - 0s 537us/step - loss: 0.6605 - accuracy: 0.7407\n",
      "Epoch 203/500\n",
      "41/41 [==============================] - 0s 547us/step - loss: 0.6571 - accuracy: 0.7407\n",
      "Epoch 204/500\n",
      "41/41 [==============================] - 0s 513us/step - loss: 0.6165 - accuracy: 0.7531\n",
      "Epoch 205/500\n",
      "41/41 [==============================] - 0s 520us/step - loss: 0.6630 - accuracy: 0.7346\n",
      "Epoch 206/500\n",
      "41/41 [==============================] - 0s 462us/step - loss: 0.6350 - accuracy: 0.7407\n",
      "Epoch 207/500\n",
      "41/41 [==============================] - 0s 529us/step - loss: 0.6202 - accuracy: 0.7438\n",
      "Epoch 208/500\n",
      "41/41 [==============================] - 0s 487us/step - loss: 0.6444 - accuracy: 0.7531\n",
      "Epoch 209/500\n",
      "41/41 [==============================] - 0s 527us/step - loss: 0.6370 - accuracy: 0.7377\n",
      "Epoch 210/500\n",
      "41/41 [==============================] - 0s 495us/step - loss: 0.6248 - accuracy: 0.7407\n",
      "Epoch 211/500\n",
      "41/41 [==============================] - 0s 523us/step - loss: 0.6543 - accuracy: 0.7438\n",
      "Epoch 212/500\n",
      "41/41 [==============================] - 0s 532us/step - loss: 0.6317 - accuracy: 0.7500\n",
      "Epoch 213/500\n",
      "41/41 [==============================] - 0s 510us/step - loss: 0.6267 - accuracy: 0.7346\n",
      "Epoch 214/500\n",
      "41/41 [==============================] - 0s 543us/step - loss: 0.6120 - accuracy: 0.7623\n",
      "Epoch 215/500\n",
      "41/41 [==============================] - 0s 467us/step - loss: 0.6060 - accuracy: 0.7623\n",
      "Epoch 216/500\n",
      "41/41 [==============================] - 0s 482us/step - loss: 0.6311 - accuracy: 0.7654\n",
      "Epoch 217/500\n",
      "41/41 [==============================] - 0s 489us/step - loss: 0.6156 - accuracy: 0.7407\n",
      "Epoch 218/500\n",
      "41/41 [==============================] - 0s 523us/step - loss: 0.6554 - accuracy: 0.7531\n",
      "Epoch 219/500\n",
      "41/41 [==============================] - 0s 516us/step - loss: 0.6222 - accuracy: 0.7469\n",
      "Epoch 220/500\n",
      "41/41 [==============================] - 0s 483us/step - loss: 0.6199 - accuracy: 0.7500\n",
      "Epoch 221/500\n",
      "41/41 [==============================] - 0s 545us/step - loss: 0.5759 - accuracy: 0.7654\n",
      "Epoch 222/500\n",
      "41/41 [==============================] - 0s 500us/step - loss: 0.6452 - accuracy: 0.7160\n",
      "Epoch 223/500\n",
      "41/41 [==============================] - 0s 535us/step - loss: 0.6372 - accuracy: 0.7284\n",
      "Epoch 224/500\n",
      "41/41 [==============================] - 0s 486us/step - loss: 0.6218 - accuracy: 0.7531\n",
      "Epoch 225/500\n",
      "41/41 [==============================] - 0s 577us/step - loss: 0.6515 - accuracy: 0.7346\n",
      "Epoch 226/500\n",
      "41/41 [==============================] - 0s 569us/step - loss: 0.6152 - accuracy: 0.7469\n",
      "Epoch 227/500\n",
      "41/41 [==============================] - 0s 522us/step - loss: 0.5893 - accuracy: 0.7809\n",
      "Epoch 228/500\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.3867 - accuracy: 0.8750"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-695b788237c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mX_cross_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cross_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cross_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cross_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cross_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpredictions_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "BATCH_SIZE = 8\n",
    "num_folds = 10\n",
    "\n",
    "\n",
    "kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "cv_results = np.array([])\n",
    "for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "    X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "    X_cross_train = scaler.fit_transform(X_cross_train)\n",
    "    X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "    X_cross_test = scaler.transform(X_cross_test)\n",
    "    model = getNetwork()\n",
    "    model.fit(X_cross_train, y_cross_train, epochs=EPOCHS, batch_size=BATCH_SIZE)  \n",
    "    y_pred = model.predict(X_cross_test)\n",
    "    predictions_categorical = np.argmax(y_pred, axis=1)\n",
    "    f1s = f1_score(y_cross_test, predictions_categorical, average=\"weighted\")\n",
    "    cv_results = np.append(cv_results, [f1s])\n",
    "\n",
    "print(f'Average score of Cross Validation: {cv_results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 3,493\n",
      "Trainable params: 3,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 1.8496 - accuracy: 0.2356 - val_loss: 1.6064 - val_accuracy: 0.2933\n",
      "Epoch 2/500\n",
      "57/57 [==============================] - 0s 714us/step - loss: 1.7696 - accuracy: 0.2311 - val_loss: 1.5777 - val_accuracy: 0.3267\n",
      "Epoch 3/500\n",
      "57/57 [==============================] - 0s 782us/step - loss: 1.7072 - accuracy: 0.2444 - val_loss: 1.5591 - val_accuracy: 0.3400\n",
      "Epoch 4/500\n",
      "57/57 [==============================] - 0s 724us/step - loss: 1.6191 - accuracy: 0.3156 - val_loss: 1.5454 - val_accuracy: 0.3333\n",
      "Epoch 5/500\n",
      "57/57 [==============================] - 0s 712us/step - loss: 1.5879 - accuracy: 0.2844 - val_loss: 1.5332 - val_accuracy: 0.3467\n",
      "Epoch 6/500\n",
      "57/57 [==============================] - 0s 717us/step - loss: 1.5801 - accuracy: 0.3044 - val_loss: 1.5251 - val_accuracy: 0.3733\n",
      "Epoch 7/500\n",
      "57/57 [==============================] - 0s 701us/step - loss: 1.5363 - accuracy: 0.2933 - val_loss: 1.5188 - val_accuracy: 0.3733\n",
      "Epoch 8/500\n",
      "57/57 [==============================] - 0s 696us/step - loss: 1.5672 - accuracy: 0.3178 - val_loss: 1.5159 - val_accuracy: 0.3733\n",
      "Epoch 9/500\n",
      "57/57 [==============================] - 0s 692us/step - loss: 1.5103 - accuracy: 0.3178 - val_loss: 1.5102 - val_accuracy: 0.3667\n",
      "Epoch 10/500\n",
      "57/57 [==============================] - 0s 684us/step - loss: 1.5394 - accuracy: 0.2867 - val_loss: 1.5061 - val_accuracy: 0.3867\n",
      "Epoch 11/500\n",
      "57/57 [==============================] - 0s 689us/step - loss: 1.5015 - accuracy: 0.3444 - val_loss: 1.5052 - val_accuracy: 0.3867\n",
      "Epoch 12/500\n",
      "57/57 [==============================] - 0s 704us/step - loss: 1.5017 - accuracy: 0.3378 - val_loss: 1.5103 - val_accuracy: 0.3600\n",
      "Epoch 13/500\n",
      "57/57 [==============================] - 0s 718us/step - loss: 1.4785 - accuracy: 0.3311 - val_loss: 1.5051 - val_accuracy: 0.3600\n",
      "Epoch 14/500\n",
      "57/57 [==============================] - 0s 672us/step - loss: 1.4818 - accuracy: 0.3356 - val_loss: 1.5039 - val_accuracy: 0.3600\n",
      "Epoch 15/500\n",
      "57/57 [==============================] - 0s 741us/step - loss: 1.4968 - accuracy: 0.3244 - val_loss: 1.5017 - val_accuracy: 0.3600\n",
      "Epoch 16/500\n",
      "57/57 [==============================] - 0s 709us/step - loss: 1.4628 - accuracy: 0.3489 - val_loss: 1.4957 - val_accuracy: 0.3733\n",
      "Epoch 17/500\n",
      "57/57 [==============================] - 0s 702us/step - loss: 1.4686 - accuracy: 0.3467 - val_loss: 1.4951 - val_accuracy: 0.3800\n",
      "Epoch 18/500\n",
      "57/57 [==============================] - 0s 717us/step - loss: 1.4522 - accuracy: 0.3644 - val_loss: 1.4966 - val_accuracy: 0.3867\n",
      "Epoch 19/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 1.4506 - accuracy: 0.4000 - val_loss: 1.4915 - val_accuracy: 0.4000\n",
      "Epoch 20/500\n",
      "57/57 [==============================] - 0s 723us/step - loss: 1.4378 - accuracy: 0.3844 - val_loss: 1.4940 - val_accuracy: 0.4000\n",
      "Epoch 21/500\n",
      "57/57 [==============================] - 0s 712us/step - loss: 1.4199 - accuracy: 0.3956 - val_loss: 1.4906 - val_accuracy: 0.4000\n",
      "Epoch 22/500\n",
      "57/57 [==============================] - 0s 753us/step - loss: 1.4340 - accuracy: 0.3556 - val_loss: 1.4850 - val_accuracy: 0.4133\n",
      "Epoch 23/500\n",
      "57/57 [==============================] - 0s 684us/step - loss: 1.4106 - accuracy: 0.3756 - val_loss: 1.4822 - val_accuracy: 0.4267\n",
      "Epoch 24/500\n",
      "57/57 [==============================] - 0s 742us/step - loss: 1.3968 - accuracy: 0.4089 - val_loss: 1.4786 - val_accuracy: 0.4200\n",
      "Epoch 25/500\n",
      "57/57 [==============================] - 0s 681us/step - loss: 1.4297 - accuracy: 0.3667 - val_loss: 1.4771 - val_accuracy: 0.4267\n",
      "Epoch 26/500\n",
      "57/57 [==============================] - 0s 692us/step - loss: 1.4264 - accuracy: 0.4022 - val_loss: 1.4764 - val_accuracy: 0.4400\n",
      "Epoch 27/500\n",
      "57/57 [==============================] - 0s 683us/step - loss: 1.3879 - accuracy: 0.4178 - val_loss: 1.4733 - val_accuracy: 0.4400\n",
      "Epoch 28/500\n",
      "57/57 [==============================] - 0s 761us/step - loss: 1.4013 - accuracy: 0.4178 - val_loss: 1.4719 - val_accuracy: 0.4533\n",
      "Epoch 29/500\n",
      "57/57 [==============================] - 0s 739us/step - loss: 1.3932 - accuracy: 0.4156 - val_loss: 1.4727 - val_accuracy: 0.4467\n",
      "Epoch 30/500\n",
      "57/57 [==============================] - 0s 819us/step - loss: 1.3918 - accuracy: 0.3889 - val_loss: 1.4701 - val_accuracy: 0.4400\n",
      "Epoch 31/500\n",
      "57/57 [==============================] - 0s 687us/step - loss: 1.3810 - accuracy: 0.4200 - val_loss: 1.4682 - val_accuracy: 0.4400\n",
      "Epoch 32/500\n",
      "57/57 [==============================] - 0s 714us/step - loss: 1.3954 - accuracy: 0.4178 - val_loss: 1.4654 - val_accuracy: 0.4533\n",
      "Epoch 33/500\n",
      "57/57 [==============================] - 0s 692us/step - loss: 1.3706 - accuracy: 0.4311 - val_loss: 1.4642 - val_accuracy: 0.4600\n",
      "Epoch 34/500\n",
      "57/57 [==============================] - 0s 679us/step - loss: 1.3932 - accuracy: 0.4067 - val_loss: 1.4633 - val_accuracy: 0.4600\n",
      "Epoch 35/500\n",
      "57/57 [==============================] - 0s 688us/step - loss: 1.3759 - accuracy: 0.4156 - val_loss: 1.4639 - val_accuracy: 0.4667\n",
      "Epoch 36/500\n",
      "57/57 [==============================] - 0s 667us/step - loss: 1.3686 - accuracy: 0.4156 - val_loss: 1.4598 - val_accuracy: 0.4600\n",
      "Epoch 37/500\n",
      "57/57 [==============================] - 0s 703us/step - loss: 1.3623 - accuracy: 0.4422 - val_loss: 1.4577 - val_accuracy: 0.4600\n",
      "Epoch 38/500\n",
      "57/57 [==============================] - 0s 695us/step - loss: 1.3348 - accuracy: 0.4511 - val_loss: 1.4587 - val_accuracy: 0.4467\n",
      "Epoch 39/500\n",
      "57/57 [==============================] - 0s 719us/step - loss: 1.3630 - accuracy: 0.4356 - val_loss: 1.4558 - val_accuracy: 0.4467\n",
      "Epoch 40/500\n",
      "57/57 [==============================] - 0s 695us/step - loss: 1.3507 - accuracy: 0.4556 - val_loss: 1.4552 - val_accuracy: 0.4600\n",
      "Epoch 41/500\n",
      "57/57 [==============================] - 0s 708us/step - loss: 1.3542 - accuracy: 0.4622 - val_loss: 1.4517 - val_accuracy: 0.4667\n",
      "Epoch 42/500\n",
      "57/57 [==============================] - 0s 708us/step - loss: 1.3679 - accuracy: 0.4000 - val_loss: 1.4516 - val_accuracy: 0.4600\n",
      "Epoch 43/500\n",
      "57/57 [==============================] - 0s 684us/step - loss: 1.3419 - accuracy: 0.4600 - val_loss: 1.4531 - val_accuracy: 0.4667\n",
      "Epoch 44/500\n",
      "57/57 [==============================] - 0s 690us/step - loss: 1.3507 - accuracy: 0.4267 - val_loss: 1.4514 - val_accuracy: 0.4733\n",
      "Epoch 45/500\n",
      "57/57 [==============================] - 0s 690us/step - loss: 1.3073 - accuracy: 0.4600 - val_loss: 1.4484 - val_accuracy: 0.4800\n",
      "Epoch 46/500\n",
      "57/57 [==============================] - 0s 708us/step - loss: 1.3389 - accuracy: 0.4422 - val_loss: 1.4423 - val_accuracy: 0.4800\n",
      "Epoch 47/500\n",
      "57/57 [==============================] - 0s 724us/step - loss: 1.3371 - accuracy: 0.4667 - val_loss: 1.4444 - val_accuracy: 0.4800\n",
      "Epoch 48/500\n",
      "57/57 [==============================] - 0s 677us/step - loss: 1.3169 - accuracy: 0.4667 - val_loss: 1.4446 - val_accuracy: 0.4867\n",
      "Epoch 49/500\n",
      "57/57 [==============================] - 0s 683us/step - loss: 1.3154 - accuracy: 0.4844 - val_loss: 1.4439 - val_accuracy: 0.4800\n",
      "Epoch 50/500\n",
      "57/57 [==============================] - 0s 734us/step - loss: 1.3280 - accuracy: 0.4622 - val_loss: 1.4466 - val_accuracy: 0.4933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "57/57 [==============================] - 0s 710us/step - loss: 1.3363 - accuracy: 0.4533 - val_loss: 1.4418 - val_accuracy: 0.4867\n",
      "Epoch 52/500\n",
      "57/57 [==============================] - 0s 721us/step - loss: 1.3326 - accuracy: 0.4622 - val_loss: 1.4349 - val_accuracy: 0.4800\n",
      "Epoch 53/500\n",
      "57/57 [==============================] - 0s 677us/step - loss: 1.3139 - accuracy: 0.4889 - val_loss: 1.4309 - val_accuracy: 0.4867\n",
      "Epoch 54/500\n",
      "57/57 [==============================] - 0s 676us/step - loss: 1.3258 - accuracy: 0.4600 - val_loss: 1.4316 - val_accuracy: 0.5000\n",
      "Epoch 55/500\n",
      "57/57 [==============================] - 0s 725us/step - loss: 1.3254 - accuracy: 0.4556 - val_loss: 1.4322 - val_accuracy: 0.4867\n",
      "Epoch 56/500\n",
      "57/57 [==============================] - 0s 716us/step - loss: 1.3174 - accuracy: 0.4711 - val_loss: 1.4324 - val_accuracy: 0.4933\n",
      "Epoch 57/500\n",
      "57/57 [==============================] - 0s 704us/step - loss: 1.3050 - accuracy: 0.4978 - val_loss: 1.4295 - val_accuracy: 0.4933\n",
      "Epoch 58/500\n",
      "57/57 [==============================] - 0s 723us/step - loss: 1.3006 - accuracy: 0.4778 - val_loss: 1.4229 - val_accuracy: 0.4867\n",
      "Epoch 59/500\n",
      "57/57 [==============================] - 0s 681us/step - loss: 1.2929 - accuracy: 0.5133 - val_loss: 1.4194 - val_accuracy: 0.4867\n",
      "Epoch 60/500\n",
      "57/57 [==============================] - 0s 684us/step - loss: 1.2975 - accuracy: 0.5133 - val_loss: 1.4164 - val_accuracy: 0.4867\n",
      "Epoch 61/500\n",
      "57/57 [==============================] - 0s 711us/step - loss: 1.2758 - accuracy: 0.5022 - val_loss: 1.4129 - val_accuracy: 0.4800\n",
      "Epoch 62/500\n",
      "57/57 [==============================] - 0s 685us/step - loss: 1.2771 - accuracy: 0.5178 - val_loss: 1.4120 - val_accuracy: 0.5000\n",
      "Epoch 63/500\n",
      "57/57 [==============================] - 0s 692us/step - loss: 1.2833 - accuracy: 0.5133 - val_loss: 1.4108 - val_accuracy: 0.4933\n",
      "Epoch 64/500\n",
      "57/57 [==============================] - 0s 713us/step - loss: 1.3139 - accuracy: 0.4778 - val_loss: 1.4092 - val_accuracy: 0.4933\n",
      "Epoch 65/500\n",
      "57/57 [==============================] - 0s 710us/step - loss: 1.2952 - accuracy: 0.4822 - val_loss: 1.4121 - val_accuracy: 0.4867\n",
      "Epoch 66/500\n",
      "57/57 [==============================] - 0s 698us/step - loss: 1.2495 - accuracy: 0.4911 - val_loss: 1.4093 - val_accuracy: 0.4867\n",
      "Epoch 67/500\n",
      "57/57 [==============================] - 0s 687us/step - loss: 1.2835 - accuracy: 0.4889 - val_loss: 1.4077 - val_accuracy: 0.4933\n",
      "Epoch 68/500\n",
      "57/57 [==============================] - 0s 665us/step - loss: 1.2510 - accuracy: 0.5267 - val_loss: 1.4093 - val_accuracy: 0.4933\n",
      "Epoch 69/500\n",
      "57/57 [==============================] - 0s 716us/step - loss: 1.2614 - accuracy: 0.4889 - val_loss: 1.4057 - val_accuracy: 0.4933\n",
      "Epoch 70/500\n",
      "57/57 [==============================] - 0s 692us/step - loss: 1.2520 - accuracy: 0.5200 - val_loss: 1.4039 - val_accuracy: 0.4867\n",
      "Epoch 71/500\n",
      "57/57 [==============================] - 0s 702us/step - loss: 1.2689 - accuracy: 0.5200 - val_loss: 1.4029 - val_accuracy: 0.4867\n",
      "Epoch 72/500\n",
      "57/57 [==============================] - 0s 711us/step - loss: 1.2943 - accuracy: 0.4844 - val_loss: 1.4012 - val_accuracy: 0.4867\n",
      "Epoch 73/500\n",
      "57/57 [==============================] - 0s 703us/step - loss: 1.2501 - accuracy: 0.5133 - val_loss: 1.3996 - val_accuracy: 0.4867\n",
      "Epoch 74/500\n",
      "57/57 [==============================] - 0s 653us/step - loss: 1.2679 - accuracy: 0.4867 - val_loss: 1.4014 - val_accuracy: 0.4867\n",
      "Epoch 75/500\n",
      "57/57 [==============================] - 0s 716us/step - loss: 1.2714 - accuracy: 0.4956 - val_loss: 1.4000 - val_accuracy: 0.4867\n",
      "Epoch 76/500\n",
      "57/57 [==============================] - 0s 717us/step - loss: 1.2488 - accuracy: 0.5156 - val_loss: 1.3938 - val_accuracy: 0.4933\n",
      "Epoch 77/500\n",
      "57/57 [==============================] - 0s 711us/step - loss: 1.2674 - accuracy: 0.5178 - val_loss: 1.3946 - val_accuracy: 0.4867\n",
      "Epoch 78/500\n",
      "57/57 [==============================] - 0s 674us/step - loss: 1.2214 - accuracy: 0.5489 - val_loss: 1.3914 - val_accuracy: 0.4933\n",
      "Epoch 79/500\n",
      "57/57 [==============================] - 0s 696us/step - loss: 1.2711 - accuracy: 0.5156 - val_loss: 1.3886 - val_accuracy: 0.4867\n",
      "Epoch 80/500\n",
      "57/57 [==============================] - 0s 722us/step - loss: 1.2478 - accuracy: 0.5289 - val_loss: 1.3869 - val_accuracy: 0.4933\n",
      "Epoch 81/500\n",
      "57/57 [==============================] - 0s 706us/step - loss: 1.2502 - accuracy: 0.5200 - val_loss: 1.3850 - val_accuracy: 0.4867\n",
      "Epoch 82/500\n",
      "57/57 [==============================] - 0s 695us/step - loss: 1.2593 - accuracy: 0.5311 - val_loss: 1.3830 - val_accuracy: 0.4867\n",
      "Epoch 83/500\n",
      "57/57 [==============================] - 0s 704us/step - loss: 1.2601 - accuracy: 0.5089 - val_loss: 1.3828 - val_accuracy: 0.5000\n",
      "Epoch 84/500\n",
      "57/57 [==============================] - 0s 702us/step - loss: 1.2249 - accuracy: 0.5089 - val_loss: 1.3808 - val_accuracy: 0.4933\n",
      "Epoch 85/500\n",
      "57/57 [==============================] - 0s 697us/step - loss: 1.2677 - accuracy: 0.4978 - val_loss: 1.3797 - val_accuracy: 0.4933\n",
      "Epoch 86/500\n",
      "57/57 [==============================] - 0s 713us/step - loss: 1.2220 - accuracy: 0.5067 - val_loss: 1.3802 - val_accuracy: 0.5000\n",
      "Epoch 87/500\n",
      "57/57 [==============================] - 0s 701us/step - loss: 1.2286 - accuracy: 0.5289 - val_loss: 1.3798 - val_accuracy: 0.5000\n",
      "Epoch 88/500\n",
      "57/57 [==============================] - 0s 706us/step - loss: 1.2543 - accuracy: 0.5222 - val_loss: 1.3775 - val_accuracy: 0.5000\n",
      "Epoch 89/500\n",
      "57/57 [==============================] - 0s 695us/step - loss: 1.2343 - accuracy: 0.5133 - val_loss: 1.3771 - val_accuracy: 0.5000\n",
      "Epoch 90/500\n",
      "57/57 [==============================] - 0s 659us/step - loss: 1.2242 - accuracy: 0.5578 - val_loss: 1.3768 - val_accuracy: 0.4933\n",
      "Epoch 91/500\n",
      "57/57 [==============================] - 0s 714us/step - loss: 1.2554 - accuracy: 0.4956 - val_loss: 1.3754 - val_accuracy: 0.5000\n",
      "Epoch 92/500\n",
      "57/57 [==============================] - 0s 697us/step - loss: 1.2368 - accuracy: 0.5378 - val_loss: 1.3709 - val_accuracy: 0.4933\n",
      "Epoch 93/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 1.2176 - accuracy: 0.5067 - val_loss: 1.3678 - val_accuracy: 0.5000\n",
      "Epoch 94/500\n",
      "57/57 [==============================] - 0s 739us/step - loss: 1.2106 - accuracy: 0.5133 - val_loss: 1.3660 - val_accuracy: 0.5067\n",
      "Epoch 95/500\n",
      "57/57 [==============================] - 0s 725us/step - loss: 1.2279 - accuracy: 0.5533 - val_loss: 1.3668 - val_accuracy: 0.5067\n",
      "Epoch 96/500\n",
      "57/57 [==============================] - 0s 673us/step - loss: 1.2221 - accuracy: 0.5000 - val_loss: 1.3666 - val_accuracy: 0.5000\n",
      "Epoch 97/500\n",
      "57/57 [==============================] - 0s 691us/step - loss: 1.2272 - accuracy: 0.5267 - val_loss: 1.3646 - val_accuracy: 0.5000\n",
      "Epoch 98/500\n",
      "57/57 [==============================] - 0s 682us/step - loss: 1.2158 - accuracy: 0.5378 - val_loss: 1.3657 - val_accuracy: 0.5067\n",
      "Epoch 99/500\n",
      "57/57 [==============================] - 0s 700us/step - loss: 1.2314 - accuracy: 0.5244 - val_loss: 1.3655 - val_accuracy: 0.5133\n",
      "Epoch 100/500\n",
      "57/57 [==============================] - 0s 697us/step - loss: 1.2231 - accuracy: 0.5267 - val_loss: 1.3649 - val_accuracy: 0.5067\n",
      "Epoch 101/500\n",
      "57/57 [==============================] - 0s 703us/step - loss: 1.2204 - accuracy: 0.5244 - val_loss: 1.3617 - val_accuracy: 0.5133\n",
      "Epoch 102/500\n",
      "57/57 [==============================] - 0s 660us/step - loss: 1.2058 - accuracy: 0.5200 - val_loss: 1.3640 - val_accuracy: 0.5133\n",
      "Epoch 103/500\n",
      "57/57 [==============================] - 0s 699us/step - loss: 1.1980 - accuracy: 0.5533 - val_loss: 1.3597 - val_accuracy: 0.5133\n",
      "Epoch 104/500\n",
      "57/57 [==============================] - 0s 688us/step - loss: 1.2328 - accuracy: 0.5400 - val_loss: 1.3608 - val_accuracy: 0.5133\n",
      "Epoch 105/500\n",
      "57/57 [==============================] - 0s 711us/step - loss: 1.1813 - accuracy: 0.5267 - val_loss: 1.3578 - val_accuracy: 0.5200\n",
      "Epoch 106/500\n",
      "57/57 [==============================] - 0s 706us/step - loss: 1.1839 - accuracy: 0.5689 - val_loss: 1.3558 - val_accuracy: 0.5200\n",
      "Epoch 107/500\n",
      "57/57 [==============================] - 0s 702us/step - loss: 1.1743 - accuracy: 0.5622 - val_loss: 1.3519 - val_accuracy: 0.5267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "57/57 [==============================] - 0s 707us/step - loss: 1.2047 - accuracy: 0.5267 - val_loss: 1.3514 - val_accuracy: 0.5267\n",
      "Epoch 109/500\n",
      "57/57 [==============================] - 0s 703us/step - loss: 1.2177 - accuracy: 0.5311 - val_loss: 1.3484 - val_accuracy: 0.5267\n",
      "Epoch 110/500\n",
      "57/57 [==============================] - 0s 669us/step - loss: 1.1844 - accuracy: 0.5467 - val_loss: 1.3459 - val_accuracy: 0.5267\n",
      "Epoch 111/500\n",
      "57/57 [==============================] - 0s 712us/step - loss: 1.1788 - accuracy: 0.5667 - val_loss: 1.3448 - val_accuracy: 0.5200\n",
      "Epoch 112/500\n",
      "57/57 [==============================] - 0s 686us/step - loss: 1.1840 - accuracy: 0.5467 - val_loss: 1.3421 - val_accuracy: 0.5267\n",
      "Epoch 113/500\n",
      "57/57 [==============================] - 0s 695us/step - loss: 1.2020 - accuracy: 0.5533 - val_loss: 1.3455 - val_accuracy: 0.5267\n",
      "Epoch 114/500\n",
      "57/57 [==============================] - 0s 703us/step - loss: 1.2105 - accuracy: 0.5444 - val_loss: 1.3459 - val_accuracy: 0.5267\n",
      "Epoch 115/500\n",
      "57/57 [==============================] - 0s 706us/step - loss: 1.1946 - accuracy: 0.5444 - val_loss: 1.3470 - val_accuracy: 0.5267\n",
      "Epoch 116/500\n",
      "57/57 [==============================] - 0s 704us/step - loss: 1.1992 - accuracy: 0.5222 - val_loss: 1.3431 - val_accuracy: 0.5267\n",
      "Epoch 117/500\n",
      "57/57 [==============================] - 0s 687us/step - loss: 1.1908 - accuracy: 0.5467 - val_loss: 1.3428 - val_accuracy: 0.5267\n",
      "Epoch 118/500\n",
      "57/57 [==============================] - 0s 732us/step - loss: 1.1832 - accuracy: 0.5511 - val_loss: 1.3421 - val_accuracy: 0.5267\n",
      "Epoch 119/500\n",
      "57/57 [==============================] - 0s 683us/step - loss: 1.1847 - accuracy: 0.5556 - val_loss: 1.3381 - val_accuracy: 0.5333\n",
      "Epoch 120/500\n",
      "57/57 [==============================] - 0s 703us/step - loss: 1.2063 - accuracy: 0.5022 - val_loss: 1.3367 - val_accuracy: 0.5333\n",
      "Epoch 121/500\n",
      "57/57 [==============================] - 0s 697us/step - loss: 1.1364 - accuracy: 0.5556 - val_loss: 1.3348 - val_accuracy: 0.5333\n",
      "Epoch 122/500\n",
      "57/57 [==============================] - 0s 739us/step - loss: 1.1758 - accuracy: 0.5644 - val_loss: 1.3312 - val_accuracy: 0.5400\n",
      "Epoch 123/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 1.1605 - accuracy: 0.5444 - val_loss: 1.3311 - val_accuracy: 0.5333\n",
      "Epoch 124/500\n",
      "57/57 [==============================] - 0s 713us/step - loss: 1.1559 - accuracy: 0.5778 - val_loss: 1.3295 - val_accuracy: 0.5333\n",
      "Epoch 125/500\n",
      "57/57 [==============================] - 0s 694us/step - loss: 1.2163 - accuracy: 0.5489 - val_loss: 1.3272 - val_accuracy: 0.5333\n",
      "Epoch 126/500\n",
      "57/57 [==============================] - 0s 710us/step - loss: 1.1721 - accuracy: 0.5422 - val_loss: 1.3238 - val_accuracy: 0.5333\n",
      "Epoch 127/500\n",
      "57/57 [==============================] - 0s 675us/step - loss: 1.1524 - accuracy: 0.5467 - val_loss: 1.3250 - val_accuracy: 0.5333\n",
      "Epoch 128/500\n",
      "57/57 [==============================] - 0s 692us/step - loss: 1.2028 - accuracy: 0.5400 - val_loss: 1.3208 - val_accuracy: 0.5400\n",
      "Epoch 129/500\n",
      "57/57 [==============================] - 0s 713us/step - loss: 1.1966 - accuracy: 0.5289 - val_loss: 1.3201 - val_accuracy: 0.5400\n",
      "Epoch 130/500\n",
      "57/57 [==============================] - 0s 678us/step - loss: 1.1838 - accuracy: 0.5756 - val_loss: 1.3217 - val_accuracy: 0.5400\n",
      "Epoch 131/500\n",
      "57/57 [==============================] - 0s 683us/step - loss: 1.1292 - accuracy: 0.5711 - val_loss: 1.3196 - val_accuracy: 0.5400\n",
      "Epoch 132/500\n",
      "57/57 [==============================] - 0s 667us/step - loss: 1.1751 - accuracy: 0.5689 - val_loss: 1.3186 - val_accuracy: 0.5333\n",
      "Epoch 133/500\n",
      "57/57 [==============================] - 0s 669us/step - loss: 1.1558 - accuracy: 0.5533 - val_loss: 1.3182 - val_accuracy: 0.5333\n",
      "Epoch 134/500\n",
      "57/57 [==============================] - 0s 670us/step - loss: 1.1881 - accuracy: 0.5378 - val_loss: 1.3157 - val_accuracy: 0.5267\n",
      "Epoch 135/500\n",
      "57/57 [==============================] - 0s 691us/step - loss: 1.1814 - accuracy: 0.5800 - val_loss: 1.3151 - val_accuracy: 0.5333\n",
      "Epoch 136/500\n",
      "57/57 [==============================] - 0s 694us/step - loss: 1.1796 - accuracy: 0.5467 - val_loss: 1.3128 - val_accuracy: 0.5400\n",
      "Epoch 137/500\n",
      "57/57 [==============================] - 0s 702us/step - loss: 1.1611 - accuracy: 0.5644 - val_loss: 1.3128 - val_accuracy: 0.5333\n",
      "Epoch 138/500\n",
      "57/57 [==============================] - 0s 688us/step - loss: 1.1463 - accuracy: 0.5644 - val_loss: 1.3115 - val_accuracy: 0.5333\n",
      "Epoch 139/500\n",
      "57/57 [==============================] - 0s 695us/step - loss: 1.1236 - accuracy: 0.5556 - val_loss: 1.3107 - val_accuracy: 0.5467\n",
      "Epoch 140/500\n",
      "57/57 [==============================] - 0s 711us/step - loss: 1.1492 - accuracy: 0.5644 - val_loss: 1.3106 - val_accuracy: 0.5467\n",
      "Epoch 141/500\n",
      "57/57 [==============================] - 0s 707us/step - loss: 1.1585 - accuracy: 0.5578 - val_loss: 1.3089 - val_accuracy: 0.5400\n",
      "Epoch 142/500\n",
      "57/57 [==============================] - 0s 706us/step - loss: 1.1230 - accuracy: 0.5956 - val_loss: 1.3089 - val_accuracy: 0.5267\n",
      "Epoch 143/500\n",
      "57/57 [==============================] - 0s 737us/step - loss: 1.1545 - accuracy: 0.5533 - val_loss: 1.3073 - val_accuracy: 0.5333\n",
      "Epoch 144/500\n",
      "57/57 [==============================] - 0s 660us/step - loss: 1.1481 - accuracy: 0.5933 - val_loss: 1.3049 - val_accuracy: 0.5400\n",
      "Epoch 145/500\n",
      "57/57 [==============================] - 0s 696us/step - loss: 1.1528 - accuracy: 0.5467 - val_loss: 1.3052 - val_accuracy: 0.5400\n",
      "Epoch 146/500\n",
      "57/57 [==============================] - 0s 708us/step - loss: 1.1235 - accuracy: 0.5667 - val_loss: 1.3032 - val_accuracy: 0.5400\n",
      "Epoch 147/500\n",
      "57/57 [==============================] - 0s 695us/step - loss: 1.1635 - accuracy: 0.5578 - val_loss: 1.3036 - val_accuracy: 0.5533\n",
      "Epoch 148/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 1.1250 - accuracy: 0.5978 - val_loss: 1.3044 - val_accuracy: 0.5533\n",
      "Epoch 149/500\n",
      "57/57 [==============================] - 0s 710us/step - loss: 1.1260 - accuracy: 0.5889 - val_loss: 1.3041 - val_accuracy: 0.5533\n",
      "Epoch 150/500\n",
      "57/57 [==============================] - 0s 708us/step - loss: 1.1500 - accuracy: 0.5622 - val_loss: 1.3042 - val_accuracy: 0.5600\n",
      "Epoch 151/500\n",
      "57/57 [==============================] - 0s 711us/step - loss: 1.1501 - accuracy: 0.5711 - val_loss: 1.3032 - val_accuracy: 0.5533\n",
      "Epoch 152/500\n",
      "57/57 [==============================] - 0s 716us/step - loss: 1.1321 - accuracy: 0.5822 - val_loss: 1.2991 - val_accuracy: 0.5600\n",
      "Epoch 153/500\n",
      "57/57 [==============================] - 0s 708us/step - loss: 1.1158 - accuracy: 0.5711 - val_loss: 1.2995 - val_accuracy: 0.5667\n",
      "Epoch 154/500\n",
      "57/57 [==============================] - 0s 698us/step - loss: 1.1405 - accuracy: 0.5756 - val_loss: 1.2987 - val_accuracy: 0.5667\n",
      "Epoch 155/500\n",
      "57/57 [==============================] - 0s 714us/step - loss: 1.1426 - accuracy: 0.5756 - val_loss: 1.2979 - val_accuracy: 0.5600\n",
      "Epoch 156/500\n",
      "57/57 [==============================] - 0s 652us/step - loss: 1.1336 - accuracy: 0.5844 - val_loss: 1.2983 - val_accuracy: 0.5600\n",
      "Epoch 157/500\n",
      "57/57 [==============================] - 0s 710us/step - loss: 1.1308 - accuracy: 0.5911 - val_loss: 1.2992 - val_accuracy: 0.5533\n",
      "Epoch 158/500\n",
      "57/57 [==============================] - 0s 693us/step - loss: 1.1110 - accuracy: 0.5778 - val_loss: 1.2985 - val_accuracy: 0.5600\n",
      "Epoch 159/500\n",
      "57/57 [==============================] - 0s 702us/step - loss: 1.1221 - accuracy: 0.5689 - val_loss: 1.2976 - val_accuracy: 0.5667\n",
      "Epoch 160/500\n",
      "57/57 [==============================] - 0s 701us/step - loss: 1.1249 - accuracy: 0.5667 - val_loss: 1.2971 - val_accuracy: 0.5667\n",
      "Epoch 161/500\n",
      "57/57 [==============================] - 0s 704us/step - loss: 1.1320 - accuracy: 0.5822 - val_loss: 1.2961 - val_accuracy: 0.5600\n",
      "Epoch 162/500\n",
      "57/57 [==============================] - 0s 656us/step - loss: 1.1117 - accuracy: 0.5933 - val_loss: 1.2974 - val_accuracy: 0.5600\n",
      "Epoch 163/500\n",
      "57/57 [==============================] - 0s 682us/step - loss: 1.1277 - accuracy: 0.5733 - val_loss: 1.2952 - val_accuracy: 0.5600\n",
      "Epoch 164/500\n",
      "57/57 [==============================] - 0s 670us/step - loss: 1.1179 - accuracy: 0.5511 - val_loss: 1.2953 - val_accuracy: 0.5467\n",
      "Epoch 165/500\n",
      "57/57 [==============================] - 0s 712us/step - loss: 1.1383 - accuracy: 0.5733 - val_loss: 1.2946 - val_accuracy: 0.5533\n",
      "Epoch 166/500\n",
      "57/57 [==============================] - 0s 660us/step - loss: 1.1264 - accuracy: 0.5889 - val_loss: 1.2915 - val_accuracy: 0.5533\n",
      "Epoch 167/500\n",
      "57/57 [==============================] - 0s 726us/step - loss: 1.1053 - accuracy: 0.5956 - val_loss: 1.2903 - val_accuracy: 0.5600\n",
      "Epoch 168/500\n",
      "57/57 [==============================] - 0s 751us/step - loss: 1.1169 - accuracy: 0.5711 - val_loss: 1.2914 - val_accuracy: 0.5800\n",
      "Epoch 169/500\n",
      "57/57 [==============================] - 0s 699us/step - loss: 1.1119 - accuracy: 0.5733 - val_loss: 1.2906 - val_accuracy: 0.5667\n",
      "Epoch 170/500\n",
      "57/57 [==============================] - 0s 680us/step - loss: 1.1505 - accuracy: 0.5689 - val_loss: 1.2890 - val_accuracy: 0.5533\n",
      "Epoch 171/500\n",
      "57/57 [==============================] - 0s 691us/step - loss: 1.1138 - accuracy: 0.5778 - val_loss: 1.2863 - val_accuracy: 0.5467\n",
      "Epoch 172/500\n",
      "57/57 [==============================] - 0s 703us/step - loss: 1.1068 - accuracy: 0.5778 - val_loss: 1.2833 - val_accuracy: 0.5467\n",
      "Epoch 173/500\n",
      "57/57 [==============================] - 0s 679us/step - loss: 1.1240 - accuracy: 0.5800 - val_loss: 1.2833 - val_accuracy: 0.5600\n",
      "Epoch 174/500\n",
      "57/57 [==============================] - 0s 694us/step - loss: 1.1237 - accuracy: 0.5667 - val_loss: 1.2828 - val_accuracy: 0.5533\n",
      "Epoch 175/500\n",
      "57/57 [==============================] - 0s 666us/step - loss: 1.1048 - accuracy: 0.5956 - val_loss: 1.2831 - val_accuracy: 0.5333\n",
      "Epoch 176/500\n",
      "57/57 [==============================] - 0s 697us/step - loss: 1.0908 - accuracy: 0.5889 - val_loss: 1.2841 - val_accuracy: 0.5333\n",
      "Epoch 177/500\n",
      "57/57 [==============================] - 0s 725us/step - loss: 1.1279 - accuracy: 0.5600 - val_loss: 1.2827 - val_accuracy: 0.5267\n",
      "Epoch 178/500\n",
      "57/57 [==============================] - 0s 686us/step - loss: 1.0955 - accuracy: 0.5911 - val_loss: 1.2813 - val_accuracy: 0.5267\n",
      "Epoch 179/500\n",
      "57/57 [==============================] - 0s 665us/step - loss: 1.0836 - accuracy: 0.5911 - val_loss: 1.2779 - val_accuracy: 0.5267\n",
      "Epoch 180/500\n",
      "57/57 [==============================] - 0s 717us/step - loss: 1.0911 - accuracy: 0.5733 - val_loss: 1.2775 - val_accuracy: 0.5400\n",
      "Epoch 181/500\n",
      "57/57 [==============================] - 0s 824us/step - loss: 1.0742 - accuracy: 0.6222 - val_loss: 1.2774 - val_accuracy: 0.5267\n",
      "Epoch 182/500\n",
      "57/57 [==============================] - 0s 706us/step - loss: 1.1037 - accuracy: 0.5844 - val_loss: 1.2736 - val_accuracy: 0.5400\n",
      "Epoch 183/500\n",
      "57/57 [==============================] - 0s 715us/step - loss: 1.1096 - accuracy: 0.5600 - val_loss: 1.2742 - val_accuracy: 0.5333\n",
      "Epoch 184/500\n",
      "57/57 [==============================] - 0s 719us/step - loss: 1.0985 - accuracy: 0.6067 - val_loss: 1.2763 - val_accuracy: 0.5533\n",
      "Epoch 185/500\n",
      "57/57 [==============================] - 0s 704us/step - loss: 1.1028 - accuracy: 0.5756 - val_loss: 1.2728 - val_accuracy: 0.5533\n",
      "Epoch 186/500\n",
      "57/57 [==============================] - 0s 718us/step - loss: 1.0868 - accuracy: 0.5978 - val_loss: 1.2737 - val_accuracy: 0.5533\n",
      "Epoch 187/500\n",
      "57/57 [==============================] - 0s 700us/step - loss: 1.1160 - accuracy: 0.5756 - val_loss: 1.2719 - val_accuracy: 0.5400\n",
      "Epoch 188/500\n",
      "57/57 [==============================] - 0s 671us/step - loss: 1.1190 - accuracy: 0.5778 - val_loss: 1.2724 - val_accuracy: 0.5333\n",
      "Epoch 189/500\n",
      "57/57 [==============================] - 0s 702us/step - loss: 1.1159 - accuracy: 0.5711 - val_loss: 1.2696 - val_accuracy: 0.5333\n",
      "Epoch 190/500\n",
      "57/57 [==============================] - 0s 710us/step - loss: 1.1068 - accuracy: 0.5889 - val_loss: 1.2693 - val_accuracy: 0.5333\n",
      "Epoch 191/500\n",
      "57/57 [==============================] - 0s 744us/step - loss: 1.1111 - accuracy: 0.5956 - val_loss: 1.2699 - val_accuracy: 0.5400\n",
      "Epoch 192/500\n",
      "57/57 [==============================] - 0s 715us/step - loss: 1.0566 - accuracy: 0.6200 - val_loss: 1.2713 - val_accuracy: 0.5467\n",
      "Epoch 193/500\n",
      "57/57 [==============================] - 0s 739us/step - loss: 1.1229 - accuracy: 0.5911 - val_loss: 1.2718 - val_accuracy: 0.5600\n",
      "Epoch 194/500\n",
      "57/57 [==============================] - 0s 683us/step - loss: 1.0743 - accuracy: 0.6089 - val_loss: 1.2712 - val_accuracy: 0.5600\n",
      "Epoch 195/500\n",
      "57/57 [==============================] - 0s 703us/step - loss: 1.0915 - accuracy: 0.6022 - val_loss: 1.2713 - val_accuracy: 0.5733\n",
      "Epoch 196/500\n",
      "57/57 [==============================] - 0s 678us/step - loss: 1.1130 - accuracy: 0.5622 - val_loss: 1.2706 - val_accuracy: 0.5667\n",
      "Epoch 197/500\n",
      "57/57 [==============================] - 0s 683us/step - loss: 1.0756 - accuracy: 0.5933 - val_loss: 1.2716 - val_accuracy: 0.5533\n",
      "Epoch 198/500\n",
      "57/57 [==============================] - 0s 687us/step - loss: 1.0646 - accuracy: 0.6111 - val_loss: 1.2723 - val_accuracy: 0.5533\n",
      "Epoch 199/500\n",
      "57/57 [==============================] - 0s 662us/step - loss: 1.0619 - accuracy: 0.6178 - val_loss: 1.2649 - val_accuracy: 0.5600\n",
      "Epoch 200/500\n",
      "57/57 [==============================] - 0s 703us/step - loss: 1.0692 - accuracy: 0.6156 - val_loss: 1.2659 - val_accuracy: 0.5600\n",
      "Epoch 201/500\n",
      "57/57 [==============================] - 0s 667us/step - loss: 1.0917 - accuracy: 0.5667 - val_loss: 1.2672 - val_accuracy: 0.5533\n",
      "Epoch 202/500\n",
      "57/57 [==============================] - 0s 726us/step - loss: 1.0833 - accuracy: 0.6222 - val_loss: 1.2688 - val_accuracy: 0.5733\n",
      "Epoch 203/500\n",
      "57/57 [==============================] - 0s 825us/step - loss: 1.0623 - accuracy: 0.5889 - val_loss: 1.2702 - val_accuracy: 0.5667\n",
      "Epoch 204/500\n",
      "57/57 [==============================] - 0s 671us/step - loss: 1.0922 - accuracy: 0.5978 - val_loss: 1.2693 - val_accuracy: 0.5533\n",
      "Epoch 205/500\n",
      "57/57 [==============================] - 0s 731us/step - loss: 1.1001 - accuracy: 0.5978 - val_loss: 1.2675 - val_accuracy: 0.5467\n",
      "Epoch 206/500\n",
      "57/57 [==============================] - 0s 696us/step - loss: 1.0810 - accuracy: 0.6111 - val_loss: 1.2712 - val_accuracy: 0.5533\n",
      "Epoch 207/500\n",
      "57/57 [==============================] - 0s 698us/step - loss: 1.0438 - accuracy: 0.6044 - val_loss: 1.2714 - val_accuracy: 0.5533\n",
      "Epoch 208/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 1.0629 - accuracy: 0.6111 - val_loss: 1.2692 - val_accuracy: 0.5600\n",
      "Epoch 209/500\n",
      "57/57 [==============================] - 0s 723us/step - loss: 1.0646 - accuracy: 0.6067 - val_loss: 1.2640 - val_accuracy: 0.5533\n",
      "Epoch 210/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 1.0782 - accuracy: 0.5867 - val_loss: 1.2638 - val_accuracy: 0.5533\n",
      "Epoch 211/500\n",
      "57/57 [==============================] - 0s 708us/step - loss: 1.0394 - accuracy: 0.6000 - val_loss: 1.2618 - val_accuracy: 0.5467\n",
      "Epoch 212/500\n",
      "57/57 [==============================] - 0s 709us/step - loss: 1.0747 - accuracy: 0.6022 - val_loss: 1.2612 - val_accuracy: 0.5467\n",
      "Epoch 213/500\n",
      "57/57 [==============================] - 0s 696us/step - loss: 1.0619 - accuracy: 0.6200 - val_loss: 1.2604 - val_accuracy: 0.5533\n",
      "Epoch 214/500\n",
      "57/57 [==============================] - 0s 700us/step - loss: 1.0561 - accuracy: 0.5867 - val_loss: 1.2568 - val_accuracy: 0.5533\n",
      "Epoch 215/500\n",
      "57/57 [==============================] - 0s 723us/step - loss: 1.0485 - accuracy: 0.6044 - val_loss: 1.2599 - val_accuracy: 0.5533\n",
      "Epoch 216/500\n",
      "57/57 [==============================] - 0s 717us/step - loss: 1.0973 - accuracy: 0.5933 - val_loss: 1.2603 - val_accuracy: 0.5400\n",
      "Epoch 217/500\n",
      "57/57 [==============================] - 0s 714us/step - loss: 1.0553 - accuracy: 0.5756 - val_loss: 1.2589 - val_accuracy: 0.5533\n",
      "Epoch 218/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 1.0590 - accuracy: 0.5956 - val_loss: 1.2564 - val_accuracy: 0.5533\n",
      "Epoch 219/500\n",
      "57/57 [==============================] - 0s 736us/step - loss: 1.0519 - accuracy: 0.6044 - val_loss: 1.2587 - val_accuracy: 0.5533\n",
      "Epoch 220/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 654us/step - loss: 1.0524 - accuracy: 0.5867 - val_loss: 1.2590 - val_accuracy: 0.5400\n",
      "Epoch 221/500\n",
      "57/57 [==============================] - 0s 712us/step - loss: 1.0545 - accuracy: 0.6044 - val_loss: 1.2565 - val_accuracy: 0.5533\n",
      "Epoch 222/500\n",
      "57/57 [==============================] - 0s 710us/step - loss: 1.0644 - accuracy: 0.6044 - val_loss: 1.2563 - val_accuracy: 0.5533\n",
      "Epoch 223/500\n",
      "57/57 [==============================] - 0s 670us/step - loss: 1.0207 - accuracy: 0.6356 - val_loss: 1.2548 - val_accuracy: 0.5533\n",
      "Epoch 224/500\n",
      "57/57 [==============================] - 0s 708us/step - loss: 1.0622 - accuracy: 0.6111 - val_loss: 1.2516 - val_accuracy: 0.5667\n",
      "Epoch 225/500\n",
      "57/57 [==============================] - 0s 684us/step - loss: 1.0768 - accuracy: 0.5911 - val_loss: 1.2529 - val_accuracy: 0.5533\n",
      "Epoch 226/500\n",
      "57/57 [==============================] - 0s 701us/step - loss: 1.0542 - accuracy: 0.6267 - val_loss: 1.2530 - val_accuracy: 0.5467\n",
      "Epoch 227/500\n",
      "57/57 [==============================] - 0s 687us/step - loss: 1.0311 - accuracy: 0.6022 - val_loss: 1.2508 - val_accuracy: 0.5467\n",
      "Epoch 228/500\n",
      "57/57 [==============================] - 0s 711us/step - loss: 1.0293 - accuracy: 0.5911 - val_loss: 1.2513 - val_accuracy: 0.5400\n",
      "Epoch 229/500\n",
      "57/57 [==============================] - 0s 678us/step - loss: 1.0621 - accuracy: 0.6067 - val_loss: 1.2543 - val_accuracy: 0.5400\n",
      "Epoch 230/500\n",
      "57/57 [==============================] - 0s 704us/step - loss: 1.0650 - accuracy: 0.5956 - val_loss: 1.2518 - val_accuracy: 0.5467\n",
      "Epoch 231/500\n",
      "57/57 [==============================] - 0s 696us/step - loss: 1.0553 - accuracy: 0.5956 - val_loss: 1.2541 - val_accuracy: 0.5467\n",
      "Epoch 232/500\n",
      "57/57 [==============================] - 0s 690us/step - loss: 1.0295 - accuracy: 0.6289 - val_loss: 1.2561 - val_accuracy: 0.5400\n",
      "Epoch 233/500\n",
      "57/57 [==============================] - 0s 693us/step - loss: 1.0340 - accuracy: 0.5778 - val_loss: 1.2562 - val_accuracy: 0.5267\n",
      "Epoch 234/500\n",
      "57/57 [==============================] - 0s 716us/step - loss: 1.0210 - accuracy: 0.6178 - val_loss: 1.2553 - val_accuracy: 0.5333\n",
      "Epoch 235/500\n",
      "57/57 [==============================] - 0s 678us/step - loss: 1.0619 - accuracy: 0.5933 - val_loss: 1.2564 - val_accuracy: 0.5333\n",
      "Epoch 236/500\n",
      "57/57 [==============================] - 0s 707us/step - loss: 1.0224 - accuracy: 0.6311 - val_loss: 1.2549 - val_accuracy: 0.5400\n",
      "Epoch 237/500\n",
      "57/57 [==============================] - 0s 677us/step - loss: 1.0476 - accuracy: 0.5867 - val_loss: 1.2520 - val_accuracy: 0.5333\n",
      "Epoch 238/500\n",
      "57/57 [==============================] - 0s 704us/step - loss: 1.0206 - accuracy: 0.6178 - val_loss: 1.2519 - val_accuracy: 0.5267\n",
      "Epoch 239/500\n",
      "57/57 [==============================] - 0s 703us/step - loss: 1.0435 - accuracy: 0.6044 - val_loss: 1.2521 - val_accuracy: 0.5467\n",
      "Epoch 240/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 1.0282 - accuracy: 0.6089 - val_loss: 1.2501 - val_accuracy: 0.5400\n",
      "Epoch 241/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 1.0110 - accuracy: 0.6178 - val_loss: 1.2525 - val_accuracy: 0.5467\n",
      "Epoch 242/500\n",
      "57/57 [==============================] - 0s 678us/step - loss: 1.0230 - accuracy: 0.6111 - val_loss: 1.2534 - val_accuracy: 0.5200\n",
      "Epoch 243/500\n",
      "57/57 [==============================] - 0s 724us/step - loss: 1.0520 - accuracy: 0.5956 - val_loss: 1.2544 - val_accuracy: 0.5467\n",
      "Epoch 244/500\n",
      "57/57 [==============================] - 0s 685us/step - loss: 1.0352 - accuracy: 0.6156 - val_loss: 1.2543 - val_accuracy: 0.5333\n",
      "Epoch 245/500\n",
      "57/57 [==============================] - 0s 757us/step - loss: 1.0349 - accuracy: 0.6178 - val_loss: 1.2517 - val_accuracy: 0.5400\n",
      "Epoch 246/500\n",
      "57/57 [==============================] - 0s 715us/step - loss: 1.0230 - accuracy: 0.6156 - val_loss: 1.2496 - val_accuracy: 0.5400\n",
      "Epoch 247/500\n",
      "57/57 [==============================] - 0s 721us/step - loss: 1.0630 - accuracy: 0.6133 - val_loss: 1.2495 - val_accuracy: 0.5333\n",
      "Epoch 248/500\n",
      "57/57 [==============================] - 0s 710us/step - loss: 1.0315 - accuracy: 0.6000 - val_loss: 1.2478 - val_accuracy: 0.5400\n",
      "Epoch 249/500\n",
      "57/57 [==============================] - 0s 711us/step - loss: 0.9865 - accuracy: 0.6444 - val_loss: 1.2494 - val_accuracy: 0.5400\n",
      "Epoch 250/500\n",
      "57/57 [==============================] - 0s 702us/step - loss: 1.0195 - accuracy: 0.6067 - val_loss: 1.2514 - val_accuracy: 0.5333\n",
      "Epoch 251/500\n",
      "57/57 [==============================] - 0s 696us/step - loss: 1.0191 - accuracy: 0.6333 - val_loss: 1.2505 - val_accuracy: 0.5333\n",
      "Epoch 252/500\n",
      "57/57 [==============================] - 0s 706us/step - loss: 1.0642 - accuracy: 0.5933 - val_loss: 1.2497 - val_accuracy: 0.5333\n",
      "Epoch 253/500\n",
      "57/57 [==============================] - 0s 708us/step - loss: 1.0109 - accuracy: 0.6311 - val_loss: 1.2489 - val_accuracy: 0.5333\n",
      "Epoch 254/500\n",
      "57/57 [==============================] - 0s 692us/step - loss: 1.0464 - accuracy: 0.5778 - val_loss: 1.2458 - val_accuracy: 0.5333\n",
      "Epoch 255/500\n",
      "57/57 [==============================] - 0s 698us/step - loss: 1.0130 - accuracy: 0.6089 - val_loss: 1.2460 - val_accuracy: 0.5467\n",
      "Epoch 256/500\n",
      "57/57 [==============================] - 0s 706us/step - loss: 0.9980 - accuracy: 0.6111 - val_loss: 1.2473 - val_accuracy: 0.5467\n",
      "Epoch 257/500\n",
      "57/57 [==============================] - 0s 687us/step - loss: 1.0344 - accuracy: 0.6200 - val_loss: 1.2455 - val_accuracy: 0.5467\n",
      "Epoch 258/500\n",
      "57/57 [==============================] - 0s 702us/step - loss: 1.0364 - accuracy: 0.6044 - val_loss: 1.2455 - val_accuracy: 0.5400\n",
      "Epoch 259/500\n",
      "57/57 [==============================] - 0s 723us/step - loss: 1.0224 - accuracy: 0.6000 - val_loss: 1.2462 - val_accuracy: 0.5467\n",
      "Epoch 260/500\n",
      "57/57 [==============================] - 0s 694us/step - loss: 1.0224 - accuracy: 0.6089 - val_loss: 1.2441 - val_accuracy: 0.5400\n",
      "Epoch 261/500\n",
      "57/57 [==============================] - 0s 687us/step - loss: 0.9998 - accuracy: 0.6267 - val_loss: 1.2447 - val_accuracy: 0.5400\n",
      "Epoch 262/500\n",
      "57/57 [==============================] - 0s 719us/step - loss: 1.0384 - accuracy: 0.6222 - val_loss: 1.2460 - val_accuracy: 0.5333\n",
      "Epoch 263/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 1.0093 - accuracy: 0.6044 - val_loss: 1.2493 - val_accuracy: 0.5333\n",
      "Epoch 264/500\n",
      "57/57 [==============================] - 0s 716us/step - loss: 1.0512 - accuracy: 0.6089 - val_loss: 1.2488 - val_accuracy: 0.5400\n",
      "Epoch 265/500\n",
      "57/57 [==============================] - 0s 668us/step - loss: 0.9991 - accuracy: 0.6289 - val_loss: 1.2467 - val_accuracy: 0.5400\n",
      "Epoch 266/500\n",
      "57/57 [==============================] - 0s 702us/step - loss: 1.0081 - accuracy: 0.5933 - val_loss: 1.2485 - val_accuracy: 0.5400\n",
      "Epoch 267/500\n",
      "57/57 [==============================] - 0s 672us/step - loss: 1.0412 - accuracy: 0.6156 - val_loss: 1.2472 - val_accuracy: 0.5400\n",
      "Epoch 268/500\n",
      "57/57 [==============================] - 0s 714us/step - loss: 1.0392 - accuracy: 0.5867 - val_loss: 1.2480 - val_accuracy: 0.5400\n",
      "Epoch 269/500\n",
      "57/57 [==============================] - 0s 710us/step - loss: 0.9947 - accuracy: 0.6022 - val_loss: 1.2487 - val_accuracy: 0.5400\n",
      "Epoch 270/500\n",
      "57/57 [==============================] - 0s 697us/step - loss: 1.0217 - accuracy: 0.6244 - val_loss: 1.2504 - val_accuracy: 0.5400\n",
      "Epoch 271/500\n",
      "57/57 [==============================] - 0s 701us/step - loss: 1.0292 - accuracy: 0.6022 - val_loss: 1.2513 - val_accuracy: 0.5400\n",
      "Epoch 272/500\n",
      "57/57 [==============================] - 0s 724us/step - loss: 1.0319 - accuracy: 0.5911 - val_loss: 1.2516 - val_accuracy: 0.5333\n",
      "Epoch 273/500\n",
      "57/57 [==============================] - 0s 713us/step - loss: 1.0189 - accuracy: 0.6133 - val_loss: 1.2475 - val_accuracy: 0.5333\n",
      "Epoch 274/500\n",
      "57/57 [==============================] - 0s 702us/step - loss: 1.0341 - accuracy: 0.6089 - val_loss: 1.2452 - val_accuracy: 0.5400\n",
      "Epoch 275/500\n",
      "57/57 [==============================] - 0s 687us/step - loss: 1.0032 - accuracy: 0.6222 - val_loss: 1.2471 - val_accuracy: 0.5467\n",
      "Epoch 276/500\n",
      "57/57 [==============================] - 0s 708us/step - loss: 0.9904 - accuracy: 0.6400 - val_loss: 1.2453 - val_accuracy: 0.5467\n",
      "Epoch 277/500\n",
      "57/57 [==============================] - 0s 740us/step - loss: 1.0331 - accuracy: 0.6067 - val_loss: 1.2412 - val_accuracy: 0.5467\n",
      "Epoch 278/500\n",
      "57/57 [==============================] - 0s 711us/step - loss: 0.9978 - accuracy: 0.6333 - val_loss: 1.2392 - val_accuracy: 0.5400\n",
      "Epoch 279/500\n",
      "57/57 [==============================] - 0s 693us/step - loss: 1.0066 - accuracy: 0.6311 - val_loss: 1.2390 - val_accuracy: 0.5400\n",
      "Epoch 280/500\n",
      "57/57 [==============================] - 0s 706us/step - loss: 0.9858 - accuracy: 0.6467 - val_loss: 1.2381 - val_accuracy: 0.5400\n",
      "Epoch 281/500\n",
      "57/57 [==============================] - 0s 710us/step - loss: 0.9767 - accuracy: 0.6467 - val_loss: 1.2377 - val_accuracy: 0.5400\n",
      "Epoch 282/500\n",
      "57/57 [==============================] - 0s 660us/step - loss: 0.9798 - accuracy: 0.6467 - val_loss: 1.2392 - val_accuracy: 0.5333\n",
      "Epoch 283/500\n",
      "57/57 [==============================] - 0s 749us/step - loss: 1.0095 - accuracy: 0.6244 - val_loss: 1.2403 - val_accuracy: 0.5400\n",
      "Epoch 284/500\n",
      "57/57 [==============================] - 0s 733us/step - loss: 0.9987 - accuracy: 0.6267 - val_loss: 1.2400 - val_accuracy: 0.5400\n",
      "Epoch 285/500\n",
      "57/57 [==============================] - 0s 707us/step - loss: 1.0048 - accuracy: 0.6067 - val_loss: 1.2402 - val_accuracy: 0.5333\n",
      "Epoch 286/500\n",
      "57/57 [==============================] - 0s 708us/step - loss: 1.0004 - accuracy: 0.6222 - val_loss: 1.2407 - val_accuracy: 0.5333\n",
      "Epoch 287/500\n",
      "57/57 [==============================] - 0s 698us/step - loss: 0.9592 - accuracy: 0.6444 - val_loss: 1.2384 - val_accuracy: 0.5467\n",
      "Epoch 288/500\n",
      "57/57 [==============================] - 0s 707us/step - loss: 1.0211 - accuracy: 0.6133 - val_loss: 1.2384 - val_accuracy: 0.5467\n",
      "Epoch 289/500\n",
      "57/57 [==============================] - 0s 691us/step - loss: 1.0056 - accuracy: 0.6089 - val_loss: 1.2394 - val_accuracy: 0.5467\n",
      "Epoch 290/500\n",
      "57/57 [==============================] - 0s 710us/step - loss: 0.9747 - accuracy: 0.6244 - val_loss: 1.2434 - val_accuracy: 0.5467\n",
      "Epoch 291/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 0.9972 - accuracy: 0.6267 - val_loss: 1.2448 - val_accuracy: 0.5467\n",
      "Epoch 292/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 0.9793 - accuracy: 0.6400 - val_loss: 1.2432 - val_accuracy: 0.5467\n",
      "Epoch 293/500\n",
      "57/57 [==============================] - 0s 716us/step - loss: 0.9954 - accuracy: 0.6444 - val_loss: 1.2451 - val_accuracy: 0.5400\n",
      "Epoch 294/500\n",
      "57/57 [==============================] - 0s 698us/step - loss: 0.9881 - accuracy: 0.6356 - val_loss: 1.2499 - val_accuracy: 0.5467\n",
      "Epoch 295/500\n",
      "57/57 [==============================] - 0s 713us/step - loss: 0.9975 - accuracy: 0.6156 - val_loss: 1.2522 - val_accuracy: 0.5467\n",
      "Epoch 296/500\n",
      "57/57 [==============================] - 0s 723us/step - loss: 1.0011 - accuracy: 0.6422 - val_loss: 1.2479 - val_accuracy: 0.5467\n",
      "Epoch 297/500\n",
      "57/57 [==============================] - 0s 708us/step - loss: 0.9970 - accuracy: 0.6378 - val_loss: 1.2476 - val_accuracy: 0.5533\n",
      "Epoch 298/500\n",
      "57/57 [==============================] - 0s 709us/step - loss: 0.9836 - accuracy: 0.6089 - val_loss: 1.2468 - val_accuracy: 0.5533\n",
      "Epoch 299/500\n",
      "57/57 [==============================] - 0s 692us/step - loss: 0.9584 - accuracy: 0.6422 - val_loss: 1.2473 - val_accuracy: 0.5467\n",
      "Epoch 300/500\n",
      "57/57 [==============================] - 0s 696us/step - loss: 0.9910 - accuracy: 0.6222 - val_loss: 1.2456 - val_accuracy: 0.5400\n",
      "Epoch 301/500\n",
      "57/57 [==============================] - 0s 691us/step - loss: 0.9812 - accuracy: 0.6311 - val_loss: 1.2449 - val_accuracy: 0.5467\n",
      "Epoch 302/500\n",
      "57/57 [==============================] - 0s 681us/step - loss: 0.9509 - accuracy: 0.6867 - val_loss: 1.2451 - val_accuracy: 0.5467\n",
      "Epoch 303/500\n",
      "57/57 [==============================] - 0s 688us/step - loss: 0.9823 - accuracy: 0.6378 - val_loss: 1.2471 - val_accuracy: 0.5467\n",
      "Epoch 304/500\n",
      "57/57 [==============================] - 0s 671us/step - loss: 1.0020 - accuracy: 0.6267 - val_loss: 1.2453 - val_accuracy: 0.5467\n",
      "Epoch 305/500\n",
      "57/57 [==============================] - 0s 672us/step - loss: 0.9736 - accuracy: 0.6333 - val_loss: 1.2470 - val_accuracy: 0.5467\n",
      "Epoch 306/500\n",
      "57/57 [==============================] - 0s 694us/step - loss: 0.9687 - accuracy: 0.6311 - val_loss: 1.2474 - val_accuracy: 0.5467\n",
      "Epoch 307/500\n",
      "57/57 [==============================] - 0s 680us/step - loss: 0.9697 - accuracy: 0.6511 - val_loss: 1.2449 - val_accuracy: 0.5467\n",
      "Epoch 308/500\n",
      "57/57 [==============================] - 0s 692us/step - loss: 0.9712 - accuracy: 0.6333 - val_loss: 1.2440 - val_accuracy: 0.5467\n",
      "Epoch 309/500\n",
      "57/57 [==============================] - 0s 673us/step - loss: 0.9840 - accuracy: 0.6511 - val_loss: 1.2446 - val_accuracy: 0.5533\n",
      "Epoch 310/500\n",
      "57/57 [==============================] - 0s 710us/step - loss: 1.0020 - accuracy: 0.6089 - val_loss: 1.2451 - val_accuracy: 0.5467\n",
      "Epoch 311/500\n",
      "57/57 [==============================] - 0s 755us/step - loss: 1.0005 - accuracy: 0.6089 - val_loss: 1.2474 - val_accuracy: 0.5467\n",
      "Epoch 312/500\n",
      "57/57 [==============================] - 0s 703us/step - loss: 0.9541 - accuracy: 0.6511 - val_loss: 1.2474 - val_accuracy: 0.5467\n",
      "Epoch 313/500\n",
      "57/57 [==============================] - 0s 685us/step - loss: 0.9909 - accuracy: 0.6467 - val_loss: 1.2466 - val_accuracy: 0.5467\n",
      "Epoch 314/500\n",
      "57/57 [==============================] - 0s 728us/step - loss: 0.9626 - accuracy: 0.6378 - val_loss: 1.2429 - val_accuracy: 0.5533\n",
      "Epoch 315/500\n",
      "57/57 [==============================] - 0s 693us/step - loss: 0.9579 - accuracy: 0.6533 - val_loss: 1.2422 - val_accuracy: 0.5467\n",
      "Epoch 316/500\n",
      "57/57 [==============================] - 0s 706us/step - loss: 0.9923 - accuracy: 0.6356 - val_loss: 1.2446 - val_accuracy: 0.5467\n",
      "Epoch 317/500\n",
      "57/57 [==============================] - 0s 727us/step - loss: 0.9920 - accuracy: 0.6311 - val_loss: 1.2479 - val_accuracy: 0.5400\n",
      "Epoch 318/500\n",
      "57/57 [==============================] - 0s 721us/step - loss: 0.9893 - accuracy: 0.6378 - val_loss: 1.2471 - val_accuracy: 0.5400\n",
      "Epoch 319/500\n",
      "57/57 [==============================] - 0s 692us/step - loss: 0.9638 - accuracy: 0.6333 - val_loss: 1.2489 - val_accuracy: 0.5400\n",
      "Epoch 320/500\n",
      "57/57 [==============================] - 0s 711us/step - loss: 0.9936 - accuracy: 0.6200 - val_loss: 1.2495 - val_accuracy: 0.5400\n",
      "Epoch 321/500\n",
      "57/57 [==============================] - 0s 713us/step - loss: 0.9829 - accuracy: 0.6444 - val_loss: 1.2491 - val_accuracy: 0.5400\n",
      "Epoch 322/500\n",
      "57/57 [==============================] - 0s 692us/step - loss: 0.9853 - accuracy: 0.6178 - val_loss: 1.2451 - val_accuracy: 0.5400\n",
      "Epoch 323/500\n",
      "57/57 [==============================] - 0s 689us/step - loss: 0.9400 - accuracy: 0.6378 - val_loss: 1.2427 - val_accuracy: 0.5400\n",
      "Epoch 324/500\n",
      "57/57 [==============================] - 0s 703us/step - loss: 0.9571 - accuracy: 0.6378 - val_loss: 1.2443 - val_accuracy: 0.5400\n",
      "Epoch 325/500\n",
      "57/57 [==============================] - 0s 718us/step - loss: 0.9390 - accuracy: 0.6511 - val_loss: 1.2475 - val_accuracy: 0.5400\n",
      "Epoch 326/500\n",
      "57/57 [==============================] - 0s 719us/step - loss: 1.0100 - accuracy: 0.6222 - val_loss: 1.2496 - val_accuracy: 0.5400\n",
      "Epoch 327/500\n",
      "57/57 [==============================] - 0s 704us/step - loss: 0.9735 - accuracy: 0.6267 - val_loss: 1.2447 - val_accuracy: 0.5400\n",
      "Epoch 328/500\n",
      "57/57 [==============================] - 0s 704us/step - loss: 0.9606 - accuracy: 0.6644 - val_loss: 1.2404 - val_accuracy: 0.5400\n",
      "Epoch 329/500\n",
      "57/57 [==============================] - 0s 722us/step - loss: 0.9823 - accuracy: 0.6267 - val_loss: 1.2408 - val_accuracy: 0.5400\n",
      "Epoch 330/500\n",
      "57/57 [==============================] - 0s 678us/step - loss: 0.9849 - accuracy: 0.6244 - val_loss: 1.2418 - val_accuracy: 0.5400\n",
      "Epoch 331/500\n",
      "57/57 [==============================] - 0s 677us/step - loss: 0.9903 - accuracy: 0.6222 - val_loss: 1.2403 - val_accuracy: 0.5333\n",
      "Epoch 332/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 677us/step - loss: 0.9242 - accuracy: 0.6511 - val_loss: 1.2390 - val_accuracy: 0.5400\n",
      "Epoch 333/500\n",
      "57/57 [==============================] - 0s 690us/step - loss: 0.9715 - accuracy: 0.6400 - val_loss: 1.2387 - val_accuracy: 0.5400\n",
      "Epoch 334/500\n",
      "57/57 [==============================] - 0s 661us/step - loss: 0.9559 - accuracy: 0.6200 - val_loss: 1.2391 - val_accuracy: 0.5333\n",
      "Epoch 335/500\n",
      "57/57 [==============================] - 0s 731us/step - loss: 0.9293 - accuracy: 0.6556 - val_loss: 1.2402 - val_accuracy: 0.5200\n",
      "Epoch 336/500\n",
      "57/57 [==============================] - 0s 657us/step - loss: 0.9381 - accuracy: 0.6667 - val_loss: 1.2387 - val_accuracy: 0.5267\n",
      "Epoch 337/500\n",
      "57/57 [==============================] - 0s 715us/step - loss: 0.9368 - accuracy: 0.6578 - val_loss: 1.2413 - val_accuracy: 0.5267\n",
      "Epoch 338/500\n",
      "57/57 [==============================] - 0s 712us/step - loss: 0.9803 - accuracy: 0.6222 - val_loss: 1.2437 - val_accuracy: 0.5400\n",
      "Epoch 339/500\n",
      "57/57 [==============================] - 0s 701us/step - loss: 0.9488 - accuracy: 0.6600 - val_loss: 1.2459 - val_accuracy: 0.5333\n",
      "Epoch 340/500\n",
      "57/57 [==============================] - 0s 709us/step - loss: 0.9709 - accuracy: 0.6178 - val_loss: 1.2507 - val_accuracy: 0.5400\n",
      "Epoch 341/500\n",
      "57/57 [==============================] - 0s 707us/step - loss: 0.9455 - accuracy: 0.6578 - val_loss: 1.2487 - val_accuracy: 0.5467\n",
      "Epoch 342/500\n",
      "57/57 [==============================] - 0s 665us/step - loss: 0.9622 - accuracy: 0.6422 - val_loss: 1.2452 - val_accuracy: 0.5400\n",
      "Epoch 343/500\n",
      "57/57 [==============================] - 0s 716us/step - loss: 0.9527 - accuracy: 0.6156 - val_loss: 1.2442 - val_accuracy: 0.5467\n",
      "Epoch 344/500\n",
      "57/57 [==============================] - 0s 706us/step - loss: 0.9711 - accuracy: 0.6156 - val_loss: 1.2421 - val_accuracy: 0.5600\n",
      "Epoch 345/500\n",
      "57/57 [==============================] - 0s 713us/step - loss: 0.9498 - accuracy: 0.6600 - val_loss: 1.2440 - val_accuracy: 0.5467\n",
      "Epoch 346/500\n",
      "57/57 [==============================] - 0s 698us/step - loss: 0.9537 - accuracy: 0.6533 - val_loss: 1.2465 - val_accuracy: 0.5400\n",
      "Epoch 347/500\n",
      "57/57 [==============================] - 0s 685us/step - loss: 0.9723 - accuracy: 0.6244 - val_loss: 1.2525 - val_accuracy: 0.5333\n",
      "Epoch 348/500\n",
      "57/57 [==============================] - 0s 729us/step - loss: 0.9538 - accuracy: 0.6533 - val_loss: 1.2502 - val_accuracy: 0.5400\n",
      "Epoch 349/500\n",
      "57/57 [==============================] - 0s 698us/step - loss: 0.9292 - accuracy: 0.6689 - val_loss: 1.2519 - val_accuracy: 0.5400\n",
      "Epoch 350/500\n",
      "57/57 [==============================] - 0s 680us/step - loss: 0.9507 - accuracy: 0.6356 - val_loss: 1.2450 - val_accuracy: 0.5400\n",
      "Epoch 351/500\n",
      "57/57 [==============================] - 0s 702us/step - loss: 0.9482 - accuracy: 0.6489 - val_loss: 1.2434 - val_accuracy: 0.5400\n",
      "Epoch 352/500\n",
      "57/57 [==============================] - 0s 691us/step - loss: 0.9561 - accuracy: 0.6444 - val_loss: 1.2458 - val_accuracy: 0.5400\n",
      "Epoch 353/500\n",
      "57/57 [==============================] - 0s 693us/step - loss: 0.9411 - accuracy: 0.6578 - val_loss: 1.2447 - val_accuracy: 0.5400\n",
      "Epoch 354/500\n",
      "57/57 [==============================] - 0s 668us/step - loss: 0.9655 - accuracy: 0.6133 - val_loss: 1.2452 - val_accuracy: 0.5400\n",
      "Epoch 355/500\n",
      "57/57 [==============================] - 0s 699us/step - loss: 0.9201 - accuracy: 0.6622 - val_loss: 1.2493 - val_accuracy: 0.5400\n",
      "Epoch 356/500\n",
      "57/57 [==============================] - 0s 695us/step - loss: 0.9145 - accuracy: 0.6711 - val_loss: 1.2526 - val_accuracy: 0.5400\n",
      "Epoch 357/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 0.9414 - accuracy: 0.6444 - val_loss: 1.2524 - val_accuracy: 0.5467\n",
      "Epoch 358/500\n",
      "57/57 [==============================] - 0s 678us/step - loss: 0.9882 - accuracy: 0.6311 - val_loss: 1.2524 - val_accuracy: 0.5467\n",
      "Epoch 359/500\n",
      "57/57 [==============================] - 0s 712us/step - loss: 0.9536 - accuracy: 0.6289 - val_loss: 1.2524 - val_accuracy: 0.5533\n",
      "Epoch 360/500\n",
      "57/57 [==============================] - 0s 717us/step - loss: 0.9796 - accuracy: 0.6244 - val_loss: 1.2528 - val_accuracy: 0.5533\n",
      "Epoch 361/500\n",
      "57/57 [==============================] - 0s 704us/step - loss: 0.9336 - accuracy: 0.6356 - val_loss: 1.2511 - val_accuracy: 0.5467\n",
      "Epoch 362/500\n",
      "57/57 [==============================] - 0s 699us/step - loss: 0.9420 - accuracy: 0.6378 - val_loss: 1.2503 - val_accuracy: 0.5533\n",
      "Epoch 363/500\n",
      "57/57 [==============================] - 0s 689us/step - loss: 0.9644 - accuracy: 0.6422 - val_loss: 1.2508 - val_accuracy: 0.5467\n",
      "Epoch 364/500\n",
      "57/57 [==============================] - 0s 704us/step - loss: 0.9436 - accuracy: 0.6422 - val_loss: 1.2486 - val_accuracy: 0.5467\n",
      "Epoch 365/500\n",
      "57/57 [==============================] - 0s 697us/step - loss: 0.9739 - accuracy: 0.6511 - val_loss: 1.2505 - val_accuracy: 0.5400\n",
      "Epoch 366/500\n",
      "57/57 [==============================] - 0s 682us/step - loss: 0.9409 - accuracy: 0.6711 - val_loss: 1.2507 - val_accuracy: 0.5400\n",
      "Epoch 367/500\n",
      "57/57 [==============================] - 0s 711us/step - loss: 0.9502 - accuracy: 0.6467 - val_loss: 1.2496 - val_accuracy: 0.5400\n",
      "Epoch 368/500\n",
      "57/57 [==============================] - 0s 697us/step - loss: 0.9311 - accuracy: 0.6778 - val_loss: 1.2498 - val_accuracy: 0.5533\n",
      "Epoch 369/500\n",
      "57/57 [==============================] - 0s 714us/step - loss: 0.9560 - accuracy: 0.6644 - val_loss: 1.2524 - val_accuracy: 0.5533\n",
      "Epoch 370/500\n",
      "57/57 [==============================] - 0s 699us/step - loss: 0.9769 - accuracy: 0.6422 - val_loss: 1.2481 - val_accuracy: 0.5467\n",
      "Epoch 371/500\n",
      "57/57 [==============================] - 0s 690us/step - loss: 0.9073 - accuracy: 0.6756 - val_loss: 1.2501 - val_accuracy: 0.5533\n",
      "Epoch 372/500\n",
      "57/57 [==============================] - 0s 685us/step - loss: 0.9120 - accuracy: 0.6489 - val_loss: 1.2452 - val_accuracy: 0.5467\n",
      "Epoch 373/500\n",
      "57/57 [==============================] - 0s 700us/step - loss: 0.9227 - accuracy: 0.6600 - val_loss: 1.2458 - val_accuracy: 0.5267\n",
      "Epoch 374/500\n",
      "57/57 [==============================] - 0s 704us/step - loss: 0.9550 - accuracy: 0.6422 - val_loss: 1.2451 - val_accuracy: 0.5467\n",
      "Epoch 375/500\n",
      "57/57 [==============================] - 0s 695us/step - loss: 0.9247 - accuracy: 0.6311 - val_loss: 1.2478 - val_accuracy: 0.5467\n",
      "Epoch 376/500\n",
      "57/57 [==============================] - 0s 673us/step - loss: 0.9560 - accuracy: 0.6444 - val_loss: 1.2456 - val_accuracy: 0.5467\n",
      "Epoch 377/500\n",
      "57/57 [==============================] - 0s 831us/step - loss: 0.9224 - accuracy: 0.6489 - val_loss: 1.2445 - val_accuracy: 0.5467\n",
      "Epoch 378/500\n",
      "57/57 [==============================] - 0s 656us/step - loss: 0.9357 - accuracy: 0.6556 - val_loss: 1.2438 - val_accuracy: 0.5467\n",
      "Epoch 379/500\n",
      "57/57 [==============================] - 0s 717us/step - loss: 0.9052 - accuracy: 0.6533 - val_loss: 1.2436 - val_accuracy: 0.5467\n",
      "Epoch 380/500\n",
      "57/57 [==============================] - 0s 704us/step - loss: 0.9400 - accuracy: 0.6422 - val_loss: 1.2462 - val_accuracy: 0.5467\n",
      "Epoch 381/500\n",
      "57/57 [==============================] - 0s 735us/step - loss: 0.9786 - accuracy: 0.6489 - val_loss: 1.2470 - val_accuracy: 0.5400\n",
      "Epoch 382/500\n",
      "57/57 [==============================] - 0s 657us/step - loss: 0.9223 - accuracy: 0.6578 - val_loss: 1.2464 - val_accuracy: 0.5467\n",
      "Epoch 383/500\n",
      "57/57 [==============================] - 0s 701us/step - loss: 0.9266 - accuracy: 0.6511 - val_loss: 1.2467 - val_accuracy: 0.5467\n",
      "Epoch 384/500\n",
      "57/57 [==============================] - 0s 695us/step - loss: 0.9051 - accuracy: 0.6644 - val_loss: 1.2441 - val_accuracy: 0.5467\n",
      "Epoch 385/500\n",
      "57/57 [==============================] - 0s 699us/step - loss: 0.9209 - accuracy: 0.6511 - val_loss: 1.2426 - val_accuracy: 0.5467\n",
      "Epoch 386/500\n",
      "57/57 [==============================] - 0s 693us/step - loss: 0.9377 - accuracy: 0.6333 - val_loss: 1.2469 - val_accuracy: 0.5467\n",
      "Epoch 387/500\n",
      "57/57 [==============================] - 0s 681us/step - loss: 0.9166 - accuracy: 0.6711 - val_loss: 1.2479 - val_accuracy: 0.5400\n",
      "Epoch 388/500\n",
      "57/57 [==============================] - 0s 694us/step - loss: 0.9429 - accuracy: 0.6644 - val_loss: 1.2447 - val_accuracy: 0.5400\n",
      "Epoch 389/500\n",
      "57/57 [==============================] - 0s 700us/step - loss: 0.9392 - accuracy: 0.6578 - val_loss: 1.2449 - val_accuracy: 0.5400\n",
      "Epoch 390/500\n",
      "57/57 [==============================] - 0s 690us/step - loss: 0.9718 - accuracy: 0.6333 - val_loss: 1.2404 - val_accuracy: 0.5533\n",
      "Epoch 391/500\n",
      "57/57 [==============================] - 0s 694us/step - loss: 0.9039 - accuracy: 0.6489 - val_loss: 1.2399 - val_accuracy: 0.5533\n",
      "Epoch 392/500\n",
      "57/57 [==============================] - 0s 677us/step - loss: 0.9029 - accuracy: 0.6778 - val_loss: 1.2407 - val_accuracy: 0.5467\n",
      "Epoch 393/500\n",
      "57/57 [==============================] - 0s 699us/step - loss: 0.9110 - accuracy: 0.6644 - val_loss: 1.2446 - val_accuracy: 0.5600\n",
      "Epoch 394/500\n",
      "57/57 [==============================] - 0s 669us/step - loss: 0.9532 - accuracy: 0.6222 - val_loss: 1.2449 - val_accuracy: 0.5533\n",
      "Epoch 395/500\n",
      "57/57 [==============================] - 0s 688us/step - loss: 0.8946 - accuracy: 0.6667 - val_loss: 1.2469 - val_accuracy: 0.5600\n",
      "Epoch 396/500\n",
      "57/57 [==============================] - 0s 686us/step - loss: 0.9233 - accuracy: 0.6667 - val_loss: 1.2473 - val_accuracy: 0.5600\n",
      "Epoch 397/500\n",
      "57/57 [==============================] - 0s 680us/step - loss: 0.9181 - accuracy: 0.6422 - val_loss: 1.2468 - val_accuracy: 0.5467\n",
      "Epoch 398/500\n",
      "57/57 [==============================] - 0s 706us/step - loss: 0.9257 - accuracy: 0.6222 - val_loss: 1.2486 - val_accuracy: 0.5467\n",
      "Epoch 399/500\n",
      "57/57 [==============================] - 0s 750us/step - loss: 0.8851 - accuracy: 0.6733 - val_loss: 1.2471 - val_accuracy: 0.5467\n",
      "Epoch 400/500\n",
      "57/57 [==============================] - 0s 657us/step - loss: 0.9287 - accuracy: 0.6333 - val_loss: 1.2478 - val_accuracy: 0.5533\n",
      "Epoch 401/500\n",
      "57/57 [==============================] - 0s 718us/step - loss: 0.8878 - accuracy: 0.6756 - val_loss: 1.2476 - val_accuracy: 0.5467\n",
      "Epoch 402/500\n",
      "57/57 [==============================] - 0s 700us/step - loss: 0.9114 - accuracy: 0.6622 - val_loss: 1.2473 - val_accuracy: 0.5333\n",
      "Epoch 403/500\n",
      "57/57 [==============================] - 0s 704us/step - loss: 0.9038 - accuracy: 0.6689 - val_loss: 1.2476 - val_accuracy: 0.5400\n",
      "Epoch 404/500\n",
      "57/57 [==============================] - 0s 712us/step - loss: 0.8908 - accuracy: 0.6644 - val_loss: 1.2462 - val_accuracy: 0.5400\n",
      "Epoch 405/500\n",
      "57/57 [==============================] - 0s 706us/step - loss: 0.9099 - accuracy: 0.6689 - val_loss: 1.2473 - val_accuracy: 0.5333\n",
      "Epoch 406/500\n",
      "57/57 [==============================] - 0s 719us/step - loss: 0.9325 - accuracy: 0.6444 - val_loss: 1.2473 - val_accuracy: 0.5400\n",
      "Epoch 407/500\n",
      "57/57 [==============================] - 0s 712us/step - loss: 0.9476 - accuracy: 0.6267 - val_loss: 1.2470 - val_accuracy: 0.5467\n",
      "Epoch 408/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 0.9257 - accuracy: 0.6511 - val_loss: 1.2527 - val_accuracy: 0.5400\n",
      "Epoch 409/500\n",
      "57/57 [==============================] - 0s 740us/step - loss: 0.9236 - accuracy: 0.6622 - val_loss: 1.2544 - val_accuracy: 0.5400\n",
      "Epoch 410/500\n",
      "57/57 [==============================] - 0s 676us/step - loss: 0.8911 - accuracy: 0.6644 - val_loss: 1.2524 - val_accuracy: 0.5467\n",
      "Epoch 411/500\n",
      "57/57 [==============================] - 0s 680us/step - loss: 0.8820 - accuracy: 0.6711 - val_loss: 1.2519 - val_accuracy: 0.5333\n",
      "Epoch 412/500\n",
      "57/57 [==============================] - 0s 676us/step - loss: 0.8952 - accuracy: 0.6556 - val_loss: 1.2527 - val_accuracy: 0.5400\n",
      "Epoch 413/500\n",
      "57/57 [==============================] - 0s 707us/step - loss: 0.9416 - accuracy: 0.6489 - val_loss: 1.2533 - val_accuracy: 0.5400\n",
      "Epoch 414/500\n",
      "57/57 [==============================] - 0s 704us/step - loss: 0.8975 - accuracy: 0.6578 - val_loss: 1.2529 - val_accuracy: 0.5333\n",
      "Epoch 415/500\n",
      "57/57 [==============================] - 0s 684us/step - loss: 0.8990 - accuracy: 0.6667 - val_loss: 1.2519 - val_accuracy: 0.5333\n",
      "Epoch 416/500\n",
      "57/57 [==============================] - 0s 714us/step - loss: 0.9290 - accuracy: 0.6622 - val_loss: 1.2565 - val_accuracy: 0.5333\n",
      "Epoch 417/500\n",
      "57/57 [==============================] - 0s 697us/step - loss: 0.9042 - accuracy: 0.6556 - val_loss: 1.2598 - val_accuracy: 0.5400\n",
      "Epoch 418/500\n",
      "57/57 [==============================] - 0s 702us/step - loss: 0.9267 - accuracy: 0.6733 - val_loss: 1.2618 - val_accuracy: 0.5467\n",
      "Epoch 419/500\n",
      "57/57 [==============================] - 0s 713us/step - loss: 0.9321 - accuracy: 0.6422 - val_loss: 1.2592 - val_accuracy: 0.5467\n",
      "Epoch 420/500\n",
      "57/57 [==============================] - 0s 663us/step - loss: 0.9161 - accuracy: 0.6644 - val_loss: 1.2585 - val_accuracy: 0.5400\n",
      "Epoch 421/500\n",
      "57/57 [==============================] - 0s 696us/step - loss: 0.9198 - accuracy: 0.6644 - val_loss: 1.2524 - val_accuracy: 0.5533\n",
      "Epoch 422/500\n",
      "57/57 [==============================] - 0s 686us/step - loss: 0.9073 - accuracy: 0.6600 - val_loss: 1.2501 - val_accuracy: 0.5467\n",
      "Epoch 423/500\n",
      "57/57 [==============================] - 0s 687us/step - loss: 0.8908 - accuracy: 0.6600 - val_loss: 1.2520 - val_accuracy: 0.5467\n",
      "Epoch 424/500\n",
      "57/57 [==============================] - 0s 722us/step - loss: 0.9123 - accuracy: 0.6800 - val_loss: 1.2521 - val_accuracy: 0.5467\n",
      "Epoch 425/500\n",
      "57/57 [==============================] - 0s 693us/step - loss: 0.8903 - accuracy: 0.6556 - val_loss: 1.2548 - val_accuracy: 0.5467\n",
      "Epoch 426/500\n",
      "57/57 [==============================] - 0s 659us/step - loss: 0.8740 - accuracy: 0.6689 - val_loss: 1.2561 - val_accuracy: 0.5467\n",
      "Epoch 427/500\n",
      "57/57 [==============================] - 0s 732us/step - loss: 0.8620 - accuracy: 0.6711 - val_loss: 1.2562 - val_accuracy: 0.5467\n",
      "Epoch 428/500\n",
      "57/57 [==============================] - 0s 745us/step - loss: 0.9131 - accuracy: 0.6644 - val_loss: 1.2534 - val_accuracy: 0.5467\n",
      "Epoch 429/500\n",
      "57/57 [==============================] - 0s 709us/step - loss: 0.9418 - accuracy: 0.6467 - val_loss: 1.2541 - val_accuracy: 0.5467\n",
      "Epoch 430/500\n",
      "57/57 [==============================] - 0s 700us/step - loss: 0.8998 - accuracy: 0.6467 - val_loss: 1.2552 - val_accuracy: 0.5467\n",
      "Epoch 431/500\n",
      "57/57 [==============================] - 0s 735us/step - loss: 0.8761 - accuracy: 0.6378 - val_loss: 1.2539 - val_accuracy: 0.5533\n",
      "Epoch 432/500\n",
      "57/57 [==============================] - 0s 703us/step - loss: 0.9395 - accuracy: 0.6533 - val_loss: 1.2524 - val_accuracy: 0.5467\n",
      "Epoch 433/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 0.9034 - accuracy: 0.6644 - val_loss: 1.2548 - val_accuracy: 0.5467\n",
      "Epoch 434/500\n",
      "57/57 [==============================] - 0s 690us/step - loss: 0.9007 - accuracy: 0.6711 - val_loss: 1.2568 - val_accuracy: 0.5467\n",
      "Epoch 435/500\n",
      "57/57 [==============================] - 0s 718us/step - loss: 0.8748 - accuracy: 0.6844 - val_loss: 1.2568 - val_accuracy: 0.5467\n",
      "Epoch 436/500\n",
      "57/57 [==============================] - 0s 696us/step - loss: 0.8847 - accuracy: 0.6911 - val_loss: 1.2575 - val_accuracy: 0.5400\n",
      "Epoch 437/500\n",
      "57/57 [==============================] - 0s 699us/step - loss: 0.8725 - accuracy: 0.6822 - val_loss: 1.2564 - val_accuracy: 0.5400\n",
      "Epoch 438/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 0.9277 - accuracy: 0.6578 - val_loss: 1.2538 - val_accuracy: 0.5400\n",
      "Epoch 439/500\n",
      "57/57 [==============================] - 0s 712us/step - loss: 0.8881 - accuracy: 0.6733 - val_loss: 1.2508 - val_accuracy: 0.5400\n",
      "Epoch 440/500\n",
      "57/57 [==============================] - 0s 705us/step - loss: 0.8637 - accuracy: 0.6711 - val_loss: 1.2494 - val_accuracy: 0.5533\n",
      "Epoch 441/500\n",
      "57/57 [==============================] - 0s 708us/step - loss: 0.9392 - accuracy: 0.6556 - val_loss: 1.2499 - val_accuracy: 0.5467\n",
      "Epoch 442/500\n",
      "57/57 [==============================] - 0s 680us/step - loss: 0.8756 - accuracy: 0.6867 - val_loss: 1.2456 - val_accuracy: 0.5467\n",
      "Epoch 443/500\n",
      "57/57 [==============================] - 0s 692us/step - loss: 0.8727 - accuracy: 0.6733 - val_loss: 1.2461 - val_accuracy: 0.5400\n",
      "Epoch 444/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 678us/step - loss: 0.8878 - accuracy: 0.6644 - val_loss: 1.2433 - val_accuracy: 0.5467\n",
      "Epoch 445/500\n",
      "57/57 [==============================] - 0s 693us/step - loss: 0.8973 - accuracy: 0.6533 - val_loss: 1.2465 - val_accuracy: 0.5467\n",
      "Epoch 446/500\n",
      "57/57 [==============================] - 0s 688us/step - loss: 0.8952 - accuracy: 0.6889 - val_loss: 1.2485 - val_accuracy: 0.5333\n",
      "Epoch 447/500\n",
      "57/57 [==============================] - 0s 722us/step - loss: 0.8726 - accuracy: 0.6822 - val_loss: 1.2493 - val_accuracy: 0.5467\n",
      "Epoch 448/500\n",
      "57/57 [==============================] - 0s 688us/step - loss: 0.8762 - accuracy: 0.6667 - val_loss: 1.2535 - val_accuracy: 0.5533\n",
      "Epoch 449/500\n",
      "57/57 [==============================] - 0s 699us/step - loss: 0.8817 - accuracy: 0.6733 - val_loss: 1.2538 - val_accuracy: 0.5467\n",
      "Epoch 450/500\n",
      "57/57 [==============================] - 0s 698us/step - loss: 0.8770 - accuracy: 0.6822 - val_loss: 1.2532 - val_accuracy: 0.5467\n",
      "Epoch 451/500\n",
      "57/57 [==============================] - 0s 708us/step - loss: 0.8763 - accuracy: 0.6778 - val_loss: 1.2546 - val_accuracy: 0.5467\n",
      "Epoch 452/500\n",
      "57/57 [==============================] - 0s 696us/step - loss: 0.8722 - accuracy: 0.6578 - val_loss: 1.2518 - val_accuracy: 0.5533\n",
      "Epoch 453/500\n",
      "57/57 [==============================] - 0s 692us/step - loss: 0.8798 - accuracy: 0.6689 - val_loss: 1.2503 - val_accuracy: 0.5400\n",
      "Epoch 454/500\n",
      "57/57 [==============================] - 0s 687us/step - loss: 0.8569 - accuracy: 0.6800 - val_loss: 1.2545 - val_accuracy: 0.5400\n",
      "Epoch 455/500\n",
      "57/57 [==============================] - 0s 725us/step - loss: 0.9345 - accuracy: 0.6511 - val_loss: 1.2578 - val_accuracy: 0.5400\n",
      "Epoch 456/500\n",
      "57/57 [==============================] - 0s 670us/step - loss: 0.8643 - accuracy: 0.6867 - val_loss: 1.2569 - val_accuracy: 0.5400\n",
      "Epoch 457/500\n",
      "57/57 [==============================] - 0s 691us/step - loss: 0.8578 - accuracy: 0.7022 - val_loss: 1.2579 - val_accuracy: 0.5467\n",
      "Epoch 458/500\n",
      "57/57 [==============================] - 0s 712us/step - loss: 0.8989 - accuracy: 0.6822 - val_loss: 1.2541 - val_accuracy: 0.5467\n",
      "Epoch 459/500\n",
      "57/57 [==============================] - 0s 683us/step - loss: 0.9131 - accuracy: 0.6644 - val_loss: 1.2551 - val_accuracy: 0.5467\n",
      "Epoch 460/500\n",
      "57/57 [==============================] - 0s 686us/step - loss: 0.8918 - accuracy: 0.6622 - val_loss: 1.2549 - val_accuracy: 0.5467\n",
      "Epoch 461/500\n",
      "57/57 [==============================] - 0s 694us/step - loss: 0.8611 - accuracy: 0.6978 - val_loss: 1.2512 - val_accuracy: 0.5467\n",
      "Epoch 462/500\n",
      "57/57 [==============================] - 0s 666us/step - loss: 0.9034 - accuracy: 0.6556 - val_loss: 1.2533 - val_accuracy: 0.5400\n",
      "Epoch 463/500\n",
      "57/57 [==============================] - 0s 711us/step - loss: 0.8456 - accuracy: 0.6733 - val_loss: 1.2552 - val_accuracy: 0.5333\n",
      "Epoch 464/500\n",
      "57/57 [==============================] - 0s 693us/step - loss: 0.8625 - accuracy: 0.6911 - val_loss: 1.2553 - val_accuracy: 0.5333\n",
      "Epoch 465/500\n",
      "57/57 [==============================] - 0s 703us/step - loss: 0.8635 - accuracy: 0.6911 - val_loss: 1.2570 - val_accuracy: 0.5467\n",
      "Epoch 466/500\n",
      "57/57 [==============================] - 0s 723us/step - loss: 0.8265 - accuracy: 0.6978 - val_loss: 1.2581 - val_accuracy: 0.5533\n",
      "Epoch 467/500\n",
      "57/57 [==============================] - 0s 667us/step - loss: 0.8863 - accuracy: 0.6711 - val_loss: 1.2580 - val_accuracy: 0.5533\n",
      "Epoch 468/500\n",
      "57/57 [==============================] - 0s 721us/step - loss: 0.8665 - accuracy: 0.7000 - val_loss: 1.2539 - val_accuracy: 0.5467\n",
      "Epoch 469/500\n",
      "57/57 [==============================] - 0s 708us/step - loss: 0.8854 - accuracy: 0.6644 - val_loss: 1.2540 - val_accuracy: 0.5533\n",
      "Epoch 470/500\n",
      "57/57 [==============================] - 0s 721us/step - loss: 0.8598 - accuracy: 0.6800 - val_loss: 1.2582 - val_accuracy: 0.5467\n",
      "Epoch 471/500\n",
      "57/57 [==============================] - 0s 674us/step - loss: 0.8844 - accuracy: 0.6644 - val_loss: 1.2558 - val_accuracy: 0.5467\n",
      "Epoch 472/500\n",
      "57/57 [==============================] - 0s 703us/step - loss: 0.8758 - accuracy: 0.6578 - val_loss: 1.2585 - val_accuracy: 0.5467\n",
      "Epoch 473/500\n",
      "57/57 [==============================] - 0s 723us/step - loss: 0.9007 - accuracy: 0.6689 - val_loss: 1.2570 - val_accuracy: 0.5467\n",
      "Epoch 474/500\n",
      "57/57 [==============================] - 0s 701us/step - loss: 0.9034 - accuracy: 0.6444 - val_loss: 1.2553 - val_accuracy: 0.5400\n",
      "Epoch 475/500\n",
      "57/57 [==============================] - 0s 708us/step - loss: 0.8631 - accuracy: 0.7178 - val_loss: 1.2581 - val_accuracy: 0.5400\n",
      "Epoch 476/500\n",
      "57/57 [==============================] - 0s 695us/step - loss: 0.8600 - accuracy: 0.6733 - val_loss: 1.2605 - val_accuracy: 0.5333\n",
      "Epoch 477/500\n",
      "57/57 [==============================] - 0s 713us/step - loss: 0.8905 - accuracy: 0.6800 - val_loss: 1.2579 - val_accuracy: 0.5400\n",
      "Epoch 478/500\n",
      "57/57 [==============================] - 0s 702us/step - loss: 0.8309 - accuracy: 0.7000 - val_loss: 1.2579 - val_accuracy: 0.5400\n",
      "Epoch 479/500\n",
      "57/57 [==============================] - 0s 718us/step - loss: 0.8847 - accuracy: 0.6622 - val_loss: 1.2560 - val_accuracy: 0.5467\n",
      "Epoch 480/500\n",
      "57/57 [==============================] - 0s 692us/step - loss: 0.8780 - accuracy: 0.6800 - val_loss: 1.2587 - val_accuracy: 0.5533\n",
      "Epoch 481/500\n",
      "57/57 [==============================] - 0s 716us/step - loss: 0.8588 - accuracy: 0.6711 - val_loss: 1.2574 - val_accuracy: 0.5467\n",
      "Epoch 482/500\n",
      "57/57 [==============================] - 0s 736us/step - loss: 0.9052 - accuracy: 0.6467 - val_loss: 1.2559 - val_accuracy: 0.5467\n",
      "Epoch 483/500\n",
      "57/57 [==============================] - 0s 677us/step - loss: 0.8714 - accuracy: 0.6667 - val_loss: 1.2557 - val_accuracy: 0.5467\n",
      "Epoch 484/500\n",
      "57/57 [==============================] - 0s 702us/step - loss: 0.8914 - accuracy: 0.6667 - val_loss: 1.2572 - val_accuracy: 0.5467\n",
      "Epoch 485/500\n",
      "57/57 [==============================] - 0s 710us/step - loss: 0.8454 - accuracy: 0.6933 - val_loss: 1.2611 - val_accuracy: 0.5467\n",
      "Epoch 486/500\n",
      "57/57 [==============================] - 0s 692us/step - loss: 0.8496 - accuracy: 0.6867 - val_loss: 1.2626 - val_accuracy: 0.5467\n",
      "Epoch 487/500\n",
      "57/57 [==============================] - 0s 657us/step - loss: 0.8940 - accuracy: 0.6733 - val_loss: 1.2661 - val_accuracy: 0.5467\n",
      "Epoch 488/500\n",
      "57/57 [==============================] - 0s 717us/step - loss: 0.8539 - accuracy: 0.6778 - val_loss: 1.2676 - val_accuracy: 0.5467\n",
      "Epoch 489/500\n",
      "57/57 [==============================] - 0s 669us/step - loss: 0.8377 - accuracy: 0.6978 - val_loss: 1.2690 - val_accuracy: 0.5467\n",
      "Epoch 490/500\n",
      "57/57 [==============================] - 0s 707us/step - loss: 0.8584 - accuracy: 0.6778 - val_loss: 1.2684 - val_accuracy: 0.5467\n",
      "Epoch 491/500\n",
      "57/57 [==============================] - 0s 727us/step - loss: 0.8697 - accuracy: 0.6756 - val_loss: 1.2643 - val_accuracy: 0.5400\n",
      "Epoch 492/500\n",
      "57/57 [==============================] - 0s 709us/step - loss: 0.8660 - accuracy: 0.6911 - val_loss: 1.2662 - val_accuracy: 0.5333\n",
      "Epoch 493/500\n",
      "57/57 [==============================] - 0s 696us/step - loss: 0.8714 - accuracy: 0.6578 - val_loss: 1.2642 - val_accuracy: 0.5400\n",
      "Epoch 494/500\n",
      "57/57 [==============================] - 0s 707us/step - loss: 0.8860 - accuracy: 0.6556 - val_loss: 1.2610 - val_accuracy: 0.5333\n",
      "Epoch 495/500\n",
      "57/57 [==============================] - 0s 685us/step - loss: 0.8543 - accuracy: 0.6778 - val_loss: 1.2546 - val_accuracy: 0.5333\n",
      "Epoch 496/500\n",
      "57/57 [==============================] - 0s 699us/step - loss: 0.8582 - accuracy: 0.6844 - val_loss: 1.2589 - val_accuracy: 0.5400\n",
      "Epoch 497/500\n",
      "57/57 [==============================] - 0s 715us/step - loss: 0.8524 - accuracy: 0.6644 - val_loss: 1.2572 - val_accuracy: 0.5333\n",
      "Epoch 498/500\n",
      "57/57 [==============================] - 0s 701us/step - loss: 0.8472 - accuracy: 0.6911 - val_loss: 1.2597 - val_accuracy: 0.5333\n",
      "Epoch 499/500\n",
      "57/57 [==============================] - 0s 714us/step - loss: 0.8539 - accuracy: 0.6933 - val_loss: 1.2583 - val_accuracy: 0.5333\n",
      "Epoch 500/500\n",
      "57/57 [==============================] - 0s 699us/step - loss: 0.8827 - accuracy: 0.6822 - val_loss: 1.2607 - val_accuracy: 0.5333\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "BATCH_SIZE = 8\n",
    "model = getNetwork()\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.40      0.43        30\n",
      "           1       0.62      0.60      0.61        30\n",
      "           2       0.46      0.40      0.43        30\n",
      "           3       0.68      0.57      0.62        30\n",
      "           4       0.50      0.73      0.59        30\n",
      "\n",
      "    accuracy                           0.54       150\n",
      "   macro avg       0.54      0.54      0.54       150\n",
      "weighted avg       0.54      0.54      0.54       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test = model.predict(X_test)\n",
    "predictions_categorical = np.argmax(pred_test, axis=1)\n",
    "print(classification_report(y_test, predictions_categorical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Models in C code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpzzai4o2p/assets\n"
     ]
    }
   ],
   "source": [
    "# Neural network with TinyMLGen\n",
    "with open('exportedModels/NNmodel.h', 'w') as f:\n",
    "    f.write(tiny.port(model, optimize=False))\n",
    "\n",
    "# Classifiers with MicroMLGen\n",
    "for name, model in models:\n",
    "    prepath = 'exportedModels/' + str(labels) + \"/\"\n",
    "    path = prepath + name + '.h'\n",
    "    x = port(model, optimize=True)\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(port(model, optimize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
