{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Comparison for TinyML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "import pickle\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,  classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold,StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dense, Input, concatenate, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from micromlgen import port\n",
    "import tinymlgen as tiny\n",
    "\n",
    "import warnings\n",
    "import seaborn as sbs\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/X_paper.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open('../data/y_paper.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X[:200], X[600:800], X[1200:]), axis=0)\n",
    "y = np.concatenate((y[:200], y[600:800], y[1200:]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Convert the labels in values like 0...n for the NN tests\n",
    "\n",
    "labels = []\n",
    "uniques = list(np.unique(y))\n",
    "\n",
    "[labels.append(uniques.index(el)) for el in y]\n",
    "\n",
    "y = np.array(labels)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  21  102  100  220    0]\n",
      " [ 100  110  101 1010    0]\n",
      " [ 102  121  111 1102    0]\n",
      " [ 111  122  112 1112    0]\n",
      " [ 111  122  120 1122    0]\n",
      " [ 100  111  102 1002    0]\n",
      " [ 102  120  111 1100    0]\n",
      " [ 100  111  101 1001    0]\n",
      " [ 111  122  112 1111    0]\n",
      " [  21  102  100  221    0]\n",
      " [  21  102  101  221    0]\n",
      " [ 112  201  121 1200    0]\n",
      " [ 110  120  111 1101    0]\n",
      " [  21  101  100  212    0]\n",
      " [  21  102  100  220    0]\n",
      " [  22  111  102 1001    0]\n",
      " [ 122  202  200 2000    0]\n",
      " [ 120  201  121 1210    0]\n",
      " [ 101  120  110 1021    0]\n",
      " [ 101  120  110 1021    0]\n",
      " [  22  110  101 1001    0]\n",
      " [ 112  201  122 1210    0]\n",
      " [ 121  202  122 1221    0]\n",
      " [ 100  111  101 1001    0]\n",
      " [ 121  210  200 1222    0]\n",
      " [ 122  210  201 2002    0]\n",
      " [ 112  200  120 1122    0]\n",
      " [ 110  122  120 1121  212]\n",
      " [ 112  200  120 1200    0]\n",
      " [ 121  202  122 1222    0]\n",
      " [ 102  121  111 1100    0]\n",
      " [ 111  201  122 1200    0]\n",
      " [  21  102  100  221    0]\n",
      " [ 120  210  201 1221    0]\n",
      " [  22  110  101 1000    0]\n",
      " [ 110  121  111 1102    0]\n",
      " [ 101  120  112 1101    0]\n",
      " [ 111  201  121 1121    0]\n",
      " [ 111  122  112 1112    0]\n",
      " [ 102  120  111 1022    0]\n",
      " [  22  110  101  222    0]\n",
      " [  22  111  102 1010    0]\n",
      " [  22  111  102 1010    0]\n",
      " [ 102  120  110 1022    0]\n",
      " [  22  102  101  222    0]\n",
      " [ 122  202  200 1222    0]\n",
      " [ 100  112  110 1011    0]\n",
      " [  22  111  102 1010    0]\n",
      " [  22  111  102 1002    0]\n",
      " [ 111  122  112 1112    0]\n",
      " [ 101  120  110 1020    0]\n",
      " [ 102  121  111 1100    0]\n",
      " [  20  101   22  211    0]\n",
      " [ 110  121  111 1102    0]\n",
      " [ 110  122  120 1120  100]\n",
      " [ 111  200  120 1120    0]\n",
      " [ 111  200  120 1121    0]\n",
      " [ 101  112  102 1011    0]\n",
      " [ 110  122  112 1110    0]\n",
      " [ 101  120  110 1020    0]\n",
      " [  22  112  110 1010    0]\n",
      " [ 100  111  102 1002    0]\n",
      " [ 101  112  110 1020    0]\n",
      " [ 121  202  122 1221    0]\n",
      " [ 110  121  111 1101    0]\n",
      " [ 120  202  200 1220    0]\n",
      " [  22  102  100  222    0]\n",
      " [ 122  202  200 2000    0]\n",
      " [ 111  200  121 1201    0]\n",
      " [ 120  202  122 1220    0]\n",
      " [ 110  121  112 1111    0]\n",
      " [ 111  122  112 1120    0]\n",
      " [ 111  201  120 1120    0]\n",
      " [ 101  112  110 1021    0]\n",
      " [ 121  201  122 1220    0]\n",
      " [  21  102  100  220    0]\n",
      " [ 121  202  122 1212    0]\n",
      " [  21  101   22  211    0]\n",
      " [ 121  202  122 1220    0]\n",
      " [ 122  210  201 2002    0]\n",
      " [ 102  121  111 1102    0]\n",
      " [ 100  111  102 1011    0]\n",
      " [ 110  121  111 1110    0]\n",
      " [ 101  120  112 1101    0]\n",
      " [ 102  121  111 1101    0]\n",
      " [  21  102  100  221    0]\n",
      " [  22  110  101  222    0]\n",
      " [ 122  210  200 1222    0]\n",
      " [  22  111  102 1002    0]\n",
      " [ 100  111  102 1011    0]\n",
      " [ 101  120  110 1021    0]\n",
      " [ 101  112  110 1020    0]\n",
      " [ 121  202  122 1221    0]\n",
      " [ 112  202  200 1212    0]\n",
      " [ 122  202  200 2000    0]\n",
      " [ 112  201  121 1200    0]\n",
      " [ 102  120  110 1100    0]\n",
      " [ 101  120  112 1101    0]\n",
      " [ 122  202  122 1222    0]\n",
      " [ 101  120  111 1022    0]\n",
      " [ 101  120  110 1021    0]\n",
      " [ 200  210  200 2001    0]\n",
      " [ 110  122  120 1120    0]\n",
      " [  21  101   22  211    0]\n",
      " [ 101  112  102 1012    0]\n",
      " [  22  111  102 1002    0]\n",
      " [ 100  111  110 1011    0]\n",
      " [ 111  122  112 1111    0]\n",
      " [ 100  112  102 1011    0]\n",
      " [  21  102  101  222    0]\n",
      " [  21  102  101  221    0]\n",
      " [ 102  120  111 1101    0]\n",
      " [ 100  111  110 1011    0]\n",
      " [ 110  122  112 1111    0]\n",
      " [ 100  112  110 1012    0]\n",
      " [ 112  200  120 1121    0]\n",
      " [  22  111  102 1010    0]\n",
      " [ 112  201  121 1201    0]\n",
      " [ 111  122  112 1111    0]\n",
      " [ 200  210  201 2011    0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Spotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 42\n",
    "scoring = 'f1_macro'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "\n",
    "#models.append(('XGB', XGBClassifier(random_state=seed)))\n",
    "models.append(('GNB', GaussianNB(var_smoothing=2e-9)))\n",
    "models.append(('LR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression(random_state=seed))])))\n",
    "models.append(('CART' , DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('SVC' , SVC(gamma=0.5, random_state=seed)))\n",
    "models.append(('RF', RandomForestClassifier(random_state=seed, n_estimators = 50)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB - 0,72 0,06\n",
      "LR - 0,79 0,07\n",
      "CART - 0,93 0,03\n",
      "SVC - 0,91 0,04\n",
      "RF - 0,94 0,02\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    # Dividere dati in n = num_folds\n",
    "    kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = np.array([])\n",
    "    for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "        X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "        X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "        model.fit(X_cross_train, y_cross_train)  \n",
    "        y_pred = model.predict(X_cross_test)\n",
    "        f1s = f1_score(y_cross_test, y_pred, average=\"weighted\")\n",
    "        cv_results = np.append(cv_results, [f1s])\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #msg = \"%s - %f - %f\" % (name, cv_results.mean(), cv_results.std())\n",
    "    msg = \"{} - {:.2f} {:.2f}\".format(name, cv_results.mean(), cv_results.std()).replace('.', ',')\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "fig.suptitle('Algorithms Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione modelli sul Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model GNB: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.45      0.61        40\n",
      "           1       0.49      0.80      0.61        40\n",
      "           2       0.81      0.72      0.76        40\n",
      "\n",
      "    accuracy                           0.66       120\n",
      "   macro avg       0.75      0.66      0.66       120\n",
      "weighted avg       0.75      0.66      0.66       120\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model LR: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.70      0.81        40\n",
      "           1       0.67      0.82      0.74        40\n",
      "           2       0.86      0.90      0.88        40\n",
      "\n",
      "    accuracy                           0.81       120\n",
      "   macro avg       0.83      0.81      0.81       120\n",
      "weighted avg       0.83      0.81      0.81       120\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model CART: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96        40\n",
      "           1       0.92      0.88      0.90        40\n",
      "           2       0.95      0.93      0.94        40\n",
      "\n",
      "    accuracy                           0.93       120\n",
      "   macro avg       0.93      0.93      0.93       120\n",
      "weighted avg       0.93      0.93      0.93       120\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model SVC: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.72      0.83        40\n",
      "           1       0.70      0.80      0.74        40\n",
      "           2       0.84      0.93      0.88        40\n",
      "\n",
      "    accuracy                           0.82       120\n",
      "   macro avg       0.83      0.82      0.82       120\n",
      "weighted avg       0.83      0.82      0.82       120\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model RF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        40\n",
      "           1       1.00      0.95      0.97        40\n",
      "           2       0.98      1.00      0.99        40\n",
      "\n",
      "    accuracy                           0.98       120\n",
      "   macro avg       0.98      0.98      0.98       120\n",
      "weighted avg       0.98      0.98      0.98       120\n",
      "\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "for name, model in models:\n",
    "    model.fit(X_train,  y_train)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    print(f\"Model {name}: \")\n",
    "    print(classification_report(y_test, pred_test))\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione Inferance Rate medio (|X_test| = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = read_csv(\"InfTimeReport.csv\")\n",
    "g = sbs.barplot(x=csv['Algoritmo'], y=csv['InfTime'])\n",
    "g.set_yscale(\"log\")\n",
    "plt.ylabel(\"Inference Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoria occupata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv = read_csv(\"MemOccupationReport.csv\")\n",
    "sbs.barplot(x=csv['Algoritmo'], y=csv['MemOccupata'])\n",
    "plt.ylabel(\"MemOccupata in Byte\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn_rate = 0.001\n",
    "def getNetwork(): \n",
    "    model = Sequential(name=\"Sequential-NN\")\n",
    "    model.add(layers.Dense(X.shape[1], activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(np.unique(y).size, activation='softmax'))\n",
    "    opt = Adam(learning_rate=learn_rate)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1536      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 2,337\n",
      "Trainable params: 2,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "87/87 [==============================] - 0s 359us/step - loss: 1.0714 - accuracy: 0.6458\n",
      "Epoch 2/500\n",
      "87/87 [==============================] - 0s 357us/step - loss: 0.6487 - accuracy: 0.7801\n",
      "Epoch 3/500\n",
      "87/87 [==============================] - 0s 342us/step - loss: 0.5662 - accuracy: 0.7894\n",
      "Epoch 4/500\n",
      "87/87 [==============================] - 0s 363us/step - loss: 0.5231 - accuracy: 0.7917\n",
      "Epoch 5/500\n",
      "87/87 [==============================] - 0s 369us/step - loss: 0.4926 - accuracy: 0.7940\n",
      "Epoch 6/500\n",
      "87/87 [==============================] - 0s 376us/step - loss: 0.4813 - accuracy: 0.7963\n",
      "Epoch 7/500\n",
      "87/87 [==============================] - 0s 343us/step - loss: 0.4665 - accuracy: 0.7940\n",
      "Epoch 8/500\n",
      "87/87 [==============================] - 0s 356us/step - loss: 0.4567 - accuracy: 0.7986\n",
      "Epoch 9/500\n",
      "87/87 [==============================] - 0s 346us/step - loss: 0.4479 - accuracy: 0.7986\n",
      "Epoch 10/500\n",
      "87/87 [==============================] - 0s 345us/step - loss: 0.4372 - accuracy: 0.8009\n",
      "Epoch 11/500\n",
      "87/87 [==============================] - 0s 344us/step - loss: 0.4358 - accuracy: 0.7940\n",
      "Epoch 12/500\n",
      "87/87 [==============================] - 0s 356us/step - loss: 0.4295 - accuracy: 0.8056\n",
      "Epoch 13/500\n",
      "87/87 [==============================] - 0s 347us/step - loss: 0.4232 - accuracy: 0.7986\n",
      "Epoch 14/500\n",
      "87/87 [==============================] - 0s 340us/step - loss: 0.4185 - accuracy: 0.8287\n",
      "Epoch 15/500\n",
      "87/87 [==============================] - 0s 368us/step - loss: 0.4164 - accuracy: 0.8171\n",
      "Epoch 16/500\n",
      "87/87 [==============================] - 0s 358us/step - loss: 0.4115 - accuracy: 0.8171\n",
      "Epoch 17/500\n",
      "87/87 [==============================] - 0s 349us/step - loss: 0.4062 - accuracy: 0.8241\n",
      "Epoch 18/500\n",
      "87/87 [==============================] - 0s 332us/step - loss: 0.4082 - accuracy: 0.8218\n",
      "Epoch 19/500\n",
      "87/87 [==============================] - 0s 358us/step - loss: 0.3986 - accuracy: 0.8356\n",
      "Epoch 20/500\n",
      "87/87 [==============================] - 0s 352us/step - loss: 0.3973 - accuracy: 0.8356\n",
      "Epoch 21/500\n",
      "87/87 [==============================] - 0s 357us/step - loss: 0.4029 - accuracy: 0.8380\n",
      "Epoch 22/500\n",
      "87/87 [==============================] - 0s 354us/step - loss: 0.3889 - accuracy: 0.8472\n",
      "Epoch 23/500\n",
      "87/87 [==============================] - 0s 341us/step - loss: 0.3880 - accuracy: 0.8495\n",
      "Epoch 24/500\n",
      "87/87 [==============================] - 0s 345us/step - loss: 0.3816 - accuracy: 0.8588\n",
      "Epoch 25/500\n",
      "87/87 [==============================] - 0s 354us/step - loss: 0.3750 - accuracy: 0.8634\n",
      "Epoch 26/500\n",
      "87/87 [==============================] - 0s 352us/step - loss: 0.3750 - accuracy: 0.8542\n",
      "Epoch 27/500\n",
      "87/87 [==============================] - 0s 363us/step - loss: 0.3702 - accuracy: 0.8542\n",
      "Epoch 28/500\n",
      "87/87 [==============================] - 0s 349us/step - loss: 0.3740 - accuracy: 0.8519\n",
      "Epoch 29/500\n",
      "87/87 [==============================] - 0s 352us/step - loss: 0.3650 - accuracy: 0.8542\n",
      "Epoch 30/500\n",
      "87/87 [==============================] - 0s 352us/step - loss: 0.3618 - accuracy: 0.8519\n",
      "Epoch 31/500\n",
      "87/87 [==============================] - 0s 344us/step - loss: 0.3514 - accuracy: 0.8611\n",
      "Epoch 32/500\n",
      "87/87 [==============================] - 0s 335us/step - loss: 0.3471 - accuracy: 0.8588\n",
      "Epoch 33/500\n",
      "87/87 [==============================] - 0s 343us/step - loss: 0.3459 - accuracy: 0.8588\n",
      "Epoch 34/500\n",
      "87/87 [==============================] - 0s 346us/step - loss: 0.3429 - accuracy: 0.8634\n",
      "Epoch 35/500\n",
      "87/87 [==============================] - 0s 376us/step - loss: 0.3374 - accuracy: 0.8611\n",
      "Epoch 36/500\n",
      "87/87 [==============================] - 0s 368us/step - loss: 0.3408 - accuracy: 0.8727\n",
      "Epoch 37/500\n",
      "87/87 [==============================] - 0s 340us/step - loss: 0.3363 - accuracy: 0.8565\n",
      "Epoch 38/500\n",
      "87/87 [==============================] - 0s 338us/step - loss: 0.3342 - accuracy: 0.8588\n",
      "Epoch 39/500\n",
      "87/87 [==============================] - 0s 349us/step - loss: 0.3292 - accuracy: 0.8750\n",
      "Epoch 40/500\n",
      "87/87 [==============================] - 0s 349us/step - loss: 0.3322 - accuracy: 0.8657\n",
      "Epoch 41/500\n",
      "87/87 [==============================] - 0s 366us/step - loss: 0.3205 - accuracy: 0.8750\n",
      "Epoch 42/500\n",
      "87/87 [==============================] - 0s 348us/step - loss: 0.3169 - accuracy: 0.8819\n",
      "Epoch 43/500\n",
      "87/87 [==============================] - 0s 335us/step - loss: 0.3188 - accuracy: 0.8727\n",
      "Epoch 44/500\n",
      "87/87 [==============================] - 0s 341us/step - loss: 0.3164 - accuracy: 0.8750\n",
      "Epoch 45/500\n",
      "87/87 [==============================] - 0s 343us/step - loss: 0.3082 - accuracy: 0.8843\n",
      "Epoch 46/500\n",
      "87/87 [==============================] - 0s 346us/step - loss: 0.3115 - accuracy: 0.8843\n",
      "Epoch 47/500\n",
      "87/87 [==============================] - 0s 358us/step - loss: 0.3076 - accuracy: 0.8727\n",
      "Epoch 48/500\n",
      "87/87 [==============================] - 0s 337us/step - loss: 0.3104 - accuracy: 0.8750\n",
      "Epoch 49/500\n",
      "87/87 [==============================] - 0s 343us/step - loss: 0.3026 - accuracy: 0.8819\n",
      "Epoch 50/500\n",
      "87/87 [==============================] - 0s 343us/step - loss: 0.2964 - accuracy: 0.8843\n",
      "Epoch 51/500\n",
      "87/87 [==============================] - 0s 351us/step - loss: 0.2941 - accuracy: 0.8773\n",
      "Epoch 52/500\n",
      "87/87 [==============================] - 0s 346us/step - loss: 0.3004 - accuracy: 0.8843\n",
      "Epoch 53/500\n",
      "87/87 [==============================] - 0s 341us/step - loss: 0.2874 - accuracy: 0.8889\n",
      "Epoch 54/500\n",
      "87/87 [==============================] - 0s 340us/step - loss: 0.2852 - accuracy: 0.8866\n",
      "Epoch 55/500\n",
      "87/87 [==============================] - 0s 344us/step - loss: 0.2948 - accuracy: 0.8681\n",
      "Epoch 56/500\n",
      "87/87 [==============================] - 0s 339us/step - loss: 0.2916 - accuracy: 0.8889\n",
      "Epoch 57/500\n",
      "87/87 [==============================] - 0s 328us/step - loss: 0.2845 - accuracy: 0.8796\n",
      "Epoch 58/500\n",
      "87/87 [==============================] - 0s 342us/step - loss: 0.2861 - accuracy: 0.8796\n",
      "Epoch 59/500\n",
      "87/87 [==============================] - 0s 344us/step - loss: 0.2814 - accuracy: 0.8889\n",
      "Epoch 60/500\n",
      "87/87 [==============================] - 0s 341us/step - loss: 0.2853 - accuracy: 0.8796\n",
      "Epoch 61/500\n",
      "87/87 [==============================] - 0s 343us/step - loss: 0.2750 - accuracy: 0.8866\n",
      "Epoch 62/500\n",
      "87/87 [==============================] - 0s 352us/step - loss: 0.2796 - accuracy: 0.8843\n",
      "Epoch 63/500\n",
      "87/87 [==============================] - 0s 350us/step - loss: 0.2699 - accuracy: 0.8843\n",
      "Epoch 64/500\n",
      "87/87 [==============================] - 0s 352us/step - loss: 0.2813 - accuracy: 0.8866\n",
      "Epoch 65/500\n",
      "87/87 [==============================] - 0s 357us/step - loss: 0.2890 - accuracy: 0.8843\n",
      "Epoch 66/500\n",
      "87/87 [==============================] - 0s 345us/step - loss: 0.2705 - accuracy: 0.8889\n",
      "Epoch 67/500\n",
      "87/87 [==============================] - 0s 352us/step - loss: 0.2789 - accuracy: 0.8819\n",
      "Epoch 68/500\n",
      "87/87 [==============================] - 0s 350us/step - loss: 0.2729 - accuracy: 0.8843\n",
      "Epoch 69/500\n",
      "87/87 [==============================] - 0s 353us/step - loss: 0.2740 - accuracy: 0.8866\n",
      "Epoch 70/500\n",
      "87/87 [==============================] - 0s 359us/step - loss: 0.2726 - accuracy: 0.8866\n",
      "Epoch 71/500\n",
      "87/87 [==============================] - 0s 347us/step - loss: 0.2631 - accuracy: 0.8958\n",
      "Epoch 72/500\n",
      "87/87 [==============================] - 0s 353us/step - loss: 0.2810 - accuracy: 0.8912\n",
      "Epoch 73/500\n",
      "87/87 [==============================] - 0s 342us/step - loss: 0.2654 - accuracy: 0.8889\n",
      "Epoch 74/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 337us/step - loss: 0.2616 - accuracy: 0.8935\n",
      "Epoch 75/500\n",
      "87/87 [==============================] - 0s 335us/step - loss: 0.2612 - accuracy: 0.8935\n",
      "Epoch 76/500\n",
      "87/87 [==============================] - 0s 336us/step - loss: 0.2685 - accuracy: 0.8819\n",
      "Epoch 77/500\n",
      "87/87 [==============================] - 0s 330us/step - loss: 0.2684 - accuracy: 0.8935\n",
      "Epoch 78/500\n",
      "87/87 [==============================] - 0s 339us/step - loss: 0.2666 - accuracy: 0.8981\n",
      "Epoch 79/500\n",
      "87/87 [==============================] - 0s 374us/step - loss: 0.2720 - accuracy: 0.8843\n",
      "Epoch 80/500\n",
      "87/87 [==============================] - 0s 359us/step - loss: 0.2623 - accuracy: 0.8958\n",
      "Epoch 81/500\n",
      "87/87 [==============================] - 0s 340us/step - loss: 0.2606 - accuracy: 0.8866\n",
      "Epoch 82/500\n",
      "87/87 [==============================] - 0s 358us/step - loss: 0.2644 - accuracy: 0.8912\n",
      "Epoch 83/500\n",
      "87/87 [==============================] - 0s 358us/step - loss: 0.2693 - accuracy: 0.8819\n",
      "Epoch 84/500\n",
      "87/87 [==============================] - 0s 346us/step - loss: 0.2537 - accuracy: 0.8912\n",
      "Epoch 85/500\n",
      "87/87 [==============================] - 0s 380us/step - loss: 0.2598 - accuracy: 0.8843\n",
      "Epoch 86/500\n",
      "87/87 [==============================] - 0s 354us/step - loss: 0.2611 - accuracy: 0.8912\n",
      "Epoch 87/500\n",
      "87/87 [==============================] - 0s 401us/step - loss: 0.2540 - accuracy: 0.8981\n",
      "Epoch 88/500\n",
      "87/87 [==============================] - 0s 349us/step - loss: 0.2558 - accuracy: 0.8843\n",
      "Epoch 89/500\n",
      "87/87 [==============================] - 0s 367us/step - loss: 0.2505 - accuracy: 0.8912\n",
      "Epoch 90/500\n",
      "87/87 [==============================] - 0s 341us/step - loss: 0.2611 - accuracy: 0.8843\n",
      "Epoch 91/500\n",
      "87/87 [==============================] - 0s 355us/step - loss: 0.2467 - accuracy: 0.9005\n",
      "Epoch 92/500\n",
      "87/87 [==============================] - 0s 333us/step - loss: 0.2515 - accuracy: 0.8912\n",
      "Epoch 93/500\n",
      "87/87 [==============================] - 0s 340us/step - loss: 0.2540 - accuracy: 0.8935\n",
      "Epoch 94/500\n",
      "87/87 [==============================] - 0s 350us/step - loss: 0.2522 - accuracy: 0.8958\n",
      "Epoch 95/500\n",
      "87/87 [==============================] - 0s 326us/step - loss: 0.2487 - accuracy: 0.9028\n",
      "Epoch 96/500\n",
      "87/87 [==============================] - 0s 334us/step - loss: 0.2425 - accuracy: 0.9051\n",
      "Epoch 97/500\n",
      "87/87 [==============================] - 0s 341us/step - loss: 0.2507 - accuracy: 0.8958\n",
      "Epoch 98/500\n",
      "87/87 [==============================] - 0s 342us/step - loss: 0.2500 - accuracy: 0.8981\n",
      "Epoch 99/500\n",
      "87/87 [==============================] - 0s 365us/step - loss: 0.2504 - accuracy: 0.8981\n",
      "Epoch 100/500\n",
      "87/87 [==============================] - 0s 341us/step - loss: 0.2534 - accuracy: 0.8981\n",
      "Epoch 101/500\n",
      "87/87 [==============================] - 0s 336us/step - loss: 0.2485 - accuracy: 0.9005\n",
      "Epoch 102/500\n",
      "87/87 [==============================] - 0s 346us/step - loss: 0.2480 - accuracy: 0.8912\n",
      "Epoch 103/500\n",
      "87/87 [==============================] - 0s 358us/step - loss: 0.2558 - accuracy: 0.8866\n",
      "Epoch 104/500\n",
      "87/87 [==============================] - 0s 350us/step - loss: 0.2375 - accuracy: 0.9051\n",
      "Epoch 105/500\n",
      "87/87 [==============================] - 0s 333us/step - loss: 0.2506 - accuracy: 0.9005\n",
      "Epoch 106/500\n",
      "87/87 [==============================] - 0s 345us/step - loss: 0.2410 - accuracy: 0.8981\n",
      "Epoch 107/500\n",
      "87/87 [==============================] - 0s 347us/step - loss: 0.2437 - accuracy: 0.9097\n",
      "Epoch 108/500\n",
      "87/87 [==============================] - 0s 333us/step - loss: 0.2536 - accuracy: 0.8958\n",
      "Epoch 109/500\n",
      "87/87 [==============================] - 0s 333us/step - loss: 0.2424 - accuracy: 0.8912\n",
      "Epoch 110/500\n",
      "87/87 [==============================] - 0s 356us/step - loss: 0.2433 - accuracy: 0.8912\n",
      "Epoch 111/500\n",
      "87/87 [==============================] - 0s 342us/step - loss: 0.2450 - accuracy: 0.8866\n",
      "Epoch 112/500\n",
      "87/87 [==============================] - 0s 338us/step - loss: 0.2442 - accuracy: 0.8935\n",
      "Epoch 113/500\n",
      "87/87 [==============================] - 0s 353us/step - loss: 0.2387 - accuracy: 0.8958\n",
      "Epoch 114/500\n",
      "87/87 [==============================] - 0s 344us/step - loss: 0.2477 - accuracy: 0.9005\n",
      "Epoch 115/500\n",
      "87/87 [==============================] - 0s 351us/step - loss: 0.2431 - accuracy: 0.9097\n",
      "Epoch 116/500\n",
      "87/87 [==============================] - 0s 338us/step - loss: 0.2349 - accuracy: 0.9028\n",
      "Epoch 117/500\n",
      "87/87 [==============================] - 0s 349us/step - loss: 0.2361 - accuracy: 0.9028\n",
      "Epoch 118/500\n",
      "87/87 [==============================] - 0s 344us/step - loss: 0.2332 - accuracy: 0.9005\n",
      "Epoch 119/500\n",
      "87/87 [==============================] - 0s 363us/step - loss: 0.2371 - accuracy: 0.9074\n",
      "Epoch 120/500\n",
      "87/87 [==============================] - 0s 346us/step - loss: 0.2487 - accuracy: 0.8912\n",
      "Epoch 121/500\n",
      "87/87 [==============================] - 0s 350us/step - loss: 0.2416 - accuracy: 0.8889\n",
      "Epoch 122/500\n",
      "87/87 [==============================] - 0s 353us/step - loss: 0.2368 - accuracy: 0.9051\n",
      "Epoch 123/500\n",
      "87/87 [==============================] - 0s 343us/step - loss: 0.2344 - accuracy: 0.9028\n",
      "Epoch 124/500\n",
      "87/87 [==============================] - 0s 348us/step - loss: 0.2279 - accuracy: 0.9028\n",
      "Epoch 125/500\n",
      "87/87 [==============================] - 0s 342us/step - loss: 0.2298 - accuracy: 0.9028\n",
      "Epoch 126/500\n",
      "87/87 [==============================] - 0s 346us/step - loss: 0.2444 - accuracy: 0.8912\n",
      "Epoch 127/500\n",
      "87/87 [==============================] - 0s 348us/step - loss: 0.2320 - accuracy: 0.8935\n",
      "Epoch 128/500\n",
      "87/87 [==============================] - 0s 349us/step - loss: 0.2426 - accuracy: 0.8981\n",
      "Epoch 129/500\n",
      "87/87 [==============================] - 0s 346us/step - loss: 0.2268 - accuracy: 0.9028\n",
      "Epoch 130/500\n",
      "87/87 [==============================] - 0s 335us/step - loss: 0.2317 - accuracy: 0.8958\n",
      "Epoch 131/500\n",
      "87/87 [==============================] - 0s 345us/step - loss: 0.2345 - accuracy: 0.8935\n",
      "Epoch 132/500\n",
      "87/87 [==============================] - 0s 355us/step - loss: 0.2394 - accuracy: 0.8958\n",
      "Epoch 133/500\n",
      "87/87 [==============================] - 0s 337us/step - loss: 0.2324 - accuracy: 0.9167\n",
      "Epoch 134/500\n",
      "87/87 [==============================] - 0s 343us/step - loss: 0.2416 - accuracy: 0.8796\n",
      "Epoch 135/500\n",
      "87/87 [==============================] - 0s 341us/step - loss: 0.2269 - accuracy: 0.9028\n",
      "Epoch 136/500\n",
      "87/87 [==============================] - 0s 329us/step - loss: 0.2306 - accuracy: 0.9074\n",
      "Epoch 137/500\n",
      "87/87 [==============================] - 0s 330us/step - loss: 0.2366 - accuracy: 0.9051\n",
      "Epoch 138/500\n",
      "87/87 [==============================] - 0s 352us/step - loss: 0.2253 - accuracy: 0.9097\n",
      "Epoch 139/500\n",
      "87/87 [==============================] - 0s 323us/step - loss: 0.2315 - accuracy: 0.8981\n",
      "Epoch 140/500\n",
      "87/87 [==============================] - 0s 338us/step - loss: 0.2252 - accuracy: 0.8958\n",
      "Epoch 141/500\n",
      "87/87 [==============================] - 0s 340us/step - loss: 0.2267 - accuracy: 0.9097\n",
      "Epoch 142/500\n",
      "87/87 [==============================] - 0s 336us/step - loss: 0.2315 - accuracy: 0.9051\n",
      "Epoch 143/500\n",
      "87/87 [==============================] - 0s 333us/step - loss: 0.2344 - accuracy: 0.9005\n",
      "Epoch 144/500\n",
      "87/87 [==============================] - 0s 343us/step - loss: 0.2242 - accuracy: 0.9051\n",
      "Epoch 145/500\n",
      "87/87 [==============================] - 0s 356us/step - loss: 0.2281 - accuracy: 0.9097\n",
      "Epoch 146/500\n",
      "87/87 [==============================] - 0s 362us/step - loss: 0.2229 - accuracy: 0.9051\n",
      "Epoch 147/500\n",
      "87/87 [==============================] - 0s 339us/step - loss: 0.2245 - accuracy: 0.9144\n",
      "Epoch 148/500\n",
      "87/87 [==============================] - 0s 343us/step - loss: 0.2271 - accuracy: 0.9097\n",
      "Epoch 149/500\n",
      "87/87 [==============================] - 0s 350us/step - loss: 0.2227 - accuracy: 0.9074\n",
      "Epoch 150/500\n",
      "87/87 [==============================] - 0s 354us/step - loss: 0.2350 - accuracy: 0.9028\n",
      "Epoch 151/500\n",
      "87/87 [==============================] - 0s 339us/step - loss: 0.2381 - accuracy: 0.9074\n",
      "Epoch 152/500\n",
      "87/87 [==============================] - 0s 330us/step - loss: 0.2241 - accuracy: 0.9051\n",
      "Epoch 153/500\n",
      "87/87 [==============================] - 0s 345us/step - loss: 0.2199 - accuracy: 0.9120\n",
      "Epoch 154/500\n",
      "87/87 [==============================] - 0s 328us/step - loss: 0.2425 - accuracy: 0.9005\n",
      "Epoch 155/500\n",
      "87/87 [==============================] - 0s 324us/step - loss: 0.2173 - accuracy: 0.9028\n",
      "Epoch 156/500\n",
      "87/87 [==============================] - 0s 335us/step - loss: 0.2296 - accuracy: 0.9051\n",
      "Epoch 157/500\n",
      "87/87 [==============================] - 0s 334us/step - loss: 0.2222 - accuracy: 0.9051\n",
      "Epoch 158/500\n",
      "87/87 [==============================] - 0s 328us/step - loss: 0.2172 - accuracy: 0.9259\n",
      "Epoch 159/500\n",
      "87/87 [==============================] - 0s 335us/step - loss: 0.2190 - accuracy: 0.9074\n",
      "Epoch 160/500\n",
      "87/87 [==============================] - 0s 321us/step - loss: 0.2390 - accuracy: 0.8981\n",
      "Epoch 161/500\n",
      "87/87 [==============================] - 0s 341us/step - loss: 0.2302 - accuracy: 0.8958\n",
      "Epoch 162/500\n",
      "87/87 [==============================] - 0s 343us/step - loss: 0.2199 - accuracy: 0.9051\n",
      "Epoch 163/500\n",
      "87/87 [==============================] - 0s 342us/step - loss: 0.2191 - accuracy: 0.9028\n",
      "Epoch 164/500\n",
      "87/87 [==============================] - 0s 347us/step - loss: 0.2212 - accuracy: 0.9074\n",
      "Epoch 165/500\n",
      "87/87 [==============================] - 0s 342us/step - loss: 0.2233 - accuracy: 0.8981\n",
      "Epoch 166/500\n",
      "87/87 [==============================] - 0s 341us/step - loss: 0.2272 - accuracy: 0.9051\n",
      "Epoch 167/500\n",
      "87/87 [==============================] - 0s 345us/step - loss: 0.2178 - accuracy: 0.9051\n",
      "Epoch 168/500\n",
      "87/87 [==============================] - 0s 337us/step - loss: 0.2215 - accuracy: 0.9190\n",
      "Epoch 169/500\n",
      "87/87 [==============================] - 0s 325us/step - loss: 0.2340 - accuracy: 0.8981\n",
      "Epoch 170/500\n",
      "87/87 [==============================] - 0s 346us/step - loss: 0.2427 - accuracy: 0.8981\n",
      "Epoch 171/500\n",
      "87/87 [==============================] - 0s 355us/step - loss: 0.2354 - accuracy: 0.9028\n",
      "Epoch 172/500\n",
      "87/87 [==============================] - 0s 327us/step - loss: 0.2245 - accuracy: 0.9051\n",
      "Epoch 173/500\n",
      "87/87 [==============================] - 0s 349us/step - loss: 0.2176 - accuracy: 0.9144\n",
      "Epoch 174/500\n",
      "87/87 [==============================] - 0s 326us/step - loss: 0.2140 - accuracy: 0.9120\n",
      "Epoch 175/500\n",
      "87/87 [==============================] - 0s 331us/step - loss: 0.2186 - accuracy: 0.8981\n",
      "Epoch 176/500\n",
      "87/87 [==============================] - 0s 336us/step - loss: 0.2070 - accuracy: 0.9213\n",
      "Epoch 177/500\n",
      "87/87 [==============================] - 0s 340us/step - loss: 0.2299 - accuracy: 0.9028\n",
      "Epoch 178/500\n",
      "87/87 [==============================] - 0s 340us/step - loss: 0.2112 - accuracy: 0.9213\n",
      "Epoch 179/500\n",
      "87/87 [==============================] - 0s 338us/step - loss: 0.2161 - accuracy: 0.9051\n",
      "Epoch 180/500\n",
      "87/87 [==============================] - 0s 339us/step - loss: 0.2209 - accuracy: 0.9028\n",
      "Epoch 181/500\n",
      "87/87 [==============================] - 0s 328us/step - loss: 0.2077 - accuracy: 0.9028\n",
      "Epoch 182/500\n",
      "87/87 [==============================] - 0s 336us/step - loss: 0.2195 - accuracy: 0.9190\n",
      "Epoch 183/500\n",
      "87/87 [==============================] - 0s 328us/step - loss: 0.2272 - accuracy: 0.9074\n",
      "Epoch 184/500\n",
      "87/87 [==============================] - 0s 331us/step - loss: 0.2173 - accuracy: 0.9120\n",
      "Epoch 185/500\n",
      "87/87 [==============================] - 0s 378us/step - loss: 0.2119 - accuracy: 0.9051\n",
      "Epoch 186/500\n",
      "87/87 [==============================] - 0s 335us/step - loss: 0.2168 - accuracy: 0.9167\n",
      "Epoch 187/500\n",
      "87/87 [==============================] - 0s 333us/step - loss: 0.2110 - accuracy: 0.9190\n",
      "Epoch 188/500\n",
      "87/87 [==============================] - 0s 406us/step - loss: 0.2164 - accuracy: 0.9097\n",
      "Epoch 189/500\n",
      "87/87 [==============================] - 0s 325us/step - loss: 0.2182 - accuracy: 0.9190\n",
      "Epoch 190/500\n",
      "87/87 [==============================] - 0s 323us/step - loss: 0.2293 - accuracy: 0.8935\n",
      "Epoch 191/500\n",
      "87/87 [==============================] - 0s 363us/step - loss: 0.2101 - accuracy: 0.9074\n",
      "Epoch 192/500\n",
      "87/87 [==============================] - 0s 334us/step - loss: 0.2112 - accuracy: 0.9167\n",
      "Epoch 193/500\n",
      "87/87 [==============================] - 0s 335us/step - loss: 0.2125 - accuracy: 0.9120\n",
      "Epoch 194/500\n",
      "87/87 [==============================] - 0s 342us/step - loss: 0.2114 - accuracy: 0.9144\n",
      "Epoch 195/500\n",
      "87/87 [==============================] - 0s 330us/step - loss: 0.2204 - accuracy: 0.9120\n",
      "Epoch 196/500\n",
      "87/87 [==============================] - 0s 355us/step - loss: 0.2326 - accuracy: 0.9097\n",
      "Epoch 197/500\n",
      "87/87 [==============================] - 0s 347us/step - loss: 0.2117 - accuracy: 0.9144\n",
      "Epoch 198/500\n",
      "87/87 [==============================] - 0s 334us/step - loss: 0.2149 - accuracy: 0.9074\n",
      "Epoch 199/500\n",
      "87/87 [==============================] - 0s 334us/step - loss: 0.2106 - accuracy: 0.9097\n",
      "Epoch 200/500\n",
      "87/87 [==============================] - 0s 340us/step - loss: 0.2149 - accuracy: 0.9051\n",
      "Epoch 201/500\n",
      "87/87 [==============================] - 0s 347us/step - loss: 0.2069 - accuracy: 0.9097\n",
      "Epoch 202/500\n",
      "87/87 [==============================] - 0s 331us/step - loss: 0.2050 - accuracy: 0.9213\n",
      "Epoch 203/500\n",
      "87/87 [==============================] - 0s 351us/step - loss: 0.2138 - accuracy: 0.9144\n",
      "Epoch 204/500\n",
      "87/87 [==============================] - 0s 334us/step - loss: 0.2157 - accuracy: 0.9167\n",
      "Epoch 205/500\n",
      "87/87 [==============================] - 0s 336us/step - loss: 0.2158 - accuracy: 0.9097\n",
      "Epoch 206/500\n",
      "87/87 [==============================] - 0s 357us/step - loss: 0.2028 - accuracy: 0.9144\n",
      "Epoch 207/500\n",
      "87/87 [==============================] - 0s 322us/step - loss: 0.2079 - accuracy: 0.9190\n",
      "Epoch 208/500\n",
      "87/87 [==============================] - 0s 339us/step - loss: 0.2171 - accuracy: 0.9144\n",
      "Epoch 209/500\n",
      "87/87 [==============================] - 0s 329us/step - loss: 0.2009 - accuracy: 0.9167\n",
      "Epoch 210/500\n",
      "87/87 [==============================] - 0s 361us/step - loss: 0.2170 - accuracy: 0.9120\n",
      "Epoch 211/500\n",
      "87/87 [==============================] - 0s 337us/step - loss: 0.2123 - accuracy: 0.9028\n",
      "Epoch 212/500\n",
      "87/87 [==============================] - 0s 349us/step - loss: 0.2143 - accuracy: 0.9190\n",
      "Epoch 213/500\n",
      "87/87 [==============================] - 0s 342us/step - loss: 0.2200 - accuracy: 0.9120\n",
      "Epoch 214/500\n",
      "87/87 [==============================] - 0s 355us/step - loss: 0.2090 - accuracy: 0.9236\n",
      "Epoch 215/500\n",
      "87/87 [==============================] - 0s 344us/step - loss: 0.2030 - accuracy: 0.9167\n",
      "Epoch 216/500\n",
      "87/87 [==============================] - 0s 343us/step - loss: 0.2147 - accuracy: 0.9259\n",
      "Epoch 217/500\n",
      "87/87 [==============================] - 0s 337us/step - loss: 0.2100 - accuracy: 0.9190\n",
      "Epoch 218/500\n",
      " 1/87 [..............................] - ETA: 0s - loss: 0.4248 - accuracy: 0.8000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6d5d31a5d553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mX_cross_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cross_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cross_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cross_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cross_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpredictions_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "BATCH_SIZE = 5\n",
    "num_folds = 10\n",
    "\n",
    "\n",
    "kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "cv_results = np.array([])\n",
    "for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "    X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "    X_cross_train = scaler.fit_transform(X_cross_train)\n",
    "    X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "    X_cross_test = scaler.transform(X_cross_test)\n",
    "    model = getNetwork()\n",
    "    model.fit(X_cross_train, y_cross_train, epochs=EPOCHS, batch_size=BATCH_SIZE)  \n",
    "    y_pred = model.predict(X_cross_test)\n",
    "    predictions_categorical = np.argmax(y_pred, axis=1)\n",
    "    f1s = f1_score(y_cross_test, predictions_categorical, average=\"weighted\")\n",
    "    cv_results = np.append(cv_results, [f1s])\n",
    "\n",
    "print(f'Average score of Cross Validation: {cv_results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               1536      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 2,337\n",
      "Trainable params: 2,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/750\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 1.0525 - accuracy: 0.6806 - val_loss: 0.7165 - val_accuracy: 0.8083\n",
      "Epoch 2/750\n",
      "72/72 [==============================] - 0s 543us/step - loss: 0.6842 - accuracy: 0.7750 - val_loss: 0.5988 - val_accuracy: 0.8333\n",
      "Epoch 3/750\n",
      "72/72 [==============================] - 0s 527us/step - loss: 0.6075 - accuracy: 0.7806 - val_loss: 0.5188 - val_accuracy: 0.8250\n",
      "Epoch 4/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.5571 - accuracy: 0.7917 - val_loss: 0.4764 - val_accuracy: 0.8250\n",
      "Epoch 5/750\n",
      "72/72 [==============================] - 0s 533us/step - loss: 0.5295 - accuracy: 0.7944 - val_loss: 0.4554 - val_accuracy: 0.8333\n",
      "Epoch 6/750\n",
      "72/72 [==============================] - 0s 504us/step - loss: 0.5095 - accuracy: 0.7944 - val_loss: 0.4368 - val_accuracy: 0.8250\n",
      "Epoch 7/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.4877 - accuracy: 0.8000 - val_loss: 0.4442 - val_accuracy: 0.8333\n",
      "Epoch 8/750\n",
      "72/72 [==============================] - 0s 524us/step - loss: 0.4772 - accuracy: 0.7972 - val_loss: 0.4222 - val_accuracy: 0.8250\n",
      "Epoch 9/750\n",
      "72/72 [==============================] - 0s 523us/step - loss: 0.4669 - accuracy: 0.8000 - val_loss: 0.4053 - val_accuracy: 0.8250\n",
      "Epoch 10/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.4606 - accuracy: 0.8000 - val_loss: 0.3967 - val_accuracy: 0.8250\n",
      "Epoch 11/750\n",
      "72/72 [==============================] - 0s 529us/step - loss: 0.4582 - accuracy: 0.8028 - val_loss: 0.3869 - val_accuracy: 0.8250\n",
      "Epoch 12/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.4487 - accuracy: 0.7972 - val_loss: 0.3894 - val_accuracy: 0.8250\n",
      "Epoch 13/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.4454 - accuracy: 0.7944 - val_loss: 0.4045 - val_accuracy: 0.8250\n",
      "Epoch 14/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.4369 - accuracy: 0.8056 - val_loss: 0.3679 - val_accuracy: 0.8750\n",
      "Epoch 15/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.4346 - accuracy: 0.8361 - val_loss: 0.3945 - val_accuracy: 0.8333\n",
      "Epoch 16/750\n",
      "72/72 [==============================] - 0s 530us/step - loss: 0.4337 - accuracy: 0.8083 - val_loss: 0.3611 - val_accuracy: 0.8833\n",
      "Epoch 17/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.4326 - accuracy: 0.8111 - val_loss: 0.3590 - val_accuracy: 0.8500\n",
      "Epoch 18/750\n",
      "72/72 [==============================] - 0s 523us/step - loss: 0.4272 - accuracy: 0.8139 - val_loss: 0.3766 - val_accuracy: 0.8333\n",
      "Epoch 19/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.4263 - accuracy: 0.8250 - val_loss: 0.3644 - val_accuracy: 0.8833\n",
      "Epoch 20/750\n",
      "72/72 [==============================] - 0s 567us/step - loss: 0.4254 - accuracy: 0.8333 - val_loss: 0.3551 - val_accuracy: 0.8333\n",
      "Epoch 21/750\n",
      "72/72 [==============================] - 0s 546us/step - loss: 0.4166 - accuracy: 0.8056 - val_loss: 0.3352 - val_accuracy: 0.8750\n",
      "Epoch 22/750\n",
      "72/72 [==============================] - 0s 500us/step - loss: 0.4237 - accuracy: 0.8333 - val_loss: 0.3400 - val_accuracy: 0.8750\n",
      "Epoch 23/750\n",
      "72/72 [==============================] - 0s 525us/step - loss: 0.4106 - accuracy: 0.8083 - val_loss: 0.3504 - val_accuracy: 0.8667\n",
      "Epoch 24/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.4041 - accuracy: 0.8306 - val_loss: 0.3375 - val_accuracy: 0.8750\n",
      "Epoch 25/750\n",
      "72/72 [==============================] - 0s 503us/step - loss: 0.3994 - accuracy: 0.8444 - val_loss: 0.3273 - val_accuracy: 0.8667\n",
      "Epoch 26/750\n",
      "72/72 [==============================] - 0s 531us/step - loss: 0.4024 - accuracy: 0.8500 - val_loss: 0.3473 - val_accuracy: 0.8667\n",
      "Epoch 27/750\n",
      "72/72 [==============================] - 0s 528us/step - loss: 0.4066 - accuracy: 0.8306 - val_loss: 0.3475 - val_accuracy: 0.8917\n",
      "Epoch 28/750\n",
      "72/72 [==============================] - 0s 537us/step - loss: 0.3902 - accuracy: 0.8444 - val_loss: 0.3258 - val_accuracy: 0.8667\n",
      "Epoch 29/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.3913 - accuracy: 0.8444 - val_loss: 0.3222 - val_accuracy: 0.8833\n",
      "Epoch 30/750\n",
      "72/72 [==============================] - 0s 534us/step - loss: 0.3922 - accuracy: 0.8389 - val_loss: 0.3058 - val_accuracy: 0.8750\n",
      "Epoch 31/750\n",
      "72/72 [==============================] - 0s 531us/step - loss: 0.3875 - accuracy: 0.8500 - val_loss: 0.3345 - val_accuracy: 0.8833\n",
      "Epoch 32/750\n",
      "72/72 [==============================] - 0s 540us/step - loss: 0.3836 - accuracy: 0.8556 - val_loss: 0.3168 - val_accuracy: 0.8917\n",
      "Epoch 33/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.3810 - accuracy: 0.8500 - val_loss: 0.3310 - val_accuracy: 0.8750\n",
      "Epoch 34/750\n",
      "72/72 [==============================] - 0s 520us/step - loss: 0.3820 - accuracy: 0.8500 - val_loss: 0.3100 - val_accuracy: 0.8833\n",
      "Epoch 35/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.3737 - accuracy: 0.8472 - val_loss: 0.3085 - val_accuracy: 0.8833\n",
      "Epoch 36/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.3845 - accuracy: 0.8472 - val_loss: 0.2994 - val_accuracy: 0.8833\n",
      "Epoch 37/750\n",
      "72/72 [==============================] - 0s 539us/step - loss: 0.3737 - accuracy: 0.8472 - val_loss: 0.2890 - val_accuracy: 0.8833\n",
      "Epoch 38/750\n",
      "72/72 [==============================] - 0s 537us/step - loss: 0.3712 - accuracy: 0.8667 - val_loss: 0.3059 - val_accuracy: 0.8750\n",
      "Epoch 39/750\n",
      "72/72 [==============================] - 0s 595us/step - loss: 0.3711 - accuracy: 0.8472 - val_loss: 0.3068 - val_accuracy: 0.8750\n",
      "Epoch 40/750\n",
      "72/72 [==============================] - 0s 546us/step - loss: 0.3623 - accuracy: 0.8556 - val_loss: 0.3078 - val_accuracy: 0.8917\n",
      "Epoch 41/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.3620 - accuracy: 0.8556 - val_loss: 0.2828 - val_accuracy: 0.8833\n",
      "Epoch 42/750\n",
      "72/72 [==============================] - 0s 492us/step - loss: 0.3612 - accuracy: 0.8611 - val_loss: 0.3131 - val_accuracy: 0.8583\n",
      "Epoch 43/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.3549 - accuracy: 0.8556 - val_loss: 0.2848 - val_accuracy: 0.8750\n",
      "Epoch 44/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.3548 - accuracy: 0.8444 - val_loss: 0.2901 - val_accuracy: 0.8583\n",
      "Epoch 45/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.3530 - accuracy: 0.8639 - val_loss: 0.2844 - val_accuracy: 0.9250\n",
      "Epoch 46/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.3496 - accuracy: 0.8583 - val_loss: 0.2832 - val_accuracy: 0.8917\n",
      "Epoch 47/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.3473 - accuracy: 0.8611 - val_loss: 0.2839 - val_accuracy: 0.8833\n",
      "Epoch 48/750\n",
      "72/72 [==============================] - 0s 503us/step - loss: 0.3474 - accuracy: 0.8583 - val_loss: 0.2907 - val_accuracy: 0.8917\n",
      "Epoch 49/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.3410 - accuracy: 0.8611 - val_loss: 0.2654 - val_accuracy: 0.9250\n",
      "Epoch 50/750\n",
      "72/72 [==============================] - 0s 531us/step - loss: 0.3503 - accuracy: 0.8556 - val_loss: 0.2752 - val_accuracy: 0.8833\n",
      "Epoch 51/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.3549 - accuracy: 0.8472 - val_loss: 0.2902 - val_accuracy: 0.8833\n",
      "Epoch 52/750\n",
      "72/72 [==============================] - 0s 532us/step - loss: 0.3329 - accuracy: 0.8778 - val_loss: 0.2557 - val_accuracy: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.3388 - accuracy: 0.8611 - val_loss: 0.2769 - val_accuracy: 0.8500\n",
      "Epoch 54/750\n",
      "72/72 [==============================] - 0s 499us/step - loss: 0.3394 - accuracy: 0.8667 - val_loss: 0.2857 - val_accuracy: 0.9167\n",
      "Epoch 55/750\n",
      "72/72 [==============================] - 0s 531us/step - loss: 0.3314 - accuracy: 0.8583 - val_loss: 0.2492 - val_accuracy: 0.8833\n",
      "Epoch 56/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.3405 - accuracy: 0.8694 - val_loss: 0.2674 - val_accuracy: 0.9167\n",
      "Epoch 57/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.3303 - accuracy: 0.8583 - val_loss: 0.2724 - val_accuracy: 0.8833\n",
      "Epoch 58/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.3371 - accuracy: 0.8611 - val_loss: 0.2739 - val_accuracy: 0.9167\n",
      "Epoch 59/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.3251 - accuracy: 0.8583 - val_loss: 0.2679 - val_accuracy: 0.9083\n",
      "Epoch 60/750\n",
      "72/72 [==============================] - 0s 525us/step - loss: 0.3381 - accuracy: 0.8667 - val_loss: 0.2498 - val_accuracy: 0.9000\n",
      "Epoch 61/750\n",
      "72/72 [==============================] - 0s 545us/step - loss: 0.3433 - accuracy: 0.8611 - val_loss: 0.2648 - val_accuracy: 0.9250\n",
      "Epoch 62/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.3291 - accuracy: 0.8611 - val_loss: 0.2490 - val_accuracy: 0.9167\n",
      "Epoch 63/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.3202 - accuracy: 0.8639 - val_loss: 0.2334 - val_accuracy: 0.9333\n",
      "Epoch 64/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.3174 - accuracy: 0.8694 - val_loss: 0.2484 - val_accuracy: 0.9250\n",
      "Epoch 65/750\n",
      "72/72 [==============================] - 0s 500us/step - loss: 0.3187 - accuracy: 0.8694 - val_loss: 0.2852 - val_accuracy: 0.9167\n",
      "Epoch 66/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.3195 - accuracy: 0.8639 - val_loss: 0.2457 - val_accuracy: 0.9333\n",
      "Epoch 67/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.3109 - accuracy: 0.8694 - val_loss: 0.2466 - val_accuracy: 0.9083\n",
      "Epoch 68/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.3101 - accuracy: 0.8639 - val_loss: 0.2468 - val_accuracy: 0.9167\n",
      "Epoch 69/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.3096 - accuracy: 0.8639 - val_loss: 0.2512 - val_accuracy: 0.9167\n",
      "Epoch 70/750\n",
      "72/72 [==============================] - 0s 494us/step - loss: 0.3117 - accuracy: 0.8611 - val_loss: 0.2443 - val_accuracy: 0.9167\n",
      "Epoch 71/750\n",
      "72/72 [==============================] - 0s 524us/step - loss: 0.3039 - accuracy: 0.8722 - val_loss: 0.2378 - val_accuracy: 0.9417\n",
      "Epoch 72/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.3182 - accuracy: 0.8694 - val_loss: 0.2523 - val_accuracy: 0.9250\n",
      "Epoch 73/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.3069 - accuracy: 0.8611 - val_loss: 0.2180 - val_accuracy: 0.9167\n",
      "Epoch 74/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.3028 - accuracy: 0.8806 - val_loss: 0.2399 - val_accuracy: 0.9167\n",
      "Epoch 75/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.2990 - accuracy: 0.8694 - val_loss: 0.2261 - val_accuracy: 0.9167\n",
      "Epoch 76/750\n",
      "72/72 [==============================] - 0s 504us/step - loss: 0.3096 - accuracy: 0.8694 - val_loss: 0.2275 - val_accuracy: 0.9250\n",
      "Epoch 77/750\n",
      "72/72 [==============================] - 0s 503us/step - loss: 0.2961 - accuracy: 0.8861 - val_loss: 0.2288 - val_accuracy: 0.9167\n",
      "Epoch 78/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.3101 - accuracy: 0.8667 - val_loss: 0.2588 - val_accuracy: 0.9250\n",
      "Epoch 79/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.2933 - accuracy: 0.8667 - val_loss: 0.2199 - val_accuracy: 0.9250\n",
      "Epoch 80/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.2944 - accuracy: 0.8722 - val_loss: 0.2497 - val_accuracy: 0.8917\n",
      "Epoch 81/750\n",
      "72/72 [==============================] - 0s 540us/step - loss: 0.2908 - accuracy: 0.8694 - val_loss: 0.2364 - val_accuracy: 0.9000\n",
      "Epoch 82/750\n",
      "72/72 [==============================] - 0s 499us/step - loss: 0.2895 - accuracy: 0.8722 - val_loss: 0.2305 - val_accuracy: 0.9083\n",
      "Epoch 83/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.2931 - accuracy: 0.8722 - val_loss: 0.2243 - val_accuracy: 0.9250\n",
      "Epoch 84/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2971 - accuracy: 0.8639 - val_loss: 0.2446 - val_accuracy: 0.9250\n",
      "Epoch 85/750\n",
      "72/72 [==============================] - 0s 510us/step - loss: 0.2823 - accuracy: 0.8722 - val_loss: 0.2313 - val_accuracy: 0.9167\n",
      "Epoch 86/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.2917 - accuracy: 0.8722 - val_loss: 0.2279 - val_accuracy: 0.9250\n",
      "Epoch 87/750\n",
      "72/72 [==============================] - 0s 535us/step - loss: 0.2933 - accuracy: 0.8722 - val_loss: 0.2272 - val_accuracy: 0.9083\n",
      "Epoch 88/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.2932 - accuracy: 0.8639 - val_loss: 0.2272 - val_accuracy: 0.9083\n",
      "Epoch 89/750\n",
      "72/72 [==============================] - 0s 497us/step - loss: 0.2951 - accuracy: 0.8694 - val_loss: 0.2234 - val_accuracy: 0.9333\n",
      "Epoch 90/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.2984 - accuracy: 0.8806 - val_loss: 0.2195 - val_accuracy: 0.9083\n",
      "Epoch 91/750\n",
      "72/72 [==============================] - 0s 530us/step - loss: 0.2859 - accuracy: 0.8639 - val_loss: 0.2167 - val_accuracy: 0.8917\n",
      "Epoch 92/750\n",
      "72/72 [==============================] - 0s 511us/step - loss: 0.2900 - accuracy: 0.8722 - val_loss: 0.2185 - val_accuracy: 0.9167\n",
      "Epoch 93/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.2949 - accuracy: 0.8861 - val_loss: 0.2077 - val_accuracy: 0.9333\n",
      "Epoch 94/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.2832 - accuracy: 0.8833 - val_loss: 0.2107 - val_accuracy: 0.9250\n",
      "Epoch 95/750\n",
      "72/72 [==============================] - 0s 496us/step - loss: 0.2773 - accuracy: 0.8917 - val_loss: 0.2141 - val_accuracy: 0.9167\n",
      "Epoch 96/750\n",
      "72/72 [==============================] - 0s 504us/step - loss: 0.2866 - accuracy: 0.8750 - val_loss: 0.2519 - val_accuracy: 0.9167\n",
      "Epoch 97/750\n",
      "72/72 [==============================] - 0s 496us/step - loss: 0.2969 - accuracy: 0.8778 - val_loss: 0.1971 - val_accuracy: 0.9167\n",
      "Epoch 98/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.2895 - accuracy: 0.8750 - val_loss: 0.2140 - val_accuracy: 0.9083\n",
      "Epoch 99/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2748 - accuracy: 0.8833 - val_loss: 0.2067 - val_accuracy: 0.9167\n",
      "Epoch 100/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.2719 - accuracy: 0.8750 - val_loss: 0.2230 - val_accuracy: 0.9250\n",
      "Epoch 101/750\n",
      "72/72 [==============================] - 0s 523us/step - loss: 0.2962 - accuracy: 0.8889 - val_loss: 0.2088 - val_accuracy: 0.9417\n",
      "Epoch 102/750\n",
      "72/72 [==============================] - 0s 499us/step - loss: 0.2872 - accuracy: 0.8750 - val_loss: 0.2387 - val_accuracy: 0.9333\n",
      "Epoch 103/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.2803 - accuracy: 0.8694 - val_loss: 0.2059 - val_accuracy: 0.9333\n",
      "Epoch 104/750\n",
      "72/72 [==============================] - 0s 596us/step - loss: 0.2751 - accuracy: 0.8806 - val_loss: 0.2347 - val_accuracy: 0.9167\n",
      "Epoch 105/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.2876 - accuracy: 0.8806 - val_loss: 0.2269 - val_accuracy: 0.9083\n",
      "Epoch 106/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.2770 - accuracy: 0.8861 - val_loss: 0.2094 - val_accuracy: 0.9333\n",
      "Epoch 107/750\n",
      "72/72 [==============================] - 0s 522us/step - loss: 0.2729 - accuracy: 0.8833 - val_loss: 0.2081 - val_accuracy: 0.9250\n",
      "Epoch 108/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.2691 - accuracy: 0.8917 - val_loss: 0.2050 - val_accuracy: 0.9333\n",
      "Epoch 109/750\n",
      "72/72 [==============================] - 0s 524us/step - loss: 0.2805 - accuracy: 0.8778 - val_loss: 0.1926 - val_accuracy: 0.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.2776 - accuracy: 0.8917 - val_loss: 0.2167 - val_accuracy: 0.9250\n",
      "Epoch 111/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.2742 - accuracy: 0.8778 - val_loss: 0.2106 - val_accuracy: 0.9167\n",
      "Epoch 112/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.2770 - accuracy: 0.8833 - val_loss: 0.2111 - val_accuracy: 0.9333\n",
      "Epoch 113/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.2716 - accuracy: 0.8861 - val_loss: 0.2251 - val_accuracy: 0.9167\n",
      "Epoch 114/750\n",
      "72/72 [==============================] - 0s 526us/step - loss: 0.2689 - accuracy: 0.8972 - val_loss: 0.2141 - val_accuracy: 0.9167\n",
      "Epoch 115/750\n",
      "72/72 [==============================] - 0s 532us/step - loss: 0.2707 - accuracy: 0.8750 - val_loss: 0.2040 - val_accuracy: 0.9250\n",
      "Epoch 116/750\n",
      "72/72 [==============================] - 0s 535us/step - loss: 0.2851 - accuracy: 0.8889 - val_loss: 0.2328 - val_accuracy: 0.9167\n",
      "Epoch 117/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.2623 - accuracy: 0.8917 - val_loss: 0.2156 - val_accuracy: 0.9000\n",
      "Epoch 118/750\n",
      "72/72 [==============================] - 0s 540us/step - loss: 0.2745 - accuracy: 0.8944 - val_loss: 0.2080 - val_accuracy: 0.9333\n",
      "Epoch 119/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.2657 - accuracy: 0.8917 - val_loss: 0.2032 - val_accuracy: 0.9333\n",
      "Epoch 120/750\n",
      "72/72 [==============================] - 0s 525us/step - loss: 0.2783 - accuracy: 0.8694 - val_loss: 0.2113 - val_accuracy: 0.9167\n",
      "Epoch 121/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.2682 - accuracy: 0.8833 - val_loss: 0.1835 - val_accuracy: 0.9333\n",
      "Epoch 122/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.2648 - accuracy: 0.8889 - val_loss: 0.2071 - val_accuracy: 0.9167\n",
      "Epoch 123/750\n",
      "72/72 [==============================] - 0s 526us/step - loss: 0.2816 - accuracy: 0.8833 - val_loss: 0.2066 - val_accuracy: 0.9250\n",
      "Epoch 124/750\n",
      "72/72 [==============================] - 0s 537us/step - loss: 0.2737 - accuracy: 0.8722 - val_loss: 0.1925 - val_accuracy: 0.9417\n",
      "Epoch 125/750\n",
      "72/72 [==============================] - 0s 545us/step - loss: 0.2688 - accuracy: 0.8833 - val_loss: 0.2146 - val_accuracy: 0.9417\n",
      "Epoch 126/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.2662 - accuracy: 0.8972 - val_loss: 0.2135 - val_accuracy: 0.9083\n",
      "Epoch 127/750\n",
      "72/72 [==============================] - 0s 532us/step - loss: 0.2623 - accuracy: 0.8944 - val_loss: 0.1929 - val_accuracy: 0.9333\n",
      "Epoch 128/750\n",
      "72/72 [==============================] - 0s 524us/step - loss: 0.2652 - accuracy: 0.8944 - val_loss: 0.2018 - val_accuracy: 0.9250\n",
      "Epoch 129/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.2650 - accuracy: 0.8861 - val_loss: 0.1954 - val_accuracy: 0.9333\n",
      "Epoch 130/750\n",
      "72/72 [==============================] - 0s 493us/step - loss: 0.2593 - accuracy: 0.8861 - val_loss: 0.2039 - val_accuracy: 0.9167\n",
      "Epoch 131/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.2581 - accuracy: 0.8917 - val_loss: 0.1907 - val_accuracy: 0.9333\n",
      "Epoch 132/750\n",
      "72/72 [==============================] - 0s 532us/step - loss: 0.2704 - accuracy: 0.8861 - val_loss: 0.1852 - val_accuracy: 0.9333\n",
      "Epoch 133/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.2775 - accuracy: 0.8889 - val_loss: 0.1997 - val_accuracy: 0.9333\n",
      "Epoch 134/750\n",
      "72/72 [==============================] - 0s 502us/step - loss: 0.2730 - accuracy: 0.8889 - val_loss: 0.1888 - val_accuracy: 0.9417\n",
      "Epoch 135/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.2600 - accuracy: 0.8917 - val_loss: 0.2030 - val_accuracy: 0.9167\n",
      "Epoch 136/750\n",
      "72/72 [==============================] - 0s 522us/step - loss: 0.2616 - accuracy: 0.8972 - val_loss: 0.1952 - val_accuracy: 0.9250\n",
      "Epoch 137/750\n",
      "72/72 [==============================] - 0s 499us/step - loss: 0.2655 - accuracy: 0.8972 - val_loss: 0.1994 - val_accuracy: 0.9333\n",
      "Epoch 138/750\n",
      "72/72 [==============================] - 0s 520us/step - loss: 0.2596 - accuracy: 0.8972 - val_loss: 0.1910 - val_accuracy: 0.9417\n",
      "Epoch 139/750\n",
      "72/72 [==============================] - 0s 511us/step - loss: 0.2615 - accuracy: 0.8889 - val_loss: 0.1976 - val_accuracy: 0.9333\n",
      "Epoch 140/750\n",
      "72/72 [==============================] - 0s 529us/step - loss: 0.2800 - accuracy: 0.8917 - val_loss: 0.2110 - val_accuracy: 0.9250\n",
      "Epoch 141/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.2775 - accuracy: 0.8972 - val_loss: 0.2035 - val_accuracy: 0.9333\n",
      "Epoch 142/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.2488 - accuracy: 0.9056 - val_loss: 0.1950 - val_accuracy: 0.9417\n",
      "Epoch 143/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.2543 - accuracy: 0.8944 - val_loss: 0.1868 - val_accuracy: 0.9333\n",
      "Epoch 144/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2540 - accuracy: 0.9028 - val_loss: 0.2158 - val_accuracy: 0.9000\n",
      "Epoch 145/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.2607 - accuracy: 0.8889 - val_loss: 0.1954 - val_accuracy: 0.9417\n",
      "Epoch 146/750\n",
      "72/72 [==============================] - 0s 522us/step - loss: 0.2535 - accuracy: 0.9028 - val_loss: 0.1846 - val_accuracy: 0.9333\n",
      "Epoch 147/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.2523 - accuracy: 0.8917 - val_loss: 0.2026 - val_accuracy: 0.9333\n",
      "Epoch 148/750\n",
      "72/72 [==============================] - 0s 544us/step - loss: 0.2594 - accuracy: 0.8944 - val_loss: 0.2001 - val_accuracy: 0.9250\n",
      "Epoch 149/750\n",
      "72/72 [==============================] - 0s 500us/step - loss: 0.2573 - accuracy: 0.8944 - val_loss: 0.1829 - val_accuracy: 0.9333\n",
      "Epoch 150/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.2531 - accuracy: 0.9028 - val_loss: 0.2014 - val_accuracy: 0.9333\n",
      "Epoch 151/750\n",
      "72/72 [==============================] - 0s 525us/step - loss: 0.2522 - accuracy: 0.8889 - val_loss: 0.1935 - val_accuracy: 0.9417\n",
      "Epoch 152/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.2575 - accuracy: 0.8889 - val_loss: 0.1885 - val_accuracy: 0.9417\n",
      "Epoch 153/750\n",
      "72/72 [==============================] - 0s 500us/step - loss: 0.2571 - accuracy: 0.9028 - val_loss: 0.2082 - val_accuracy: 0.9083\n",
      "Epoch 154/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.2556 - accuracy: 0.8889 - val_loss: 0.2010 - val_accuracy: 0.9167\n",
      "Epoch 155/750\n",
      "72/72 [==============================] - 0s 496us/step - loss: 0.2644 - accuracy: 0.8944 - val_loss: 0.2003 - val_accuracy: 0.9417\n",
      "Epoch 156/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.2643 - accuracy: 0.8833 - val_loss: 0.1900 - val_accuracy: 0.9333\n",
      "Epoch 157/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.2583 - accuracy: 0.8917 - val_loss: 0.2276 - val_accuracy: 0.9167\n",
      "Epoch 158/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.2597 - accuracy: 0.8889 - val_loss: 0.1926 - val_accuracy: 0.9417\n",
      "Epoch 159/750\n",
      "72/72 [==============================] - 0s 522us/step - loss: 0.2510 - accuracy: 0.9000 - val_loss: 0.2011 - val_accuracy: 0.9250\n",
      "Epoch 160/750\n",
      "72/72 [==============================] - 0s 522us/step - loss: 0.2504 - accuracy: 0.9083 - val_loss: 0.1897 - val_accuracy: 0.9417\n",
      "Epoch 161/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.2499 - accuracy: 0.9000 - val_loss: 0.1819 - val_accuracy: 0.9333\n",
      "Epoch 162/750\n",
      "72/72 [==============================] - 0s 495us/step - loss: 0.2526 - accuracy: 0.9000 - val_loss: 0.1925 - val_accuracy: 0.9333\n",
      "Epoch 163/750\n",
      "72/72 [==============================] - 0s 527us/step - loss: 0.2551 - accuracy: 0.8889 - val_loss: 0.1897 - val_accuracy: 0.9417\n",
      "Epoch 164/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.2481 - accuracy: 0.9056 - val_loss: 0.1870 - val_accuracy: 0.9417\n",
      "Epoch 165/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.2402 - accuracy: 0.8944 - val_loss: 0.1877 - val_accuracy: 0.9417\n",
      "Epoch 166/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 498us/step - loss: 0.2437 - accuracy: 0.8944 - val_loss: 0.1911 - val_accuracy: 0.9250\n",
      "Epoch 167/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.2443 - accuracy: 0.9083 - val_loss: 0.1944 - val_accuracy: 0.9417\n",
      "Epoch 168/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.2529 - accuracy: 0.8917 - val_loss: 0.1804 - val_accuracy: 0.9333\n",
      "Epoch 169/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.2471 - accuracy: 0.9194 - val_loss: 0.1841 - val_accuracy: 0.9417\n",
      "Epoch 170/750\n",
      "72/72 [==============================] - 0s 523us/step - loss: 0.2483 - accuracy: 0.9028 - val_loss: 0.1828 - val_accuracy: 0.9333\n",
      "Epoch 171/750\n",
      "72/72 [==============================] - 0s 520us/step - loss: 0.2472 - accuracy: 0.9083 - val_loss: 0.1932 - val_accuracy: 0.9417\n",
      "Epoch 172/750\n",
      "72/72 [==============================] - 0s 484us/step - loss: 0.2647 - accuracy: 0.8861 - val_loss: 0.1846 - val_accuracy: 0.9333\n",
      "Epoch 173/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.2559 - accuracy: 0.8917 - val_loss: 0.2047 - val_accuracy: 0.9333\n",
      "Epoch 174/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2377 - accuracy: 0.9111 - val_loss: 0.1959 - val_accuracy: 0.9417\n",
      "Epoch 175/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.2417 - accuracy: 0.8972 - val_loss: 0.1823 - val_accuracy: 0.9333\n",
      "Epoch 176/750\n",
      "72/72 [==============================] - 0s 497us/step - loss: 0.2398 - accuracy: 0.9167 - val_loss: 0.2117 - val_accuracy: 0.9333\n",
      "Epoch 177/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.2478 - accuracy: 0.8972 - val_loss: 0.1929 - val_accuracy: 0.9333\n",
      "Epoch 178/750\n",
      "72/72 [==============================] - 0s 492us/step - loss: 0.2440 - accuracy: 0.9056 - val_loss: 0.1761 - val_accuracy: 0.9417\n",
      "Epoch 179/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.2349 - accuracy: 0.9111 - val_loss: 0.1958 - val_accuracy: 0.9250\n",
      "Epoch 180/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.2479 - accuracy: 0.8944 - val_loss: 0.1942 - val_accuracy: 0.9333\n",
      "Epoch 181/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.2334 - accuracy: 0.9028 - val_loss: 0.2501 - val_accuracy: 0.8833\n",
      "Epoch 182/750\n",
      "72/72 [==============================] - 0s 484us/step - loss: 0.2471 - accuracy: 0.8778 - val_loss: 0.1883 - val_accuracy: 0.9417\n",
      "Epoch 183/750\n",
      "72/72 [==============================] - 0s 499us/step - loss: 0.2406 - accuracy: 0.8972 - val_loss: 0.2073 - val_accuracy: 0.9333\n",
      "Epoch 184/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.2373 - accuracy: 0.9000 - val_loss: 0.1823 - val_accuracy: 0.9333\n",
      "Epoch 185/750\n",
      "72/72 [==============================] - 0s 500us/step - loss: 0.2436 - accuracy: 0.8861 - val_loss: 0.1816 - val_accuracy: 0.9333\n",
      "Epoch 186/750\n",
      "72/72 [==============================] - 0s 510us/step - loss: 0.2337 - accuracy: 0.8972 - val_loss: 0.1945 - val_accuracy: 0.9333\n",
      "Epoch 187/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.2358 - accuracy: 0.9167 - val_loss: 0.1852 - val_accuracy: 0.9417\n",
      "Epoch 188/750\n",
      "72/72 [==============================] - 0s 525us/step - loss: 0.2355 - accuracy: 0.9028 - val_loss: 0.1971 - val_accuracy: 0.9417\n",
      "Epoch 189/750\n",
      "72/72 [==============================] - 0s 533us/step - loss: 0.2352 - accuracy: 0.9083 - val_loss: 0.1939 - val_accuracy: 0.9417\n",
      "Epoch 190/750\n",
      "72/72 [==============================] - 0s 526us/step - loss: 0.2338 - accuracy: 0.9194 - val_loss: 0.1999 - val_accuracy: 0.9417\n",
      "Epoch 191/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.2347 - accuracy: 0.9111 - val_loss: 0.2003 - val_accuracy: 0.9417\n",
      "Epoch 192/750\n",
      "72/72 [==============================] - 0s 491us/step - loss: 0.2495 - accuracy: 0.9000 - val_loss: 0.1864 - val_accuracy: 0.9417\n",
      "Epoch 193/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.2337 - accuracy: 0.9000 - val_loss: 0.2064 - val_accuracy: 0.9083\n",
      "Epoch 194/750\n",
      "72/72 [==============================] - 0s 486us/step - loss: 0.2442 - accuracy: 0.8861 - val_loss: 0.1852 - val_accuracy: 0.9417\n",
      "Epoch 195/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.2286 - accuracy: 0.9111 - val_loss: 0.2137 - val_accuracy: 0.9167\n",
      "Epoch 196/750\n",
      "72/72 [==============================] - 0s 528us/step - loss: 0.2408 - accuracy: 0.9056 - val_loss: 0.1972 - val_accuracy: 0.9417\n",
      "Epoch 197/750\n",
      "72/72 [==============================] - 0s 549us/step - loss: 0.2295 - accuracy: 0.9056 - val_loss: 0.1777 - val_accuracy: 0.9333\n",
      "Epoch 198/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2407 - accuracy: 0.9000 - val_loss: 0.1947 - val_accuracy: 0.9417\n",
      "Epoch 199/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.2281 - accuracy: 0.9139 - val_loss: 0.1976 - val_accuracy: 0.9333\n",
      "Epoch 200/750\n",
      "72/72 [==============================] - 0s 511us/step - loss: 0.2321 - accuracy: 0.9139 - val_loss: 0.1773 - val_accuracy: 0.9333\n",
      "Epoch 201/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2386 - accuracy: 0.9056 - val_loss: 0.1894 - val_accuracy: 0.9500\n",
      "Epoch 202/750\n",
      "72/72 [==============================] - 0s 494us/step - loss: 0.2289 - accuracy: 0.9167 - val_loss: 0.1873 - val_accuracy: 0.9417\n",
      "Epoch 203/750\n",
      "72/72 [==============================] - 0s 540us/step - loss: 0.2308 - accuracy: 0.9056 - val_loss: 0.1763 - val_accuracy: 0.9417\n",
      "Epoch 204/750\n",
      "72/72 [==============================] - 0s 620us/step - loss: 0.2287 - accuracy: 0.9194 - val_loss: 0.1795 - val_accuracy: 0.9333\n",
      "Epoch 205/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.2275 - accuracy: 0.9083 - val_loss: 0.1769 - val_accuracy: 0.9417\n",
      "Epoch 206/750\n",
      "72/72 [==============================] - 0s 494us/step - loss: 0.2291 - accuracy: 0.9056 - val_loss: 0.1722 - val_accuracy: 0.9417\n",
      "Epoch 207/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2482 - accuracy: 0.9000 - val_loss: 0.1730 - val_accuracy: 0.9417\n",
      "Epoch 208/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.2258 - accuracy: 0.9167 - val_loss: 0.2032 - val_accuracy: 0.9417\n",
      "Epoch 209/750\n",
      "72/72 [==============================] - 0s 500us/step - loss: 0.2352 - accuracy: 0.9056 - val_loss: 0.1984 - val_accuracy: 0.9500\n",
      "Epoch 210/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.2221 - accuracy: 0.9056 - val_loss: 0.1712 - val_accuracy: 0.9500\n",
      "Epoch 211/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.2304 - accuracy: 0.8917 - val_loss: 0.1730 - val_accuracy: 0.9417\n",
      "Epoch 212/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.2415 - accuracy: 0.9139 - val_loss: 0.1870 - val_accuracy: 0.9417\n",
      "Epoch 213/750\n",
      "72/72 [==============================] - 0s 497us/step - loss: 0.2304 - accuracy: 0.9056 - val_loss: 0.1775 - val_accuracy: 0.9417\n",
      "Epoch 214/750\n",
      "72/72 [==============================] - 0s 502us/step - loss: 0.2302 - accuracy: 0.8972 - val_loss: 0.1828 - val_accuracy: 0.9583\n",
      "Epoch 215/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.2325 - accuracy: 0.9167 - val_loss: 0.1884 - val_accuracy: 0.9417\n",
      "Epoch 216/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.2244 - accuracy: 0.9167 - val_loss: 0.1807 - val_accuracy: 0.9417\n",
      "Epoch 217/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.2331 - accuracy: 0.9000 - val_loss: 0.1906 - val_accuracy: 0.9333\n",
      "Epoch 218/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.2273 - accuracy: 0.9028 - val_loss: 0.1978 - val_accuracy: 0.9167\n",
      "Epoch 219/750\n",
      "72/72 [==============================] - 0s 502us/step - loss: 0.2347 - accuracy: 0.9083 - val_loss: 0.1883 - val_accuracy: 0.9500\n",
      "Epoch 220/750\n",
      "72/72 [==============================] - 0s 511us/step - loss: 0.2217 - accuracy: 0.9139 - val_loss: 0.1922 - val_accuracy: 0.9167\n",
      "Epoch 221/750\n",
      "72/72 [==============================] - 0s 495us/step - loss: 0.2251 - accuracy: 0.9083 - val_loss: 0.1883 - val_accuracy: 0.9333\n",
      "Epoch 222/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 527us/step - loss: 0.2334 - accuracy: 0.9056 - val_loss: 0.1828 - val_accuracy: 0.9417\n",
      "Epoch 223/750\n",
      "72/72 [==============================] - 0s 529us/step - loss: 0.2288 - accuracy: 0.9083 - val_loss: 0.1967 - val_accuracy: 0.9417\n",
      "Epoch 224/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.2302 - accuracy: 0.8944 - val_loss: 0.1982 - val_accuracy: 0.9333\n",
      "Epoch 225/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.2271 - accuracy: 0.9194 - val_loss: 0.1772 - val_accuracy: 0.9500\n",
      "Epoch 226/750\n",
      "72/72 [==============================] - 0s 522us/step - loss: 0.2275 - accuracy: 0.9111 - val_loss: 0.1853 - val_accuracy: 0.9583\n",
      "Epoch 227/750\n",
      "72/72 [==============================] - 0s 553us/step - loss: 0.2278 - accuracy: 0.9167 - val_loss: 0.1802 - val_accuracy: 0.9417\n",
      "Epoch 228/750\n",
      "72/72 [==============================] - 0s 523us/step - loss: 0.2342 - accuracy: 0.9083 - val_loss: 0.1811 - val_accuracy: 0.9500\n",
      "Epoch 229/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2215 - accuracy: 0.9056 - val_loss: 0.1941 - val_accuracy: 0.9417\n",
      "Epoch 230/750\n",
      "72/72 [==============================] - 0s 525us/step - loss: 0.2280 - accuracy: 0.9056 - val_loss: 0.1773 - val_accuracy: 0.9500\n",
      "Epoch 231/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.2216 - accuracy: 0.9222 - val_loss: 0.1827 - val_accuracy: 0.9417\n",
      "Epoch 232/750\n",
      "72/72 [==============================] - 0s 496us/step - loss: 0.2191 - accuracy: 0.9194 - val_loss: 0.1816 - val_accuracy: 0.9417\n",
      "Epoch 233/750\n",
      "72/72 [==============================] - 0s 529us/step - loss: 0.2235 - accuracy: 0.8972 - val_loss: 0.1805 - val_accuracy: 0.9333\n",
      "Epoch 234/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.2163 - accuracy: 0.9278 - val_loss: 0.1727 - val_accuracy: 0.9417\n",
      "Epoch 235/750\n",
      "72/72 [==============================] - 0s 533us/step - loss: 0.2223 - accuracy: 0.9056 - val_loss: 0.1772 - val_accuracy: 0.9500\n",
      "Epoch 236/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.2130 - accuracy: 0.9278 - val_loss: 0.2177 - val_accuracy: 0.9250\n",
      "Epoch 237/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.2385 - accuracy: 0.9056 - val_loss: 0.1735 - val_accuracy: 0.9500\n",
      "Epoch 238/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.2215 - accuracy: 0.9028 - val_loss: 0.1794 - val_accuracy: 0.9500\n",
      "Epoch 239/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2335 - accuracy: 0.8972 - val_loss: 0.1922 - val_accuracy: 0.9333\n",
      "Epoch 240/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.2691 - accuracy: 0.8861 - val_loss: 0.1788 - val_accuracy: 0.9500\n",
      "Epoch 241/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.2200 - accuracy: 0.9111 - val_loss: 0.1746 - val_accuracy: 0.9500\n",
      "Epoch 242/750\n",
      "72/72 [==============================] - 0s 522us/step - loss: 0.2217 - accuracy: 0.9194 - val_loss: 0.1741 - val_accuracy: 0.9417\n",
      "Epoch 243/750\n",
      "72/72 [==============================] - 0s 510us/step - loss: 0.2191 - accuracy: 0.9000 - val_loss: 0.1933 - val_accuracy: 0.9417\n",
      "Epoch 244/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2159 - accuracy: 0.9167 - val_loss: 0.1947 - val_accuracy: 0.9333\n",
      "Epoch 245/750\n",
      "72/72 [==============================] - 0s 504us/step - loss: 0.2206 - accuracy: 0.9000 - val_loss: 0.1882 - val_accuracy: 0.9417\n",
      "Epoch 246/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.2111 - accuracy: 0.9361 - val_loss: 0.2005 - val_accuracy: 0.9333\n",
      "Epoch 247/750\n",
      "72/72 [==============================] - 0s 544us/step - loss: 0.2158 - accuracy: 0.9111 - val_loss: 0.2015 - val_accuracy: 0.9333\n",
      "Epoch 248/750\n",
      "72/72 [==============================] - 0s 520us/step - loss: 0.2329 - accuracy: 0.9111 - val_loss: 0.1814 - val_accuracy: 0.9500\n",
      "Epoch 249/750\n",
      "72/72 [==============================] - 0s 541us/step - loss: 0.2133 - accuracy: 0.9250 - val_loss: 0.1716 - val_accuracy: 0.9500\n",
      "Epoch 250/750\n",
      "72/72 [==============================] - 0s 498us/step - loss: 0.2206 - accuracy: 0.9111 - val_loss: 0.1659 - val_accuracy: 0.9500\n",
      "Epoch 251/750\n",
      "72/72 [==============================] - 0s 524us/step - loss: 0.2165 - accuracy: 0.9194 - val_loss: 0.2142 - val_accuracy: 0.9083\n",
      "Epoch 252/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2193 - accuracy: 0.9111 - val_loss: 0.1765 - val_accuracy: 0.9417\n",
      "Epoch 253/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.2122 - accuracy: 0.9222 - val_loss: 0.2059 - val_accuracy: 0.9333\n",
      "Epoch 254/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.2221 - accuracy: 0.9083 - val_loss: 0.1980 - val_accuracy: 0.9250\n",
      "Epoch 255/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.2070 - accuracy: 0.9111 - val_loss: 0.1716 - val_accuracy: 0.9417\n",
      "Epoch 256/750\n",
      "72/72 [==============================] - 0s 495us/step - loss: 0.2130 - accuracy: 0.9250 - val_loss: 0.1720 - val_accuracy: 0.9417\n",
      "Epoch 257/750\n",
      "72/72 [==============================] - 0s 523us/step - loss: 0.2266 - accuracy: 0.9167 - val_loss: 0.1778 - val_accuracy: 0.9500\n",
      "Epoch 258/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2145 - accuracy: 0.9167 - val_loss: 0.1723 - val_accuracy: 0.9417\n",
      "Epoch 259/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2289 - accuracy: 0.9083 - val_loss: 0.1800 - val_accuracy: 0.9500\n",
      "Epoch 260/750\n",
      "72/72 [==============================] - 0s 498us/step - loss: 0.2090 - accuracy: 0.9278 - val_loss: 0.1692 - val_accuracy: 0.9417\n",
      "Epoch 261/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.2150 - accuracy: 0.9167 - val_loss: 0.1664 - val_accuracy: 0.9500\n",
      "Epoch 262/750\n",
      "72/72 [==============================] - 0s 499us/step - loss: 0.2213 - accuracy: 0.9250 - val_loss: 0.1781 - val_accuracy: 0.9417\n",
      "Epoch 263/750\n",
      "72/72 [==============================] - 0s 491us/step - loss: 0.2157 - accuracy: 0.9139 - val_loss: 0.2064 - val_accuracy: 0.9333\n",
      "Epoch 264/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.2168 - accuracy: 0.9056 - val_loss: 0.1915 - val_accuracy: 0.9417\n",
      "Epoch 265/750\n",
      "72/72 [==============================] - 0s 498us/step - loss: 0.2101 - accuracy: 0.9194 - val_loss: 0.1779 - val_accuracy: 0.9417\n",
      "Epoch 266/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.2182 - accuracy: 0.9167 - val_loss: 0.1886 - val_accuracy: 0.9250\n",
      "Epoch 267/750\n",
      "72/72 [==============================] - 0s 481us/step - loss: 0.2148 - accuracy: 0.9194 - val_loss: 0.1820 - val_accuracy: 0.9250\n",
      "Epoch 268/750\n",
      "72/72 [==============================] - 0s 511us/step - loss: 0.2117 - accuracy: 0.9194 - val_loss: 0.2491 - val_accuracy: 0.9000\n",
      "Epoch 269/750\n",
      "72/72 [==============================] - 0s 489us/step - loss: 0.2171 - accuracy: 0.9083 - val_loss: 0.1925 - val_accuracy: 0.9500\n",
      "Epoch 270/750\n",
      "72/72 [==============================] - 0s 500us/step - loss: 0.2191 - accuracy: 0.9194 - val_loss: 0.1725 - val_accuracy: 0.9417\n",
      "Epoch 271/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.2136 - accuracy: 0.9139 - val_loss: 0.1739 - val_accuracy: 0.9417\n",
      "Epoch 272/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.2097 - accuracy: 0.9194 - val_loss: 0.1876 - val_accuracy: 0.9417\n",
      "Epoch 273/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2198 - accuracy: 0.9083 - val_loss: 0.1712 - val_accuracy: 0.9583\n",
      "Epoch 274/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.2094 - accuracy: 0.9194 - val_loss: 0.1824 - val_accuracy: 0.9417\n",
      "Epoch 275/750\n",
      "72/72 [==============================] - 0s 484us/step - loss: 0.2108 - accuracy: 0.9222 - val_loss: 0.1810 - val_accuracy: 0.9417\n",
      "Epoch 276/750\n",
      "72/72 [==============================] - 0s 502us/step - loss: 0.2095 - accuracy: 0.9111 - val_loss: 0.1776 - val_accuracy: 0.9333\n",
      "Epoch 277/750\n",
      "72/72 [==============================] - 0s 483us/step - loss: 0.2064 - accuracy: 0.9250 - val_loss: 0.1836 - val_accuracy: 0.9417\n",
      "Epoch 278/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 528us/step - loss: 0.2057 - accuracy: 0.9194 - val_loss: 0.1639 - val_accuracy: 0.9417\n",
      "Epoch 279/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.2120 - accuracy: 0.9167 - val_loss: 0.1972 - val_accuracy: 0.9250\n",
      "Epoch 280/750\n",
      "72/72 [==============================] - 0s 498us/step - loss: 0.2036 - accuracy: 0.9278 - val_loss: 0.2261 - val_accuracy: 0.9083\n",
      "Epoch 281/750\n",
      "72/72 [==============================] - 0s 495us/step - loss: 0.2118 - accuracy: 0.9139 - val_loss: 0.1899 - val_accuracy: 0.9417\n",
      "Epoch 282/750\n",
      "72/72 [==============================] - 0s 482us/step - loss: 0.2035 - accuracy: 0.9111 - val_loss: 0.1803 - val_accuracy: 0.9417\n",
      "Epoch 283/750\n",
      "72/72 [==============================] - 0s 488us/step - loss: 0.2063 - accuracy: 0.9222 - val_loss: 0.1700 - val_accuracy: 0.9500\n",
      "Epoch 284/750\n",
      "72/72 [==============================] - 0s 487us/step - loss: 0.2155 - accuracy: 0.9194 - val_loss: 0.1812 - val_accuracy: 0.9417\n",
      "Epoch 285/750\n",
      "72/72 [==============================] - 0s 474us/step - loss: 0.2059 - accuracy: 0.9250 - val_loss: 0.2048 - val_accuracy: 0.9167\n",
      "Epoch 286/750\n",
      "72/72 [==============================] - 0s 510us/step - loss: 0.2150 - accuracy: 0.9056 - val_loss: 0.1708 - val_accuracy: 0.9583\n",
      "Epoch 287/750\n",
      "72/72 [==============================] - 0s 493us/step - loss: 0.2072 - accuracy: 0.9306 - val_loss: 0.1703 - val_accuracy: 0.9417\n",
      "Epoch 288/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.2122 - accuracy: 0.9222 - val_loss: 0.1804 - val_accuracy: 0.9250\n",
      "Epoch 289/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.2185 - accuracy: 0.9139 - val_loss: 0.1645 - val_accuracy: 0.9417\n",
      "Epoch 290/750\n",
      "72/72 [==============================] - 0s 549us/step - loss: 0.2108 - accuracy: 0.9083 - val_loss: 0.1807 - val_accuracy: 0.9500\n",
      "Epoch 291/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.2050 - accuracy: 0.9222 - val_loss: 0.1728 - val_accuracy: 0.9500\n",
      "Epoch 292/750\n",
      "72/72 [==============================] - 0s 543us/step - loss: 0.1997 - accuracy: 0.9194 - val_loss: 0.1898 - val_accuracy: 0.9333\n",
      "Epoch 293/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.2158 - accuracy: 0.9278 - val_loss: 0.1892 - val_accuracy: 0.9417\n",
      "Epoch 294/750\n",
      "72/72 [==============================] - 0s 511us/step - loss: 0.2086 - accuracy: 0.9278 - val_loss: 0.1882 - val_accuracy: 0.9417\n",
      "Epoch 295/750\n",
      "72/72 [==============================] - 0s 535us/step - loss: 0.2178 - accuracy: 0.9083 - val_loss: 0.1647 - val_accuracy: 0.9500\n",
      "Epoch 296/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.2080 - accuracy: 0.9194 - val_loss: 0.1954 - val_accuracy: 0.9167\n",
      "Epoch 297/750\n",
      "72/72 [==============================] - 0s 510us/step - loss: 0.2173 - accuracy: 0.9194 - val_loss: 0.1767 - val_accuracy: 0.9417\n",
      "Epoch 298/750\n",
      "72/72 [==============================] - 0s 531us/step - loss: 0.2017 - accuracy: 0.9222 - val_loss: 0.1892 - val_accuracy: 0.9333\n",
      "Epoch 299/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2172 - accuracy: 0.9111 - val_loss: 0.1918 - val_accuracy: 0.9167\n",
      "Epoch 300/750\n",
      "72/72 [==============================] - 0s 548us/step - loss: 0.2190 - accuracy: 0.9194 - val_loss: 0.1666 - val_accuracy: 0.9500\n",
      "Epoch 301/750\n",
      "72/72 [==============================] - 0s 526us/step - loss: 0.2088 - accuracy: 0.9222 - val_loss: 0.1999 - val_accuracy: 0.9167\n",
      "Epoch 302/750\n",
      "72/72 [==============================] - 0s 527us/step - loss: 0.2075 - accuracy: 0.9222 - val_loss: 0.1691 - val_accuracy: 0.9500\n",
      "Epoch 303/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.2068 - accuracy: 0.9194 - val_loss: 0.1738 - val_accuracy: 0.9500\n",
      "Epoch 304/750\n",
      "72/72 [==============================] - 0s 524us/step - loss: 0.1979 - accuracy: 0.9306 - val_loss: 0.1750 - val_accuracy: 0.9583\n",
      "Epoch 305/750\n",
      "72/72 [==============================] - 0s 522us/step - loss: 0.2012 - accuracy: 0.9167 - val_loss: 0.1843 - val_accuracy: 0.9500\n",
      "Epoch 306/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.2082 - accuracy: 0.9139 - val_loss: 0.1743 - val_accuracy: 0.9250\n",
      "Epoch 307/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.2152 - accuracy: 0.9000 - val_loss: 0.1696 - val_accuracy: 0.9500\n",
      "Epoch 308/750\n",
      "72/72 [==============================] - 0s 497us/step - loss: 0.2143 - accuracy: 0.9111 - val_loss: 0.1750 - val_accuracy: 0.9250\n",
      "Epoch 309/750\n",
      "72/72 [==============================] - 0s 534us/step - loss: 0.2072 - accuracy: 0.9222 - val_loss: 0.1750 - val_accuracy: 0.9500\n",
      "Epoch 310/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.2117 - accuracy: 0.9139 - val_loss: 0.1982 - val_accuracy: 0.9333\n",
      "Epoch 311/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.2014 - accuracy: 0.9222 - val_loss: 0.1655 - val_accuracy: 0.9500\n",
      "Epoch 312/750\n",
      "72/72 [==============================] - 0s 531us/step - loss: 0.2019 - accuracy: 0.9167 - val_loss: 0.1891 - val_accuracy: 0.9333\n",
      "Epoch 313/750\n",
      "72/72 [==============================] - 0s 530us/step - loss: 0.2033 - accuracy: 0.9194 - val_loss: 0.1791 - val_accuracy: 0.9417\n",
      "Epoch 314/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.2099 - accuracy: 0.9028 - val_loss: 0.1920 - val_accuracy: 0.9417\n",
      "Epoch 315/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.2043 - accuracy: 0.9278 - val_loss: 0.1817 - val_accuracy: 0.9417\n",
      "Epoch 316/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.2077 - accuracy: 0.9083 - val_loss: 0.1758 - val_accuracy: 0.9250\n",
      "Epoch 317/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.2060 - accuracy: 0.9167 - val_loss: 0.1804 - val_accuracy: 0.9250\n",
      "Epoch 318/750\n",
      "72/72 [==============================] - 0s 533us/step - loss: 0.2184 - accuracy: 0.9111 - val_loss: 0.1802 - val_accuracy: 0.9500\n",
      "Epoch 319/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.2126 - accuracy: 0.9167 - val_loss: 0.1757 - val_accuracy: 0.9500\n",
      "Epoch 320/750\n",
      "72/72 [==============================] - 0s 520us/step - loss: 0.2102 - accuracy: 0.9167 - val_loss: 0.2038 - val_accuracy: 0.9333\n",
      "Epoch 321/750\n",
      "72/72 [==============================] - 0s 510us/step - loss: 0.1943 - accuracy: 0.9250 - val_loss: 0.1934 - val_accuracy: 0.9250\n",
      "Epoch 322/750\n",
      "72/72 [==============================] - 0s 532us/step - loss: 0.2015 - accuracy: 0.9250 - val_loss: 0.1886 - val_accuracy: 0.9500\n",
      "Epoch 323/750\n",
      "72/72 [==============================] - 0s 511us/step - loss: 0.2022 - accuracy: 0.9167 - val_loss: 0.1688 - val_accuracy: 0.9417\n",
      "Epoch 324/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.2007 - accuracy: 0.9250 - val_loss: 0.1723 - val_accuracy: 0.9500\n",
      "Epoch 325/750\n",
      "72/72 [==============================] - 0s 552us/step - loss: 0.2121 - accuracy: 0.9222 - val_loss: 0.1679 - val_accuracy: 0.9500\n",
      "Epoch 326/750\n",
      "72/72 [==============================] - 0s 520us/step - loss: 0.2107 - accuracy: 0.9194 - val_loss: 0.2054 - val_accuracy: 0.9333\n",
      "Epoch 327/750\n",
      "72/72 [==============================] - 0s 535us/step - loss: 0.2094 - accuracy: 0.9111 - val_loss: 0.1634 - val_accuracy: 0.9417\n",
      "Epoch 328/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.2032 - accuracy: 0.9167 - val_loss: 0.2165 - val_accuracy: 0.9167\n",
      "Epoch 329/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2114 - accuracy: 0.9167 - val_loss: 0.2004 - val_accuracy: 0.9417\n",
      "Epoch 330/750\n",
      "72/72 [==============================] - 0s 527us/step - loss: 0.2081 - accuracy: 0.9139 - val_loss: 0.1846 - val_accuracy: 0.9500\n",
      "Epoch 331/750\n",
      "72/72 [==============================] - 0s 524us/step - loss: 0.2032 - accuracy: 0.9250 - val_loss: 0.1870 - val_accuracy: 0.9333\n",
      "Epoch 332/750\n",
      "72/72 [==============================] - 0s 537us/step - loss: 0.2024 - accuracy: 0.9139 - val_loss: 0.1915 - val_accuracy: 0.9500\n",
      "Epoch 333/750\n",
      "72/72 [==============================] - 0s 523us/step - loss: 0.2012 - accuracy: 0.9278 - val_loss: 0.2027 - val_accuracy: 0.9333\n",
      "Epoch 334/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 508us/step - loss: 0.2088 - accuracy: 0.9222 - val_loss: 0.1858 - val_accuracy: 0.9500\n",
      "Epoch 335/750\n",
      "72/72 [==============================] - 0s 524us/step - loss: 0.2022 - accuracy: 0.9306 - val_loss: 0.1718 - val_accuracy: 0.9583\n",
      "Epoch 336/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.2246 - accuracy: 0.8972 - val_loss: 0.2078 - val_accuracy: 0.9250\n",
      "Epoch 337/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.2042 - accuracy: 0.9333 - val_loss: 0.1829 - val_accuracy: 0.9333\n",
      "Epoch 338/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.2181 - accuracy: 0.9222 - val_loss: 0.1831 - val_accuracy: 0.9500\n",
      "Epoch 339/750\n",
      "72/72 [==============================] - 0s 525us/step - loss: 0.2189 - accuracy: 0.8889 - val_loss: 0.1661 - val_accuracy: 0.9583\n",
      "Epoch 340/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.1961 - accuracy: 0.9278 - val_loss: 0.1946 - val_accuracy: 0.9417\n",
      "Epoch 341/750\n",
      "72/72 [==============================] - 0s 531us/step - loss: 0.1930 - accuracy: 0.9306 - val_loss: 0.1890 - val_accuracy: 0.9417\n",
      "Epoch 342/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.2083 - accuracy: 0.9250 - val_loss: 0.1875 - val_accuracy: 0.9417\n",
      "Epoch 343/750\n",
      "72/72 [==============================] - 0s 503us/step - loss: 0.2215 - accuracy: 0.9111 - val_loss: 0.1797 - val_accuracy: 0.9250\n",
      "Epoch 344/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.2010 - accuracy: 0.9139 - val_loss: 0.1704 - val_accuracy: 0.9500\n",
      "Epoch 345/750\n",
      "72/72 [==============================] - 0s 537us/step - loss: 0.2014 - accuracy: 0.9194 - val_loss: 0.1642 - val_accuracy: 0.9583\n",
      "Epoch 346/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.2018 - accuracy: 0.9222 - val_loss: 0.2199 - val_accuracy: 0.9250\n",
      "Epoch 347/750\n",
      "72/72 [==============================] - 0s 523us/step - loss: 0.1952 - accuracy: 0.9250 - val_loss: 0.1578 - val_accuracy: 0.9583\n",
      "Epoch 348/750\n",
      "72/72 [==============================] - 0s 526us/step - loss: 0.2052 - accuracy: 0.9194 - val_loss: 0.1748 - val_accuracy: 0.9500\n",
      "Epoch 349/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.2151 - accuracy: 0.9194 - val_loss: 0.1984 - val_accuracy: 0.9333\n",
      "Epoch 350/750\n",
      "72/72 [==============================] - 0s 537us/step - loss: 0.2206 - accuracy: 0.9111 - val_loss: 0.1699 - val_accuracy: 0.9500\n",
      "Epoch 351/750\n",
      "72/72 [==============================] - 0s 527us/step - loss: 0.1999 - accuracy: 0.9250 - val_loss: 0.1837 - val_accuracy: 0.9417\n",
      "Epoch 352/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.1923 - accuracy: 0.9167 - val_loss: 0.2185 - val_accuracy: 0.9333\n",
      "Epoch 353/750\n",
      "72/72 [==============================] - 0s 540us/step - loss: 0.2072 - accuracy: 0.9139 - val_loss: 0.1924 - val_accuracy: 0.9167\n",
      "Epoch 354/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.2074 - accuracy: 0.9167 - val_loss: 0.1985 - val_accuracy: 0.9167\n",
      "Epoch 355/750\n",
      "72/72 [==============================] - 0s 543us/step - loss: 0.2003 - accuracy: 0.9250 - val_loss: 0.1984 - val_accuracy: 0.9333\n",
      "Epoch 356/750\n",
      "72/72 [==============================] - 0s 510us/step - loss: 0.1932 - accuracy: 0.9222 - val_loss: 0.2018 - val_accuracy: 0.9167\n",
      "Epoch 357/750\n",
      "72/72 [==============================] - 0s 526us/step - loss: 0.1926 - accuracy: 0.9222 - val_loss: 0.1592 - val_accuracy: 0.9417\n",
      "Epoch 358/750\n",
      "72/72 [==============================] - 0s 526us/step - loss: 0.1973 - accuracy: 0.9306 - val_loss: 0.1719 - val_accuracy: 0.9500\n",
      "Epoch 359/750\n",
      "72/72 [==============================] - 0s 522us/step - loss: 0.2018 - accuracy: 0.9222 - val_loss: 0.1954 - val_accuracy: 0.9333\n",
      "Epoch 360/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.1961 - accuracy: 0.9361 - val_loss: 0.1745 - val_accuracy: 0.9333\n",
      "Epoch 361/750\n",
      "72/72 [==============================] - 0s 523us/step - loss: 0.1885 - accuracy: 0.9361 - val_loss: 0.1699 - val_accuracy: 0.9500\n",
      "Epoch 362/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2021 - accuracy: 0.9222 - val_loss: 0.1557 - val_accuracy: 0.9500\n",
      "Epoch 363/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.2012 - accuracy: 0.9194 - val_loss: 0.1592 - val_accuracy: 0.9500\n",
      "Epoch 364/750\n",
      "72/72 [==============================] - 0s 530us/step - loss: 0.2011 - accuracy: 0.9306 - val_loss: 0.1840 - val_accuracy: 0.9417\n",
      "Epoch 365/750\n",
      "72/72 [==============================] - 0s 539us/step - loss: 0.2009 - accuracy: 0.9333 - val_loss: 0.1870 - val_accuracy: 0.9333\n",
      "Epoch 366/750\n",
      "72/72 [==============================] - 0s 502us/step - loss: 0.2063 - accuracy: 0.9333 - val_loss: 0.1735 - val_accuracy: 0.9500\n",
      "Epoch 367/750\n",
      "72/72 [==============================] - 0s 534us/step - loss: 0.1965 - accuracy: 0.9278 - val_loss: 0.1531 - val_accuracy: 0.9500\n",
      "Epoch 368/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.2243 - accuracy: 0.9194 - val_loss: 0.1598 - val_accuracy: 0.9583\n",
      "Epoch 369/750\n",
      "72/72 [==============================] - 0s 539us/step - loss: 0.1958 - accuracy: 0.9333 - val_loss: 0.1722 - val_accuracy: 0.9333\n",
      "Epoch 370/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.2179 - accuracy: 0.9222 - val_loss: 0.1701 - val_accuracy: 0.9417\n",
      "Epoch 371/750\n",
      "72/72 [==============================] - 0s 528us/step - loss: 0.2008 - accuracy: 0.9278 - val_loss: 0.2074 - val_accuracy: 0.9167\n",
      "Epoch 372/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.1910 - accuracy: 0.9333 - val_loss: 0.1614 - val_accuracy: 0.9583\n",
      "Epoch 373/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2093 - accuracy: 0.9278 - val_loss: 0.1754 - val_accuracy: 0.9583\n",
      "Epoch 374/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.1982 - accuracy: 0.9083 - val_loss: 0.1966 - val_accuracy: 0.9333\n",
      "Epoch 375/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.1923 - accuracy: 0.9278 - val_loss: 0.1586 - val_accuracy: 0.9500\n",
      "Epoch 376/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.1980 - accuracy: 0.9278 - val_loss: 0.1780 - val_accuracy: 0.9417\n",
      "Epoch 377/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.2001 - accuracy: 0.9139 - val_loss: 0.1755 - val_accuracy: 0.9417\n",
      "Epoch 378/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.2161 - accuracy: 0.9056 - val_loss: 0.1957 - val_accuracy: 0.9417\n",
      "Epoch 379/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.2148 - accuracy: 0.9056 - val_loss: 0.1801 - val_accuracy: 0.9500\n",
      "Epoch 380/750\n",
      "72/72 [==============================] - 0s 511us/step - loss: 0.1976 - accuracy: 0.9139 - val_loss: 0.1662 - val_accuracy: 0.9583\n",
      "Epoch 381/750\n",
      "72/72 [==============================] - 0s 526us/step - loss: 0.1998 - accuracy: 0.9306 - val_loss: 0.1896 - val_accuracy: 0.9250\n",
      "Epoch 382/750\n",
      "72/72 [==============================] - 0s 511us/step - loss: 0.1904 - accuracy: 0.9306 - val_loss: 0.2003 - val_accuracy: 0.9333\n",
      "Epoch 383/750\n",
      "72/72 [==============================] - 0s 500us/step - loss: 0.1998 - accuracy: 0.9139 - val_loss: 0.1693 - val_accuracy: 0.9417\n",
      "Epoch 384/750\n",
      "72/72 [==============================] - 0s 486us/step - loss: 0.2010 - accuracy: 0.9250 - val_loss: 0.1725 - val_accuracy: 0.9250\n",
      "Epoch 385/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.1989 - accuracy: 0.9250 - val_loss: 0.1810 - val_accuracy: 0.9500\n",
      "Epoch 386/750\n",
      "72/72 [==============================] - 0s 502us/step - loss: 0.1971 - accuracy: 0.9250 - val_loss: 0.1737 - val_accuracy: 0.9500\n",
      "Epoch 387/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.2125 - accuracy: 0.9028 - val_loss: 0.1712 - val_accuracy: 0.9583\n",
      "Epoch 388/750\n",
      "72/72 [==============================] - 0s 497us/step - loss: 0.1836 - accuracy: 0.9444 - val_loss: 0.1840 - val_accuracy: 0.9417\n",
      "Epoch 389/750\n",
      "72/72 [==============================] - 0s 537us/step - loss: 0.2011 - accuracy: 0.9167 - val_loss: 0.2137 - val_accuracy: 0.9250\n",
      "Epoch 390/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 508us/step - loss: 0.1960 - accuracy: 0.9333 - val_loss: 0.1668 - val_accuracy: 0.9417\n",
      "Epoch 391/750\n",
      "72/72 [==============================] - 0s 586us/step - loss: 0.1985 - accuracy: 0.9278 - val_loss: 0.1616 - val_accuracy: 0.9500\n",
      "Epoch 392/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.1883 - accuracy: 0.9306 - val_loss: 0.1766 - val_accuracy: 0.9333\n",
      "Epoch 393/750\n",
      "72/72 [==============================] - 0s 498us/step - loss: 0.1937 - accuracy: 0.9333 - val_loss: 0.2023 - val_accuracy: 0.9167\n",
      "Epoch 394/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2045 - accuracy: 0.9139 - val_loss: 0.1745 - val_accuracy: 0.9500\n",
      "Epoch 395/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.1959 - accuracy: 0.9222 - val_loss: 0.2219 - val_accuracy: 0.9167\n",
      "Epoch 396/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.1985 - accuracy: 0.9167 - val_loss: 0.1696 - val_accuracy: 0.9500\n",
      "Epoch 397/750\n",
      "72/72 [==============================] - 0s 510us/step - loss: 0.2048 - accuracy: 0.9222 - val_loss: 0.1743 - val_accuracy: 0.9500\n",
      "Epoch 398/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.1850 - accuracy: 0.9278 - val_loss: 0.2216 - val_accuracy: 0.9083\n",
      "Epoch 399/750\n",
      "72/72 [==============================] - 0s 558us/step - loss: 0.2008 - accuracy: 0.9139 - val_loss: 0.1893 - val_accuracy: 0.9250\n",
      "Epoch 400/750\n",
      "72/72 [==============================] - 0s 499us/step - loss: 0.1991 - accuracy: 0.9222 - val_loss: 0.1870 - val_accuracy: 0.9417\n",
      "Epoch 401/750\n",
      "72/72 [==============================] - 0s 503us/step - loss: 0.1901 - accuracy: 0.9278 - val_loss: 0.1696 - val_accuracy: 0.9500\n",
      "Epoch 402/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.1881 - accuracy: 0.9306 - val_loss: 0.1584 - val_accuracy: 0.9500\n",
      "Epoch 403/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.1906 - accuracy: 0.9306 - val_loss: 0.1639 - val_accuracy: 0.9500\n",
      "Epoch 404/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.2029 - accuracy: 0.9250 - val_loss: 0.1628 - val_accuracy: 0.9583\n",
      "Epoch 405/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.1938 - accuracy: 0.9194 - val_loss: 0.1896 - val_accuracy: 0.9250\n",
      "Epoch 406/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2053 - accuracy: 0.9194 - val_loss: 0.2045 - val_accuracy: 0.9250\n",
      "Epoch 407/750\n",
      "72/72 [==============================] - 0s 525us/step - loss: 0.1886 - accuracy: 0.9222 - val_loss: 0.1859 - val_accuracy: 0.9083\n",
      "Epoch 408/750\n",
      "72/72 [==============================] - 0s 496us/step - loss: 0.1830 - accuracy: 0.9389 - val_loss: 0.2161 - val_accuracy: 0.9167\n",
      "Epoch 409/750\n",
      "72/72 [==============================] - 0s 502us/step - loss: 0.2060 - accuracy: 0.9083 - val_loss: 0.1725 - val_accuracy: 0.9333\n",
      "Epoch 410/750\n",
      "72/72 [==============================] - 0s 520us/step - loss: 0.1963 - accuracy: 0.9250 - val_loss: 0.1549 - val_accuracy: 0.9500\n",
      "Epoch 411/750\n",
      "72/72 [==============================] - 0s 495us/step - loss: 0.1959 - accuracy: 0.9278 - val_loss: 0.1657 - val_accuracy: 0.9583\n",
      "Epoch 412/750\n",
      "72/72 [==============================] - 0s 520us/step - loss: 0.1890 - accuracy: 0.9222 - val_loss: 0.1605 - val_accuracy: 0.9583\n",
      "Epoch 413/750\n",
      "72/72 [==============================] - 0s 497us/step - loss: 0.2022 - accuracy: 0.9139 - val_loss: 0.1637 - val_accuracy: 0.9500\n",
      "Epoch 414/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.1957 - accuracy: 0.9222 - val_loss: 0.1675 - val_accuracy: 0.9500\n",
      "Epoch 415/750\n",
      "72/72 [==============================] - 0s 528us/step - loss: 0.1909 - accuracy: 0.9250 - val_loss: 0.1881 - val_accuracy: 0.9250\n",
      "Epoch 416/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.2048 - accuracy: 0.9333 - val_loss: 0.1946 - val_accuracy: 0.9167\n",
      "Epoch 417/750\n",
      "72/72 [==============================] - 0s 524us/step - loss: 0.1853 - accuracy: 0.9278 - val_loss: 0.1608 - val_accuracy: 0.9500\n",
      "Epoch 418/750\n",
      "72/72 [==============================] - 0s 520us/step - loss: 0.1879 - accuracy: 0.9278 - val_loss: 0.1747 - val_accuracy: 0.9167\n",
      "Epoch 419/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.1930 - accuracy: 0.9222 - val_loss: 0.1936 - val_accuracy: 0.9500\n",
      "Epoch 420/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.2128 - accuracy: 0.9083 - val_loss: 0.1792 - val_accuracy: 0.9500\n",
      "Epoch 421/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.1895 - accuracy: 0.9278 - val_loss: 0.1703 - val_accuracy: 0.9250\n",
      "Epoch 422/750\n",
      "72/72 [==============================] - 0s 524us/step - loss: 0.1940 - accuracy: 0.9194 - val_loss: 0.2379 - val_accuracy: 0.9250\n",
      "Epoch 423/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.2108 - accuracy: 0.9111 - val_loss: 0.1735 - val_accuracy: 0.9500\n",
      "Epoch 424/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.1872 - accuracy: 0.9278 - val_loss: 0.1784 - val_accuracy: 0.9417\n",
      "Epoch 425/750\n",
      "72/72 [==============================] - 0s 504us/step - loss: 0.1894 - accuracy: 0.9250 - val_loss: 0.1720 - val_accuracy: 0.9500\n",
      "Epoch 426/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.2006 - accuracy: 0.9167 - val_loss: 0.1752 - val_accuracy: 0.9167\n",
      "Epoch 427/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.2025 - accuracy: 0.9278 - val_loss: 0.1702 - val_accuracy: 0.9583\n",
      "Epoch 428/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.1961 - accuracy: 0.9167 - val_loss: 0.1996 - val_accuracy: 0.9250\n",
      "Epoch 429/750\n",
      "72/72 [==============================] - 0s 497us/step - loss: 0.1882 - accuracy: 0.9250 - val_loss: 0.1853 - val_accuracy: 0.9500\n",
      "Epoch 430/750\n",
      "72/72 [==============================] - 0s 499us/step - loss: 0.2001 - accuracy: 0.9333 - val_loss: 0.1695 - val_accuracy: 0.9583\n",
      "Epoch 431/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.1918 - accuracy: 0.9250 - val_loss: 0.1738 - val_accuracy: 0.9333\n",
      "Epoch 432/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.1967 - accuracy: 0.9167 - val_loss: 0.1907 - val_accuracy: 0.9333\n",
      "Epoch 433/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.1942 - accuracy: 0.9167 - val_loss: 0.1915 - val_accuracy: 0.9250\n",
      "Epoch 434/750\n",
      "72/72 [==============================] - 0s 523us/step - loss: 0.1961 - accuracy: 0.9194 - val_loss: 0.1980 - val_accuracy: 0.9500\n",
      "Epoch 435/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.1919 - accuracy: 0.9306 - val_loss: 0.1932 - val_accuracy: 0.9500\n",
      "Epoch 436/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.1981 - accuracy: 0.9167 - val_loss: 0.1759 - val_accuracy: 0.9583\n",
      "Epoch 437/750\n",
      "72/72 [==============================] - 0s 504us/step - loss: 0.1872 - accuracy: 0.9333 - val_loss: 0.1702 - val_accuracy: 0.9500\n",
      "Epoch 438/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.1900 - accuracy: 0.9194 - val_loss: 0.1789 - val_accuracy: 0.9333\n",
      "Epoch 439/750\n",
      "72/72 [==============================] - 0s 502us/step - loss: 0.1997 - accuracy: 0.9194 - val_loss: 0.1810 - val_accuracy: 0.9583\n",
      "Epoch 440/750\n",
      "72/72 [==============================] - 0s 495us/step - loss: 0.1886 - accuracy: 0.9333 - val_loss: 0.1801 - val_accuracy: 0.9500\n",
      "Epoch 441/750\n",
      "72/72 [==============================] - 0s 524us/step - loss: 0.2024 - accuracy: 0.9222 - val_loss: 0.1712 - val_accuracy: 0.9250\n",
      "Epoch 442/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.1897 - accuracy: 0.9278 - val_loss: 0.1752 - val_accuracy: 0.9500\n",
      "Epoch 443/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.2049 - accuracy: 0.9194 - val_loss: 0.1909 - val_accuracy: 0.9417\n",
      "Epoch 444/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.2125 - accuracy: 0.9167 - val_loss: 0.1781 - val_accuracy: 0.9500\n",
      "Epoch 445/750\n",
      "72/72 [==============================] - 0s 496us/step - loss: 0.1999 - accuracy: 0.9250 - val_loss: 0.1604 - val_accuracy: 0.9500\n",
      "Epoch 446/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 500us/step - loss: 0.1886 - accuracy: 0.9222 - val_loss: 0.1882 - val_accuracy: 0.9250\n",
      "Epoch 447/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.1904 - accuracy: 0.9222 - val_loss: 0.1784 - val_accuracy: 0.9333\n",
      "Epoch 448/750\n",
      "72/72 [==============================] - 0s 502us/step - loss: 0.1936 - accuracy: 0.9333 - val_loss: 0.1761 - val_accuracy: 0.9250\n",
      "Epoch 449/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.1870 - accuracy: 0.9333 - val_loss: 0.1735 - val_accuracy: 0.9500\n",
      "Epoch 450/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.1970 - accuracy: 0.9222 - val_loss: 0.1766 - val_accuracy: 0.9250\n",
      "Epoch 451/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.1866 - accuracy: 0.9333 - val_loss: 0.1727 - val_accuracy: 0.9333\n",
      "Epoch 452/750\n",
      "72/72 [==============================] - 0s 493us/step - loss: 0.2021 - accuracy: 0.9194 - val_loss: 0.2003 - val_accuracy: 0.9167\n",
      "Epoch 453/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.1980 - accuracy: 0.9222 - val_loss: 0.1673 - val_accuracy: 0.9250\n",
      "Epoch 454/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.1936 - accuracy: 0.9278 - val_loss: 0.2227 - val_accuracy: 0.9333\n",
      "Epoch 455/750\n",
      "72/72 [==============================] - 0s 504us/step - loss: 0.1893 - accuracy: 0.9306 - val_loss: 0.1855 - val_accuracy: 0.9083\n",
      "Epoch 456/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.1851 - accuracy: 0.9333 - val_loss: 0.2407 - val_accuracy: 0.9333\n",
      "Epoch 457/750\n",
      "72/72 [==============================] - 0s 498us/step - loss: 0.1991 - accuracy: 0.9250 - val_loss: 0.1931 - val_accuracy: 0.9500\n",
      "Epoch 458/750\n",
      "72/72 [==============================] - 0s 500us/step - loss: 0.1982 - accuracy: 0.9250 - val_loss: 0.1600 - val_accuracy: 0.9583\n",
      "Epoch 459/750\n",
      "72/72 [==============================] - 0s 497us/step - loss: 0.1893 - accuracy: 0.9361 - val_loss: 0.1934 - val_accuracy: 0.9083\n",
      "Epoch 460/750\n",
      "72/72 [==============================] - 0s 520us/step - loss: 0.1999 - accuracy: 0.9222 - val_loss: 0.1781 - val_accuracy: 0.9417\n",
      "Epoch 461/750\n",
      "72/72 [==============================] - 0s 497us/step - loss: 0.1921 - accuracy: 0.9250 - val_loss: 0.1630 - val_accuracy: 0.9500\n",
      "Epoch 462/750\n",
      "72/72 [==============================] - 0s 510us/step - loss: 0.1943 - accuracy: 0.9194 - val_loss: 0.1839 - val_accuracy: 0.9417\n",
      "Epoch 463/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.1931 - accuracy: 0.9222 - val_loss: 0.1904 - val_accuracy: 0.9250\n",
      "Epoch 464/750\n",
      "72/72 [==============================] - 0s 526us/step - loss: 0.2072 - accuracy: 0.9083 - val_loss: 0.1568 - val_accuracy: 0.9583\n",
      "Epoch 465/750\n",
      "72/72 [==============================] - 0s 522us/step - loss: 0.1811 - accuracy: 0.9361 - val_loss: 0.2485 - val_accuracy: 0.9250\n",
      "Epoch 466/750\n",
      "72/72 [==============================] - 0s 496us/step - loss: 0.2009 - accuracy: 0.9250 - val_loss: 0.1841 - val_accuracy: 0.9333\n",
      "Epoch 467/750\n",
      "72/72 [==============================] - 0s 522us/step - loss: 0.1941 - accuracy: 0.9194 - val_loss: 0.1707 - val_accuracy: 0.9250\n",
      "Epoch 468/750\n",
      "72/72 [==============================] - 0s 493us/step - loss: 0.1875 - accuracy: 0.9333 - val_loss: 0.1699 - val_accuracy: 0.9500\n",
      "Epoch 469/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.1902 - accuracy: 0.9250 - val_loss: 0.1830 - val_accuracy: 0.9250\n",
      "Epoch 470/750\n",
      "72/72 [==============================] - 0s 510us/step - loss: 0.1815 - accuracy: 0.9278 - val_loss: 0.1943 - val_accuracy: 0.9250\n",
      "Epoch 471/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.1791 - accuracy: 0.9333 - val_loss: 0.1581 - val_accuracy: 0.9500\n",
      "Epoch 472/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.2128 - accuracy: 0.9222 - val_loss: 0.1519 - val_accuracy: 0.9583\n",
      "Epoch 473/750\n",
      "72/72 [==============================] - 0s 489us/step - loss: 0.1963 - accuracy: 0.9222 - val_loss: 0.2332 - val_accuracy: 0.9167\n",
      "Epoch 474/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.2115 - accuracy: 0.9139 - val_loss: 0.1796 - val_accuracy: 0.9417\n",
      "Epoch 475/750\n",
      "72/72 [==============================] - 0s 531us/step - loss: 0.1857 - accuracy: 0.9306 - val_loss: 0.1618 - val_accuracy: 0.9583\n",
      "Epoch 476/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.1927 - accuracy: 0.9278 - val_loss: 0.1809 - val_accuracy: 0.9250\n",
      "Epoch 477/750\n",
      "72/72 [==============================] - 0s 492us/step - loss: 0.1909 - accuracy: 0.9167 - val_loss: 0.2055 - val_accuracy: 0.9250\n",
      "Epoch 478/750\n",
      "72/72 [==============================] - 0s 524us/step - loss: 0.1820 - accuracy: 0.9278 - val_loss: 0.1577 - val_accuracy: 0.9500\n",
      "Epoch 479/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.1744 - accuracy: 0.9417 - val_loss: 0.2299 - val_accuracy: 0.9083\n",
      "Epoch 480/750\n",
      "72/72 [==============================] - 0s 504us/step - loss: 0.1903 - accuracy: 0.9167 - val_loss: 0.1960 - val_accuracy: 0.9250\n",
      "Epoch 481/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.1934 - accuracy: 0.9306 - val_loss: 0.1676 - val_accuracy: 0.9500\n",
      "Epoch 482/750\n",
      "72/72 [==============================] - 0s 528us/step - loss: 0.1928 - accuracy: 0.9222 - val_loss: 0.1954 - val_accuracy: 0.9083\n",
      "Epoch 483/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.2073 - accuracy: 0.9083 - val_loss: 0.1697 - val_accuracy: 0.9417\n",
      "Epoch 484/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.1948 - accuracy: 0.9306 - val_loss: 0.1703 - val_accuracy: 0.9500\n",
      "Epoch 485/750\n",
      "72/72 [==============================] - 0s 496us/step - loss: 0.1889 - accuracy: 0.9194 - val_loss: 0.1748 - val_accuracy: 0.9417\n",
      "Epoch 486/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.1922 - accuracy: 0.9139 - val_loss: 0.1669 - val_accuracy: 0.9333\n",
      "Epoch 487/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.1969 - accuracy: 0.9222 - val_loss: 0.1997 - val_accuracy: 0.9167\n",
      "Epoch 488/750\n",
      "72/72 [==============================] - 0s 534us/step - loss: 0.1835 - accuracy: 0.9306 - val_loss: 0.2122 - val_accuracy: 0.9250\n",
      "Epoch 489/750\n",
      "72/72 [==============================] - 0s 589us/step - loss: 0.1984 - accuracy: 0.9167 - val_loss: 0.1864 - val_accuracy: 0.9333\n",
      "Epoch 490/750\n",
      "72/72 [==============================] - 0s 498us/step - loss: 0.1816 - accuracy: 0.9417 - val_loss: 0.1688 - val_accuracy: 0.9167\n",
      "Epoch 491/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.1909 - accuracy: 0.9333 - val_loss: 0.1692 - val_accuracy: 0.9500\n",
      "Epoch 492/750\n",
      "72/72 [==============================] - 0s 496us/step - loss: 0.1942 - accuracy: 0.9222 - val_loss: 0.1845 - val_accuracy: 0.9250\n",
      "Epoch 493/750\n",
      "72/72 [==============================] - 0s 493us/step - loss: 0.1778 - accuracy: 0.9389 - val_loss: 0.1690 - val_accuracy: 0.9583\n",
      "Epoch 494/750\n",
      "72/72 [==============================] - 0s 504us/step - loss: 0.2049 - accuracy: 0.9139 - val_loss: 0.1687 - val_accuracy: 0.9500\n",
      "Epoch 495/750\n",
      "72/72 [==============================] - 0s 496us/step - loss: 0.1850 - accuracy: 0.9194 - val_loss: 0.1997 - val_accuracy: 0.9250\n",
      "Epoch 496/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.1905 - accuracy: 0.9278 - val_loss: 0.1784 - val_accuracy: 0.9417\n",
      "Epoch 497/750\n",
      "72/72 [==============================] - 0s 488us/step - loss: 0.1929 - accuracy: 0.9194 - val_loss: 0.1673 - val_accuracy: 0.9417\n",
      "Epoch 498/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.1922 - accuracy: 0.9222 - val_loss: 0.1736 - val_accuracy: 0.9250\n",
      "Epoch 499/750\n",
      "72/72 [==============================] - 0s 510us/step - loss: 0.1812 - accuracy: 0.9361 - val_loss: 0.1752 - val_accuracy: 0.9250\n",
      "Epoch 500/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.1945 - accuracy: 0.9111 - val_loss: 0.1758 - val_accuracy: 0.9500\n",
      "Epoch 501/750\n",
      "72/72 [==============================] - 0s 569us/step - loss: 0.1983 - accuracy: 0.9083 - val_loss: 0.2108 - val_accuracy: 0.9250\n",
      "Epoch 502/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 528us/step - loss: 0.1786 - accuracy: 0.9250 - val_loss: 0.1699 - val_accuracy: 0.9250\n",
      "Epoch 503/750\n",
      "72/72 [==============================] - 0s 544us/step - loss: 0.2021 - accuracy: 0.9139 - val_loss: 0.1572 - val_accuracy: 0.9583\n",
      "Epoch 504/750\n",
      "72/72 [==============================] - 0s 524us/step - loss: 0.1950 - accuracy: 0.9222 - val_loss: 0.1607 - val_accuracy: 0.9333\n",
      "Epoch 505/750\n",
      "72/72 [==============================] - 0s 497us/step - loss: 0.1889 - accuracy: 0.9278 - val_loss: 0.1655 - val_accuracy: 0.9417\n",
      "Epoch 506/750\n",
      "72/72 [==============================] - 0s 534us/step - loss: 0.1783 - accuracy: 0.9278 - val_loss: 0.2267 - val_accuracy: 0.9167\n",
      "Epoch 507/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.1875 - accuracy: 0.9306 - val_loss: 0.1810 - val_accuracy: 0.9250\n",
      "Epoch 508/750\n",
      "72/72 [==============================] - 0s 495us/step - loss: 0.1796 - accuracy: 0.9361 - val_loss: 0.1740 - val_accuracy: 0.9333\n",
      "Epoch 509/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.1948 - accuracy: 0.9222 - val_loss: 0.1882 - val_accuracy: 0.9250\n",
      "Epoch 510/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.1782 - accuracy: 0.9417 - val_loss: 0.1738 - val_accuracy: 0.9417\n",
      "Epoch 511/750\n",
      "72/72 [==============================] - 0s 546us/step - loss: 0.1945 - accuracy: 0.9139 - val_loss: 0.1585 - val_accuracy: 0.9583\n",
      "Epoch 512/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.1889 - accuracy: 0.9250 - val_loss: 0.1580 - val_accuracy: 0.9167\n",
      "Epoch 513/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.1822 - accuracy: 0.9306 - val_loss: 0.1634 - val_accuracy: 0.9500\n",
      "Epoch 514/750\n",
      "72/72 [==============================] - 0s 499us/step - loss: 0.2033 - accuracy: 0.9222 - val_loss: 0.1594 - val_accuracy: 0.9583\n",
      "Epoch 515/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.1852 - accuracy: 0.9333 - val_loss: 0.1557 - val_accuracy: 0.9333\n",
      "Epoch 516/750\n",
      "72/72 [==============================] - 0s 485us/step - loss: 0.1876 - accuracy: 0.9222 - val_loss: 0.2510 - val_accuracy: 0.8833\n",
      "Epoch 517/750\n",
      "72/72 [==============================] - 0s 493us/step - loss: 0.2081 - accuracy: 0.9167 - val_loss: 0.1884 - val_accuracy: 0.9250\n",
      "Epoch 518/750\n",
      "72/72 [==============================] - 0s 499us/step - loss: 0.1852 - accuracy: 0.9278 - val_loss: 0.1587 - val_accuracy: 0.9583\n",
      "Epoch 519/750\n",
      "72/72 [==============================] - 0s 502us/step - loss: 0.1892 - accuracy: 0.9306 - val_loss: 0.1741 - val_accuracy: 0.9500\n",
      "Epoch 520/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.1863 - accuracy: 0.9250 - val_loss: 0.2078 - val_accuracy: 0.9333\n",
      "Epoch 521/750\n",
      "72/72 [==============================] - 0s 485us/step - loss: 0.1948 - accuracy: 0.9222 - val_loss: 0.1645 - val_accuracy: 0.9167\n",
      "Epoch 522/750\n",
      "72/72 [==============================] - 0s 481us/step - loss: 0.1845 - accuracy: 0.9250 - val_loss: 0.1553 - val_accuracy: 0.9250\n",
      "Epoch 523/750\n",
      "72/72 [==============================] - 0s 489us/step - loss: 0.1886 - accuracy: 0.9250 - val_loss: 0.1675 - val_accuracy: 0.9583\n",
      "Epoch 524/750\n",
      "72/72 [==============================] - 0s 500us/step - loss: 0.1936 - accuracy: 0.9083 - val_loss: 0.1829 - val_accuracy: 0.9250\n",
      "Epoch 525/750\n",
      "72/72 [==============================] - 0s 491us/step - loss: 0.1879 - accuracy: 0.9222 - val_loss: 0.1694 - val_accuracy: 0.9250\n",
      "Epoch 526/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.1741 - accuracy: 0.9333 - val_loss: 0.1872 - val_accuracy: 0.9500\n",
      "Epoch 527/750\n",
      "72/72 [==============================] - 0s 497us/step - loss: 0.1866 - accuracy: 0.9306 - val_loss: 0.1627 - val_accuracy: 0.9250\n",
      "Epoch 528/750\n",
      "72/72 [==============================] - 0s 491us/step - loss: 0.1897 - accuracy: 0.9278 - val_loss: 0.1715 - val_accuracy: 0.9417\n",
      "Epoch 529/750\n",
      "72/72 [==============================] - 0s 496us/step - loss: 0.1959 - accuracy: 0.9278 - val_loss: 0.2051 - val_accuracy: 0.9167\n",
      "Epoch 530/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.1848 - accuracy: 0.9250 - val_loss: 0.1906 - val_accuracy: 0.9250\n",
      "Epoch 531/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.1820 - accuracy: 0.9278 - val_loss: 0.1757 - val_accuracy: 0.9250\n",
      "Epoch 532/750\n",
      "72/72 [==============================] - 0s 536us/step - loss: 0.1857 - accuracy: 0.9306 - val_loss: 0.1901 - val_accuracy: 0.9333\n",
      "Epoch 533/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.1783 - accuracy: 0.9222 - val_loss: 0.1634 - val_accuracy: 0.9500\n",
      "Epoch 534/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.1868 - accuracy: 0.9167 - val_loss: 0.1794 - val_accuracy: 0.9167\n",
      "Epoch 535/750\n",
      "72/72 [==============================] - 0s 493us/step - loss: 0.1908 - accuracy: 0.9278 - val_loss: 0.1658 - val_accuracy: 0.9333\n",
      "Epoch 536/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.1778 - accuracy: 0.9306 - val_loss: 0.1849 - val_accuracy: 0.9250\n",
      "Epoch 537/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.1948 - accuracy: 0.9222 - val_loss: 0.2048 - val_accuracy: 0.9167\n",
      "Epoch 538/750\n",
      "72/72 [==============================] - 0s 495us/step - loss: 0.1794 - accuracy: 0.9333 - val_loss: 0.1946 - val_accuracy: 0.9250\n",
      "Epoch 539/750\n",
      "72/72 [==============================] - 0s 535us/step - loss: 0.1833 - accuracy: 0.9278 - val_loss: 0.1939 - val_accuracy: 0.9417\n",
      "Epoch 540/750\n",
      "72/72 [==============================] - 0s 498us/step - loss: 0.1940 - accuracy: 0.9194 - val_loss: 0.1912 - val_accuracy: 0.9333\n",
      "Epoch 541/750\n",
      "72/72 [==============================] - 0s 499us/step - loss: 0.1851 - accuracy: 0.9278 - val_loss: 0.2322 - val_accuracy: 0.9333\n",
      "Epoch 542/750\n",
      "72/72 [==============================] - 0s 488us/step - loss: 0.1996 - accuracy: 0.9167 - val_loss: 0.1580 - val_accuracy: 0.9583\n",
      "Epoch 543/750\n",
      "72/72 [==============================] - 0s 496us/step - loss: 0.1933 - accuracy: 0.9361 - val_loss: 0.1664 - val_accuracy: 0.9500\n",
      "Epoch 544/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.1778 - accuracy: 0.9306 - val_loss: 0.1842 - val_accuracy: 0.9417\n",
      "Epoch 545/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.1880 - accuracy: 0.9306 - val_loss: 0.1566 - val_accuracy: 0.9500\n",
      "Epoch 546/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.1781 - accuracy: 0.9333 - val_loss: 0.1737 - val_accuracy: 0.9417\n",
      "Epoch 547/750\n",
      "72/72 [==============================] - 0s 480us/step - loss: 0.1906 - accuracy: 0.9306 - val_loss: 0.1901 - val_accuracy: 0.9250\n",
      "Epoch 548/750\n",
      "72/72 [==============================] - 0s 477us/step - loss: 0.1946 - accuracy: 0.9306 - val_loss: 0.2352 - val_accuracy: 0.9417\n",
      "Epoch 549/750\n",
      "72/72 [==============================] - 0s 504us/step - loss: 0.1805 - accuracy: 0.9278 - val_loss: 0.1818 - val_accuracy: 0.9250\n",
      "Epoch 550/750\n",
      "72/72 [==============================] - 0s 486us/step - loss: 0.1755 - accuracy: 0.9333 - val_loss: 0.1727 - val_accuracy: 0.9333\n",
      "Epoch 551/750\n",
      "72/72 [==============================] - 0s 511us/step - loss: 0.1867 - accuracy: 0.9167 - val_loss: 0.1513 - val_accuracy: 0.9583\n",
      "Epoch 552/750\n",
      "72/72 [==============================] - 0s 482us/step - loss: 0.1813 - accuracy: 0.9417 - val_loss: 0.2059 - val_accuracy: 0.9333\n",
      "Epoch 553/750\n",
      "72/72 [==============================] - 0s 487us/step - loss: 0.1933 - accuracy: 0.9250 - val_loss: 0.1553 - val_accuracy: 0.9500\n",
      "Epoch 554/750\n",
      "72/72 [==============================] - 0s 525us/step - loss: 0.1888 - accuracy: 0.9333 - val_loss: 0.1637 - val_accuracy: 0.9500\n",
      "Epoch 555/750\n",
      "72/72 [==============================] - 0s 558us/step - loss: 0.1946 - accuracy: 0.9306 - val_loss: 0.1866 - val_accuracy: 0.9250\n",
      "Epoch 556/750\n",
      "72/72 [==============================] - 0s 578us/step - loss: 0.1967 - accuracy: 0.9194 - val_loss: 0.1633 - val_accuracy: 0.9583\n",
      "Epoch 557/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.2008 - accuracy: 0.9083 - val_loss: 0.1598 - val_accuracy: 0.9583\n",
      "Epoch 558/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 538us/step - loss: 0.1989 - accuracy: 0.9250 - val_loss: 0.1630 - val_accuracy: 0.9583\n",
      "Epoch 559/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.1784 - accuracy: 0.9306 - val_loss: 0.1817 - val_accuracy: 0.9250\n",
      "Epoch 560/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.1845 - accuracy: 0.9222 - val_loss: 0.1942 - val_accuracy: 0.9250\n",
      "Epoch 561/750\n",
      "72/72 [==============================] - 0s 531us/step - loss: 0.1811 - accuracy: 0.9250 - val_loss: 0.1586 - val_accuracy: 0.9583\n",
      "Epoch 562/750\n",
      "72/72 [==============================] - 0s 522us/step - loss: 0.1774 - accuracy: 0.9417 - val_loss: 0.2348 - val_accuracy: 0.9000\n",
      "Epoch 563/750\n",
      "72/72 [==============================] - 0s 526us/step - loss: 0.1888 - accuracy: 0.9250 - val_loss: 0.1944 - val_accuracy: 0.9250\n",
      "Epoch 564/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.1877 - accuracy: 0.9333 - val_loss: 0.2440 - val_accuracy: 0.9167\n",
      "Epoch 565/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.1786 - accuracy: 0.9333 - val_loss: 0.1940 - val_accuracy: 0.9250\n",
      "Epoch 566/750\n",
      "72/72 [==============================] - 0s 522us/step - loss: 0.1867 - accuracy: 0.9278 - val_loss: 0.1912 - val_accuracy: 0.9250\n",
      "Epoch 567/750\n",
      "72/72 [==============================] - 0s 531us/step - loss: 0.2073 - accuracy: 0.9250 - val_loss: 0.1738 - val_accuracy: 0.9500\n",
      "Epoch 568/750\n",
      "72/72 [==============================] - 0s 585us/step - loss: 0.1854 - accuracy: 0.9278 - val_loss: 0.2068 - val_accuracy: 0.9333\n",
      "Epoch 569/750\n",
      "72/72 [==============================] - 0s 599us/step - loss: 0.1771 - accuracy: 0.9333 - val_loss: 0.2259 - val_accuracy: 0.9333\n",
      "Epoch 570/750\n",
      "72/72 [==============================] - 0s 528us/step - loss: 0.1863 - accuracy: 0.9222 - val_loss: 0.1955 - val_accuracy: 0.9250\n",
      "Epoch 571/750\n",
      "72/72 [==============================] - 0s 665us/step - loss: 0.1832 - accuracy: 0.9389 - val_loss: 0.1810 - val_accuracy: 0.9417\n",
      "Epoch 572/750\n",
      "72/72 [==============================] - 0s 543us/step - loss: 0.1792 - accuracy: 0.9306 - val_loss: 0.1718 - val_accuracy: 0.9250\n",
      "Epoch 573/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.1895 - accuracy: 0.9222 - val_loss: 0.1748 - val_accuracy: 0.9250\n",
      "Epoch 574/750\n",
      "72/72 [==============================] - 0s 565us/step - loss: 0.1929 - accuracy: 0.9194 - val_loss: 0.1609 - val_accuracy: 0.9500\n",
      "Epoch 575/750\n",
      "72/72 [==============================] - 0s 542us/step - loss: 0.1801 - accuracy: 0.9361 - val_loss: 0.1739 - val_accuracy: 0.9500\n",
      "Epoch 576/750\n",
      "72/72 [==============================] - 0s 504us/step - loss: 0.1802 - accuracy: 0.9222 - val_loss: 0.1545 - val_accuracy: 0.9583\n",
      "Epoch 577/750\n",
      "72/72 [==============================] - 0s 545us/step - loss: 0.1840 - accuracy: 0.9389 - val_loss: 0.1793 - val_accuracy: 0.9500\n",
      "Epoch 578/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.1857 - accuracy: 0.9306 - val_loss: 0.1925 - val_accuracy: 0.9250\n",
      "Epoch 579/750\n",
      "72/72 [==============================] - 0s 571us/step - loss: 0.1880 - accuracy: 0.9361 - val_loss: 0.1533 - val_accuracy: 0.9583\n",
      "Epoch 580/750\n",
      "72/72 [==============================] - 0s 524us/step - loss: 0.1834 - accuracy: 0.9306 - val_loss: 0.1760 - val_accuracy: 0.9500\n",
      "Epoch 581/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.1779 - accuracy: 0.9278 - val_loss: 0.1539 - val_accuracy: 0.9583\n",
      "Epoch 582/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.1858 - accuracy: 0.9306 - val_loss: 0.1548 - val_accuracy: 0.9583\n",
      "Epoch 583/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.2116 - accuracy: 0.9250 - val_loss: 0.1685 - val_accuracy: 0.9500\n",
      "Epoch 584/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.1838 - accuracy: 0.9222 - val_loss: 0.1464 - val_accuracy: 0.9583\n",
      "Epoch 585/750\n",
      "72/72 [==============================] - 0s 533us/step - loss: 0.1883 - accuracy: 0.9278 - val_loss: 0.1564 - val_accuracy: 0.9500\n",
      "Epoch 586/750\n",
      "72/72 [==============================] - 0s 554us/step - loss: 0.1776 - accuracy: 0.9333 - val_loss: 0.1679 - val_accuracy: 0.9333\n",
      "Epoch 587/750\n",
      "72/72 [==============================] - 0s 545us/step - loss: 0.1780 - accuracy: 0.9361 - val_loss: 0.1720 - val_accuracy: 0.9500\n",
      "Epoch 588/750\n",
      "72/72 [==============================] - 0s 531us/step - loss: 0.1765 - accuracy: 0.9389 - val_loss: 0.2771 - val_accuracy: 0.9083\n",
      "Epoch 589/750\n",
      "72/72 [==============================] - 0s 547us/step - loss: 0.2146 - accuracy: 0.9222 - val_loss: 0.1673 - val_accuracy: 0.9500\n",
      "Epoch 590/750\n",
      "72/72 [==============================] - 0s 543us/step - loss: 0.1854 - accuracy: 0.9222 - val_loss: 0.2023 - val_accuracy: 0.9250\n",
      "Epoch 591/750\n",
      "72/72 [==============================] - 0s 545us/step - loss: 0.1740 - accuracy: 0.9306 - val_loss: 0.1630 - val_accuracy: 0.9500\n",
      "Epoch 592/750\n",
      "72/72 [==============================] - 0s 525us/step - loss: 0.1763 - accuracy: 0.9306 - val_loss: 0.1698 - val_accuracy: 0.9500\n",
      "Epoch 593/750\n",
      "72/72 [==============================] - 0s 534us/step - loss: 0.1887 - accuracy: 0.9167 - val_loss: 0.1573 - val_accuracy: 0.9583\n",
      "Epoch 594/750\n",
      "72/72 [==============================] - 0s 526us/step - loss: 0.1766 - accuracy: 0.9278 - val_loss: 0.1859 - val_accuracy: 0.9417\n",
      "Epoch 595/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.1932 - accuracy: 0.9222 - val_loss: 0.1668 - val_accuracy: 0.9583\n",
      "Epoch 596/750\n",
      "72/72 [==============================] - 0s 528us/step - loss: 0.2158 - accuracy: 0.9111 - val_loss: 0.2165 - val_accuracy: 0.9167\n",
      "Epoch 597/750\n",
      "72/72 [==============================] - 0s 508us/step - loss: 0.1872 - accuracy: 0.9333 - val_loss: 0.1648 - val_accuracy: 0.9250\n",
      "Epoch 598/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.1691 - accuracy: 0.9361 - val_loss: 0.1960 - val_accuracy: 0.9083\n",
      "Epoch 599/750\n",
      "72/72 [==============================] - 0s 524us/step - loss: 0.1762 - accuracy: 0.9306 - val_loss: 0.1676 - val_accuracy: 0.9250\n",
      "Epoch 600/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.1787 - accuracy: 0.9250 - val_loss: 0.1765 - val_accuracy: 0.9250\n",
      "Epoch 601/750\n",
      "72/72 [==============================] - 0s 530us/step - loss: 0.1822 - accuracy: 0.9306 - val_loss: 0.2050 - val_accuracy: 0.9167\n",
      "Epoch 602/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.1909 - accuracy: 0.9139 - val_loss: 0.1809 - val_accuracy: 0.9417\n",
      "Epoch 603/750\n",
      "72/72 [==============================] - 0s 546us/step - loss: 0.1922 - accuracy: 0.9167 - val_loss: 0.1537 - val_accuracy: 0.9583\n",
      "Epoch 604/750\n",
      "72/72 [==============================] - 0s 526us/step - loss: 0.1943 - accuracy: 0.9222 - val_loss: 0.1609 - val_accuracy: 0.9333\n",
      "Epoch 605/750\n",
      "72/72 [==============================] - 0s 546us/step - loss: 0.1808 - accuracy: 0.9194 - val_loss: 0.1666 - val_accuracy: 0.9500\n",
      "Epoch 606/750\n",
      "72/72 [==============================] - 0s 566us/step - loss: 0.1781 - accuracy: 0.9361 - val_loss: 0.1763 - val_accuracy: 0.9250\n",
      "Epoch 607/750\n",
      "72/72 [==============================] - 0s 574us/step - loss: 0.1689 - accuracy: 0.9417 - val_loss: 0.1538 - val_accuracy: 0.9500\n",
      "Epoch 608/750\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1884 - accuracy: 0.9222 - val_loss: 0.1848 - val_accuracy: 0.9333\n",
      "Epoch 609/750\n",
      "72/72 [==============================] - 0s 692us/step - loss: 0.1822 - accuracy: 0.9306 - val_loss: 0.1768 - val_accuracy: 0.9250\n",
      "Epoch 610/750\n",
      "72/72 [==============================] - 0s 531us/step - loss: 0.1908 - accuracy: 0.9194 - val_loss: 0.1615 - val_accuracy: 0.9333\n",
      "Epoch 611/750\n",
      "72/72 [==============================] - 0s 532us/step - loss: 0.1871 - accuracy: 0.9306 - val_loss: 0.2620 - val_accuracy: 0.8833\n",
      "Epoch 612/750\n",
      "72/72 [==============================] - 0s 596us/step - loss: 0.1828 - accuracy: 0.9361 - val_loss: 0.1678 - val_accuracy: 0.9583\n",
      "Epoch 613/750\n",
      "72/72 [==============================] - 0s 534us/step - loss: 0.1853 - accuracy: 0.9222 - val_loss: 0.1986 - val_accuracy: 0.9167\n",
      "Epoch 614/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 572us/step - loss: 0.1758 - accuracy: 0.9250 - val_loss: 0.1556 - val_accuracy: 0.9500\n",
      "Epoch 615/750\n",
      "72/72 [==============================] - 0s 522us/step - loss: 0.1969 - accuracy: 0.9222 - val_loss: 0.1824 - val_accuracy: 0.9417\n",
      "Epoch 616/750\n",
      "72/72 [==============================] - 0s 499us/step - loss: 0.1862 - accuracy: 0.9306 - val_loss: 0.2271 - val_accuracy: 0.9250\n",
      "Epoch 617/750\n",
      "72/72 [==============================] - 0s 523us/step - loss: 0.1810 - accuracy: 0.9194 - val_loss: 0.1538 - val_accuracy: 0.9583\n",
      "Epoch 618/750\n",
      "72/72 [==============================] - 0s 498us/step - loss: 0.1875 - accuracy: 0.9194 - val_loss: 0.1606 - val_accuracy: 0.9583\n",
      "Epoch 619/750\n",
      "72/72 [==============================] - 0s 511us/step - loss: 0.1856 - accuracy: 0.9306 - val_loss: 0.1719 - val_accuracy: 0.9250\n",
      "Epoch 620/750\n",
      "72/72 [==============================] - 0s 561us/step - loss: 0.1787 - accuracy: 0.9306 - val_loss: 0.1709 - val_accuracy: 0.9333\n",
      "Epoch 621/750\n",
      "72/72 [==============================] - 0s 513us/step - loss: 0.1835 - accuracy: 0.9306 - val_loss: 0.1963 - val_accuracy: 0.9167\n",
      "Epoch 622/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.1832 - accuracy: 0.9222 - val_loss: 0.2087 - val_accuracy: 0.9250\n",
      "Epoch 623/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.1754 - accuracy: 0.9333 - val_loss: 0.1784 - val_accuracy: 0.9250\n",
      "Epoch 624/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.1768 - accuracy: 0.9333 - val_loss: 0.1883 - val_accuracy: 0.9333\n",
      "Epoch 625/750\n",
      "72/72 [==============================] - 0s 497us/step - loss: 0.1750 - accuracy: 0.9278 - val_loss: 0.1684 - val_accuracy: 0.9583\n",
      "Epoch 626/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.1926 - accuracy: 0.9250 - val_loss: 0.1803 - val_accuracy: 0.9333\n",
      "Epoch 627/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.1870 - accuracy: 0.9250 - val_loss: 0.1997 - val_accuracy: 0.9250\n",
      "Epoch 628/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.1860 - accuracy: 0.9222 - val_loss: 0.2026 - val_accuracy: 0.9250\n",
      "Epoch 629/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.1878 - accuracy: 0.9194 - val_loss: 0.1760 - val_accuracy: 0.9583\n",
      "Epoch 630/750\n",
      "72/72 [==============================] - 0s 525us/step - loss: 0.1786 - accuracy: 0.9250 - val_loss: 0.2530 - val_accuracy: 0.9250\n",
      "Epoch 631/750\n",
      "72/72 [==============================] - 0s 542us/step - loss: 0.1889 - accuracy: 0.9167 - val_loss: 0.2282 - val_accuracy: 0.9333\n",
      "Epoch 632/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.1823 - accuracy: 0.9361 - val_loss: 0.2087 - val_accuracy: 0.9167\n",
      "Epoch 633/750\n",
      "72/72 [==============================] - 0s 521us/step - loss: 0.1805 - accuracy: 0.9333 - val_loss: 0.1886 - val_accuracy: 0.9333\n",
      "Epoch 634/750\n",
      "72/72 [==============================] - 0s 528us/step - loss: 0.1792 - accuracy: 0.9250 - val_loss: 0.1653 - val_accuracy: 0.9583\n",
      "Epoch 635/750\n",
      "72/72 [==============================] - 0s 566us/step - loss: 0.1819 - accuracy: 0.9250 - val_loss: 0.1971 - val_accuracy: 0.9333\n",
      "Epoch 636/750\n",
      "72/72 [==============================] - 0s 526us/step - loss: 0.1776 - accuracy: 0.9194 - val_loss: 0.2801 - val_accuracy: 0.9333\n",
      "Epoch 637/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.1923 - accuracy: 0.9250 - val_loss: 0.1970 - val_accuracy: 0.9417\n",
      "Epoch 638/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.1933 - accuracy: 0.9222 - val_loss: 0.1851 - val_accuracy: 0.9333\n",
      "Epoch 639/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.1809 - accuracy: 0.9222 - val_loss: 0.1971 - val_accuracy: 0.9333\n",
      "Epoch 640/750\n",
      "72/72 [==============================] - 0s 535us/step - loss: 0.1837 - accuracy: 0.9333 - val_loss: 0.2098 - val_accuracy: 0.9250\n",
      "Epoch 641/750\n",
      "72/72 [==============================] - 0s 528us/step - loss: 0.1795 - accuracy: 0.9250 - val_loss: 0.1832 - val_accuracy: 0.9250\n",
      "Epoch 642/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.1753 - accuracy: 0.9389 - val_loss: 0.1745 - val_accuracy: 0.9500\n",
      "Epoch 643/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.1862 - accuracy: 0.9306 - val_loss: 0.2051 - val_accuracy: 0.9250\n",
      "Epoch 644/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.1959 - accuracy: 0.9222 - val_loss: 0.1704 - val_accuracy: 0.9333\n",
      "Epoch 645/750\n",
      "72/72 [==============================] - 0s 540us/step - loss: 0.1882 - accuracy: 0.9306 - val_loss: 0.1769 - val_accuracy: 0.9500\n",
      "Epoch 646/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.1814 - accuracy: 0.9278 - val_loss: 0.1903 - val_accuracy: 0.9250\n",
      "Epoch 647/750\n",
      "72/72 [==============================] - 0s 504us/step - loss: 0.1755 - accuracy: 0.9250 - val_loss: 0.2077 - val_accuracy: 0.9333\n",
      "Epoch 648/750\n",
      "72/72 [==============================] - 0s 552us/step - loss: 0.1753 - accuracy: 0.9306 - val_loss: 0.1723 - val_accuracy: 0.9333\n",
      "Epoch 649/750\n",
      "72/72 [==============================] - 0s 504us/step - loss: 0.1866 - accuracy: 0.9222 - val_loss: 0.1832 - val_accuracy: 0.9500\n",
      "Epoch 650/750\n",
      "72/72 [==============================] - 0s 524us/step - loss: 0.1733 - accuracy: 0.9389 - val_loss: 0.2143 - val_accuracy: 0.9250\n",
      "Epoch 651/750\n",
      "72/72 [==============================] - 0s 523us/step - loss: 0.1718 - accuracy: 0.9361 - val_loss: 0.2092 - val_accuracy: 0.9250\n",
      "Epoch 652/750\n",
      "72/72 [==============================] - 0s 498us/step - loss: 0.1847 - accuracy: 0.9250 - val_loss: 0.1797 - val_accuracy: 0.9417\n",
      "Epoch 653/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.1941 - accuracy: 0.9139 - val_loss: 0.1974 - val_accuracy: 0.9250\n",
      "Epoch 654/750\n",
      "72/72 [==============================] - 0s 503us/step - loss: 0.1720 - accuracy: 0.9306 - val_loss: 0.2120 - val_accuracy: 0.9167\n",
      "Epoch 655/750\n",
      "72/72 [==============================] - 0s 512us/step - loss: 0.1822 - accuracy: 0.9306 - val_loss: 0.1867 - val_accuracy: 0.9500\n",
      "Epoch 656/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.2068 - accuracy: 0.9139 - val_loss: 0.1768 - val_accuracy: 0.9250\n",
      "Epoch 657/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.1794 - accuracy: 0.9444 - val_loss: 0.1715 - val_accuracy: 0.9417\n",
      "Epoch 658/750\n",
      "72/72 [==============================] - 0s 530us/step - loss: 0.1859 - accuracy: 0.9194 - val_loss: 0.2053 - val_accuracy: 0.9167\n",
      "Epoch 659/750\n",
      "72/72 [==============================] - 0s 538us/step - loss: 0.1740 - accuracy: 0.9417 - val_loss: 0.1847 - val_accuracy: 0.9250\n",
      "Epoch 660/750\n",
      "72/72 [==============================] - 0s 511us/step - loss: 0.1660 - accuracy: 0.9444 - val_loss: 0.2596 - val_accuracy: 0.9333\n",
      "Epoch 661/750\n",
      "72/72 [==============================] - 0s 659us/step - loss: 0.1807 - accuracy: 0.9333 - val_loss: 0.1693 - val_accuracy: 0.9417\n",
      "Epoch 662/750\n",
      "72/72 [==============================] - 0s 525us/step - loss: 0.1814 - accuracy: 0.9306 - val_loss: 0.1871 - val_accuracy: 0.9500\n",
      "Epoch 663/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.1807 - accuracy: 0.9250 - val_loss: 0.2047 - val_accuracy: 0.9250\n",
      "Epoch 664/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.1712 - accuracy: 0.9361 - val_loss: 0.2019 - val_accuracy: 0.9250\n",
      "Epoch 665/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.1822 - accuracy: 0.9222 - val_loss: 0.1791 - val_accuracy: 0.9333\n",
      "Epoch 666/750\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.1844 - accuracy: 0.9306 - val_loss: 0.1624 - val_accuracy: 0.9583\n",
      "Epoch 667/750\n",
      "72/72 [==============================] - 0s 532us/step - loss: 0.1753 - accuracy: 0.9361 - val_loss: 0.1851 - val_accuracy: 0.9333\n",
      "Epoch 668/750\n",
      "72/72 [==============================] - 0s 551us/step - loss: 0.1720 - accuracy: 0.9417 - val_loss: 0.2324 - val_accuracy: 0.9333\n",
      "Epoch 669/750\n",
      "72/72 [==============================] - 0s 535us/step - loss: 0.1803 - accuracy: 0.9278 - val_loss: 0.1897 - val_accuracy: 0.9417\n",
      "Epoch 670/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 512us/step - loss: 0.1802 - accuracy: 0.9333 - val_loss: 0.1682 - val_accuracy: 0.9500\n",
      "Epoch 671/750\n",
      "72/72 [==============================] - 0s 531us/step - loss: 0.1859 - accuracy: 0.9111 - val_loss: 0.2159 - val_accuracy: 0.9167\n",
      "Epoch 672/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.1789 - accuracy: 0.9333 - val_loss: 0.1631 - val_accuracy: 0.9500\n",
      "Epoch 673/750\n",
      "72/72 [==============================] - 0s 520us/step - loss: 0.1794 - accuracy: 0.9306 - val_loss: 0.1720 - val_accuracy: 0.9333\n",
      "Epoch 674/750\n",
      "72/72 [==============================] - 0s 519us/step - loss: 0.1657 - accuracy: 0.9417 - val_loss: 0.2288 - val_accuracy: 0.9417\n",
      "Epoch 675/750\n",
      "72/72 [==============================] - 0s 520us/step - loss: 0.1793 - accuracy: 0.9306 - val_loss: 0.2266 - val_accuracy: 0.9167\n",
      "Epoch 676/750\n",
      "72/72 [==============================] - 0s 542us/step - loss: 0.1996 - accuracy: 0.9250 - val_loss: 0.1866 - val_accuracy: 0.9250\n",
      "Epoch 677/750\n",
      "72/72 [==============================] - 0s 539us/step - loss: 0.1901 - accuracy: 0.9167 - val_loss: 0.1738 - val_accuracy: 0.9333\n",
      "Epoch 678/750\n",
      "72/72 [==============================] - 0s 533us/step - loss: 0.1743 - accuracy: 0.9361 - val_loss: 0.1752 - val_accuracy: 0.9500\n",
      "Epoch 679/750\n",
      "72/72 [==============================] - 0s 518us/step - loss: 0.1870 - accuracy: 0.9278 - val_loss: 0.1612 - val_accuracy: 0.9583\n",
      "Epoch 680/750\n",
      "72/72 [==============================] - 0s 522us/step - loss: 0.1755 - accuracy: 0.9333 - val_loss: 0.1737 - val_accuracy: 0.9500\n",
      "Epoch 681/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.1660 - accuracy: 0.9333 - val_loss: 0.1711 - val_accuracy: 0.9583\n",
      "Epoch 682/750\n",
      "72/72 [==============================] - 0s 572us/step - loss: 0.1731 - accuracy: 0.9333 - val_loss: 0.1794 - val_accuracy: 0.9167\n",
      "Epoch 683/750\n",
      "72/72 [==============================] - 0s 530us/step - loss: 0.1731 - accuracy: 0.9361 - val_loss: 0.1891 - val_accuracy: 0.9250\n",
      "Epoch 684/750\n",
      "72/72 [==============================] - 0s 514us/step - loss: 0.1770 - accuracy: 0.9306 - val_loss: 0.1776 - val_accuracy: 0.9500\n",
      "Epoch 685/750\n",
      "72/72 [==============================] - 0s 525us/step - loss: 0.1740 - accuracy: 0.9333 - val_loss: 0.1575 - val_accuracy: 0.9583\n",
      "Epoch 686/750\n",
      "72/72 [==============================] - 0s 520us/step - loss: 0.1755 - accuracy: 0.9361 - val_loss: 0.1690 - val_accuracy: 0.9167\n",
      "Epoch 687/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.1708 - accuracy: 0.9444 - val_loss: 0.1739 - val_accuracy: 0.9500\n",
      "Epoch 688/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.1748 - accuracy: 0.9333 - val_loss: 0.1997 - val_accuracy: 0.9250\n",
      "Epoch 689/750\n",
      "72/72 [==============================] - 0s 536us/step - loss: 0.1794 - accuracy: 0.9222 - val_loss: 0.2087 - val_accuracy: 0.9333\n",
      "Epoch 690/750\n",
      "72/72 [==============================] - 0s 500us/step - loss: 0.1767 - accuracy: 0.9361 - val_loss: 0.2029 - val_accuracy: 0.9250\n",
      "Epoch 691/750\n",
      "72/72 [==============================] - 0s 526us/step - loss: 0.1741 - accuracy: 0.9222 - val_loss: 0.2442 - val_accuracy: 0.9083\n",
      "Epoch 692/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.1795 - accuracy: 0.9444 - val_loss: 0.1757 - val_accuracy: 0.9500\n",
      "Epoch 693/750\n",
      "72/72 [==============================] - 0s 499us/step - loss: 0.1720 - accuracy: 0.9278 - val_loss: 0.1685 - val_accuracy: 0.9500\n",
      "Epoch 694/750\n",
      "72/72 [==============================] - 0s 498us/step - loss: 0.1824 - accuracy: 0.9250 - val_loss: 0.1735 - val_accuracy: 0.9333\n",
      "Epoch 695/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.1766 - accuracy: 0.9417 - val_loss: 0.2141 - val_accuracy: 0.9250\n",
      "Epoch 696/750\n",
      "72/72 [==============================] - 0s 503us/step - loss: 0.1633 - accuracy: 0.9306 - val_loss: 0.1974 - val_accuracy: 0.9250\n",
      "Epoch 697/750\n",
      "72/72 [==============================] - 0s 486us/step - loss: 0.1893 - accuracy: 0.9250 - val_loss: 0.1866 - val_accuracy: 0.9250\n",
      "Epoch 698/750\n",
      "72/72 [==============================] - 0s 492us/step - loss: 0.1782 - accuracy: 0.9306 - val_loss: 0.1936 - val_accuracy: 0.9167\n",
      "Epoch 699/750\n",
      "72/72 [==============================] - 0s 503us/step - loss: 0.1862 - accuracy: 0.9278 - val_loss: 0.1622 - val_accuracy: 0.9500\n",
      "Epoch 700/750\n",
      "72/72 [==============================] - 0s 493us/step - loss: 0.2116 - accuracy: 0.9028 - val_loss: 0.2396 - val_accuracy: 0.9250\n",
      "Epoch 701/750\n",
      "72/72 [==============================] - 0s 493us/step - loss: 0.1780 - accuracy: 0.9278 - val_loss: 0.2245 - val_accuracy: 0.9250\n",
      "Epoch 702/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.1751 - accuracy: 0.9333 - val_loss: 0.1939 - val_accuracy: 0.9250\n",
      "Epoch 703/750\n",
      "72/72 [==============================] - 0s 547us/step - loss: 0.1729 - accuracy: 0.9222 - val_loss: 0.2279 - val_accuracy: 0.9250\n",
      "Epoch 704/750\n",
      "72/72 [==============================] - 0s 525us/step - loss: 0.1767 - accuracy: 0.9278 - val_loss: 0.1903 - val_accuracy: 0.9250\n",
      "Epoch 705/750\n",
      "72/72 [==============================] - 0s 543us/step - loss: 0.1673 - accuracy: 0.9389 - val_loss: 0.1844 - val_accuracy: 0.9250\n",
      "Epoch 706/750\n",
      "72/72 [==============================] - 0s 490us/step - loss: 0.1732 - accuracy: 0.9361 - val_loss: 0.1714 - val_accuracy: 0.9333\n",
      "Epoch 707/750\n",
      "72/72 [==============================] - 0s 490us/step - loss: 0.1733 - accuracy: 0.9361 - val_loss: 0.1637 - val_accuracy: 0.9583\n",
      "Epoch 708/750\n",
      "72/72 [==============================] - 0s 498us/step - loss: 0.1895 - accuracy: 0.9250 - val_loss: 0.2014 - val_accuracy: 0.9167\n",
      "Epoch 709/750\n",
      "72/72 [==============================] - 0s 494us/step - loss: 0.1814 - accuracy: 0.9306 - val_loss: 0.1770 - val_accuracy: 0.9333\n",
      "Epoch 710/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.1763 - accuracy: 0.9389 - val_loss: 0.1617 - val_accuracy: 0.9583\n",
      "Epoch 711/750\n",
      "72/72 [==============================] - 0s 494us/step - loss: 0.1925 - accuracy: 0.9361 - val_loss: 0.1805 - val_accuracy: 0.9333\n",
      "Epoch 712/750\n",
      "72/72 [==============================] - 0s 497us/step - loss: 0.1716 - accuracy: 0.9306 - val_loss: 0.1993 - val_accuracy: 0.9250\n",
      "Epoch 713/750\n",
      "72/72 [==============================] - 0s 485us/step - loss: 0.1675 - accuracy: 0.9306 - val_loss: 0.1593 - val_accuracy: 0.9500\n",
      "Epoch 714/750\n",
      "72/72 [==============================] - 0s 490us/step - loss: 0.1824 - accuracy: 0.9306 - val_loss: 0.1790 - val_accuracy: 0.9250\n",
      "Epoch 715/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.1669 - accuracy: 0.9389 - val_loss: 0.1616 - val_accuracy: 0.9500\n",
      "Epoch 716/750\n",
      "72/72 [==============================] - 0s 505us/step - loss: 0.1800 - accuracy: 0.9222 - val_loss: 0.1554 - val_accuracy: 0.9583\n",
      "Epoch 717/750\n",
      "72/72 [==============================] - 0s 511us/step - loss: 0.1689 - accuracy: 0.9333 - val_loss: 0.1670 - val_accuracy: 0.9500\n",
      "Epoch 718/750\n",
      "72/72 [==============================] - 0s 475us/step - loss: 0.1756 - accuracy: 0.9333 - val_loss: 0.1548 - val_accuracy: 0.9583\n",
      "Epoch 719/750\n",
      "72/72 [==============================] - 0s 492us/step - loss: 0.1838 - accuracy: 0.9250 - val_loss: 0.1563 - val_accuracy: 0.9583\n",
      "Epoch 720/750\n",
      "72/72 [==============================] - 0s 499us/step - loss: 0.1810 - accuracy: 0.9222 - val_loss: 0.1728 - val_accuracy: 0.9333\n",
      "Epoch 721/750\n",
      "72/72 [==============================] - 0s 486us/step - loss: 0.1778 - accuracy: 0.9306 - val_loss: 0.1744 - val_accuracy: 0.9333\n",
      "Epoch 722/750\n",
      "72/72 [==============================] - 0s 498us/step - loss: 0.1719 - accuracy: 0.9361 - val_loss: 0.1791 - val_accuracy: 0.9250\n",
      "Epoch 723/750\n",
      "72/72 [==============================] - 0s 504us/step - loss: 0.1741 - accuracy: 0.9222 - val_loss: 0.1828 - val_accuracy: 0.9250\n",
      "Epoch 724/750\n",
      "72/72 [==============================] - 0s 495us/step - loss: 0.1673 - accuracy: 0.9444 - val_loss: 0.1736 - val_accuracy: 0.9333\n",
      "Epoch 725/750\n",
      "72/72 [==============================] - 0s 496us/step - loss: 0.1670 - accuracy: 0.9444 - val_loss: 0.1675 - val_accuracy: 0.9333\n",
      "Epoch 726/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 507us/step - loss: 0.1739 - accuracy: 0.9389 - val_loss: 0.2172 - val_accuracy: 0.9000\n",
      "Epoch 727/750\n",
      "72/72 [==============================] - 0s 492us/step - loss: 0.1885 - accuracy: 0.9306 - val_loss: 0.2004 - val_accuracy: 0.9250\n",
      "Epoch 728/750\n",
      "72/72 [==============================] - 0s 479us/step - loss: 0.1747 - accuracy: 0.9278 - val_loss: 0.2449 - val_accuracy: 0.9333\n",
      "Epoch 729/750\n",
      "72/72 [==============================] - 0s 500us/step - loss: 0.1840 - accuracy: 0.9361 - val_loss: 0.2014 - val_accuracy: 0.9250\n",
      "Epoch 730/750\n",
      "72/72 [==============================] - 0s 495us/step - loss: 0.1694 - accuracy: 0.9306 - val_loss: 0.1903 - val_accuracy: 0.9250\n",
      "Epoch 731/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.1644 - accuracy: 0.9444 - val_loss: 0.2063 - val_accuracy: 0.9250\n",
      "Epoch 732/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.1655 - accuracy: 0.9361 - val_loss: 0.1753 - val_accuracy: 0.9500\n",
      "Epoch 733/750\n",
      "72/72 [==============================] - 0s 485us/step - loss: 0.1699 - accuracy: 0.9389 - val_loss: 0.2682 - val_accuracy: 0.9250\n",
      "Epoch 734/750\n",
      "72/72 [==============================] - 0s 532us/step - loss: 0.1840 - accuracy: 0.9361 - val_loss: 0.1577 - val_accuracy: 0.9583\n",
      "Epoch 735/750\n",
      "72/72 [==============================] - 0s 507us/step - loss: 0.1920 - accuracy: 0.9278 - val_loss: 0.1701 - val_accuracy: 0.9417\n",
      "Epoch 736/750\n",
      "72/72 [==============================] - 0s 546us/step - loss: 0.1867 - accuracy: 0.9250 - val_loss: 0.2203 - val_accuracy: 0.9250\n",
      "Epoch 737/750\n",
      "72/72 [==============================] - 0s 503us/step - loss: 0.1744 - accuracy: 0.9306 - val_loss: 0.2017 - val_accuracy: 0.9250\n",
      "Epoch 738/750\n",
      "72/72 [==============================] - 0s 487us/step - loss: 0.1789 - accuracy: 0.9278 - val_loss: 0.2290 - val_accuracy: 0.9167\n",
      "Epoch 739/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.1698 - accuracy: 0.9333 - val_loss: 0.1776 - val_accuracy: 0.9250\n",
      "Epoch 740/750\n",
      "72/72 [==============================] - 0s 517us/step - loss: 0.1717 - accuracy: 0.9333 - val_loss: 0.2782 - val_accuracy: 0.9167\n",
      "Epoch 741/750\n",
      "72/72 [==============================] - 0s 601us/step - loss: 0.1942 - accuracy: 0.9278 - val_loss: 0.1898 - val_accuracy: 0.9250\n",
      "Epoch 742/750\n",
      "72/72 [==============================] - 0s 535us/step - loss: 0.1809 - accuracy: 0.9222 - val_loss: 0.2132 - val_accuracy: 0.9250\n",
      "Epoch 743/750\n",
      "72/72 [==============================] - 0s 642us/step - loss: 0.1734 - accuracy: 0.9306 - val_loss: 0.2205 - val_accuracy: 0.9000\n",
      "Epoch 744/750\n",
      "72/72 [==============================] - 0s 510us/step - loss: 0.1762 - accuracy: 0.9361 - val_loss: 0.2153 - val_accuracy: 0.9250\n",
      "Epoch 745/750\n",
      "72/72 [==============================] - 0s 501us/step - loss: 0.1794 - accuracy: 0.9278 - val_loss: 0.1633 - val_accuracy: 0.9583\n",
      "Epoch 746/750\n",
      "72/72 [==============================] - 0s 515us/step - loss: 0.1781 - accuracy: 0.9306 - val_loss: 0.1752 - val_accuracy: 0.9333\n",
      "Epoch 747/750\n",
      "72/72 [==============================] - 0s 494us/step - loss: 0.1764 - accuracy: 0.9222 - val_loss: 0.2135 - val_accuracy: 0.9333\n",
      "Epoch 748/750\n",
      "72/72 [==============================] - 0s 509us/step - loss: 0.1791 - accuracy: 0.9306 - val_loss: 0.2026 - val_accuracy: 0.9250\n",
      "Epoch 749/750\n",
      "72/72 [==============================] - 0s 497us/step - loss: 0.1672 - accuracy: 0.9417 - val_loss: 0.1838 - val_accuracy: 0.9500\n",
      "Epoch 750/750\n",
      "72/72 [==============================] - 0s 506us/step - loss: 0.1697 - accuracy: 0.9333 - val_loss: 0.1690 - val_accuracy: 0.9333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        40\n",
      "           1       0.93      0.93      0.93        40\n",
      "           2       0.97      0.97      0.97        40\n",
      "\n",
      "    accuracy                           0.95       120\n",
      "   macro avg       0.95      0.95      0.95       120\n",
      "weighted avg       0.95      0.95      0.95       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 750\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "model = getNetwork()\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.25)\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Models in C code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models in Pipeline\n",
    "modelsInPipeline = []\n",
    "modelsInPipeline.append('LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpx7x9ouh0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpx7x9ouh0/assets\n"
     ]
    }
   ],
   "source": [
    "# Neural network with TinyMLGen\n",
    "with open('exportedModels/NNmodel.h', 'w') as f:\n",
    "    f.write(tiny.port(model, optimize=False))\n",
    "\n",
    "# Classifiers with MicroMLGen\n",
    "for name, model in models:\n",
    "    prepath = 'exportedModels/'\n",
    "    path = prepath + name + '.h'\n",
    "    if name in modelsInPipeline:\n",
    "        model = model[1]\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(port(model, optimize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
