{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Comparison for TinyML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "import pickle\n",
    "\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,  classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold,StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dense, Input, concatenate, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from micromlgen import port\n",
    "import tinymlgen as tiny\n",
    "\n",
    "import warnings\n",
    "import sys\n",
    "import seaborn as sbs\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change 'chosenIndex' to change the chosen Test (s/s3/s6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataXPath = ['../data/X.pkl', '../data/XS3.pkl', '../data/XS6.pkl']\n",
    "dataYPath = ['../data/y.pkl', '../data/yS3.pkl', '../data/yS6.pkl']\n",
    "choosenIndex = 2\n",
    "\n",
    "with open(dataXPath[choosenIndex], 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open(dataYPath[choosenIndex], 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000]\n",
      "['2000', '2100', '2200', '2300', '2400', '2500', '2600', '2700', '2800', '2900', '3000']\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y))\n",
    "labels = [str(el) for el in list(np.unique(y))]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3  3\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  6  6  6  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  9  9  9  9  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n"
     ]
    }
   ],
   "source": [
    "# Convert the labels in values like 0...n for the NN tests\n",
    "\n",
    "labels = []\n",
    "uniques = list(np.unique(y))\n",
    "\n",
    "[labels.append(uniques.index(el)) for el in y]\n",
    "\n",
    "y = np.array(labels)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4285.7  4294.7  4306.07 4293.05 4286.88 4312.93 4301.12 4287.5  4284.17\n",
      "  4290.04 4285.91 4269.82 4283.57 4264.58 4259.01 4268.87 4246.28 4241.\n",
      "  4245.03 4236.4  4226.57 4236.02 4231.11 4218.93 4228.91 4229.2  4215.02\n",
      "  4215.3  4214.23 4204.33 4204.88 4209.74]\n",
      " [4002.17 4015.39 4018.22 4015.45 4016.49 4028.62 4032.57 4026.54 4027.87\n",
      "  4029.12 4038.97 4038.42 4029.67 4036.72 4036.27 4039.25 4029.92 4038.12\n",
      "  4039.1  4041.64 4040.98 4042.86 4039.91 4048.86 4053.43 4053.   4054.18\n",
      "  4053.5  4055.97 4057.14 4057.02 4063.76]\n",
      " [5257.57 5255.8  5257.39 5218.43 5261.56 5252.22 5241.1  5202.98 5263.5\n",
      "  5186.01 5237.17 5252.8  5204.57 5229.22 5210.17 5241.79 5208.13 5210.38\n",
      "  5218.85 5226.97 5264.2  5170.76 5247.67 5202.49 5187.13 5157.64 5175.31\n",
      "  5161.65 5183.45 5260.1  5194.25 5213.14]\n",
      " [4651.56 4657.4  4666.16 4644.15 4662.31 4664.16 4654.97 4659.11 4669.47\n",
      "  4650.75 4661.73 4665.12 4661.75 4659.81 4674.65 4656.95 4659.17 4670.53\n",
      "  4666.99 4663.3  4676.72 4664.78 4666.05 4681.82 4683.45 4668.1  4689.23\n",
      "  4682.67 4673.25 4691.56 4700.54 4679.8 ]\n",
      " [4735.3  4729.42 4717.07 4722.76 4712.48 4727.77 4795.11 4794.24 4859.97\n",
      "  4719.84 4691.06 4739.58 4693.46 4722.77 4753.11 4727.86 4698.81 4711.94\n",
      "  4708.91 4686.69 4719.82 4711.56 4693.49 4731.29 4709.92 4740.18 4676.35\n",
      "  4697.   4786.77 4715.63 4692.2  4745.64]\n",
      " [4765.68 4742.69 4764.18 4766.98 4753.5  4765.03 4777.96 4776.65 4762.14\n",
      "  4775.39 4760.58 4773.51 4777.7  4762.86 4764.15 4779.97 4770.71 4758.22\n",
      "  4777.26 4766.52 4764.66 4772.45 4779.05 4756.2  4783.25 4777.67 4771.36\n",
      "  4777.55 4787.29 4769.71 4784.51 4785.54]\n",
      " [4002.47 4010.66 4005.25 4011.19 4013.33 4012.11 4009.63 4024.49 4018.02\n",
      "  4018.7  4028.64 4029.98 4026.63 4037.55 4036.71 4043.96 4041.27 4047.02\n",
      "  4047.47 4049.88 4049.34 4044.36 4051.23 4059.64 4059.76 4061.42 4066.79\n",
      "  4068.62 4065.62 4077.77 4078.07 4077.16]\n",
      " [4820.02 4941.13 4787.46 4905.74 4830.28 4836.14 4813.37 4893.81 4809.42\n",
      "  4810.19 4821.27 4803.58 4841.25 4835.8  4813.65 4798.87 4814.53 4820.08\n",
      "  4806.26 4815.5  4811.94 4810.81 4850.12 4827.5  4820.11 4827.92 4816.76\n",
      "  4794.   4818.17 4811.19 4820.62 4830.26]\n",
      " [4664.23 4602.29 4614.51 4721.94 4589.06 4597.87 4612.86 4604.48 4601.48\n",
      "  4583.23 4608.76 4616.14 4613.56 4636.3  4633.15 4587.39 4616.32 4575.76\n",
      "  4602.66 4624.45 4702.51 4693.34 4705.25 4583.44 4613.92 4593.66 4595.71\n",
      "  4617.23 4664.8  4647.6  4603.59 4605.26]\n",
      " [3420.3  3471.7  3453.37 3460.44 3421.77 3467.86 3444.45 3431.95 3425.22\n",
      "  3443.57 3419.61 3437.86 3415.95 3441.99 3447.86 3395.02 3439.68 3405.78\n",
      "  3402.81 3576.6  3448.41 3411.03 3439.68 3433.46 3418.4  3403.06 3443.6\n",
      "  3440.73 3393.42 3428.64 3462.34 3408.88]\n",
      " [3534.76 3407.07 3472.43 3439.88 3389.8  3459.44 3445.7  3435.68 3412.32\n",
      "  3377.38 3472.32 3451.13 3444.22 3385.5  3377.67 3427.09 3451.98 3391.1\n",
      "  3440.78 3394.05 3406.37 3380.76 3380.84 3356.54 3462.29 3395.96 3399.81\n",
      "  3423.79 3443.24 3384.91 3401.64 3360.22]\n",
      " [5191.13 5194.17 5179.63 5290.21 5148.35 5163.3  5142.59 5150.89 5093.9\n",
      "  5177.19 5110.95 5133.1  5095.89 5132.07 5142.18 5109.25 5125.3  5183.34\n",
      "  5066.84 5105.3  5088.96 5058.95 5106.06 5092.76 5114.53 5095.59 5105.46\n",
      "  5196.08 5046.21 5202.38 5031.64 5074.97]\n",
      " [4865.58 4943.24 4885.97 4896.96 4842.37 4886.88 4849.64 4834.77 4909.31\n",
      "  4855.25 4930.09 4881.89 4936.58 4826.33 4843.53 4858.44 4856.07 4836.49\n",
      "  4853.73 4831.6  4893.07 4897.62 4891.65 4869.94 4896.91 4861.16 4880.22\n",
      "  4894.24 4861.36 4843.61 4884.29 4933.94]\n",
      " [4254.18 4242.38 4223.12 4210.77 4326.77 4269.2  4267.28 4247.9  4253.01\n",
      "  4253.75 4245.53 4210.58 4223.53 4209.31 4242.13 4244.26 4218.62 4292.42\n",
      "  4189.36 4219.81 4322.94 4197.92 4238.33 4224.01 4244.71 4224.76 4215.24\n",
      "  4192.82 4234.32 4235.28 4211.8  4191.02]\n",
      " [5150.5  5164.47 5159.41 5140.03 5171.83 5139.82 5150.98 5182.42 5144.68\n",
      "  5158.65 5160.27 5144.38 5145.73 5162.4  5135.74 5145.82 5146.84 5151.85\n",
      "  5155.42 5159.39 5128.59 5163.74 5179.62 5153.2  5283.92 5149.83 5115.69\n",
      "  5125.16 5154.91 5127.91 5128.34 5136.82]\n",
      " [5115.28 5100.14 5140.86 5103.59 5111.23 5104.   5177.86 5095.19 5152.26\n",
      "  5146.83 5119.66 5115.37 5130.94 5114.77 5126.37 5116.97 5125.33 5103.63\n",
      "  5126.59 5112.95 5135.58 5118.36 5142.06 5124.39 5144.45 5094.82 5133.73\n",
      "  5137.12 5143.46 5106.89 5166.78 5102.06]\n",
      " [4752.5  4748.46 4781.07 4712.68 4733.74 4721.6  4775.09 4732.71 4829.22\n",
      "  4756.3  4726.25 4835.06 4716.55 4722.02 4772.16 4747.23 4821.45 4810.81\n",
      "  4788.8  4829.42 4733.39 4716.1  4790.02 4692.82 4710.27 4762.85 4791.99\n",
      "  4724.04 4721.56 4700.05 4709.76 4708.64]\n",
      " [3457.11 3440.68 3425.24 3389.65 3427.69 3391.66 3401.73 3348.58 3377.16\n",
      "  3384.58 3384.15 3458.96 3383.7  3369.54 3386.63 3379.35 3466.23 3390.46\n",
      "  3416.7  3400.61 3397.6  3350.04 3481.69 3366.18 3367.78 3404.51 3371.62\n",
      "  3420.17 3378.68 3435.37 3385.76 3357.21]\n",
      " [4679.7  4777.33 4755.85 4735.23 4692.14 4711.15 4707.25 4747.79 4804.29\n",
      "  4705.44 4679.45 4712.42 4700.79 4684.75 4742.83 4725.22 4685.69 4758.27\n",
      "  4739.43 4733.49 4713.97 4732.17 4747.39 4701.97 4739.73 4728.02 4697.88\n",
      "  4702.53 4703.69 4688.31 4713.13 4681.43]\n",
      " [4270.12 4278.75 4279.86 4270.87 4272.37 4268.41 4261.5  4268.99 4257.7\n",
      "  4253.92 4259.34 4255.53 4244.94 4251.93 4246.96 4243.18 4242.38 4249.07\n",
      "  4235.28 4232.97 4243.99 4224.87 4225.93 4238.12 4231.85 4215.54 4249.51\n",
      "  4228.06 4219.23 4223.69 4214.31 4218.31]\n",
      " [3340.74 3333.96 3352.55 3367.47 3356.27 3331.28 3372.61 3329.7  3354.12\n",
      "  3329.81 3359.57 3360.44 3373.48 3333.25 3331.84 3319.45 3325.1  3351.54\n",
      "  3364.07 3324.72 3359.44 3317.77 3327.83 3324.62 3325.11 3307.52 3313.27\n",
      "  3312.06 3319.08 3294.34 3309.55 3294.65]\n",
      " [4924.29 4967.02 4938.77 4957.79 4955.72 4974.13 4944.19 4965.16 4943.18\n",
      "  4959.14 4927.71 4953.   4898.14 4923.66 4906.06 4889.64 4858.89 4877.42\n",
      "  4819.19 4826.44 4812.5  4815.16 4784.86 4813.27 4783.44 4807.07 4817.37\n",
      "  4837.42 4812.71 4884.95 4843.17 4859.87]\n",
      " [5275.51 5312.64 5327.85 5281.37 5264.73 5277.93 5265.26 5246.33 5252.56\n",
      "  5264.74 5228.95 5246.81 5305.25 5239.67 5237.7  5271.23 5298.48 5248.39\n",
      "  5257.11 5240.54 5256.98 5254.62 5229.22 5273.58 5293.88 5249.31 5262.99\n",
      "  5271.79 5264.37 5275.21 5282.36 5292.8 ]\n",
      " [4750.57 4774.14 4777.3  4741.59 4745.97 4788.41 4749.17 4868.37 4767.9\n",
      "  4752.1  4752.32 4766.03 4731.67 4731.21 4770.52 4740.97 4725.05 4721.65\n",
      "  4778.73 4709.39 4758.1  4716.25 4723.83 4723.33 4774.37 4683.37 4788.55\n",
      "  4718.01 4695.3  4699.25 4729.94 4704.67]\n",
      " [3478.55 3493.23 3465.95 3486.2  3475.95 3487.38 3464.87 3484.39 3475.04\n",
      "  3488.39 3461.96 3479.34 3463.85 3499.5  3468.01 3501.8  3466.98 3494.05\n",
      "  3458.8  3481.4  3475.21 3480.84 3468.15 3492.25 3468.55 3492.18 3477.07\n",
      "  3496.67 3499.37 3492.85 3465.29 3483.11]\n",
      " [4112.87 4114.2  4129.02 4146.03 4202.02 4164.43 4182.07 4202.28 4206.4\n",
      "  4180.35 4239.43 4242.76 4284.8  4326.21 4382.43 4329.35 4372.68 4461.37\n",
      "  4437.74 4397.61 4459.84 4348.29 4364.28 4370.   4350.68 4312.58 4314.72\n",
      "  4348.65 4276.31 4264.81 4275.13 4254.56]\n",
      " [4153.73 4130.63 4126.78 4146.77 4109.95 4110.43 4189.23 4126.23 4081.9\n",
      "  4187.12 4115.   4196.57 4091.1  4166.24 4081.5  4131.9  4101.34 4150.82\n",
      "  4089.49 4125.47 4142.65 4146.74 4133.97 4148.9  4123.5  4163.61 4156.79\n",
      "  4135.38 4128.2  4199.4  4155.58 4160.09]\n",
      " [4359.69 4395.7  4352.16 4499.05 4525.27 4374.56 4379.99 4375.98 4342.5\n",
      "  4399.45 4377.29 4428.61 4347.26 4422.98 4339.84 4362.41 4356.04 4356.98\n",
      "  4332.62 4367.72 4344.28 4373.22 4315.38 4374.43 4347.51 4339.81 4364.91\n",
      "  4471.74 4451.46 4363.99 4306.87 4346.41]\n",
      " [4307.52 4559.18 4270.34 4316.13 4278.87 4439.45 4196.18 4363.03 4210.08\n",
      "  4257.27 4215.7  4246.51 4302.35 4414.07 4436.26 4416.25 4473.64 4306.71\n",
      "  4251.65 4300.12 4208.48 4553.79 4244.55 4368.72 4225.75 4247.62 4223.32\n",
      "  4272.41 4199.41 4280.62 4345.38 4529.19]\n",
      " [4345.76 4347.77 4358.19 4344.57 4362.89 4359.02 4354.61 4377.75 4386.73\n",
      "  4363.75 4389.35 4384.35 4387.63 4382.88 4412.42 4406.08 4394.62 4410.67\n",
      "  4403.07 4396.45 4427.96 4401.97 4395.77 4423.38 4400.75 4401.78 4439.74\n",
      "  4406.39 4417.78 4406.24 4413.69 4421.78]\n",
      " [4095.94 4103.27 4098.79 4082.23 4084.71 4082.13 4074.8  4080.52 4082.14\n",
      "  4076.12 4069.03 4077.21 4079.78 4076.3  4080.34 4079.21 4072.46 4081.62\n",
      "  4082.64 4075.47 4098.6  4081.35 4073.76 4085.81 4080.68 4075.61 4079.66\n",
      "  4075.09 4069.73 4081.01 4099.79 4069.97]\n",
      " [4376.98 4373.45 4352.69 4393.36 4357.2  4388.48 4386.78 4396.32 4382.52\n",
      "  4434.76 4410.97 4460.8  4460.44 4482.06 4454.65 4498.46 4487.73 4487.73\n",
      "  4472.04 4485.48 4450.98 4473.96 4451.46 4448.7  4425.34 4442.9  4406.7\n",
      "  4429.38 4403.38 4406.04 4380.32 4404.35]\n",
      " [4797.16 4807.69 4811.41 4795.26 4943.98 4817.04 4797.49 4817.23 4917.55\n",
      "  4786.97 4833.74 4827.47 4800.43 4893.22 4831.76 4793.54 4875.63 4820.86\n",
      "  4793.77 4811.42 4811.12 4797.51 4866.96 4828.44 4793.59 4793.34 4831.19\n",
      "  4834.23 4833.72 4822.07 4789.6  4781.12]\n",
      " [4326.5  4313.28 4334.06 4377.97 4348.72 4339.06 4377.83 4341.31 4396.3\n",
      "  4336.89 4357.39 4374.14 4346.4  4351.34 4374.67 4341.64 4503.5  4325.51\n",
      "  4391.36 4313.47 4349.49 4309.36 4317.95 4329.54 4363.59 4314.09 4313.33\n",
      "  4305.59 4316.62 4300.16 4371.72 4303.  ]\n",
      " [4350.69 4380.19 4347.42 4368.03 4344.8  4355.59 4349.48 4374.36 4336.41\n",
      "  4367.51 4355.29 4353.55 4347.08 4379.53 4333.05 4358.9  4349.86 4371.24\n",
      "  4339.95 4379.58 4334.74 4358.96 4349.   4364.19 4333.17 4364.09 4329.73\n",
      "  4351.47 4344.39 4357.79 4321.39 4358.84]\n",
      " [3139.56 3145.24 3156.18 3146.81 3165.   3168.77 3157.09 3153.37 3142.03\n",
      "  3118.46 3113.23 3096.71 3079.39 3072.83 3068.36 3052.63 3047.87 3048.32\n",
      "  3041.72 3026.71 3041.03 3024.61 3046.64 3028.91 3031.67 3025.35 3039.48\n",
      "  3047.61 3039.29 3022.45 3020.3  3007.02]\n",
      " [5119.93 5079.43 5140.83 5103.77 5109.05 5107.13 5113.04 5086.16 5104.14\n",
      "  5095.59 5098.54 5090.65 5122.6  5081.5  5102.06 5098.99 5094.2  5082.89\n",
      "  5111.5  5096.5  5086.74 5091.81 5112.38 5083.05 5107.41 5101.81 5106.95\n",
      "  5093.78 5102.72 5078.46 5114.43 5100.44]\n",
      " [3457.59 3473.45 3451.91 3475.61 3465.55 3471.38 3456.21 3480.47 3456.19\n",
      "  3478.24 3461.24 3497.56 3464.95 3474.71 3463.07 3482.16 3464.88 3480.55\n",
      "  3454.96 3481.15 3462.21 3489.01 3462.61 3481.6  3459.68 3487.87 3465.76\n",
      "  3478.38 3461.52 3485.99 3459.53 3487.02]\n",
      " [4555.19 4558.4  4568.49 4560.54 4572.85 4578.34 4569.73 4572.08 4585.08\n",
      "  4580.6  4591.27 4607.13 4616.53 4634.12 4664.33 4669.08 4686.11 4709.41\n",
      "  4732.39 4743.03 4780.16 4791.41 4817.91 4830.38 4856.13 4865.35 4895.63\n",
      "  4914.54 4910.56 4916.06 4925.28 4935.6 ]\n",
      " [3466.59 3447.47 3452.28 3457.73 3464.07 3452.46 3465.74 3455.13 3468.09\n",
      "  3452.34 3460.42 3454.73 3471.83 3455.93 3459.06 3455.48 3466.64 3455.16\n",
      "  3462.27 3453.01 3462.28 3452.76 3464.37 3456.86 3467.78 3459.48 3466.13\n",
      "  3460.67 3472.15 3451.17 3456.15 3453.21]\n",
      " [3490.86 3417.89 3437.13 3449.86 3500.27 3448.81 3455.06 3454.55 3401.51\n",
      "  3410.71 3411.61 3464.13 3472.06 3431.53 3413.51 3401.16 3440.57 3523.88\n",
      "  3421.51 3437.09 3569.49 3436.42 3423.62 3450.31 3431.76 3444.18 3463.39\n",
      "  3458.05 3479.85 3466.26 3436.82 3529.58]\n",
      " [4809.63 4827.15 4821.63 4822.46 4822.57 4834.29 4813.53 4825.55 4836.94\n",
      "  4830.8  4825.39 4829.9  4829.5  4848.92 4834.13 4827.88 4820.99 4832.51\n",
      "  4831.54 4844.2  4836.97 4832.64 4837.1  4837.93 4836.71 4835.11 4838.05\n",
      "  4826.76 4825.33 4840.72 4829.8  4820.88]\n",
      " [4739.72 4786.17 4784.74 4774.3  4832.83 4772.58 4800.19 4880.47 4806.61\n",
      "  4884.52 4818.68 4798.81 4765.96 4781.76 4740.41 4757.42 4771.06 4748.08\n",
      "  4746.15 4763.89 4787.83 4762.62 4759.93 4796.91 4779.49 4772.13 4714.18\n",
      "  4752.15 4790.36 4753.05 4718.41 4750.89]\n",
      " [5122.73 5127.31 5108.74 5117.47 5133.94 5120.53 5134.29 5139.88 5126.79\n",
      "  5128.34 5145.07 5134.28 5128.45 5154.02 5159.86 5129.63 5145.63 5133.63\n",
      "  5143.87 5141.34 5141.39 5131.01 5150.76 5155.29 5130.88 5134.03 5146.19\n",
      "  5125.66 5139.74 5143.43 5132.19 5124.32]\n",
      " [5141.3  5178.74 5193.21 5221.78 5153.85 5172.79 5199.5  5167.09 5176.07\n",
      "  5171.94 5246.73 5180.73 5181.85 5192.92 5189.98 5184.74 5181.82 5169.34\n",
      "  5149.46 5250.59 5173.06 5169.02 5155.33 5184.67 5144.28 5184.   5197.82\n",
      "  5232.59 5207.76 5164.19 5150.66 5202.27]\n",
      " [4104.29 4125.42 4090.69 4115.67 4126.93 4124.05 4128.15 4145.56 4117.43\n",
      "  4181.76 4131.53 4136.7  4100.95 4136.17 4118.77 4123.94 4144.36 4155.53\n",
      "  4095.   4159.59 4086.52 4120.76 4063.21 4123.13 4088.3  4125.28 4066.25\n",
      "  4079.52 4060.81 4128.38 4088.45 4100.62]\n",
      " [4436.73 4458.57 4453.48 4452.73 4449.1  4473.09 4445.98 4467.97 4455.01\n",
      "  4459.47 4432.29 4458.73 4434.12 4436.22 4437.74 4445.24 4419.05 4450.04\n",
      "  4438.14 4439.45 4443.15 4459.28 4431.36 4455.52 4450.23 4448.53 4447.89\n",
      "  4471.02 4454.86 4468.51 4468.86 4472.95]\n",
      " [4222.8  4243.97 4216.22 4260.41 4267.95 4258.96 4198.2  4241.33 4201.91\n",
      "  4211.57 4170.52 4216.69 4221.86 4218.76 4203.71 4235.46 4203.77 4251.42\n",
      "  4203.09 4189.91 4192.78 4188.11 4239.3  4228.64 4270.26 4396.84 4369.27\n",
      "  4384.77 4357.79 4403.54 4448.78 4560.48]\n",
      " [3503.56 3535.53 3513.47 3537.61 3529.33 3551.51 3530.83 3567.04 3551.93\n",
      "  3574.7  3563.49 3606.7  3596.18 3656.31 3630.2  3668.47 3665.95 3724.14\n",
      "  3677.69 3709.75 3700.52 3725.59 3695.82 3731.6  3695.82 3706.79 3686.97\n",
      "  3689.79 3660.7  3703.09 3690.12 3740.75]\n",
      " [4736.99 4586.85 4635.36 4572.12 4598.31 4602.42 4598.17 4655.31 4655.51\n",
      "  4583.37 4598.74 4657.48 4616.14 4636.51 4675.13 4610.08 4620.56 4587.78\n",
      "  4654.47 4603.82 4624.57 4590.98 4603.97 4604.36 4592.93 4570.48 4601.\n",
      "  4681.72 4601.25 4599.84 4600.46 4608.59]\n",
      " [5170.73 5182.02 5181.82 5188.6  5163.47 5178.6  5209.28 5175.79 5156.03\n",
      "  5178.18 5169.58 5262.89 5160.94 5262.22 5172.01 5175.31 5192.28 5169.78\n",
      "  5165.78 5213.58 5268.22 5168.63 5168.13 5188.98 5167.69 5174.11 5181.31\n",
      "  5205.57 5201.68 5202.16 5185.35 5216.08]\n",
      " [2943.23 2934.87 2939.47 2936.32 2934.86 2931.84 2945.77 2924.5  2935.37\n",
      "  2930.54 2943.31 2934.45 2946.32 2951.45 2945.31 2936.31 2933.86 2931.45\n",
      "  2945.94 2927.81 2938.77 2939.38 2951.77 2946.97 2944.75 2932.79 2944.36\n",
      "  2938.89 2948.67 2928.74 2942.19 2934.57]\n",
      " [4789.03 4793.18 4785.75 4797.48 4837.8  4788.01 4795.47 4759.14 4819.49\n",
      "  4819.13 4786.9  4755.72 4808.2  4835.46 4788.35 4773.55 4773.96 4764.91\n",
      "  4776.32 4762.8  4765.24 4806.88 4800.29 4809.03 4760.35 4840.61 4751.24\n",
      "  4771.56 4763.85 4760.79 4735.32 4775.93]\n",
      " [4287.35 4271.45 4256.54 4295.47 4254.78 4238.26 4249.32 4235.07 4219.68\n",
      "  4218.14 4213.93 4206.08 4192.37 4185.68 4169.37 4176.26 4178.99 4150.04\n",
      "  4152.72 4155.48 4148.3  4150.37 4149.74 4138.06 4148.39 4151.17 4145.09\n",
      "  4145.85 4149.71 4148.49 4148.89 4155.89]\n",
      " [4725.82 4851.83 4802.74 4704.9  4678.74 4720.5  4689.1  4725.43 4700.09\n",
      "  4703.97 4820.97 4709.54 4765.   4724.33 4752.74 4725.76 4678.24 4806.11\n",
      "  4710.9  4697.72 4685.41 4721.24 4674.26 4741.82 4725.9  4716.58 4704.64\n",
      "  4700.35 4677.03 4698.46 4705.54 4709.89]\n",
      " [4003.54 4015.01 4035.22 4042.33 3993.47 4037.93 4044.44 4005.24 4019.8\n",
      "  4055.4  4017.   4014.86 4035.51 4041.02 4076.74 4053.41 4042.22 4041.02\n",
      "  4052.93 4055.74 4042.59 4105.98 4093.65 4046.97 4056.64 3984.83 4031.18\n",
      "  4008.44 4003.73 3984.12 4021.04 3985.68]\n",
      " [2939.35 2947.   2956.6  2952.13 2937.14 2940.   2926.55 2952.98 2935.78\n",
      "  2951.92 2932.31 2954.64 2927.75 2954.73 2931.67 2937.91 2928.31 2940.03\n",
      "  2929.99 2942.56 2926.62 2933.7  2926.05 2944.78 2922.8  2938.83 2939.74\n",
      "  2944.95 2934.32 2942.4  2932.66 2964.01]\n",
      " [4368.83 4365.47 4358.91 4397.31 4341.68 4377.52 4341.13 4356.19 4350.26\n",
      "  4359.04 4346.94 4362.36 4330.71 4334.11 4331.06 4345.36 4321.26 4341.35\n",
      "  4321.79 4326.72 4329.84 4346.67 4316.01 4338.75 4324.44 4329.34 4310.85\n",
      "  4340.19 4312.26 4322.71 4323.66 4331.59]\n",
      " [4855.35 4787.33 4787.64 4784.21 4791.72 4803.55 4788.88 4791.84 4850.25\n",
      "  4833.18 4797.74 4899.43 4806.73 4798.83 4806.78 4855.92 4912.15 4876.73\n",
      "  4792.82 4807.14 4772.03 4788.93 4803.16 4792.55 4819.56 4805.27 4811.32\n",
      "  4802.47 4822.47 4857.83 4994.88 4845.  ]\n",
      " [3528.39 3456.13 3445.39 3444.02 3502.26 3493.14 3475.21 3515.52 3516.71\n",
      "  3501.44 3470.78 3454.31 3436.17 3426.77 3519.81 3440.3  3488.03 3494.34\n",
      "  3434.5  3451.61 3434.9  3455.25 3458.6  3413.61 3483.53 3420.43 3413.2\n",
      "  3437.83 3563.6  3437.64 3426.59 3537.3 ]\n",
      " [4343.48 4370.37 4359.61 4359.33 4375.31 4373.85 4346.06 4373.01 4359.28\n",
      "  4358.1  4353.59 4368.13 4345.73 4369.43 4362.62 4360.3  4363.92 4373.71\n",
      "  4357.04 4380.88 4369.29 4356.06 4350.11 4373.01 4361.82 4358.4  4360.76\n",
      "  4358.93 4352.1  4365.37 4353.84 4346.96]\n",
      " [4857.1  4894.18 4842.91 4865.68 4933.94 4853.46 4849.79 4971.5  4832.89\n",
      "  5003.18 4912.25 4868.81 4863.   4852.09 4841.14 4861.57 4868.69 4837.15\n",
      "  4867.31 4869.86 4848.   4865.58 4850.71 4880.11 4847.89 4876.92 4867.66\n",
      "  4875.06 4876.47 4862.16 4861.54 4854.55]\n",
      " [4272.84 4277.48 4308.22 4280.13 4278.37 4282.   4292.89 4278.4  4298.38\n",
      "  4274.4  4283.93 4289.19 4295.58 4278.02 4303.87 4288.67 4288.69 4295.25\n",
      "  4302.56 4278.25 4298.31 4297.46 4296.38 4311.08 4313.52 4292.67 4294.32\n",
      "  4308.38 4299.86 4288.11 4318.13 4295.93]\n",
      " [3022.82 3006.89 3016.5  2993.52 3012.8  2980.41 3006.97 2990.26 2998.76\n",
      "  2986.7  3012.19 2986.08 3003.85 3007.9  3007.04 2984.96 3001.81 3002.98\n",
      "  3031.56 2990.99 2994.72 3002.69 3013.18 3000.91 2997.84 2978.72 2995.78\n",
      "  2975.74 2992.99 3040.6  3013.51 2981.34]\n",
      " [4443.96 4325.08 4346.18 4318.01 4365.98 4390.04 4387.93 4437.65 4345.98\n",
      "  4322.65 4349.04 4323.94 4410.27 4345.1  4354.93 4416.28 4375.89 4337.52\n",
      "  4364.77 4323.53 4361.55 4340.79 4340.74 4330.32 4367.08 4352.68 4358.1\n",
      "  4335.62 4363.41 4331.92 4356.68 4334.71]\n",
      " [4002.16 4014.91 4009.26 4033.3  4008.21 4019.82 4033.35 4024.27 4054.26\n",
      "  4041.95 3992.9  4044.   4033.3  4031.66 4015.41 4049.69 3994.21 4049.56\n",
      "  4034.53 4062.37 4039.32 4025.84 4067.71 4023.92 4063.55 4023.17 4001.04\n",
      "  4051.97 4060.68 4089.95 4013.36 4091.71]\n",
      " [4566.27 4622.07 4552.01 4589.47 4548.31 4551.3  4558.06 4678.86 4666.49\n",
      "  4567.51 4542.53 4564.06 4555.42 4583.9  4549.82 4553.93 4550.91 4592.35\n",
      "  4540.98 4607.29 4560.06 4639.17 4562.61 4636.74 4555.72 4607.52 4571.36\n",
      "  4596.26 4560.65 4594.04 4554.18 4557.  ]\n",
      " [4749.46 4701.58 4704.79 4713.58 4718.67 4751.57 4701.11 4718.08 4715.09\n",
      "  4819.12 4714.49 4709.28 4746.   4817.17 4713.66 4783.05 4697.97 4688.71\n",
      "  4744.   4697.52 4686.31 4695.58 4809.73 4719.07 4716.77 4688.84 4688.62\n",
      "  4685.77 4687.51 4707.42 4705.4  4829.58]\n",
      " [4859.01 4886.11 4851.97 4847.39 4873.13 4816.38 4881.38 4875.08 4855.19\n",
      "  4852.39 4933.7  4894.39 4864.75 4856.96 4846.34 4854.87 4863.21 4814.6\n",
      "  4884.55 4842.97 4931.71 4893.59 4870.11 4854.56 4836.88 4846.57 4851.28\n",
      "  4858.86 4864.17 4820.83 4859.3  4864.36]\n",
      " [4841.8  4865.39 4915.53 4857.6  5077.96 4922.64 4882.25 4909.13 4908.7\n",
      "  4935.88 4979.43 5028.16 4951.6  4943.19 5091.8  4992.78 4952.23 4970.86\n",
      "  4961.39 5026.58 4950.85 4930.52 4921.36 5149.36 4909.95 4905.15 4932.35\n",
      "  4932.05 4908.65 4919.82 4906.27 4888.14]\n",
      " [3425.13 3445.79 3607.06 3470.42 3437.15 3430.98 3425.96 3428.81 3463.31\n",
      "  3426.59 3434.87 3442.57 3441.56 3443.   3429.98 3435.04 3442.1  3442.41\n",
      "  3405.11 3460.02 3473.26 3438.8  3477.31 3424.7  3418.71 3463.05 3409.26\n",
      "  3399.28 3438.9  3480.71 3448.55 3402.38]\n",
      " [3379.47 3335.28 3419.18 3354.84 3380.3  3372.75 3470.73 3370.31 3408.79\n",
      "  3351.29 3338.38 3336.42 3401.35 3324.06 3345.01 3353.88 3358.66 3334.46\n",
      "  3354.61 3353.11 3355.72 3414.9  3354.91 3416.67 3395.95 3372.64 3388.58\n",
      "  3361.99 3355.01 3346.83 3353.17 3330.64]\n",
      " [5235.52 5255.65 5224.64 5274.27 5243.22 5330.41 5272.23 5259.56 5249.34\n",
      "  5259.1  5228.87 5262.02 5295.56 5234.87 5241.74 5239.22 5205.93 5263.11\n",
      "  5193.84 5244.04 5206.39 5213.39 5199.71 5236.73 5207.49 5229.84 5177.51\n",
      "  5229.72 5235.47 5296.56 5197.1  5284.45]\n",
      " [3444.69 3422.68 3456.54 3495.59 3562.23 3446.74 3429.63 3497.88 3437.7\n",
      "  3424.53 3417.   3426.74 3421.21 3442.93 3427.54 3435.67 3433.81 3444.38\n",
      "  3420.87 3416.9  3530.52 3512.84 3433.67 3435.53 3500.76 3531.65 3419.78\n",
      "  3422.7  3484.88 3448.   3452.5  3414.29]\n",
      " [3389.34 3400.56 3357.29 3413.64 3384.86 3403.48 3380.52 3349.29 3402.92\n",
      "  3377.83 3373.57 3377.78 3491.69 3365.49 3384.3  3369.59 3371.58 3385.93\n",
      "  3433.33 3453.98 3434.61 3378.25 3420.08 3356.77 3351.04 3390.63 3391.9\n",
      "  3573.44 3435.13 3366.89 3390.44 3351.61]\n",
      " [3461.42 3481.88 3469.19 3489.3  3465.58 3483.37 3461.92 3481.82 3472.9\n",
      "  3502.52 3459.34 3480.99 3465.83 3483.29 3464.09 3484.77 3462.42 3479.52\n",
      "  3462.32 3486.06 3456.58 3489.45 3462.3  3481.86 3456.05 3479.41 3454.39\n",
      "  3476.09 3464.41 3474.91 3457.15 3479.11]\n",
      " [4795.28 4831.77 4816.32 4778.99 4798.05 4791.   4787.12 4802.84 4784.13\n",
      "  4783.49 4796.51 4798.48 4777.28 4799.14 4814.27 4815.63 4798.29 4802.29\n",
      "  4795.3  4792.18 4825.82 4796.79 4786.37 4810.21 4785.04 4793.49 4804.08\n",
      "  4799.93 4784.93 4812.89 4784.06 4796.15]\n",
      " [3283.57 3288.58 3260.38 3321.66 3310.78 3289.62 3281.01 3290.89 3443.25\n",
      "  3373.07 3432.72 3299.6  3295.27 3295.88 3432.02 3308.48 3338.76 3304.41\n",
      "  3391.35 3353.98 3303.2  3291.57 3303.03 3297.97 3289.39 3308.96 3323.81\n",
      "  3348.54 3313.65 3379.54 3312.9  3326.81]\n",
      " [4711.9  4698.75 4714.76 4686.84 4720.88 4689.14 4722.75 4709.29 4736.76\n",
      "  4704.53 4753.2  4710.68 4723.72 4731.27 4741.59 4714.45 4772.3  4755.41\n",
      "  4802.37 4782.15 4852.55 4799.73 4858.39 4854.72 4861.16 4859.39 4916.77\n",
      "  4852.39 4900.98 4880.09 4870.12 4850.03]\n",
      " [4858.83 4812.46 4850.38 4836.02 4842.4  4833.2  4864.05 4826.29 4850.39\n",
      "  4843.65 4850.37 4823.52 4864.36 4839.39 4848.98 4840.88 4859.79 4829.55\n",
      "  4857.51 4834.77 4844.14 4843.77 4862.45 4827.78 4860.98 4843.62 4850.18\n",
      "  4848.05 4865.94 4829.04 4855.49 4850.6 ]\n",
      " [5100.73 5211.89 5120.86 5158.67 5205.75 5166.41 5108.7  5183.48 5127.32\n",
      "  5194.13 5140.56 5155.81 5131.25 5163.05 5159.54 5172.98 5151.17 5256.62\n",
      "  5138.96 5179.39 5164.83 5172.66 5174.98 5161.11 5144.35 5183.24 5232.25\n",
      "  5165.02 5136.3  5161.04 5174.98 5255.35]\n",
      " [4704.39 4718.11 4814.63 4679.6  4676.51 4774.53 4711.21 4732.44 4710.38\n",
      "  4721.54 4711.89 4740.19 4732.77 4719.56 4712.63 4733.72 4748.02 4720.78\n",
      "  4738.   4719.85 4713.26 4738.58 4727.34 4773.4  4718.86 4775.12 4704.85\n",
      "  4714.37 4708.39 4739.22 4707.18 4706.54]\n",
      " [3483.89 3476.85 3498.37 3498.34 3486.32 3482.57 3502.62 3495.92 3506.31\n",
      "  3475.98 3497.66 3496.87 3508.68 3511.08 3524.88 3501.97 3504.8  3547.68\n",
      "  3526.91 3498.45 3499.14 3536.44 3495.36 3493.51 3498.17 3506.05 3494.92\n",
      "  3486.46 3496.84 3488.95 3497.64 3484.12]\n",
      " [4704.24 4677.11 4705.89 4680.08 4719.71 4686.58 4667.96 4677.94 4666.68\n",
      "  4665.41 4681.82 4722.63 4730.99 4769.49 4668.28 4657.34 4739.27 4719.54\n",
      "  4731.55 4673.63 4654.87 4705.36 4737.49 4836.72 4648.23 4737.91 4671.85\n",
      "  4682.56 4677.14 4668.04 4639.6  4706.23]\n",
      " [4247.59 4217.75 4145.46 4151.51 4161.01 4244.96 4278.13 4306.34 4248.42\n",
      "  4314.91 4298.88 4152.21 4144.73 4138.09 4064.28 4359.53 4382.2  4350.11\n",
      "  4355.62 4334.62 4265.43 4237.07 4211.2  4103.65 4035.98 4106.93 4075.69\n",
      "  4188.84 4254.58 4313.88 4372.53 4340.13]\n",
      " [3382.69 3362.19 3385.65 3362.53 3378.06 3360.7  3378.23 3352.19 3371.49\n",
      "  3348.61 3355.83 3333.07 3352.5  3327.74 3349.69 3332.8  3347.94 3333.53\n",
      "  3356.09 3321.72 3338.48 3330.79 3370.85 3334.29 3374.7  3350.36 3374.52\n",
      "  3381.72 3400.02 3391.66 3421.04 3403.86]\n",
      " [4835.32 4798.38 4823.89 4813.13 4819.42 4802.55 4836.68 4803.77 4829.1\n",
      "  4817.77 4834.17 4815.13 4845.83 4818.29 4826.7  4817.22 4847.15 4808.93\n",
      "  4844.56 4825.62 4831.01 4820.18 4845.55 4815.42 4846.46 4825.82 4841.27\n",
      "  4821.66 4858.08 4814.05 4836.   4826.47]\n",
      " [4656.94 4634.58 4641.58 4639.43 4645.11 4626.11 4654.64 4630.2  4638.02\n",
      "  4629.55 4646.33 4626.98 4651.37 4629.03 4631.82 4627.54 4644.54 4614.12\n",
      "  4640.91 4626.16 4627.6  4610.37 4638.97 4609.62 4626.12 4619.44 4628.79\n",
      "  4607.14 4641.2  4609.43 4622.93 4611.05]\n",
      " [4694.96 4713.65 4680.24 4701.96 4701.85 4699.04 4683.05 4712.84 4671.26\n",
      "  4701.1  4708.29 4700.13 4684.47 4713.74 4680.57 4698.58 4692.57 4708.92\n",
      "  4694.94 4711.34 4697.96 4699.63 4698.94 4717.54 4695.73 4721.58 4717.17\n",
      "  4726.88 4698.18 4727.05 4695.95 4731.55]\n",
      " [4649.04 4638.78 4583.5  4638.35 4639.19 4648.66 4623.03 4654.51 4587.15\n",
      "  4634.97 4635.22 4626.87 4656.63 4630.14 4702.34 4657.75 4625.57 4683.38\n",
      "  4813.83 4694.53 4664.46 4673.75 4703.61 4678.64 4751.11 4703.02 4683.09\n",
      "  4706.98 4716.31 4699.59 4705.45 4791.75]\n",
      " [4979.41 5073.   4964.23 4996.83 5083.45 4957.59 4946.41 4948.52 4955.53\n",
      "  4971.86 4963.04 4925.9  4953.53 4982.23 4948.92 4985.22 5013.57 5103.63\n",
      "  5035.21 5064.01 5063.06 5113.22 5146.66 5198.06 5190.12 5244.77 5252.04\n",
      "  5368.19 5477.78 5312.04 5405.34 5271.56]\n",
      " [4668.59 4650.92 4656.08 4667.55 4669.88 4642.58 4660.98 4652.95 4658.15\n",
      "  4646.87 4672.26 4642.23 4670.61 4658.79 4662.24 4646.19 4675.74 4657.26\n",
      "  4658.23 4662.17 4675.03 4652.63 4688.19 4668.11 4666.67 4669.03 4686.36\n",
      "  4649.95 4675.37 4674.38 4668.75 4671.64]\n",
      " [4085.56 4101.86 4088.27 4078.45 4084.69 4086.5  4078.1  4084.1  4081.64\n",
      "  4072.27 4064.96 4067.96 4059.69 4054.89 4058.25 4047.67 4049.13 4051.48\n",
      "  4044.77 4038.36 4048.69 4036.48 4042.33 4055.08 4062.19 4033.72 4057.47\n",
      "  4064.09 4036.38 4040.94 4049.85 4035.64]\n",
      " [4408.33 4419.75 4349.43 4369.33 4366.   4362.23 4349.48 4390.88 4378.32\n",
      "  4348.03 4317.97 4325.88 4343.18 4366.64 4357.59 4335.33 4371.75 4449.2\n",
      "  4484.23 4357.61 4308.57 4348.04 4340.34 4435.52 4332.99 4322.66 4398.13\n",
      "  4363.58 4325.06 4333.93 4332.34 4449.3 ]\n",
      " [4218.47 4230.68 4227.56 4220.4  4213.14 4219.94 4199.4  4203.06 4202.18\n",
      "  4188.3  4197.36 4207.21 4199.05 4202.33 4219.21 4218.82 4225.65 4279.5\n",
      "  4285.4  4321.56 4338.73 4369.06 4376.43 4418.43 4442.36 4437.55 4455.86\n",
      "  4481.92 4465.58 4510.98 4492.8  4492.97]\n",
      " [3477.09 3462.07 3473.6  3459.39 3475.61 3458.14 3480.05 3455.14 3471.68\n",
      "  3460.85 3475.45 3456.7  3475.87 3460.94 3476.57 3471.75 3478.74 3456.49\n",
      "  3477.79 3462.69 3474.31 3459.8  3480.29 3462.34 3478.59 3477.01 3477.04\n",
      "  3466.79 3484.35 3459.84 3488.84 3463.13]\n",
      " [4213.85 4213.75 4221.48 4217.9  4222.71 4229.88 4225.27 4217.77 4227.7\n",
      "  4223.63 4216.87 4235.79 4231.31 4224.17 4233.65 4252.11 4226.95 4235.33\n",
      "  4243.8  4236.93 4258.1  4245.56 4246.08 4245.18 4254.16 4260.4  4250.96\n",
      "  4258.47 4266.96 4266.72 4262.91 4265.16]\n",
      " [4634.13 4773.01 4662.66 4761.46 4708.18 4658.82 4659.81 4739.3  4655.47\n",
      "  4645.38 4643.58 4681.22 4609.73 4654.5  4608.94 4637.85 4643.58 4661.06\n",
      "  4622.11 4616.56 4633.43 4646.4  4600.9  4637.5  4598.9  4652.98 4607.91\n",
      "  4603.61 4612.46 4608.79 4604.01 4643.61]\n",
      " [4816.78 4794.12 4820.19 4806.26 4806.68 4795.44 4815.24 4792.31 4840.99\n",
      "  4853.97 4874.01 4799.4  4824.38 4797.49 4813.05 4810.4  4818.15 4830.17\n",
      "  4860.37 4804.13 4801.4  4799.   4814.39 4819.62 4831.27 4807.15 4793.93\n",
      "  4798.3  4810.08 4787.87 4815.37 4799.49]\n",
      " [4645.83 4687.47 4764.07 4666.86 4707.09 4674.98 4656.26 4735.2  4671.67\n",
      "  4666.63 4742.37 4663.68 4703.46 4692.99 4658.93 4667.57 4689.3  4703.75\n",
      "  4704.13 4689.61 4681.13 4695.1  4727.36 4753.31 4715.78 4753.54 4718.23\n",
      "  4722.99 4710.34 4757.73 4746.47 4738.15]\n",
      " [4315.5  4304.29 4310.06 4314.18 4333.96 4295.46 4361.34 4309.29 4320.92\n",
      "  4342.44 4329.01 4306.53 4325.76 4311.83 4312.93 4292.17 4335.96 4310.52\n",
      "  4303.39 4305.93 4314.32 4282.38 4304.78 4287.12 4300.17 4305.86 4322.5\n",
      "  4282.62 4308.37 4293.67 4318.07 4300.99]\n",
      " [3499.85 3489.02 3507.33 3490.   3497.99 3485.33 3513.14 3486.32 3495.87\n",
      "  3492.64 3502.24 3486.22 3510.23 3493.78 3507.85 3493.7  3504.43 3486.37\n",
      "  3504.5  3495.03 3505.86 3497.45 3502.38 3490.22 3505.05 3500.04 3501.36\n",
      "  3494.5  3507.69 3491.63 3503.21 3499.09]\n",
      " [4653.97 4632.72 4619.03 4620.4  4608.19 4601.52 4609.74 4600.52 4583.28\n",
      "  4585.28 4585.74 4569.36 4580.65 4575.27 4576.08 4580.86 4595.01 4572.34\n",
      "  4578.37 4585.97 4570.54 4577.41 4585.38 4581.55 4583.13 4592.04 4570.39\n",
      "  4571.05 4567.33 4548.03 4509.26 4499.65]\n",
      " [4683.58 4791.32 4713.44 4709.   4708.43 4734.52 4698.89 4716.08 4696.09\n",
      "  4706.01 4704.06 4707.7  4733.15 4784.96 4678.15 4739.09 4682.88 4694.83\n",
      "  4683.35 4687.45 4677.61 4694.9  4675.33 4667.4  4759.68 4677.44 4681.82\n",
      "  4700.85 4676.23 4697.05 4761.92 4692.28]\n",
      " [5283.41 5314.32 5265.98 5294.11 5310.21 5284.53 5285.84 5296.63 5260.66\n",
      "  5311.93 5311.25 5331.58 5267.8  5299.06 5275.37 5305.48 5320.36 5289.03\n",
      "  5270.61 5320.58 5263.86 5308.43 5374.94 5284.81 5291.21 5303.25 5311.74\n",
      "  5274.19 5277.51 5287.58 5268.41 5289.32]\n",
      " [4189.53 4227.73 4199.03 4209.76 4229.29 4216.19 4198.85 4242.62 4221.19\n",
      "  4263.3  4239.91 4307.56 4203.6  4216.57 4221.08 4204.88 4217.39 4218.29\n",
      "  4248.75 4258.53 4260.51 4237.46 4243.11 4239.48 4218.77 4236.94 4279.71\n",
      "  4254.08 4251.83 4304.75 4223.03 4252.56]\n",
      " [4846.98 4826.82 4789.96 4834.16 4834.95 4847.17 4817.88 4918.97 4787.35\n",
      "  4795.48 4813.92 4860.58 4816.9  4853.15 4798.28 4805.89 4832.06 4823.83\n",
      "  4840.88 4835.27 4810.62 4814.76 4816.11 4831.82 4829.58 4822.28 4826.38\n",
      "  4793.75 4823.52 4915.24 4890.7  4818.85]\n",
      " [4564.07 4517.82 4533.58 4483.67 4633.4  4494.59 4609.   4482.75 4547.11\n",
      "  4517.16 4529.32 4597.14 4542.36 4495.41 4568.72 4708.25 4595.74 4511.72\n",
      "  4511.57 4583.94 4573.44 4506.48 4517.15 4489.38 4610.77 4601.57 4554.72\n",
      "  4500.39 4508.32 4499.86 4500.99 4490.1 ]\n",
      " [3437.39 3413.56 3438.16 3434.38 3422.73 3432.95 3438.2  3419.85 3420.91\n",
      "  3480.28 3581.63 3416.36 3428.41 3591.27 3440.17 3401.12 3418.03 3486.55\n",
      "  3440.14 3427.2  3569.86 3460.59 3408.28 3430.28 3447.97 3417.9  3423.67\n",
      "  3408.56 3409.21 3487.39 3482.   3452.13]\n",
      " [4125.47 4177.23 4093.27 4154.9  4130.82 4139.66 4130.6  4159.63 4187.15\n",
      "  4100.83 4094.41 4118.65 4086.13 4136.52 4080.69 4162.52 4166.99 4150.36\n",
      "  4104.06 4135.31 4096.31 4150.07 4117.5  4145.48 4108.42 4119.2  4123.27\n",
      "  4132.4  4110.15 4147.49 4116.22 4155.76]\n",
      " [4251.91 4231.04 4211.25 4407.37 4241.13 4184.19 4249.27 4245.64 4224.17\n",
      "  4216.35 4234.6  4227.68 4238.92 4301.9  4225.94 4184.31 4278.94 4241.41\n",
      "  4242.68 4234.44 4222.92 4271.78 4242.83 4217.47 4232.77 4243.39 4304.8\n",
      "  4221.08 4259.76 4190.93 4308.49 4219.78]\n",
      " [3346.15 3351.03 3347.85 3348.77 3334.66 3366.99 3356.8  3348.53 3338.63\n",
      "  3352.49 3331.83 3356.48 3333.87 3335.02 3328.38 3340.72 3325.21 3335.11\n",
      "  3330.81 3331.33 3328.39 3346.46 3319.35 3327.08 3323.82 3323.17 3318.57\n",
      "  3344.35 3329.22 3325.86 3312.5  3318.87]\n",
      " [3508.57 3510.81 3528.09 3492.45 3522.03 3499.62 3517.81 3500.66 3513.55\n",
      "  3494.39 3522.55 3515.5  3532.48 3497.68 3515.25 3503.76 3528.23 3511.08\n",
      "  3517.99 3503.65 3519.74 3502.69 3512.33 3511.46 3524.04 3503.22 3545.26\n",
      "  3520.82 3545.56 3513.68 3526.76 3505.99]\n",
      " [4400.25 4420.85 4403.39 4430.74 4393.17 4407.14 4401.35 4417.32 4391.47\n",
      "  4426.22 4397.57 4412.08 4405.13 4430.58 4389.99 4422.89 4412.66 4415.1\n",
      "  4400.89 4420.76 4386.18 4419.38 4397.04 4409.02 4393.7  4423.54 4385.48\n",
      "  4407.63 4401.54 4415.98 4387.74 4415.7 ]\n",
      " [5046.86 5023.3  5046.02 5046.77 5021.21 5033.84 5047.41 5018.7  5022.66\n",
      "  5028.3  5018.98 5021.13 5032.08 5005.1  5018.88 5027.16 5024.23 5024.45\n",
      "  5035.82 5007.71 5017.2  5034.44 5028.33 5018.27 5044.12 5022.74 5021.98\n",
      "  5030.06 5035.97 5014.36 5057.21 5028.22]\n",
      " [4923.37 4922.29 4916.32 4922.6  4947.08 4909.91 4919.24 4895.82 4902.78\n",
      "  4887.02 4921.54 4876.23 4895.7  4912.18 4886.1  4885.08 4889.67 4877.6\n",
      "  4889.64 4881.45 4874.31 4853.01 4910.32 4885.55 4883.21 4892.46 4895.28\n",
      "  4866.08 4902.98 4877.91 4889.73 4876.94]\n",
      " [3408.15 3453.01 3465.69 3400.33 3461.34 3400.81 3394.17 3381.94 3583.43\n",
      "  3373.87 3415.38 3463.1  3413.18 3402.48 3453.72 3426.21 3398.59 3524.18\n",
      "  3407.08 3445.91 3400.43 3442.18 3396.58 3371.35 3480.82 3370.12 3438.25\n",
      "  3477.1  3412.48 3433.84 3453.75 3391.83]\n",
      " [3468.53 3464.92 3461.8  3463.59 3464.71 3460.98 3462.7  3459.22 3461.33\n",
      "  3457.6  3464.47 3452.67 3456.73 3459.09 3458.44 3459.06 3458.7  3456.68\n",
      "  3464.69 3457.36 3454.11 3448.54 3460.67 3454.25 3446.29 3455.71 3455.36\n",
      "  3455.23 3457.82 3459.04 3451.8  3459.77]\n",
      " [5122.52 5130.6  5168.34 5203.53 5141.61 5194.39 5113.81 5141.32 5129.31\n",
      "  5163.49 5151.04 5137.57 5132.03 5185.29 5123.21 5128.17 5140.09 5114.48\n",
      "  5112.96 5163.53 5114.59 5119.03 5156.12 5103.47 5091.66 5135.   5114.06\n",
      "  5122.24 5130.06 5099.37 5094.09 5112.96]\n",
      " [5173.27 5181.81 5178.9  5197.49 5161.4  5220.57 5160.09 5170.82 5170.34\n",
      "  5183.27 5193.44 5197.93 5188.66 5184.45 5195.93 5226.43 5203.72 5166.52\n",
      "  5181.67 5174.76 5181.58 5178.99 5199.4  5209.58 5191.43 5240.6  5233.7\n",
      "  5259.9  5245.28 5179.26 5200.94 5171.12]\n",
      " [4555.87 4560.8  4576.56 4565.73 4567.32 4585.57 4574.2  4577.78 4582.93\n",
      "  4569.53 4578.12 4578.6  4577.69 4569.57 4577.09 4582.99 4575.12 4577.22\n",
      "  4580.71 4583.99 4578.5  4583.75 4571.31 4567.83 4576.52 4577.36 4571.55\n",
      "  4576.4  4568.85 4567.23 4583.13 4578.76]\n",
      " [3466.9  3464.63 3494.25 3464.79 3486.17 3462.78 3473.27 3468.63 3486.31\n",
      "  3462.83 3475.49 3477.11 3476.89 3463.3  3485.61 3467.14 3477.5  3469.47\n",
      "  3476.22 3462.13 3483.27 3472.93 3476.84 3466.99 3482.41 3460.39 3478.69\n",
      "  3464.63 3479.14 3463.41 3483.64 3464.15]\n",
      " [3294.89 3270.02 3311.02 3266.8  3280.21 3258.06 3288.19 3259.08 3262.34\n",
      "  3241.47 3266.79 3229.19 3267.26 3226.96 3238.68 3225.58 3264.61 3208.4\n",
      "  3225.93 3204.63 3227.44 3211.42 3219.15 3192.93 3197.86 3191.55 3251.63\n",
      "  3185.29 3208.22 3209.24 3214.28 3211.9 ]\n",
      " [3351.29 3366.15 3340.66 3363.33 3360.8  3356.23 3344.96 3354.76 3336.81\n",
      "  3347.52 3345.27 3344.28 3339.98 3346.   3333.24 3337.65 3333.55 3333.6\n",
      "  3326.66 3336.05 3328.36 3328.43 3327.83 3326.13 3316.01 3327.84 3323.31\n",
      "  3317.61 3323.47 3328.72 3323.36 3328.23]\n",
      " [4632.12 4703.21 4632.14 4684.41 4628.6  4676.17 4638.65 4645.41 4665.11\n",
      "  4667.35 4622.55 4711.59 4670.46 4689.11 4640.44 4648.46 4622.61 4670.37\n",
      "  4714.15 4651.16 4616.98 4653.2  4639.63 4676.98 4629.35 4657.68 4599.53\n",
      "  4644.6  4694.39 4643.91 4605.96 4676.58]\n",
      " [4786.31 4767.72 4763.16 4762.86 4761.15 4748.43 4750.36 4768.34 4770.88\n",
      "  4765.94 4792.72 4781.71 4737.92 4875.34 4814.33 4905.74 4763.9  4809.36\n",
      "  4820.08 4787.86 4794.38 4748.09 4823.67 4815.54 4776.42 4767.16 4818.87\n",
      "  4766.54 4810.89 4795.43 4828.59 4866.5 ]\n",
      " [5050.4  5005.18 5033.18 5022.68 5026.96 5006.35 5040.21 5009.31 5025.03\n",
      "  5011.06 5021.64 4991.56 5038.17 5007.59 5017.93 5018.49 5027.26 4991.55\n",
      "  5035.6  5020.5  5021.97 5017.56 5040.46 5009.15 5040.56 5026.69 5029.81\n",
      "  5015.32 5060.9  5017.79 5045.27 5035.81]\n",
      " [4732.01 4717.68 4735.41 4707.36 4750.56 4719.63 4733.43 4711.23 4738.19\n",
      "  4698.61 4735.55 4715.14 4717.75 4702.31 4732.13 4710.05 4718.04 4709.3\n",
      "  4714.22 4692.4  4730.15 4698.49 4723.53 4708.07 4725.98 4694.88 4735.59\n",
      "  4708.26 4709.55 4706.   4720.55 4692.09]\n",
      " [3270.23 3289.52 3281.25 3283.87 3296.79 3301.49 3361.59 3392.76 3263.5\n",
      "  3349.75 3312.26 3264.16 3264.96 3306.11 3277.27 3315.94 3323.1  3361.19\n",
      "  3324.02 3417.27 3307.64 3318.31 3429.88 3282.12 3440.71 3305.88 3294.56\n",
      "  3350.77 3314.59 3343.39 3337.31 3304.21]\n",
      " [3484.86 3406.23 3428.8  3472.55 3495.38 3489.48 3449.38 3453.44 3399.1\n",
      "  3417.2  3481.53 3459.91 3409.75 3433.55 3447.27 3416.79 3430.44 3417.89\n",
      "  3424.64 3501.89 3417.59 3435.17 3439.53 3488.59 3433.33 3440.48 3440.06\n",
      "  3397.92 3414.4  3592.73 3524.42 3494.1 ]\n",
      " [3008.16 2998.17 3004.64 3002.11 3005.54 2996.59 3015.58 3003.86 3008.49\n",
      "  3010.46 3012.52 3001.04 3008.97 3009.43 3007.81 3006.16 3011.75 3006.74\n",
      "  3017.66 3010.65 3013.58 3006.34 3018.23 2998.33 3009.89 3003.96 3004.15\n",
      "  3001.06 3014.5  3004.55 3011.89 3009.05]\n",
      " [4088.46 4095.93 4109.79 4102.89 4107.8  4103.05 4107.21 4114.94 4112.58\n",
      "  4123.64 4120.66 4132.62 4117.51 4128.42 4121.35 4135.88 4132.05 4132.42\n",
      "  4118.62 4147.96 4133.78 4127.52 4132.43 4143.06 4133.93 4144.35 4148.87\n",
      "  4147.6  4155.53 4167.83 4166.23 4185.89]\n",
      " [4104.95 4080.01 4144.83 4152.52 4127.33 4166.84 4166.83 4126.64 4137.49\n",
      "  4116.31 4145.48 4138.21 4199.55 4176.68 4124.54 4163.61 4154.92 4188.49\n",
      "  4120.8  4143.37 4148.37 4097.86 4177.25 4232.12 4188.13 4156.27 4169.35\n",
      "  4183.5  4223.1  4275.63 4281.2  4292.11]\n",
      " [3588.25 3588.35 3631.72 3574.38 3579.48 3581.45 3657.65 3607.31 3614.17\n",
      "  3674.89 3678.23 3611.64 3548.51 3569.21 3553.86 3560.31 3628.71 3552.27\n",
      "  3617.16 3565.6  3594.53 3553.38 3553.84 3580.48 3606.77 3586.61 3559.64\n",
      "  3606.7  3732.4  3570.96 3539.58 3576.13]\n",
      " [4312.1  4299.56 4318.85 4288.35 4300.14 4300.54 4312.96 4280.5  4312.95\n",
      "  4291.43 4300.37 4291.53 4313.58 4284.53 4320.24 4317.24 4310.96 4291.9\n",
      "  4311.74 4284.05 4315.07 4312.03 4311.13 4293.3  4318.35 4294.34 4306.1\n",
      "  4311.88 4313.99 4296.17 4333.91 4309.29]\n",
      " [3463.19 3486.98 3465.69 3484.44 3465.56 3491.57 3463.02 3487.18 3462.18\n",
      "  3483.06 3461.75 3488.   3464.21 3478.92 3462.16 3485.16 3473.69 3482.51\n",
      "  3458.99 3484.25 3459.27 3482.45 3457.54 3480.46 3451.7  3480.46 3474.79\n",
      "  3474.95 3451.98 3483.14 3461.34 3482.96]\n",
      " [4391.18 4355.98 4360.23 4409.22 4502.06 4403.92 4364.36 4365.67 4388.26\n",
      "  4384.68 4353.19 4374.29 4383.88 4428.54 4394.35 4387.23 4394.08 4338.54\n",
      "  4367.56 4363.44 4358.25 4353.49 4376.58 4347.08 4346.22 4360.15 4415.4\n",
      "  4344.68 4393.95 4359.56 4370.88 4364.71]\n",
      " [4378.38 4388.55 4265.17 4169.21 4207.29 4175.49 4271.46 4252.13 4299.34\n",
      "  4308.71 4302.82 4304.25 4416.65 4289.75 4370.67 4384.37 4462.96 4362.92\n",
      "  4377.75 4355.43 4428.33 4335.81 4348.99 4394.37 4324.61 4300.07 4310.41\n",
      "  4312.8  4424.54 4308.11 4340.11 4313.4 ]\n",
      " [4399.56 4383.35 4344.62 4344.94 4366.86 4395.58 4402.57 4393.96 4377.64\n",
      "  4340.96 4374.3  4352.16 4499.03 4314.68 4366.29 4334.04 4368.77 4353.26\n",
      "  4348.49 4344.75 4385.26 4501.24 4354.29 4341.97 4345.5  4334.7  4408.75\n",
      "  4421.93 4352.5  4348.87 4382.73 4344.42]\n",
      " [4728.38 4724.57 4775.85 4716.22 4740.22 4750.39 4742.38 4758.11 4816.93\n",
      "  4710.82 4824.32 4726.15 4762.74 4789.43 4783.93 4734.47 4780.79 4748.25\n",
      "  4744.3  4737.69 4759.64 4742.68 4769.55 4738.42 4906.72 4723.54 4821.33\n",
      "  4763.64 4768.74 4735.06 4781.73 4717.38]\n",
      " [3607.7  3585.7  3598.79 3581.7  3603.94 3581.92 3596.32 3597.02 3599.39\n",
      "  3580.11 3602.65 3585.04 3593.27 3605.26 3594.55 3590.01 3623.31 3582.58\n",
      "  3597.61 3574.29 3616.1  3578.85 3590.34 3582.8  3617.51 3584.3  3598.35\n",
      "  3584.99 3592.88 3589.29 3590.54 3577.09]\n",
      " [4703.28 4684.56 4717.95 4664.46 4663.72 4714.44 4685.54 4680.33 4631.7\n",
      "  4703.15 4664.96 4805.79 4675.7  4699.91 4655.44 4639.2  4680.25 4651.61\n",
      "  4641.15 4708.95 4645.61 4665.94 4654.32 4714.42 4684.59 4683.05 4707.86\n",
      "  4793.09 4709.64 4695.54 4644.92 4663.26]\n",
      " [4147.47 4186.18 4290.81 4221.17 4196.5  4242.94 4175.   4153.82 4181.91\n",
      "  4202.47 4153.63 4170.47 4160.83 4194.75 4179.08 4174.49 4166.4  4170.33\n",
      "  4217.47 4168.94 4160.62 4183.53 4195.62 4175.25 4242.43 4179.14 4243.18\n",
      "  4203.64 4223.27 4235.25 4176.65 4179.78]\n",
      " [3846.49 3855.18 3886.02 3886.62 3947.02 3917.18 3911.85 3906.3  3912.79\n",
      "  3970.94 3956.48 3934.23 3938.61 3938.89 3957.56 4021.3  4058.68 3970.92\n",
      "  4001.3  4021.12 4039.99 4034.51 4035.18 4016.23 4038.15 4102.56 4059.05\n",
      "  4058.13 4219.7  4159.88 4273.65 4186.51]\n",
      " [4832.23 4800.8  4827.27 4829.17 4822.04 4812.19 4826.68 4803.48 4819.11\n",
      "  4822.13 4810.71 4802.57 4830.77 4807.53 4813.66 4814.26 4811.47 4803.52\n",
      "  4824.42 4807.27 4808.57 4809.87 4816.73 4797.75 4823.32 4810.33 4804.95\n",
      "  4807.06 4819.71 4793.7  4814.04 4813.13]\n",
      " [2980.89 2970.08 2977.76 2968.04 2986.94 2972.68 2979.57 2967.29 2996.01\n",
      "  2975.72 2987.46 2966.46 2986.49 2966.03 2997.33 2967.54 2979.12 2975.03\n",
      "  3040.17 2966.31 2984.2  2971.84 2982.48 2965.5  2985.12 2966.63 2980.36\n",
      "  2971.84 2976.25 2963.32 2984.45 2960.58]\n",
      " [4450.96 4434.52 4438.71 4447.3  4429.56 4430.22 4437.   4430.71 4423.52\n",
      "  4432.13 4421.89 4418.77 4425.6  4417.05 4404.58 4416.53 4419.63 4401.55\n",
      "  4406.88 4399.82 4403.89 4399.44 4395.98 4387.11 4390.26 4392.79 4384.13\n",
      "  4380.91 4381.13 4370.82 4386.89 4380.98]\n",
      " [4187.63 4247.11 4176.51 4176.12 4197.25 4229.85 4212.74 4151.81 4219.08\n",
      "  4176.35 4169.55 4214.41 4161.77 4159.55 4175.68 4199.34 4207.73 4226.1\n",
      "  4213.46 4276.85 4406.07 4177.09 4295.34 4294.36 4204.34 4205.41 4258.12\n",
      "  4193.98 4210.47 4254.17 4221.89 4221.63]\n",
      " [3476.94 3475.6  3466.62 3488.05 3466.63 3485.08 3465.27 3479.54 3463.03\n",
      "  3487.81 3465.91 3485.38 3465.13 3488.97 3461.39 3486.08 3475.61 3485.68\n",
      "  3468.01 3485.91 3465.82 3487.21 3465.59 3485.13 3475.41 3481.63 3468.08\n",
      "  3498.46 3499.24 3489.44 3486.29 3491.39]\n",
      " [4382.17 4388.4  4370.05 4405.84 4383.26 4387.74 4379.57 4402.95 4378.21\n",
      "  4418.42 4380.66 4395.06 4406.02 4398.81 4370.59 4390.85 4379.49 4382.98\n",
      "  4366.97 4393.04 4368.84 4383.35 4374.12 4385.52 4365.97 4429.83 4375.73\n",
      "  4378.87 4377.05 4397.68 4380.76 4412.16]\n",
      " [3682.72 3689.29 3769.13 3716.27 3777.45 3776.38 3714.93 3663.46 3674.32\n",
      "  3692.75 3701.82 3668.19 3676.94 3730.51 3747.54 3704.53 3697.89 3832.09\n",
      "  3710.3  3654.18 3684.91 3655.09 3710.37 3692.57 3684.86 3678.48 3713.91\n",
      "  3712.93 3680.84 3699.98 3683.45 3688.84]\n",
      " [3555.99 3644.19 3565.53 3570.74 3602.29 3622.49 3667.03 3662.66 3697.84\n",
      "  3597.78 3571.46 3614.29 3632.12 3565.61 3633.44 3606.19 3597.83 3637.\n",
      "  3564.09 3598.16 3563.3  3566.63 3553.67 3596.57 3593.19 3608.72 3645.85\n",
      "  3595.19 3581.89 3577.85 3565.49 3579.95]\n",
      " [4392.45 4384.06 4376.02 4382.81 4387.26 4368.22 4389.12 4385.25 4378.78\n",
      "  4377.7  4391.3  4376.89 4387.1  4390.45 4376.25 4371.71 4391.   4374.43\n",
      "  4382.75 4387.97 4382.91 4371.47 4394.82 4378.16 4380.64 4381.87 4391.56\n",
      "  4366.62 4395.56 4381.02 4373.55 4376.65]\n",
      " [5280.09 5356.54 5252.66 5319.28 5267.39 5250.06 5396.2  5362.71 5281.91\n",
      "  5257.94 5259.99 5302.93 5307.86 5287.28 5273.16 5239.71 5283.94 5324.06\n",
      "  5324.06 5268.69 5255.6  5245.31 5258.03 5275.96 5247.02 5270.74 5256.46\n",
      "  5273.19 5268.2  5328.26 5320.12 5264.48]\n",
      " [5098.96 5099.14 5119.71 5103.36 5097.7  5121.16 5109.52 5103.93 5109.07\n",
      "  5107.36 5105.76 5108.09 5103.81 5089.84 5092.31 5111.25 5083.23 5098.52\n",
      "  5098.74 5090.47 5086.72 5101.11 5078.41 5089.69 5101.17 5091.93 5078.83\n",
      "  5108.19 5089.92 5093.69 5100.24 5104.94]\n",
      " [5346.6  5326.18 5389.32 5350.32 5405.92 5331.67 5353.16 5323.08 5358.99\n",
      "  5331.76 5338.   5344.13 5348.7  5313.8  5390.44 5320.23 5373.24 5392.13\n",
      "  5433.01 5364.5  5364.89 5361.17 5385.62 5348.91 5364.01 5354.8  5351.44\n",
      "  5349.55 5336.57 5325.41 5366.17 5342.61]\n",
      " [4438.91 4433.82 4455.96 4404.09 4392.06 4482.16 4387.37 4419.64 4400.68\n",
      "  4419.47 4412.65 4419.27 4435.81 4437.23 4410.17 4473.81 4403.99 4440.18\n",
      "  4424.73 4427.85 4393.81 4421.57 4417.79 4435.6  4442.29 4656.14 4426.84\n",
      "  4440.3  4461.05 4427.45 4390.26 4447.85]\n",
      " [3440.58 3431.76 3458.62 3431.59 3438.85 3425.08 3446.99 3432.09 3448.64\n",
      "  3423.19 3444.76 3425.02 3439.73 3422.66 3433.91 3419.07 3422.11 3416.07\n",
      "  3431.84 3404.4  3428.6  3414.08 3422.62 3407.65 3427.67 3404.64 3427.02\n",
      "  3429.79 3420.17 3398.99 3417.79 3389.15]\n",
      " [2994.53 3006.32 2999.89 3007.1  2990.01 3011.39 3007.6  2987.03 2980.05\n",
      "  2998.94 2978.01 2986.48 2970.43 2972.66 2959.82 2968.94 2966.67 2954.73\n",
      "  2952.11 2947.72 2947.08 2944.41 2935.4  2934.03 2930.75 2941.85 2926.95\n",
      "  2933.3  2929.55 2926.21 2924.4  2928.85]\n",
      " [4632.58 4641.37 4626.28 4607.96 4597.06 4631.39 4618.39 4624.63 4620.\n",
      "  4617.74 4583.8  4611.49 4594.74 4592.02 4672.97 4631.01 4574.02 4612.71\n",
      "  4700.97 4595.74 4580.79 4606.04 4614.41 4630.97 4618.02 4627.48 4620.1\n",
      "  4634.92 4784.23 4581.31 4616.58 4610.25]\n",
      " [3539.81 3450.45 3455.73 3582.68 3498.28 3478.31 3445.69 3431.37 3483.01\n",
      "  3499.93 3543.02 3406.46 3445.3  3436.89 3450.51 3408.07 3470.83 3445.46\n",
      "  3457.74 3408.2  3485.4  3429.83 3471.48 3474.25 3575.54 3438.1  3453.1\n",
      "  3518.81 3444.31 3513.46 3479.95 3434.05]\n",
      " [4635.88 4646.13 4642.41 4645.27 4645.7  4623.82 4637.68 4638.41 4668.17\n",
      "  4639.12 4654.17 4726.17 4673.72 4633.29 4712.68 4609.35 4658.95 4645.98\n",
      "  4627.73 4618.22 4638.2  4676.99 4655.03 4638.7  4622.08 4614.58 4667.87\n",
      "  4613.55 4618.43 4638.07 4705.35 4612.99]\n",
      " [4526.98 4572.83 4556.87 4568.31 4517.47 4514.65 4636.86 4516.05 4532.33\n",
      "  4492.38 4545.92 4535.68 4593.15 4561.63 4528.79 4524.3  4568.72 4548.4\n",
      "  4654.5  4524.4  4592.45 4518.67 4626.22 4506.7  4514.5  4483.41 4515.22\n",
      "  4542.93 4510.75 4474.47 4516.51 4482.53]\n",
      " [2992.58 3013.42 2994.9  3009.64 3000.23 3013.64 2995.14 3007.82 2999.76\n",
      "  3004.58 2995.6  3011.93 2990.71 3010.88 2998.51 3004.49 2996.66 3010.\n",
      "  2995.44 3009.07 2997.45 3008.07 2999.29 3012.   2999.85 3007.73 3006.37\n",
      "  3013.1  3005.11 3015.1  3004.55 3009.75]\n",
      " [3005.47 2996.94 3007.24 3003.06 3014.87 2998.61 3008.03 3003.42 3017.9\n",
      "  2996.03 3018.42 3004.15 3009.31 3005.53 3015.37 3016.06 3014.5  2999.92\n",
      "  3008.64 3003.48 3018.34 3013.39 3024.29 3006.75 3010.28 3008.41 3018.22\n",
      "  3007.93 3029.51 3004.63 3020.21 3004.53]\n",
      " [4068.41 3989.   3997.43 4023.25 4007.49 4043.99 4114.35 4011.61 4065.11\n",
      "  4019.05 4100.27 4037.75 4082.67 4034.76 4134.32 4087.05 4015.17 4138.01\n",
      "  4050.38 4042.37 4024.81 4069.48 4049.26 4092.   4118.85 4121.62 4024.68\n",
      "  4023.44 4030.06 4014.05 4046.66 4012.48]\n",
      " [4103.37 4113.94 4111.85 4108.31 4108.06 4120.85 4124.18 4118.31 4126.3\n",
      "  4127.11 4121.12 4132.72 4139.43 4131.55 4141.13 4143.63 4138.39 4142.77\n",
      "  4149.53 4138.66 4143.16 4148.66 4148.38 4146.02 4145.85 4144.15 4138.46\n",
      "  4149.42 4158.46 4152.65 4153.96 4150.99]\n",
      " [2993.25 2987.62 2994.53 2979.29 3003.74 2985.24 2999.87 2994.79 3008.88\n",
      "  2987.97 3008.12 2998.18 3010.95 3005.99 3029.54 3001.94 3022.13 3011.19\n",
      "  3024.95 3012.19 3033.98 3019.88 3033.83 3024.03 3029.56 3017.82 3044.7\n",
      "  3022.61 3039.07 3028.08 3038.43 3037.53]\n",
      " [4057.48 4033.41 4086.84 4022.15 4068.18 4047.75 4053.2  4064.78 4101.1\n",
      "  4027.59 4087.65 4149.32 4044.79 4010.2  4064.9  4022.67 4154.01 4074.05\n",
      "  4039.46 4052.25 4108.22 4025.34 4047.41 4024.15 4072.27 4018.08 4046.76\n",
      "  4052.28 4051.41 4027.77 4058.04 4022.95]\n",
      " [4457.32 4462.45 4468.01 4444.07 4502.33 4393.24 4397.65 4429.56 4404.88\n",
      "  4485.54 4426.2  4415.07 4434.01 4436.19 4428.73 4481.06 4470.32 4422.59\n",
      "  4463.4  4628.81 4429.05 4430.02 4426.35 4435.44 4439.75 4415.63 4464.82\n",
      "  4416.61 4434.09 4425.08 4438.46 4431.01]\n",
      " [4423.56 4414.35 4418.72 4417.01 4424.79 4405.56 4439.62 4418.76 4426.56\n",
      "  4424.17 4462.67 4420.4  4442.08 4441.13 4459.41 4449.16 4496.62 4498.52\n",
      "  4499.61 4527.18 4536.36 4521.58 4546.   4530.28 4534.93 4522.19 4532.45\n",
      "  4496.85 4512.41 4497.03 4489.7  4478.5 ]\n",
      " [3469.52 3453.01 3469.64 3446.51 3474.42 3447.09 3466.46 3453.07 3470.15\n",
      "  3446.4  3469.11 3450.85 3460.22 3451.55 3465.19 3447.55 3462.68 3441.86\n",
      "  3470.75 3444.42 3463.8  3439.65 3459.15 3452.01 3463.64 3440.01 3461.98\n",
      "  3443.86 3466.2  3449.84 3457.76 3438.23]\n",
      " [4645.49 4669.84 4665.98 4662.03 4656.98 4677.14 4655.01 4669.99 4662.48\n",
      "  4669.64 4657.28 4681.38 4658.83 4665.7  4668.37 4672.04 4653.11 4680.33\n",
      "  4658.58 4665.7  4667.08 4674.39 4649.61 4665.29 4653.26 4657.92 4655.12\n",
      "  4666.06 4645.68 4673.79 4655.24 4651.42]\n",
      " [3378.92 3386.75 3363.34 3392.07 3366.31 3403.18 3335.18 3323.   3439.18\n",
      "  3365.85 3377.97 3351.04 3444.73 3346.96 3409.03 3334.36 3389.42 3332.43\n",
      "  3381.35 3386.47 3340.22 3347.69 3433.91 3475.85 3443.88 3534.21 3354.55\n",
      "  3395.2  3316.84 3339.93 3336.2  3319.27]\n",
      " [3443.79 3451.37 3447.77 3450.09 3444.86 3448.38 3445.81 3447.63 3452.22\n",
      "  3451.7  3444.05 3450.51 3452.24 3446.73 3452.92 3451.87 3445.87 3456.67\n",
      "  3453.81 3445.92 3458.57 3450.51 3450.28 3451.23 3454.2  3453.73 3465.44\n",
      "  3455.08 3454.22 3452.03 3454.55 3448.05]\n",
      " [4067.84 4122.82 4041.88 4094.44 4155.1  4019.63 3997.34 4059.51 3999.89\n",
      "  4012.57 4029.37 4037.02 4012.82 4088.09 4011.07 4061.39 4030.63 4151.91\n",
      "  3996.92 4263.81 4057.98 4067.14 4088.93 4008.94 4017.25 4039.   4009.49\n",
      "  4069.26 4030.3  4013.87 4078.83 4040.15]\n",
      " [4239.81 4297.5  4287.48 4310.69 4263.36 4306.22 4252.59 4262.9  4243.17\n",
      "  4376.7  4245.06 4331.46 4255.82 4290.95 4269.71 4263.83 4276.07 4322.13\n",
      "  4254.23 4299.95 4247.45 4265.65 4282.49 4324.95 4320.28 4331.42 4279.12\n",
      "  4313.18 4307.86 4355.88 4331.43 4304.13]\n",
      " [2937.02 2927.73 2942.15 2934.02 2924.33 2924.38 2933.17 2923.76 2915.83\n",
      "  2928.03 2914.59 2934.3  2911.05 2920.82 2933.48 2941.38 2900.91 2891.63\n",
      "  2909.49 2901.91 2895.31 2880.15 2876.5  2862.36 2853.79 2864.39 2858.31\n",
      "  2858.26 2861.56 2853.31 2847.33 2864.46]\n",
      " [3037.84 3043.26 3037.   3047.65 3026.86 3041.31 3026.82 3032.32 3025.76\n",
      "  3028.14 3008.52 3021.4  3007.53 3011.22 2999.49 3020.11 3011.59 3009.5\n",
      "  2998.59 3015.21 3005.11 3018.12 2994.09 3006.03 2997.87 3007.32 2999.77\n",
      "  3021.99 2991.18 3003.06 2994.11 3020.87]\n",
      " [4135.54 4137.29 4117.18 4126.89 4113.39 4109.   4113.89 4103.52 4106.25\n",
      "  4108.74 4096.59 4096.58 4096.86 4088.52 4083.95 4086.03 4081.03 4079.15\n",
      "  4082.14 4080.82 4073.79 4075.7  4079.47 4077.1  4078.72 4089.26 4079.83\n",
      "  4084.29 4104.23 4085.77 4084.41 4108.82]\n",
      " [4355.59 4405.65 4350.69 4328.93 4404.66 4330.76 4333.92 4346.01 4340.08\n",
      "  4351.64 4336.27 4381.85 4347.64 4334.12 4338.68 4346.49 4402.36 4332.08\n",
      "  4334.59 4430.6  4351.   4361.93 4373.78 4359.14 4367.1  4354.05 4355.45\n",
      "  4360.29 4351.65 4406.23 4411.36 4486.39]\n",
      " [5246.8  5245.06 5242.41 5214.57 5229.33 5238.57 5265.44 5210.96 5248.49\n",
      "  5257.44 5237.75 5240.11 5261.55 5226.73 5245.65 5222.92 5342.47 5233.09\n",
      "  5266.08 5230.67 5271.57 5247.52 5348.5  5232.46 5275.82 5252.42 5254.67\n",
      "  5264.61 5270.34 5238.94 5276.09 5226.21]\n",
      " [3474.27 3453.32 3458.36 3442.5  3473.44 3471.79 3464.66 3443.36 3462.02\n",
      "  3441.18 3466.   3449.25 3475.42 3444.09 3463.13 3446.53 3460.46 3448.77\n",
      "  3456.56 3442.83 3470.17 3447.85 3461.62 3436.76 3462.18 3445.15 3471.75\n",
      "  3447.77 3456.95 3447.18 3470.32 3450.49]\n",
      " [4255.64 4233.25 4266.11 4238.01 4248.65 4237.89 4261.2  4230.35 4264.88\n",
      "  4239.96 4254.29 4243.4  4264.98 4237.15 4260.19 4248.42 4254.61 4241.52\n",
      "  4268.97 4245.62 4259.54 4262.21 4254.98 4237.58 4268.26 4241.9  4264.88\n",
      "  4252.57 4286.48 4237.31 4265.54 4244.24]\n",
      " [3935.69 3946.84 3942.98 3942.12 3937.69 3947.89 3943.42 3940.45 3963.1\n",
      "  3947.4  3941.44 3943.05 3945.3  3938.83 3943.12 3943.01 3941.28 3945.34\n",
      "  3945.68 3938.97 3943.46 3940.85 3929.98 3931.79 3926.65 3915.12 3914.42\n",
      "  3922.26 3904.49 3901.79 3905.27 3898.51]\n",
      " [4350.87 4316.95 4340.91 4323.97 4334.1  4316.67 4343.24 4310.2  4333.7\n",
      "  4318.03 4341.52 4317.14 4352.6  4334.94 4340.9  4336.7  4351.07 4326.4\n",
      "  4351.91 4340.63 4345.21 4324.89 4351.02 4319.   4342.56 4327.99 4342.08\n",
      "  4319.52 4351.04 4328.26 4342.8  4333.27]\n",
      " [4715.47 4710.39 4783.47 4678.53 4740.05 4682.99 4694.93 4684.33 4703.54\n",
      "  4697.6  4757.81 4725.24 4727.71 4732.68 4777.74 4702.28 4734.14 4743.07\n",
      "  4701.58 4906.65 4752.98 4696.26 4702.76 4858.44 4719.75 4806.54 4731.8\n",
      "  4709.89 4702.83 4762.86 4695.25 4685.61]\n",
      " [4702.81 4715.39 4728.16 4722.94 4700.99 4738.59 4714.53 4727.38 4705.\n",
      "  4713.51 4688.58 4735.91 4721.15 4745.27 4702.2  4705.97 4690.1  4704.29\n",
      "  4735.59 4721.15 4724.13 4709.3  4764.61 4749.16 4827.97 4731.04 4743.11\n",
      "  4700.48 4717.63 4677.75 4695.26 4698.18]\n",
      " [4024.57 4066.29 4033.06 4057.85 4057.57 4058.2  4138.84 4063.62 4150.75\n",
      "  4151.92 4030.35 4168.43 4085.13 4110.49 4035.19 4081.86 4132.89 4130.23\n",
      "  4205.75 4093.39 4109.92 4077.36 4091.62 4106.69 4062.97 4092.09 4050.93\n",
      "  4130.62 4090.34 4069.53 4034.66 4072.18]\n",
      " [3497.27 3468.1  3501.81 3455.54 3481.08 3458.26 3481.16 3467.57 3480.\n",
      "  3462.16 3487.84 3463.41 3489.8  3462.82 3481.98 3468.31 3488.04 3459.56\n",
      "  3483.83 3463.35 3480.14 3460.05 3488.75 3459.58 3480.82 3466.81 3485.11\n",
      "  3466.29 3485.09 3462.28 3485.37 3466.38]\n",
      " [4739.76 4727.22 4837.41 4707.91 4730.66 4706.51 4737.74 4706.7  4732.\n",
      "  4709.39 4731.95 4746.04 4738.33 4712.16 4745.28 4714.59 4731.48 4690.34\n",
      "  4855.96 4719.67 4738.75 4708.8  4735.19 4749.11 4743.64 4730.19 4740.08\n",
      "  4722.21 4776.37 4798.88 4768.3  4835.65]\n",
      " [4288.57 4361.2  4339.95 4275.23 4394.47 4317.1  4288.42 4219.66 4295.34\n",
      "  4244.81 4243.51 4241.58 4231.9  4226.42 4289.1  4218.48 4282.96 4133.71\n",
      "  4091.49 4064.93 4045.78 4108.48 4044.   4013.12 4065.23 4000.13 4020.9\n",
      "  4028.99 4057.83 4040.24 4095.32 4110.1 ]\n",
      " [3253.41 3259.43 3272.4  3252.21 3250.37 3242.24 3276.14 3262.91 3353.52\n",
      "  3269.21 3291.28 3400.12 3265.64 3263.08 3251.35 3306.54 3291.03 3285.47\n",
      "  3277.99 3260.8  3263.13 3260.55 3315.25 3267.96 3337.84 3288.93 3292.22\n",
      "  3297.82 3263.64 3272.44 3302.47 3331.04]\n",
      " [4969.91 4974.92 4983.22 4980.04 4969.9  4991.62 4979.32 4978.66 4993.55\n",
      "  4987.49 4983.51 4990.34 4994.62 4991.44 4994.85 5009.29 4999.44 4994.99\n",
      "  4994.83 5001.56 5040.26 5002.79 4986.55 5007.08 4989.39 4987.5  4989.21\n",
      "  4992.82 4977.   4982.71 4986.31 4974.44]\n",
      " [4417.18 4307.35 4313.12 4293.02 4313.93 4339.4  4310.77 4284.03 4315.88\n",
      "  4299.66 4306.6  4340.25 4274.22 4280.71 4349.03 4349.64 4380.3  4319.41\n",
      "  4367.9  4293.   4318.02 4336.79 4328.1  4338.21 4420.78 4356.57 4359.07\n",
      "  4324.07 4348.81 4331.15 4368.21 4308.2 ]\n",
      " [4221.82 4251.1  4233.09 4235.09 4209.84 4242.1  4227.26 4242.49 4232.84\n",
      "  4247.3  4211.38 4242.88 4226.81 4244.5  4223.1  4243.77 4217.36 4233.92\n",
      "  4227.83 4236.01 4227.43 4248.31 4217.28 4236.51 4234.27 4243.57 4223.12\n",
      "  4250.59 4228.65 4236.57 4234.49 4246.83]\n",
      " [4790.78 4785.53 4771.79 4774.76 4787.54 4764.67 4783.8  4784.02 4772.72\n",
      "  4769.85 4776.11 4767.58 4768.77 4787.39 4764.01 4768.55 4782.71 4769.49\n",
      "  4772.53 4785.58 4770.31 4766.43 4791.67 4788.24 4762.33 4781.98 4775.8\n",
      "  4772.57 4795.49 4799.51 4783.25 4808.99]\n",
      " [5090.67 5046.46 5075.17 5132.94 5066.22 5053.84 5090.81 5049.91 5108.74\n",
      "  5058.35 5083.54 5063.51 5080.88 5048.02 5085.66 5099.18 5134.7  5073.21\n",
      "  5122.77 5094.58 5073.52 5083.03 5089.68 5058.05 5133.44 5079.41 5074.95\n",
      "  5088.63 5142.58 5068.14 5124.44 5082.93]\n",
      " [4133.54 3986.82 4026.59 4010.07 4257.04 4384.9  4581.4  4690.82 4772.2\n",
      "  4759.65 4717.81 4669.97 4525.21 4446.72 4136.76 3977.76 3832.07 3723.07\n",
      "  3713.48 3651.57 3636.2  3664.3  3777.5  3873.74 3963.23 4025.58 4136.13\n",
      "  4352.33 4418.78 4437.36 4406.52 4477.44]\n",
      " [3433.79 3437.31 3454.54 3423.67 3430.5  3404.23 3365.94 3486.69 3382.31\n",
      "  3388.74 3401.94 3352.76 3432.52 3399.54 3521.04 3526.16 3455.66 3445.37\n",
      "  3513.06 3460.9  3576.42 3472.71 3407.11 3381.83 3402.35 3385.42 3616.72\n",
      "  3391.12 3458.63 3372.74 3354.4  3390.55]\n",
      " [4176.6  4189.83 4185.88 4174.35 4191.41 4190.17 4175.48 4186.32 4186.54\n",
      "  4186.22 4178.76 4188.85 4185.39 4186.12 4186.73 4177.2  4187.29 4183.56\n",
      "  4176.38 4178.22 4181.28 4170.27 4175.63 4181.6  4165.82 4160.34 4170.19\n",
      "  4163.15 4153.94 4161.76 4160.1  4145.65]\n",
      " [5154.84 5166.92 5141.48 5175.19 5127.18 5237.52 5216.88 5173.35 5139.47\n",
      "  5172.06 5140.6  5190.31 5155.53 5169.91 5141.52 5236.87 5161.87 5221.95\n",
      "  5264.53 5183.73 5174.89 5209.52 5178.   5214.85 5201.13 5382.2  5174.46\n",
      "  5248.78 5159.7  5173.32 5144.82 5192.24]\n",
      " [4846.8  4864.78 4847.22 4813.84 4825.29 4817.   4814.18 4862.05 4819.14\n",
      "  4843.95 4864.08 4894.71 4821.35 4872.69 4853.55 4922.29 4841.72 4841.02\n",
      "  4845.98 4847.84 4913.32 4819.89 4885.38 4893.2  4830.81 4849.52 4839.29\n",
      "  4824.83 4871.13 4870.35 4835.33 4850.17]\n",
      " [3265.78 3249.77 3258.71 3255.   3264.37 3249.46 3270.94 3244.48 3269.59\n",
      "  3250.89 3272.13 3239.19 3259.24 3257.23 3256.58 3233.11 3251.5  3229.11\n",
      "  3246.44 3245.94 3246.85 3253.61 3256.17 3234.05 3249.71 3239.56 3253.96\n",
      "  3240.83 3279.1  3238.38 3260.22 3253.8 ]\n",
      " [4279.75 4255.68 4232.08 4213.22 4223.32 4189.14 4160.57 4127.32 4079.52\n",
      "  4104.9  4096.69 4053.13 4068.87 4276.42 4211.27 4362.14 4380.   4534.13\n",
      "  4662.51 4803.75 4825.32 4636.13 4391.01 4151.99 4170.88 4392.7  4434.67\n",
      "  4503.75 4439.22 4322.94 4277.26 4257.52]\n",
      " [4600.74 4581.25 4604.2  4563.88 4569.73 4548.23 4544.54 4575.22 4551.34\n",
      "  4587.01 4535.38 4530.29 4583.93 4567.42 4544.3  4703.52 4551.54 4566.89\n",
      "  4576.5  4500.94 4665.52 4514.84 4577.27 4633.8  4565.97 4585.92 4692.24\n",
      "  4689.58 4623.49 4582.07 4641.39 4662.51]\n",
      " [5285.55 5282.18 5275.48 5296.83 5272.31 5290.69 5341.41 5268.65 5394.69\n",
      "  5278.17 5273.84 5239.54 5296.05 5252.53 5268.62 5244.68 5262.12 5223.75\n",
      "  5279.61 5245.22 5239.48 5233.62 5291.22 5219.05 5335.8  5254.07 5272.85\n",
      "  5246.19 5287.28 5248.86 5285.16 5238.74]\n",
      " [5307.88 5213.49 5198.98 5235.97 5199.83 5220.19 5203.77 5215.71 5191.85\n",
      "  5250.54 5199.78 5251.68 5200.12 5267.64 5210.59 5273.81 5207.4  5223.54\n",
      "  5231.7  5225.01 5191.65 5246.71 5199.84 5233.88 5242.22 5272.77 5244.94\n",
      "  5239.84 5206.26 5223.63 5216.14 5309.34]\n",
      " [4706.44 4702.91 4713.08 4703.84 4687.84 4752.25 4750.44 4743.33 4805.18\n",
      "  4760.33 4702.08 4688.86 4912.85 4705.71 4692.07 4708.87 4705.16 4704.29\n",
      "  4722.33 4715.33 4720.   4793.73 4855.14 4789.83 4764.61 4808.4  4728.26\n",
      "  4787.18 4752.76 4749.38 4732.47 4793.22]\n",
      " [4209.91 4305.31 4220.88 4238.75 4253.13 4291.76 4279.74 4330.96 4264.89\n",
      "  4309.9  4325.15 4269.98 4240.02 4214.99 4283.82 4172.44 4203.37 4179.06\n",
      "  4157.79 4154.53 4181.97 4172.92 4198.88 4199.04 4296.73 4181.56 4229.56\n",
      "  4182.35 4224.66 4205.9  4211.27 4260.  ]\n",
      " [3480.26 3471.51 3469.72 3473.74 3470.91 3471.99 3484.27 3475.62 3471.87\n",
      "  3466.02 3485.   3469.37 3475.41 3473.04 3486.28 3468.39 3477.27 3465.42\n",
      "  3476.83 3472.87 3477.3  3471.32 3483.31 3491.89 3480.21 3476.28 3478.91\n",
      "  3470.11 3482.29 3478.49 3496.49 3473.93]\n",
      " [4652.46 4650.89 4695.93 4649.42 4698.41 4774.86 4667.48 4713.71 4679.14\n",
      "  4675.88 4698.3  4649.93 4667.52 4627.43 4702.54 4647.82 4638.7  4635.31\n",
      "  4710.21 4711.43 4706.27 4703.81 4759.02 4646.14 4689.31 4633.65 4657.69\n",
      "  4658.16 4670.   4663.07 4691.47 4680.79]\n",
      " [4335.51 4344.18 4352.99 4356.16 4324.72 4348.37 4340.75 4354.15 4323.59\n",
      "  4352.92 4325.17 4357.36 4339.74 4349.29 4320.02 4348.08 4335.31 4335.53\n",
      "  4336.23 4351.59 4316.66 4343.31 4324.46 4333.77 4325.57 4352.03 4313.\n",
      "  4341.52 4328.9  4343.33 4322.53 4349.01]\n",
      " [4609.34 4513.51 4539.77 4503.62 4552.52 4601.3  4545.8  4502.33 4566.77\n",
      "  4563.45 4590.96 4539.5  4526.36 4544.66 4587.81 4542.76 4593.12 4592.53\n",
      "  4583.63 4529.8  4551.58 4573.63 4528.17 4544.26 4552.24 4529.28 4625.72\n",
      "  4607.04 4737.63 4557.49 4554.22 4529.91]\n",
      " [4618.25 4564.32 4559.44 4604.17 4566.06 4585.01 4592.3  4606.11 4644.23\n",
      "  4733.4  4647.77 4626.66 4609.16 4581.48 4597.86 4664.3  4597.26 4615.43\n",
      "  4606.53 4604.16 4586.19 4667.96 4592.64 4640.65 4618.68 4634.86 4602.43\n",
      "  4676.04 4624.89 4689.11 4655.36 4669.67]\n",
      " [5067.87 5071.12 5050.21 5063.58 5066.49 5063.59 5052.65 5067.23 5046.64\n",
      "  5052.82 5068.6  5058.76 5049.82 5074.46 5051.93 5049.27 5064.59 5066.11\n",
      "  5048.9  5069.64 5068.82 5053.63 5066.82 5073.41 5056.15 5069.15 5069.85\n",
      "  5064.69 5071.56 5090.14 5057.89 5072.95]\n",
      " [3476.57 3493.95 3470.84 3490.21 3464.87 3484.58 3473.92 3484.53 3468.17\n",
      "  3491.33 3466.   3485.1  3479.13 3490.03 3467.85 3494.22 3477.94 3480.79\n",
      "  3466.93 3480.23 3465.11 3489.49 3466.66 3483.81 3467.93 3489.01 3459.92\n",
      "  3478.99 3474.33 3483.63 3468.86 3490.39]\n",
      " [5115.59 5120.5  5125.92 5117.15 5120.08 5129.12 5107.5  5116.68 5123.58\n",
      "  5119.62 5108.93 5135.29 5106.84 5108.73 5127.59 5120.34 5107.1  5119.49\n",
      "  5105.11 5100.74 5111.04 5114.73 5100.49 5108.44 5105.99 5099.66 5105.32\n",
      "  5109.21 5090.25 5101.62 5106.55 5094.3 ]\n",
      " [3456.85 3468.39 3457.19 3490.95 3456.14 3477.19 3475.04 3487.94 3454.96\n",
      "  3470.77 3447.29 3468.72 3453.58 3455.7  3449.22 3460.93 3443.33 3470.91\n",
      "  3456.1  3476.51 3464.19 3459.63 3450.51 3455.96 3452.38 3456.66 3442.76\n",
      "  3468.85 3448.47 3458.52 3455.42 3459.25]\n",
      " [4335.91 4351.99 4364.61 4370.35 4305.96 4333.53 4313.11 4380.19 4364.22\n",
      "  4326.17 4334.86 4343.79 4310.05 4370.67 4349.76 4364.26 4327.44 4356.63\n",
      "  4307.21 4360.73 4325.47 4362.73 4560.88 4346.86 4429.17 4352.34 4329.06\n",
      "  4348.85 4341.57 4364.92 4352.6  4339.96]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  5  6  7  8  9  5 10  8  0  0  6 10  4  6  6 10  0 10  5  3  9  6 10\n",
      "  1  4  4  2  2  5  5  7 10  2  7  3  9  1  5  1  0  5  8  9  6  4  7  4\n",
      "  1  8  6  3 10  5  8  2  3  7 10  0  7 10  7  3  2  2  8 10  8 10  0  0\n",
      "  6  0  0  1  9  0  9  9  6 10  1 10  4  3  9  7  9  8  6  9  5  2  7  1\n",
      "  5  8  9 10  7  1  5  8  6  4 10  2  0  4  2  3  1  7  9  9  0  1  6  6\n",
      "  5  1  3  3  8 10  9  9  0  0  3  5  4  0  7  1  2  4  2  8  3  8  2  4\n",
      "  9  3  5  4  1  7  4  0  7  6  9  6  4  3  3  8  0  8  8  3  3  4  5  3\n",
      "  2  2  7  1  7  0  1  2  4  3  3  5  2  6  1  7  5  7 10 10  2  1  8  4\n",
      "  0  5  4  7  9  6  4  0  5  6 10  3  2  2  6  6 10  4  1  8  7  8  8  9\n",
      "  1  9  1  2]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Spotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 42\n",
    "scoring = 'f1_macro'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "\n",
    "#models.append(('XGB', XGBClassifier(random_state=seed)))\n",
    "models.append(('GNB', GaussianNB(var_smoothing=2e-9)))\n",
    "models.append(('LR', LogisticRegression(random_state=seed)))\n",
    "models.append(('CART' , DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('SVC' , SVC(gamma=0.0001, random_state=seed)))\n",
    "models.append(('RF', RandomForestClassifier(random_state=seed, n_estimators = 50)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB - 0,53 0,04\n",
      "LR - 0,10 0,03\n",
      "CART - 0,55 0,05\n",
      "SVC - 0,61 0,04\n",
      "RF - 0,64 0,04\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    # Dividere dati in n = num_folds\n",
    "    kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = np.array([])\n",
    "    for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "        X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "        X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "        model.fit(X_cross_train, y_cross_train)  \n",
    "        y_pred = model.predict(X_cross_test)\n",
    "        f1s = f1_score(y_cross_test, y_pred, average=\"weighted\")\n",
    "        cv_results = np.append(cv_results, [f1s])\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #msg = \"%s - %f - %f\" % (name, cv_results.mean(), cv_results.std())\n",
    "    msg = \"{} - {:.2f} {:.2f}\".format(name, cv_results.mean(), cv_results.std()).replace('.', ',')\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "fig.suptitle('Algorithms Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione dei modelli sul Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model GNB: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85        20\n",
      "           1       1.00      0.90      0.95        20\n",
      "           2       0.18      0.10      0.13        20\n",
      "           3       0.88      0.70      0.78        20\n",
      "           4       0.37      0.50      0.43        20\n",
      "           5       0.00      0.00      0.00        20\n",
      "           6       0.79      0.95      0.86        20\n",
      "           7       0.47      0.70      0.56        20\n",
      "           8       0.54      0.70      0.61        20\n",
      "           9       0.40      0.20      0.27        20\n",
      "          10       0.50      0.75      0.60        20\n",
      "\n",
      "    accuracy                           0.59       220\n",
      "   macro avg       0.53      0.59      0.55       220\n",
      "weighted avg       0.53      0.59      0.55       220\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model LR: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.15      0.18        20\n",
      "           1       0.00      0.00      0.00        20\n",
      "           2       0.33      0.10      0.15        20\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.23      0.15      0.18        20\n",
      "           5       0.11      0.05      0.07        20\n",
      "           6       0.10      0.30      0.15        20\n",
      "           7       0.06      0.05      0.05        20\n",
      "           8       0.09      0.10      0.09        20\n",
      "           9       0.13      0.35      0.19        20\n",
      "          10       0.30      0.35      0.33        20\n",
      "\n",
      "    accuracy                           0.15       220\n",
      "   macro avg       0.14      0.15      0.13       220\n",
      "weighted avg       0.14      0.15      0.13       220\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model CART: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.55      0.65        20\n",
      "           1       0.86      0.95      0.90        20\n",
      "           2       0.22      0.20      0.21        20\n",
      "           3       0.68      0.85      0.76        20\n",
      "           4       0.25      0.15      0.19        20\n",
      "           5       0.24      0.25      0.24        20\n",
      "           6       0.80      0.80      0.80        20\n",
      "           7       0.46      0.60      0.52        20\n",
      "           8       0.41      0.35      0.38        20\n",
      "           9       0.43      0.50      0.47        20\n",
      "          10       0.50      0.55      0.52        20\n",
      "\n",
      "    accuracy                           0.52       220\n",
      "   macro avg       0.51      0.52      0.51       220\n",
      "weighted avg       0.51      0.52      0.51       220\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model SVC: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.25      0.36        20\n",
      "           1       1.00      0.85      0.92        20\n",
      "           2       0.70      0.35      0.47        20\n",
      "           3       0.93      0.70      0.80        20\n",
      "           4       0.20      0.90      0.32        20\n",
      "           5       0.86      0.60      0.71        20\n",
      "           6       1.00      0.55      0.71        20\n",
      "           7       0.87      0.65      0.74        20\n",
      "           8       0.44      0.35      0.39        20\n",
      "           9       0.79      0.75      0.77        20\n",
      "          10       0.50      0.10      0.17        20\n",
      "\n",
      "    accuracy                           0.55       220\n",
      "   macro avg       0.72      0.55      0.58       220\n",
      "weighted avg       0.72      0.55      0.58       220\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model RF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91        20\n",
      "           1       1.00      0.95      0.97        20\n",
      "           2       0.50      0.35      0.41        20\n",
      "           3       1.00      0.85      0.92        20\n",
      "           4       0.55      0.30      0.39        20\n",
      "           5       0.43      0.45      0.44        20\n",
      "           6       1.00      0.90      0.95        20\n",
      "           7       0.56      0.70      0.62        20\n",
      "           8       0.52      0.55      0.54        20\n",
      "           9       0.50      0.60      0.55        20\n",
      "          10       0.42      0.55      0.48        20\n",
      "\n",
      "    accuracy                           0.65       220\n",
      "   macro avg       0.66      0.65      0.65       220\n",
      "weighted avg       0.66      0.65      0.65       220\n",
      "\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tasks = ['S', 'S3', 'S6']\n",
    "def classification_report_csv(report, model_name):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    index = 0\n",
    "    row = lines[-4].split('    ')\n",
    "    accuracy = row[-2]\n",
    "    for line in lines[2:-5]:\n",
    "        row = {}\n",
    "        row_data = line.split('      ')\n",
    "        row['class'] = uniques[index]\n",
    "        row['precision'] = float(row_data[2]) \n",
    "        row['recall'] = float(row_data[3]) \n",
    "        row['f1_score'] = float(row_data[4])\n",
    "        row['accuracy'] = accuracy\n",
    "        report_data.append(row)\n",
    "        index += 1\n",
    "    dataframe = pd.DataFrame.from_dict(report_data)\n",
    "    dataframe.to_csv(tasks[choosenIndex]+ '/classificationReports/'+'classification_report' + model_name +  '.csv', index = False)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)\n",
    "for name, model in models:\n",
    "    model.fit(X_train,  y_train)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    print(f\"Model {name}: \")\n",
    "    report = classification_report(y_test, pred_test)\n",
    "    print(report)\n",
    "    classification_report_csv(report, name)\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNetwork(): \n",
    "    model = Sequential(name=\"Sequential-NN\")\n",
    "    model.add(layers.Dense(X.shape[1], activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dense(np.unique(y).size, activation='softmax'))\n",
    "    learn_rate = 0.001\n",
    "    opt = Adam(learning_rate=learn_rate)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "BATCH_SIZE = 8\n",
    "num_folds = 10\n",
    "\n",
    "\n",
    "kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "cv_results = np.array([])\n",
    "for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "    X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "    X_cross_train = scaler.fit_transform(X_cross_train)\n",
    "    X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "    X_cross_test = scaler.transform(X_cross_test)\n",
    "    model = getNetwork()\n",
    "    model.fit(X_cross_train, y_cross_train, epochs=EPOCHS, batch_size=BATCH_SIZE)  \n",
    "    y_pred = model.predict(X_cross_test)\n",
    "    predictions_categorical = np.argmax(y_pred, axis=1)\n",
    "    f1s = f1_score(y_cross_test, predictions_categorical, average=\"weighted\")\n",
    "    cv_results = np.append(cv_results, [f1s])\n",
    "\n",
    "print(f'Average score of Cross Validation: {cv_results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_72 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 512)               16896     \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 11)                5643      \n",
      "=================================================================\n",
      "Total params: 23,595\n",
      "Trainable params: 23,595\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 539.1151 - accuracy: 0.1076 - val_loss: 315.2131 - val_accuracy: 0.0864\n",
      "Epoch 2/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 190.4827 - accuracy: 0.0939 - val_loss: 119.6305 - val_accuracy: 0.1045\n",
      "Epoch 3/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 81.0722 - accuracy: 0.0803 - val_loss: 75.1410 - val_accuracy: 0.0864\n",
      "Epoch 4/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 47.8274 - accuracy: 0.0955 - val_loss: 51.7341 - val_accuracy: 0.0545\n",
      "Epoch 5/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 48.8820 - accuracy: 0.0985 - val_loss: 82.9606 - val_accuracy: 0.0909\n",
      "Epoch 6/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 67.5679 - accuracy: 0.1015 - val_loss: 50.0337 - val_accuracy: 0.0636\n",
      "Epoch 7/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 49.7303 - accuracy: 0.0864 - val_loss: 44.5409 - val_accuracy: 0.0545\n",
      "Epoch 8/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 35.0472 - accuracy: 0.0909 - val_loss: 28.8369 - val_accuracy: 0.1091\n",
      "Epoch 9/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 28.7081 - accuracy: 0.0970 - val_loss: 33.9767 - val_accuracy: 0.0591\n",
      "Epoch 10/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 30.1886 - accuracy: 0.1061 - val_loss: 34.0036 - val_accuracy: 0.0636\n",
      "Epoch 11/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 19.8341 - accuracy: 0.0939 - val_loss: 14.6718 - val_accuracy: 0.0455\n",
      "Epoch 12/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 10.4333 - accuracy: 0.0727 - val_loss: 8.8642 - val_accuracy: 0.0909\n",
      "Epoch 13/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 13.7234 - accuracy: 0.0667 - val_loss: 18.5786 - val_accuracy: 0.0864\n",
      "Epoch 14/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 14.7318 - accuracy: 0.0909 - val_loss: 17.0399 - val_accuracy: 0.1045\n",
      "Epoch 15/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 11.5113 - accuracy: 0.0773 - val_loss: 8.6467 - val_accuracy: 0.0545\n",
      "Epoch 16/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 8.7290 - accuracy: 0.0909 - val_loss: 6.9663 - val_accuracy: 0.0909\n",
      "Epoch 17/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 7.0408 - accuracy: 0.0894 - val_loss: 6.7062 - val_accuracy: 0.0636\n",
      "Epoch 18/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 6.0542 - accuracy: 0.0924 - val_loss: 5.8999 - val_accuracy: 0.1227\n",
      "Epoch 19/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 4.9770 - accuracy: 0.1000 - val_loss: 5.7164 - val_accuracy: 0.0727\n",
      "Epoch 20/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 5.2315 - accuracy: 0.0985 - val_loss: 6.5118 - val_accuracy: 0.0500\n",
      "Epoch 21/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 5.4528 - accuracy: 0.1167 - val_loss: 5.4730 - val_accuracy: 0.1000\n",
      "Epoch 22/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 4.9946 - accuracy: 0.1348 - val_loss: 5.3329 - val_accuracy: 0.0955\n",
      "Epoch 23/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 4.3695 - accuracy: 0.1455 - val_loss: 5.2534 - val_accuracy: 0.0727\n",
      "Epoch 24/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 4.1610 - accuracy: 0.1076 - val_loss: 5.1748 - val_accuracy: 0.0864\n",
      "Epoch 25/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 4.1852 - accuracy: 0.1197 - val_loss: 4.9590 - val_accuracy: 0.0773\n",
      "Epoch 26/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 3.9052 - accuracy: 0.1030 - val_loss: 4.6448 - val_accuracy: 0.0636\n",
      "Epoch 27/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 3.5776 - accuracy: 0.1091 - val_loss: 4.6378 - val_accuracy: 0.1182\n",
      "Epoch 28/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 3.9431 - accuracy: 0.1364 - val_loss: 4.3245 - val_accuracy: 0.0591\n",
      "Epoch 29/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 3.5249 - accuracy: 0.1439 - val_loss: 3.6064 - val_accuracy: 0.1227\n",
      "Epoch 30/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 3.4134 - accuracy: 0.1091 - val_loss: 4.2172 - val_accuracy: 0.0636\n",
      "Epoch 31/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.9595 - accuracy: 0.1318 - val_loss: 3.8449 - val_accuracy: 0.1045\n",
      "Epoch 32/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 3.0271 - accuracy: 0.1606 - val_loss: 3.4317 - val_accuracy: 0.1136\n",
      "Epoch 33/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.7487 - accuracy: 0.1621 - val_loss: 3.4436 - val_accuracy: 0.0682\n",
      "Epoch 34/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.8042 - accuracy: 0.1591 - val_loss: 3.3864 - val_accuracy: 0.1091\n",
      "Epoch 35/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.6969 - accuracy: 0.1591 - val_loss: 3.1647 - val_accuracy: 0.1000\n",
      "Epoch 36/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4874 - accuracy: 0.1742 - val_loss: 3.0246 - val_accuracy: 0.0864\n",
      "Epoch 37/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4585 - accuracy: 0.1924 - val_loss: 3.2340 - val_accuracy: 0.1500\n",
      "Epoch 38/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.5458 - accuracy: 0.1879 - val_loss: 3.2868 - val_accuracy: 0.1136\n",
      "Epoch 39/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.6465 - accuracy: 0.1470 - val_loss: 3.1762 - val_accuracy: 0.0955\n",
      "Epoch 40/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3974 - accuracy: 0.1697 - val_loss: 3.2577 - val_accuracy: 0.0909\n",
      "Epoch 41/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4428 - accuracy: 0.1894 - val_loss: 2.9685 - val_accuracy: 0.1136\n",
      "Epoch 42/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3811 - accuracy: 0.1924 - val_loss: 3.1678 - val_accuracy: 0.0864\n",
      "Epoch 43/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3691 - accuracy: 0.1833 - val_loss: 3.1697 - val_accuracy: 0.1000\n",
      "Epoch 44/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3853 - accuracy: 0.1924 - val_loss: 3.1891 - val_accuracy: 0.1045\n",
      "Epoch 45/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2848 - accuracy: 0.1970 - val_loss: 3.3675 - val_accuracy: 0.0682\n",
      "Epoch 46/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2790 - accuracy: 0.2288 - val_loss: 3.0596 - val_accuracy: 0.1545\n",
      "Epoch 47/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2224 - accuracy: 0.2030 - val_loss: 3.3176 - val_accuracy: 0.1091\n",
      "Epoch 48/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2402 - accuracy: 0.2273 - val_loss: 3.2117 - val_accuracy: 0.1182\n",
      "Epoch 49/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2306 - accuracy: 0.2197 - val_loss: 3.2288 - val_accuracy: 0.1364\n",
      "Epoch 50/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1597 - accuracy: 0.2318 - val_loss: 3.1919 - val_accuracy: 0.1545\n",
      "Epoch 51/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1556 - accuracy: 0.2167 - val_loss: 3.1339 - val_accuracy: 0.1636\n",
      "Epoch 52/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2835 - accuracy: 0.2258 - val_loss: 3.1985 - val_accuracy: 0.1091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2265 - accuracy: 0.2303 - val_loss: 3.1715 - val_accuracy: 0.1636\n",
      "Epoch 54/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1267 - accuracy: 0.2485 - val_loss: 3.0652 - val_accuracy: 0.2227\n",
      "Epoch 55/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0546 - accuracy: 0.2485 - val_loss: 2.9698 - val_accuracy: 0.2227\n",
      "Epoch 56/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0197 - accuracy: 0.2697 - val_loss: 3.0451 - val_accuracy: 0.1455\n",
      "Epoch 57/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0133 - accuracy: 0.2848 - val_loss: 3.0828 - val_accuracy: 0.1545\n",
      "Epoch 58/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0477 - accuracy: 0.2545 - val_loss: 3.0075 - val_accuracy: 0.1818\n",
      "Epoch 59/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0699 - accuracy: 0.2500 - val_loss: 3.1090 - val_accuracy: 0.1227\n",
      "Epoch 60/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0867 - accuracy: 0.2530 - val_loss: 3.1156 - val_accuracy: 0.1864\n",
      "Epoch 61/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2184 - accuracy: 0.2348 - val_loss: 3.0832 - val_accuracy: 0.1773\n",
      "Epoch 62/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1026 - accuracy: 0.2303 - val_loss: 2.9237 - val_accuracy: 0.1636\n",
      "Epoch 63/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1064 - accuracy: 0.2394 - val_loss: 3.1631 - val_accuracy: 0.1500\n",
      "Epoch 64/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0363 - accuracy: 0.2652 - val_loss: 3.0015 - val_accuracy: 0.1409\n",
      "Epoch 65/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2486 - accuracy: 0.2106 - val_loss: 3.1021 - val_accuracy: 0.1318\n",
      "Epoch 66/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1900 - accuracy: 0.2242 - val_loss: 3.1494 - val_accuracy: 0.1909\n",
      "Epoch 67/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0318 - accuracy: 0.2379 - val_loss: 3.0505 - val_accuracy: 0.1500\n",
      "Epoch 68/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9546 - accuracy: 0.2652 - val_loss: 2.9366 - val_accuracy: 0.1955\n",
      "Epoch 69/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9383 - accuracy: 0.2667 - val_loss: 2.9579 - val_accuracy: 0.2182\n",
      "Epoch 70/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9390 - accuracy: 0.2833 - val_loss: 3.0537 - val_accuracy: 0.1591\n",
      "Epoch 71/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0048 - accuracy: 0.2833 - val_loss: 2.8076 - val_accuracy: 0.2409\n",
      "Epoch 72/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9446 - accuracy: 0.2970 - val_loss: 3.2401 - val_accuracy: 0.1455\n",
      "Epoch 73/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9964 - accuracy: 0.2530 - val_loss: 3.0589 - val_accuracy: 0.0955\n",
      "Epoch 74/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0952 - accuracy: 0.2333 - val_loss: 3.2700 - val_accuracy: 0.1000\n",
      "Epoch 75/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1132 - accuracy: 0.2515 - val_loss: 2.9364 - val_accuracy: 0.1955\n",
      "Epoch 76/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9890 - accuracy: 0.2909 - val_loss: 3.0672 - val_accuracy: 0.1773\n",
      "Epoch 77/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9652 - accuracy: 0.2833 - val_loss: 2.9622 - val_accuracy: 0.1545\n",
      "Epoch 78/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0518 - accuracy: 0.2667 - val_loss: 3.0341 - val_accuracy: 0.1455\n",
      "Epoch 79/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8795 - accuracy: 0.3379 - val_loss: 2.9934 - val_accuracy: 0.1864\n",
      "Epoch 80/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9832 - accuracy: 0.2894 - val_loss: 3.2452 - val_accuracy: 0.1500\n",
      "Epoch 81/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1567 - accuracy: 0.2455 - val_loss: 3.2187 - val_accuracy: 0.1091\n",
      "Epoch 82/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0212 - accuracy: 0.2545 - val_loss: 3.3285 - val_accuracy: 0.1318\n",
      "Epoch 83/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0950 - accuracy: 0.2500 - val_loss: 3.0526 - val_accuracy: 0.1500\n",
      "Epoch 84/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0730 - accuracy: 0.2606 - val_loss: 3.0244 - val_accuracy: 0.2182\n",
      "Epoch 85/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0154 - accuracy: 0.2606 - val_loss: 2.9217 - val_accuracy: 0.2000\n",
      "Epoch 86/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9187 - accuracy: 0.3136 - val_loss: 3.0103 - val_accuracy: 0.1909\n",
      "Epoch 87/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1777 - accuracy: 0.2076 - val_loss: 3.2262 - val_accuracy: 0.1273\n",
      "Epoch 88/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3052 - accuracy: 0.2015 - val_loss: 3.1048 - val_accuracy: 0.2000\n",
      "Epoch 89/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0584 - accuracy: 0.2848 - val_loss: 2.9182 - val_accuracy: 0.2136\n",
      "Epoch 90/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1794 - accuracy: 0.2333 - val_loss: 3.2248 - val_accuracy: 0.2409\n",
      "Epoch 91/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2985 - accuracy: 0.2121 - val_loss: 3.3944 - val_accuracy: 0.0864\n",
      "Epoch 92/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2348 - accuracy: 0.2182 - val_loss: 3.2164 - val_accuracy: 0.1955\n",
      "Epoch 93/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1253 - accuracy: 0.2242 - val_loss: 2.9767 - val_accuracy: 0.2091\n",
      "Epoch 94/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0432 - accuracy: 0.2712 - val_loss: 2.7973 - val_accuracy: 0.2455\n",
      "Epoch 95/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9942 - accuracy: 0.2894 - val_loss: 2.9184 - val_accuracy: 0.1409\n",
      "Epoch 96/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9631 - accuracy: 0.2727 - val_loss: 3.1043 - val_accuracy: 0.1500\n",
      "Epoch 97/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1294 - accuracy: 0.2303 - val_loss: 3.2383 - val_accuracy: 0.1682\n",
      "Epoch 98/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0994 - accuracy: 0.2258 - val_loss: 3.1303 - val_accuracy: 0.1864\n",
      "Epoch 99/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0464 - accuracy: 0.2409 - val_loss: 2.9255 - val_accuracy: 0.2364\n",
      "Epoch 100/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0600 - accuracy: 0.2742 - val_loss: 2.9234 - val_accuracy: 0.2500\n",
      "Epoch 101/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8914 - accuracy: 0.3182 - val_loss: 2.9175 - val_accuracy: 0.1591\n",
      "Epoch 102/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0004 - accuracy: 0.2727 - val_loss: 3.0870 - val_accuracy: 0.1773\n",
      "Epoch 103/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0013 - accuracy: 0.2667 - val_loss: 3.0589 - val_accuracy: 0.1909\n",
      "Epoch 104/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1018 - accuracy: 0.2545 - val_loss: 3.1613 - val_accuracy: 0.2682\n",
      "Epoch 105/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0351 - accuracy: 0.3015 - val_loss: 3.1865 - val_accuracy: 0.0864\n",
      "Epoch 106/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2015 - accuracy: 0.2333 - val_loss: 3.2570 - val_accuracy: 0.1045\n",
      "Epoch 107/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0668 - accuracy: 0.2500 - val_loss: 3.0287 - val_accuracy: 0.2273\n",
      "Epoch 108/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9648 - accuracy: 0.2727 - val_loss: 3.2316 - val_accuracy: 0.1455\n",
      "Epoch 109/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1168 - accuracy: 0.2515 - val_loss: 3.0942 - val_accuracy: 0.1500\n",
      "Epoch 110/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0479 - accuracy: 0.2561 - val_loss: 3.1474 - val_accuracy: 0.1818\n",
      "Epoch 111/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2608 - accuracy: 0.2152 - val_loss: 3.1524 - val_accuracy: 0.2091\n",
      "Epoch 112/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4992 - accuracy: 0.1606 - val_loss: 4.1400 - val_accuracy: 0.0955\n",
      "Epoch 113/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4013 - accuracy: 0.2197 - val_loss: 3.5019 - val_accuracy: 0.1545\n",
      "Epoch 114/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0977 - accuracy: 0.2621 - val_loss: 3.0958 - val_accuracy: 0.1591\n",
      "Epoch 115/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9751 - accuracy: 0.3045 - val_loss: 2.7436 - val_accuracy: 0.2682\n",
      "Epoch 116/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1299 - accuracy: 0.2621 - val_loss: 2.9774 - val_accuracy: 0.1545\n",
      "Epoch 117/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1557 - accuracy: 0.2652 - val_loss: 3.1065 - val_accuracy: 0.1545\n",
      "Epoch 118/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0449 - accuracy: 0.2788 - val_loss: 3.4016 - val_accuracy: 0.1591\n",
      "Epoch 119/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1906 - accuracy: 0.2500 - val_loss: 2.9552 - val_accuracy: 0.2182\n",
      "Epoch 120/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3048 - accuracy: 0.1970 - val_loss: 3.0048 - val_accuracy: 0.2136\n",
      "Epoch 121/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0972 - accuracy: 0.2591 - val_loss: 2.9895 - val_accuracy: 0.2409\n",
      "Epoch 122/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9050 - accuracy: 0.2909 - val_loss: 2.7857 - val_accuracy: 0.2000\n",
      "Epoch 123/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8931 - accuracy: 0.3136 - val_loss: 2.8791 - val_accuracy: 0.1955\n",
      "Epoch 124/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1268 - accuracy: 0.2606 - val_loss: 3.0352 - val_accuracy: 0.1727\n",
      "Epoch 125/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1496 - accuracy: 0.2242 - val_loss: 3.0131 - val_accuracy: 0.1136\n",
      "Epoch 126/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1166 - accuracy: 0.2394 - val_loss: 3.0156 - val_accuracy: 0.1682\n",
      "Epoch 127/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2136 - accuracy: 0.2212 - val_loss: 2.8720 - val_accuracy: 0.2000\n",
      "Epoch 128/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1810 - accuracy: 0.2242 - val_loss: 2.8022 - val_accuracy: 0.2182\n",
      "Epoch 129/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1762 - accuracy: 0.2515 - val_loss: 2.7292 - val_accuracy: 0.2273\n",
      "Epoch 130/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1226 - accuracy: 0.2424 - val_loss: 2.8017 - val_accuracy: 0.2500\n",
      "Epoch 131/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0538 - accuracy: 0.2606 - val_loss: 2.7109 - val_accuracy: 0.2136\n",
      "Epoch 132/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1372 - accuracy: 0.2773 - val_loss: 3.0131 - val_accuracy: 0.1000\n",
      "Epoch 133/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2761 - accuracy: 0.1955 - val_loss: 2.7387 - val_accuracy: 0.2091\n",
      "Epoch 134/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1149 - accuracy: 0.2439 - val_loss: 3.0173 - val_accuracy: 0.1045\n",
      "Epoch 135/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0940 - accuracy: 0.2348 - val_loss: 2.7419 - val_accuracy: 0.2091\n",
      "Epoch 136/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0255 - accuracy: 0.2727 - val_loss: 2.7730 - val_accuracy: 0.1909\n",
      "Epoch 137/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0961 - accuracy: 0.2242 - val_loss: 3.0037 - val_accuracy: 0.1045\n",
      "Epoch 138/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3361 - accuracy: 0.2227 - val_loss: 3.2340 - val_accuracy: 0.0636\n",
      "Epoch 139/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4855 - accuracy: 0.1727 - val_loss: 3.0638 - val_accuracy: 0.2045\n",
      "Epoch 140/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2632 - accuracy: 0.2136 - val_loss: 2.8060 - val_accuracy: 0.2273\n",
      "Epoch 141/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1337 - accuracy: 0.2303 - val_loss: 3.6116 - val_accuracy: 0.1409\n",
      "Epoch 142/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3164 - accuracy: 0.1758 - val_loss: 2.8806 - val_accuracy: 0.1045\n",
      "Epoch 143/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1398 - accuracy: 0.2333 - val_loss: 2.9180 - val_accuracy: 0.1682\n",
      "Epoch 144/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0365 - accuracy: 0.2515 - val_loss: 2.7062 - val_accuracy: 0.2591\n",
      "Epoch 145/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9897 - accuracy: 0.2667 - val_loss: 2.6747 - val_accuracy: 0.2182\n",
      "Epoch 146/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9829 - accuracy: 0.2909 - val_loss: 2.6483 - val_accuracy: 0.2091\n",
      "Epoch 147/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0530 - accuracy: 0.2591 - val_loss: 2.8927 - val_accuracy: 0.1909\n",
      "Epoch 148/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2678 - accuracy: 0.2242 - val_loss: 2.8733 - val_accuracy: 0.1591\n",
      "Epoch 149/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1455 - accuracy: 0.2561 - val_loss: 2.8365 - val_accuracy: 0.1545\n",
      "Epoch 150/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1869 - accuracy: 0.2409 - val_loss: 3.1019 - val_accuracy: 0.0591\n",
      "Epoch 151/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1863 - accuracy: 0.2515 - val_loss: 2.8440 - val_accuracy: 0.2364\n",
      "Epoch 152/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0596 - accuracy: 0.2636 - val_loss: 2.8712 - val_accuracy: 0.2636\n",
      "Epoch 153/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0342 - accuracy: 0.2833 - val_loss: 3.0401 - val_accuracy: 0.1591\n",
      "Epoch 154/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0692 - accuracy: 0.2439 - val_loss: 2.8657 - val_accuracy: 0.1727\n",
      "Epoch 155/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1934 - accuracy: 0.2288 - val_loss: 2.7094 - val_accuracy: 0.2500\n",
      "Epoch 156/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0747 - accuracy: 0.2500 - val_loss: 2.7434 - val_accuracy: 0.1864\n",
      "Epoch 157/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0653 - accuracy: 0.2515 - val_loss: 2.9222 - val_accuracy: 0.2045\n",
      "Epoch 158/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2078 - accuracy: 0.2348 - val_loss: 2.7448 - val_accuracy: 0.1955\n",
      "Epoch 159/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3647 - accuracy: 0.2227 - val_loss: 2.7231 - val_accuracy: 0.2318\n",
      "Epoch 160/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1350 - accuracy: 0.2394 - val_loss: 2.8373 - val_accuracy: 0.2000\n",
      "Epoch 161/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2570 - accuracy: 0.2167 - val_loss: 2.7846 - val_accuracy: 0.1500\n",
      "Epoch 162/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1149 - accuracy: 0.2439 - val_loss: 2.7354 - val_accuracy: 0.2136\n",
      "Epoch 163/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0620 - accuracy: 0.2712 - val_loss: 2.6265 - val_accuracy: 0.2318\n",
      "Epoch 164/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0591 - accuracy: 0.2455 - val_loss: 2.6695 - val_accuracy: 0.2273\n",
      "Epoch 165/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9785 - accuracy: 0.2909 - val_loss: 3.0928 - val_accuracy: 0.1227\n",
      "Epoch 166/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2331 - accuracy: 0.2303 - val_loss: 3.0015 - val_accuracy: 0.0955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2395 - accuracy: 0.2303 - val_loss: 2.4432 - val_accuracy: 0.2500\n",
      "Epoch 168/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1661 - accuracy: 0.2333 - val_loss: 2.5099 - val_accuracy: 0.1500\n",
      "Epoch 169/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2308 - accuracy: 0.2061 - val_loss: 2.4412 - val_accuracy: 0.2136\n",
      "Epoch 170/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1286 - accuracy: 0.2197 - val_loss: 2.4788 - val_accuracy: 0.1636\n",
      "Epoch 171/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1216 - accuracy: 0.1848 - val_loss: 2.6034 - val_accuracy: 0.1500\n",
      "Epoch 172/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3803 - accuracy: 0.1894 - val_loss: 2.5625 - val_accuracy: 0.1818\n",
      "Epoch 173/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1649 - accuracy: 0.2303 - val_loss: 2.5390 - val_accuracy: 0.1864\n",
      "Epoch 174/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1224 - accuracy: 0.2364 - val_loss: 2.6400 - val_accuracy: 0.1545\n",
      "Epoch 175/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0747 - accuracy: 0.2212 - val_loss: 3.0011 - val_accuracy: 0.1682\n",
      "Epoch 176/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2271 - accuracy: 0.2045 - val_loss: 3.2332 - val_accuracy: 0.0727\n",
      "Epoch 177/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 3.6394 - accuracy: 0.1273 - val_loss: 3.5229 - val_accuracy: 0.1727\n",
      "Epoch 178/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4560 - accuracy: 0.1591 - val_loss: 2.6659 - val_accuracy: 0.1455\n",
      "Epoch 179/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.5198 - accuracy: 0.1636 - val_loss: 3.0912 - val_accuracy: 0.0727\n",
      "Epoch 180/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.6589 - accuracy: 0.1258 - val_loss: 2.6963 - val_accuracy: 0.1364\n",
      "Epoch 181/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2850 - accuracy: 0.2061 - val_loss: 2.3793 - val_accuracy: 0.1636\n",
      "Epoch 182/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3587 - accuracy: 0.1621 - val_loss: 2.3449 - val_accuracy: 0.1045\n",
      "Epoch 183/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1747 - accuracy: 0.1970 - val_loss: 2.2346 - val_accuracy: 0.2182\n",
      "Epoch 184/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2142 - accuracy: 0.1924 - val_loss: 2.6927 - val_accuracy: 0.0727\n",
      "Epoch 185/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2270 - accuracy: 0.1848 - val_loss: 2.3503 - val_accuracy: 0.2000\n",
      "Epoch 186/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1730 - accuracy: 0.2197 - val_loss: 2.2418 - val_accuracy: 0.1864\n",
      "Epoch 187/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1530 - accuracy: 0.2091 - val_loss: 2.2356 - val_accuracy: 0.2000\n",
      "Epoch 188/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1179 - accuracy: 0.2333 - val_loss: 2.2853 - val_accuracy: 0.1818\n",
      "Epoch 189/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2460 - accuracy: 0.2000 - val_loss: 2.7426 - val_accuracy: 0.0864\n",
      "Epoch 190/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3507 - accuracy: 0.1788 - val_loss: 2.5593 - val_accuracy: 0.1864\n",
      "Epoch 191/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3122 - accuracy: 0.1788 - val_loss: 2.3642 - val_accuracy: 0.1818\n",
      "Epoch 192/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2724 - accuracy: 0.1939 - val_loss: 2.2893 - val_accuracy: 0.2091\n",
      "Epoch 193/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2717 - accuracy: 0.1576 - val_loss: 2.5002 - val_accuracy: 0.1955\n",
      "Epoch 194/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2940 - accuracy: 0.1712 - val_loss: 2.2472 - val_accuracy: 0.2500\n",
      "Epoch 195/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2681 - accuracy: 0.1864 - val_loss: 2.4009 - val_accuracy: 0.1273\n",
      "Epoch 196/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2541 - accuracy: 0.1864 - val_loss: 2.4008 - val_accuracy: 0.1091\n",
      "Epoch 197/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2660 - accuracy: 0.1818 - val_loss: 2.3330 - val_accuracy: 0.1591\n",
      "Epoch 198/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2188 - accuracy: 0.1924 - val_loss: 2.3354 - val_accuracy: 0.1318\n",
      "Epoch 199/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2328 - accuracy: 0.2061 - val_loss: 2.2542 - val_accuracy: 0.2091\n",
      "Epoch 200/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2568 - accuracy: 0.1864 - val_loss: 2.3517 - val_accuracy: 0.1136\n",
      "Epoch 201/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2469 - accuracy: 0.1803 - val_loss: 2.4244 - val_accuracy: 0.1227\n",
      "Epoch 202/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1940 - accuracy: 0.2348 - val_loss: 2.2633 - val_accuracy: 0.2091\n",
      "Epoch 203/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2203 - accuracy: 0.2061 - val_loss: 2.2723 - val_accuracy: 0.1773\n",
      "Epoch 204/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4605 - accuracy: 0.1439 - val_loss: 2.2677 - val_accuracy: 0.1909\n",
      "Epoch 205/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3591 - accuracy: 0.1788 - val_loss: 2.2409 - val_accuracy: 0.2455\n",
      "Epoch 206/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2290 - accuracy: 0.2212 - val_loss: 2.2998 - val_accuracy: 0.2091\n",
      "Epoch 207/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1895 - accuracy: 0.2061 - val_loss: 2.2606 - val_accuracy: 0.1955\n",
      "Epoch 208/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2625 - accuracy: 0.1758 - val_loss: 2.2762 - val_accuracy: 0.1591\n",
      "Epoch 209/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2525 - accuracy: 0.1773 - val_loss: 2.2833 - val_accuracy: 0.1545\n",
      "Epoch 210/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2477 - accuracy: 0.1909 - val_loss: 2.5573 - val_accuracy: 0.1182\n",
      "Epoch 211/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2397 - accuracy: 0.1955 - val_loss: 2.2363 - val_accuracy: 0.2318\n",
      "Epoch 212/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2022 - accuracy: 0.2136 - val_loss: 2.2689 - val_accuracy: 0.2091\n",
      "Epoch 213/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1925 - accuracy: 0.1939 - val_loss: 2.2663 - val_accuracy: 0.1455\n",
      "Epoch 214/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1948 - accuracy: 0.2106 - val_loss: 2.3712 - val_accuracy: 0.0864\n",
      "Epoch 215/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2118 - accuracy: 0.1879 - val_loss: 2.3119 - val_accuracy: 0.1182\n",
      "Epoch 216/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1874 - accuracy: 0.1652 - val_loss: 2.4629 - val_accuracy: 0.0818\n",
      "Epoch 217/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2676 - accuracy: 0.1758 - val_loss: 2.2554 - val_accuracy: 0.1864\n",
      "Epoch 218/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1891 - accuracy: 0.2091 - val_loss: 2.3077 - val_accuracy: 0.1409\n",
      "Epoch 219/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3694 - accuracy: 0.1424 - val_loss: 2.3466 - val_accuracy: 0.1864\n",
      "Epoch 220/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2727 - accuracy: 0.1697 - val_loss: 2.3356 - val_accuracy: 0.2136\n",
      "Epoch 221/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1883 - accuracy: 0.1924 - val_loss: 2.2320 - val_accuracy: 0.2091\n",
      "Epoch 222/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2605 - accuracy: 0.1667 - val_loss: 2.4695 - val_accuracy: 0.1045\n",
      "Epoch 223/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2645 - accuracy: 0.1955 - val_loss: 2.5121 - val_accuracy: 0.0909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3488 - accuracy: 0.1530 - val_loss: 2.3496 - val_accuracy: 0.2364\n",
      "Epoch 225/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2668 - accuracy: 0.1545 - val_loss: 2.2813 - val_accuracy: 0.2045\n",
      "Epoch 226/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1496 - accuracy: 0.1894 - val_loss: 2.3582 - val_accuracy: 0.1091\n",
      "Epoch 227/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2290 - accuracy: 0.1833 - val_loss: 2.3180 - val_accuracy: 0.2045\n",
      "Epoch 228/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2202 - accuracy: 0.1955 - val_loss: 2.5784 - val_accuracy: 0.0955\n",
      "Epoch 229/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2929 - accuracy: 0.1894 - val_loss: 2.3116 - val_accuracy: 0.1727\n",
      "Epoch 230/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2171 - accuracy: 0.1894 - val_loss: 2.4001 - val_accuracy: 0.1136\n",
      "Epoch 231/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1843 - accuracy: 0.1848 - val_loss: 2.3220 - val_accuracy: 0.1818\n",
      "Epoch 232/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3079 - accuracy: 0.1864 - val_loss: 2.5038 - val_accuracy: 0.1227\n",
      "Epoch 233/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2528 - accuracy: 0.1818 - val_loss: 2.2745 - val_accuracy: 0.1727\n",
      "Epoch 234/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1740 - accuracy: 0.2197 - val_loss: 2.2163 - val_accuracy: 0.1545\n",
      "Epoch 235/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1566 - accuracy: 0.2106 - val_loss: 2.3025 - val_accuracy: 0.1773\n",
      "Epoch 236/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1238 - accuracy: 0.2333 - val_loss: 2.2811 - val_accuracy: 0.1864\n",
      "Epoch 237/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3167 - accuracy: 0.1591 - val_loss: 2.3787 - val_accuracy: 0.1909\n",
      "Epoch 238/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2827 - accuracy: 0.1803 - val_loss: 2.2360 - val_accuracy: 0.2182\n",
      "Epoch 239/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2503 - accuracy: 0.1788 - val_loss: 2.4143 - val_accuracy: 0.2364\n",
      "Epoch 240/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2289 - accuracy: 0.2106 - val_loss: 2.2671 - val_accuracy: 0.2091\n",
      "Epoch 241/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2890 - accuracy: 0.1773 - val_loss: 2.1952 - val_accuracy: 0.1818\n",
      "Epoch 242/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1545 - accuracy: 0.2182 - val_loss: 2.2619 - val_accuracy: 0.2000\n",
      "Epoch 243/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2970 - accuracy: 0.1545 - val_loss: 2.2989 - val_accuracy: 0.1455\n",
      "Epoch 244/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2477 - accuracy: 0.1742 - val_loss: 2.4818 - val_accuracy: 0.1455\n",
      "Epoch 245/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2872 - accuracy: 0.1939 - val_loss: 2.4587 - val_accuracy: 0.1500\n",
      "Epoch 246/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2438 - accuracy: 0.2030 - val_loss: 2.2917 - val_accuracy: 0.1227\n",
      "Epoch 247/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1833 - accuracy: 0.1909 - val_loss: 2.2306 - val_accuracy: 0.1318\n",
      "Epoch 248/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1991 - accuracy: 0.2076 - val_loss: 2.3175 - val_accuracy: 0.1364\n",
      "Epoch 249/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1545 - accuracy: 0.1939 - val_loss: 2.5055 - val_accuracy: 0.1545\n",
      "Epoch 250/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3175 - accuracy: 0.1742 - val_loss: 2.3637 - val_accuracy: 0.1500\n",
      "Epoch 251/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3571 - accuracy: 0.1758 - val_loss: 2.2753 - val_accuracy: 0.1727\n",
      "Epoch 252/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2039 - accuracy: 0.1894 - val_loss: 2.2669 - val_accuracy: 0.1273\n",
      "Epoch 253/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1527 - accuracy: 0.2121 - val_loss: 2.1973 - val_accuracy: 0.1409\n",
      "Epoch 254/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2983 - accuracy: 0.1803 - val_loss: 2.3046 - val_accuracy: 0.1591\n",
      "Epoch 255/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2226 - accuracy: 0.2303 - val_loss: 2.3342 - val_accuracy: 0.1636\n",
      "Epoch 256/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1554 - accuracy: 0.2061 - val_loss: 2.4236 - val_accuracy: 0.1000\n",
      "Epoch 257/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1973 - accuracy: 0.1818 - val_loss: 2.2871 - val_accuracy: 0.1227\n",
      "Epoch 258/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1563 - accuracy: 0.2182 - val_loss: 2.1929 - val_accuracy: 0.2409\n",
      "Epoch 259/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1249 - accuracy: 0.2242 - val_loss: 2.1837 - val_accuracy: 0.2318\n",
      "Epoch 260/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4698 - accuracy: 0.1667 - val_loss: 2.8168 - val_accuracy: 0.1682\n",
      "Epoch 261/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4291 - accuracy: 0.2121 - val_loss: 2.4852 - val_accuracy: 0.1500\n",
      "Epoch 262/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2542 - accuracy: 0.1939 - val_loss: 2.2288 - val_accuracy: 0.1864\n",
      "Epoch 263/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2154 - accuracy: 0.2061 - val_loss: 2.2326 - val_accuracy: 0.1727\n",
      "Epoch 264/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2521 - accuracy: 0.1818 - val_loss: 2.6724 - val_accuracy: 0.0773\n",
      "Epoch 265/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4048 - accuracy: 0.1818 - val_loss: 2.3859 - val_accuracy: 0.1727\n",
      "Epoch 266/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4369 - accuracy: 0.1424 - val_loss: 2.5227 - val_accuracy: 0.0818\n",
      "Epoch 267/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2941 - accuracy: 0.1621 - val_loss: 2.3069 - val_accuracy: 0.1455\n",
      "Epoch 268/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1934 - accuracy: 0.1773 - val_loss: 2.3329 - val_accuracy: 0.1682\n",
      "Epoch 269/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4408 - accuracy: 0.1697 - val_loss: 2.3938 - val_accuracy: 0.1227\n",
      "Epoch 270/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2672 - accuracy: 0.1773 - val_loss: 2.2935 - val_accuracy: 0.1955\n",
      "Epoch 271/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3145 - accuracy: 0.1803 - val_loss: 2.3388 - val_accuracy: 0.1455\n",
      "Epoch 272/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1946 - accuracy: 0.1924 - val_loss: 2.2168 - val_accuracy: 0.1955\n",
      "Epoch 273/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2324 - accuracy: 0.2030 - val_loss: 2.3300 - val_accuracy: 0.1227\n",
      "Epoch 274/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2506 - accuracy: 0.1803 - val_loss: 2.2923 - val_accuracy: 0.1864\n",
      "Epoch 275/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2561 - accuracy: 0.1727 - val_loss: 2.2499 - val_accuracy: 0.1955\n",
      "Epoch 276/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4437 - accuracy: 0.1364 - val_loss: 2.4664 - val_accuracy: 0.2045\n",
      "Epoch 277/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2750 - accuracy: 0.2091 - val_loss: 2.2461 - val_accuracy: 0.1773\n",
      "Epoch 278/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2667 - accuracy: 0.1939 - val_loss: 2.4721 - val_accuracy: 0.1182\n",
      "Epoch 279/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2320 - accuracy: 0.1667 - val_loss: 2.2554 - val_accuracy: 0.2364\n",
      "Epoch 280/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1514 - accuracy: 0.2061 - val_loss: 2.2086 - val_accuracy: 0.2273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1295 - accuracy: 0.2333 - val_loss: 2.2628 - val_accuracy: 0.1545\n",
      "Epoch 282/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1069 - accuracy: 0.2409 - val_loss: 2.2600 - val_accuracy: 0.2045\n",
      "Epoch 283/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3376 - accuracy: 0.1955 - val_loss: 2.5351 - val_accuracy: 0.2273\n",
      "Epoch 284/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2432 - accuracy: 0.2076 - val_loss: 2.2951 - val_accuracy: 0.1455\n",
      "Epoch 285/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2241 - accuracy: 0.1985 - val_loss: 2.5897 - val_accuracy: 0.1182\n",
      "Epoch 286/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.8141 - accuracy: 0.1303 - val_loss: 2.3168 - val_accuracy: 0.2091\n",
      "Epoch 287/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2959 - accuracy: 0.1758 - val_loss: 2.3345 - val_accuracy: 0.2318\n",
      "Epoch 288/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3662 - accuracy: 0.1606 - val_loss: 2.3633 - val_accuracy: 0.1455\n",
      "Epoch 289/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1808 - accuracy: 0.1985 - val_loss: 2.3212 - val_accuracy: 0.1500\n",
      "Epoch 290/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1388 - accuracy: 0.2212 - val_loss: 2.2830 - val_accuracy: 0.1636\n",
      "Epoch 291/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1716 - accuracy: 0.2273 - val_loss: 2.1300 - val_accuracy: 0.2455\n",
      "Epoch 292/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0721 - accuracy: 0.2379 - val_loss: 2.4297 - val_accuracy: 0.1273\n",
      "Epoch 293/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2803 - accuracy: 0.2061 - val_loss: 2.8914 - val_accuracy: 0.0955\n",
      "Epoch 294/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3578 - accuracy: 0.1606 - val_loss: 2.2749 - val_accuracy: 0.1864\n",
      "Epoch 295/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2315 - accuracy: 0.1758 - val_loss: 2.2569 - val_accuracy: 0.2273\n",
      "Epoch 296/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1883 - accuracy: 0.1955 - val_loss: 2.4689 - val_accuracy: 0.1455\n",
      "Epoch 297/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2354 - accuracy: 0.1970 - val_loss: 2.2355 - val_accuracy: 0.2000\n",
      "Epoch 298/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2672 - accuracy: 0.1970 - val_loss: 2.3526 - val_accuracy: 0.1773\n",
      "Epoch 299/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.5172 - accuracy: 0.1288 - val_loss: 2.7552 - val_accuracy: 0.1455\n",
      "Epoch 300/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4685 - accuracy: 0.1894 - val_loss: 2.1501 - val_accuracy: 0.2136\n",
      "Epoch 301/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2114 - accuracy: 0.2061 - val_loss: 2.3142 - val_accuracy: 0.1864\n",
      "Epoch 302/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2790 - accuracy: 0.1682 - val_loss: 2.3587 - val_accuracy: 0.1364\n",
      "Epoch 303/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2017 - accuracy: 0.2015 - val_loss: 2.2504 - val_accuracy: 0.2000\n",
      "Epoch 304/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2295 - accuracy: 0.1758 - val_loss: 2.2982 - val_accuracy: 0.2091\n",
      "Epoch 305/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1321 - accuracy: 0.2167 - val_loss: 2.2142 - val_accuracy: 0.1909\n",
      "Epoch 306/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1692 - accuracy: 0.2076 - val_loss: 2.2041 - val_accuracy: 0.2500\n",
      "Epoch 307/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1854 - accuracy: 0.2061 - val_loss: 2.2722 - val_accuracy: 0.2182\n",
      "Epoch 308/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1712 - accuracy: 0.2121 - val_loss: 2.3374 - val_accuracy: 0.1500\n",
      "Epoch 309/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2202 - accuracy: 0.1742 - val_loss: 2.2508 - val_accuracy: 0.2364\n",
      "Epoch 310/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2843 - accuracy: 0.1606 - val_loss: 2.1206 - val_accuracy: 0.2545\n",
      "Epoch 311/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1950 - accuracy: 0.1985 - val_loss: 2.0996 - val_accuracy: 0.2273\n",
      "Epoch 312/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1279 - accuracy: 0.2333 - val_loss: 2.2087 - val_accuracy: 0.2091\n",
      "Epoch 313/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1629 - accuracy: 0.2061 - val_loss: 2.1224 - val_accuracy: 0.2409\n",
      "Epoch 314/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0908 - accuracy: 0.2561 - val_loss: 2.4978 - val_accuracy: 0.1273\n",
      "Epoch 315/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1013 - accuracy: 0.2227 - val_loss: 2.1895 - val_accuracy: 0.2000\n",
      "Epoch 316/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0594 - accuracy: 0.2439 - val_loss: 2.1864 - val_accuracy: 0.1818\n",
      "Epoch 317/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0900 - accuracy: 0.2364 - val_loss: 2.1392 - val_accuracy: 0.2591\n",
      "Epoch 318/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1498 - accuracy: 0.2258 - val_loss: 2.4707 - val_accuracy: 0.1773\n",
      "Epoch 319/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1421 - accuracy: 0.2364 - val_loss: 2.1489 - val_accuracy: 0.2682\n",
      "Epoch 320/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1697 - accuracy: 0.2106 - val_loss: 2.2217 - val_accuracy: 0.2273\n",
      "Epoch 321/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0944 - accuracy: 0.2212 - val_loss: 2.5499 - val_accuracy: 0.1773\n",
      "Epoch 322/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2478 - accuracy: 0.1848 - val_loss: 2.1978 - val_accuracy: 0.2000\n",
      "Epoch 323/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3466 - accuracy: 0.1576 - val_loss: 3.0893 - val_accuracy: 0.0818\n",
      "Epoch 324/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.6367 - accuracy: 0.1379 - val_loss: 2.4523 - val_accuracy: 0.1955\n",
      "Epoch 325/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4278 - accuracy: 0.1621 - val_loss: 2.4675 - val_accuracy: 0.1545\n",
      "Epoch 326/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2814 - accuracy: 0.1636 - val_loss: 2.2363 - val_accuracy: 0.1636\n",
      "Epoch 327/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2026 - accuracy: 0.1985 - val_loss: 2.4056 - val_accuracy: 0.1318\n",
      "Epoch 328/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1728 - accuracy: 0.2197 - val_loss: 2.2589 - val_accuracy: 0.1364\n",
      "Epoch 329/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1653 - accuracy: 0.2273 - val_loss: 2.2223 - val_accuracy: 0.1636\n",
      "Epoch 330/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1883 - accuracy: 0.2394 - val_loss: 2.3867 - val_accuracy: 0.1955\n",
      "Epoch 331/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1995 - accuracy: 0.1924 - val_loss: 2.2073 - val_accuracy: 0.2636\n",
      "Epoch 332/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1452 - accuracy: 0.2242 - val_loss: 2.1300 - val_accuracy: 0.2182\n",
      "Epoch 333/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1881 - accuracy: 0.2091 - val_loss: 2.2042 - val_accuracy: 0.1682\n",
      "Epoch 334/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1324 - accuracy: 0.2242 - val_loss: 2.2387 - val_accuracy: 0.2182\n",
      "Epoch 335/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3968 - accuracy: 0.1667 - val_loss: 2.6256 - val_accuracy: 0.1227\n",
      "Epoch 336/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2468 - accuracy: 0.2197 - val_loss: 2.3413 - val_accuracy: 0.1500\n",
      "Epoch 337/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2186 - accuracy: 0.1848 - val_loss: 2.1661 - val_accuracy: 0.2727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 338/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1589 - accuracy: 0.2030 - val_loss: 2.2634 - val_accuracy: 0.1636\n",
      "Epoch 339/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1870 - accuracy: 0.2045 - val_loss: 2.4989 - val_accuracy: 0.2000\n",
      "Epoch 340/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1964 - accuracy: 0.2152 - val_loss: 2.2517 - val_accuracy: 0.1682\n",
      "Epoch 341/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1351 - accuracy: 0.1985 - val_loss: 2.2473 - val_accuracy: 0.2000\n",
      "Epoch 342/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2600 - accuracy: 0.1848 - val_loss: 2.2320 - val_accuracy: 0.1500\n",
      "Epoch 343/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3510 - accuracy: 0.1803 - val_loss: 2.6621 - val_accuracy: 0.0864\n",
      "Epoch 344/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3529 - accuracy: 0.1621 - val_loss: 2.4199 - val_accuracy: 0.1591\n",
      "Epoch 345/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3154 - accuracy: 0.1773 - val_loss: 2.3874 - val_accuracy: 0.1636\n",
      "Epoch 346/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2275 - accuracy: 0.1879 - val_loss: 2.5071 - val_accuracy: 0.1455\n",
      "Epoch 347/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2777 - accuracy: 0.1439 - val_loss: 2.3068 - val_accuracy: 0.2409\n",
      "Epoch 348/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1374 - accuracy: 0.2227 - val_loss: 2.1826 - val_accuracy: 0.2318\n",
      "Epoch 349/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1252 - accuracy: 0.2273 - val_loss: 2.2580 - val_accuracy: 0.2636\n",
      "Epoch 350/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1080 - accuracy: 0.2409 - val_loss: 2.2405 - val_accuracy: 0.2045\n",
      "Epoch 351/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.5730 - accuracy: 0.1712 - val_loss: 2.4027 - val_accuracy: 0.2273\n",
      "Epoch 352/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3886 - accuracy: 0.1758 - val_loss: 2.5698 - val_accuracy: 0.1045\n",
      "Epoch 353/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3676 - accuracy: 0.1697 - val_loss: 2.3145 - val_accuracy: 0.1136\n",
      "Epoch 354/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2239 - accuracy: 0.2091 - val_loss: 2.2378 - val_accuracy: 0.2364\n",
      "Epoch 355/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4650 - accuracy: 0.1803 - val_loss: 2.6511 - val_accuracy: 0.0727\n",
      "Epoch 356/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2294 - accuracy: 0.2136 - val_loss: 2.2703 - val_accuracy: 0.2545\n",
      "Epoch 357/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1796 - accuracy: 0.2197 - val_loss: 2.4374 - val_accuracy: 0.2091\n",
      "Epoch 358/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1525 - accuracy: 0.2197 - val_loss: 2.3280 - val_accuracy: 0.1636\n",
      "Epoch 359/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 4.2082 - accuracy: 0.1439 - val_loss: 3.6283 - val_accuracy: 0.1136\n",
      "Epoch 360/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.7645 - accuracy: 0.1333 - val_loss: 2.4564 - val_accuracy: 0.1182\n",
      "Epoch 361/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4181 - accuracy: 0.1470 - val_loss: 2.2760 - val_accuracy: 0.1000\n",
      "Epoch 362/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3449 - accuracy: 0.1545 - val_loss: 2.2852 - val_accuracy: 0.1455\n",
      "Epoch 363/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3579 - accuracy: 0.1470 - val_loss: 2.2630 - val_accuracy: 0.2091\n",
      "Epoch 364/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3559 - accuracy: 0.1606 - val_loss: 2.4909 - val_accuracy: 0.1136\n",
      "Epoch 365/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3730 - accuracy: 0.1727 - val_loss: 2.2554 - val_accuracy: 0.2091\n",
      "Epoch 366/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3163 - accuracy: 0.1712 - val_loss: 2.2844 - val_accuracy: 0.1136\n",
      "Epoch 367/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3397 - accuracy: 0.1818 - val_loss: 2.2750 - val_accuracy: 0.1409\n",
      "Epoch 368/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3153 - accuracy: 0.1848 - val_loss: 2.4860 - val_accuracy: 0.1091\n",
      "Epoch 369/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3105 - accuracy: 0.1667 - val_loss: 2.2735 - val_accuracy: 0.1500\n",
      "Epoch 370/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3474 - accuracy: 0.1515 - val_loss: 2.2495 - val_accuracy: 0.2000\n",
      "Epoch 371/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3018 - accuracy: 0.1939 - val_loss: 2.3331 - val_accuracy: 0.1455\n",
      "Epoch 372/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3071 - accuracy: 0.1864 - val_loss: 2.3351 - val_accuracy: 0.1364\n",
      "Epoch 373/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2870 - accuracy: 0.1788 - val_loss: 2.2732 - val_accuracy: 0.1273\n",
      "Epoch 374/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4087 - accuracy: 0.1470 - val_loss: 2.5778 - val_accuracy: 0.1273\n",
      "Epoch 375/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4749 - accuracy: 0.1394 - val_loss: 2.4703 - val_accuracy: 0.1591\n",
      "Epoch 376/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3990 - accuracy: 0.1576 - val_loss: 2.2612 - val_accuracy: 0.1864\n",
      "Epoch 377/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2894 - accuracy: 0.1818 - val_loss: 2.2400 - val_accuracy: 0.1818\n",
      "Epoch 378/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3342 - accuracy: 0.1606 - val_loss: 2.1989 - val_accuracy: 0.2636\n",
      "Epoch 379/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3334 - accuracy: 0.1652 - val_loss: 2.3437 - val_accuracy: 0.1409\n",
      "Epoch 380/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3180 - accuracy: 0.1742 - val_loss: 2.2921 - val_accuracy: 0.1455\n",
      "Epoch 381/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4155 - accuracy: 0.1273 - val_loss: 2.2856 - val_accuracy: 0.1682\n",
      "Epoch 382/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3631 - accuracy: 0.1606 - val_loss: 2.2548 - val_accuracy: 0.1364\n",
      "Epoch 383/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2988 - accuracy: 0.1773 - val_loss: 2.2551 - val_accuracy: 0.1364\n",
      "Epoch 384/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3750 - accuracy: 0.1667 - val_loss: 2.2245 - val_accuracy: 0.2318\n",
      "Epoch 385/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3874 - accuracy: 0.1545 - val_loss: 2.3283 - val_accuracy: 0.1136\n",
      "Epoch 386/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3766 - accuracy: 0.1621 - val_loss: 2.2694 - val_accuracy: 0.1409\n",
      "Epoch 387/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3009 - accuracy: 0.1955 - val_loss: 2.3135 - val_accuracy: 0.1591\n",
      "Epoch 388/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2998 - accuracy: 0.1621 - val_loss: 2.3978 - val_accuracy: 0.1045\n",
      "Epoch 389/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3157 - accuracy: 0.1530 - val_loss: 2.7588 - val_accuracy: 0.0909\n",
      "Epoch 390/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3217 - accuracy: 0.1864 - val_loss: 2.2239 - val_accuracy: 0.1318\n",
      "Epoch 391/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2977 - accuracy: 0.1803 - val_loss: 2.2764 - val_accuracy: 0.2000\n",
      "Epoch 392/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2887 - accuracy: 0.1697 - val_loss: 2.2547 - val_accuracy: 0.1364\n",
      "Epoch 393/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3041 - accuracy: 0.1682 - val_loss: 2.3333 - val_accuracy: 0.1273\n",
      "Epoch 394/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3474 - accuracy: 0.1561 - val_loss: 2.4266 - val_accuracy: 0.0864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3115 - accuracy: 0.1500 - val_loss: 2.2122 - val_accuracy: 0.1455\n",
      "Epoch 396/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3474 - accuracy: 0.1455 - val_loss: 2.2497 - val_accuracy: 0.1727\n",
      "Epoch 397/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3204 - accuracy: 0.1606 - val_loss: 2.2234 - val_accuracy: 0.1500\n",
      "Epoch 398/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2856 - accuracy: 0.1485 - val_loss: 2.3445 - val_accuracy: 0.2000\n",
      "Epoch 399/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3385 - accuracy: 0.1773 - val_loss: 2.2461 - val_accuracy: 0.1182\n",
      "Epoch 400/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3076 - accuracy: 0.1561 - val_loss: 2.4245 - val_accuracy: 0.1000\n",
      "Epoch 401/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3410 - accuracy: 0.1727 - val_loss: 2.2560 - val_accuracy: 0.1727\n",
      "Epoch 402/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3470 - accuracy: 0.1697 - val_loss: 2.2545 - val_accuracy: 0.1545\n",
      "Epoch 403/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3357 - accuracy: 0.1303 - val_loss: 2.2374 - val_accuracy: 0.1136\n",
      "Epoch 404/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2978 - accuracy: 0.1621 - val_loss: 2.1886 - val_accuracy: 0.1864\n",
      "Epoch 405/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2644 - accuracy: 0.1818 - val_loss: 2.5616 - val_accuracy: 0.0773\n",
      "Epoch 406/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3166 - accuracy: 0.1409 - val_loss: 2.2715 - val_accuracy: 0.1818\n",
      "Epoch 407/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3535 - accuracy: 0.1470 - val_loss: 2.2949 - val_accuracy: 0.1455\n",
      "Epoch 408/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2758 - accuracy: 0.1712 - val_loss: 2.2734 - val_accuracy: 0.1500\n",
      "Epoch 409/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3034 - accuracy: 0.1591 - val_loss: 2.3375 - val_accuracy: 0.1545\n",
      "Epoch 410/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3560 - accuracy: 0.1485 - val_loss: 2.3924 - val_accuracy: 0.0955\n",
      "Epoch 411/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3259 - accuracy: 0.1682 - val_loss: 2.2223 - val_accuracy: 0.1455\n",
      "Epoch 412/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3499 - accuracy: 0.1561 - val_loss: 2.3016 - val_accuracy: 0.1091\n",
      "Epoch 413/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3228 - accuracy: 0.1591 - val_loss: 2.3865 - val_accuracy: 0.1091\n",
      "Epoch 414/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3202 - accuracy: 0.1439 - val_loss: 2.3397 - val_accuracy: 0.1409\n",
      "Epoch 415/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3018 - accuracy: 0.1682 - val_loss: 2.2659 - val_accuracy: 0.2045\n",
      "Epoch 416/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2830 - accuracy: 0.1652 - val_loss: 2.1901 - val_accuracy: 0.1500\n",
      "Epoch 417/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2479 - accuracy: 0.1924 - val_loss: 2.2566 - val_accuracy: 0.2409\n",
      "Epoch 418/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3314 - accuracy: 0.1394 - val_loss: 2.4826 - val_accuracy: 0.1091\n",
      "Epoch 419/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4825 - accuracy: 0.1348 - val_loss: 2.2431 - val_accuracy: 0.2636\n",
      "Epoch 420/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3078 - accuracy: 0.1545 - val_loss: 2.3111 - val_accuracy: 0.1909\n",
      "Epoch 421/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3708 - accuracy: 0.1409 - val_loss: 2.5441 - val_accuracy: 0.2455\n",
      "Epoch 422/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4383 - accuracy: 0.1333 - val_loss: 2.3133 - val_accuracy: 0.1500\n",
      "Epoch 423/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2747 - accuracy: 0.1788 - val_loss: 2.4405 - val_accuracy: 0.1273\n",
      "Epoch 424/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3502 - accuracy: 0.1470 - val_loss: 2.3808 - val_accuracy: 0.1136\n",
      "Epoch 425/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2967 - accuracy: 0.1879 - val_loss: 2.1750 - val_accuracy: 0.2318\n",
      "Epoch 426/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2797 - accuracy: 0.1742 - val_loss: 2.3365 - val_accuracy: 0.1955\n",
      "Epoch 427/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3160 - accuracy: 0.1697 - val_loss: 2.2598 - val_accuracy: 0.1045\n",
      "Epoch 428/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3074 - accuracy: 0.1773 - val_loss: 2.1955 - val_accuracy: 0.1727\n",
      "Epoch 429/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2498 - accuracy: 0.1773 - val_loss: 2.2804 - val_accuracy: 0.1909\n",
      "Epoch 430/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3254 - accuracy: 0.1864 - val_loss: 2.4248 - val_accuracy: 0.1364\n",
      "Epoch 431/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3063 - accuracy: 0.1667 - val_loss: 2.2815 - val_accuracy: 0.1591\n",
      "Epoch 432/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3664 - accuracy: 0.1455 - val_loss: 2.2355 - val_accuracy: 0.2136\n",
      "Epoch 433/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3121 - accuracy: 0.1515 - val_loss: 2.3435 - val_accuracy: 0.1727\n",
      "Epoch 434/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3568 - accuracy: 0.1561 - val_loss: 2.2775 - val_accuracy: 0.1545\n",
      "Epoch 435/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3017 - accuracy: 0.1682 - val_loss: 2.1946 - val_accuracy: 0.2318\n",
      "Epoch 436/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2824 - accuracy: 0.1576 - val_loss: 2.2224 - val_accuracy: 0.1955\n",
      "Epoch 437/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2674 - accuracy: 0.2076 - val_loss: 2.3407 - val_accuracy: 0.1500\n",
      "Epoch 438/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2492 - accuracy: 0.1924 - val_loss: 2.2405 - val_accuracy: 0.1409\n",
      "Epoch 439/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2626 - accuracy: 0.1909 - val_loss: 2.2346 - val_accuracy: 0.1409\n",
      "Epoch 440/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2146 - accuracy: 0.2000 - val_loss: 2.2015 - val_accuracy: 0.1682\n",
      "Epoch 441/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3311 - accuracy: 0.1576 - val_loss: 2.2726 - val_accuracy: 0.1364\n",
      "Epoch 442/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3086 - accuracy: 0.1545 - val_loss: 2.3046 - val_accuracy: 0.1227\n",
      "Epoch 443/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2610 - accuracy: 0.1924 - val_loss: 2.2488 - val_accuracy: 0.2000\n",
      "Epoch 444/1000\n",
      "21/21 [==============================] - 0s 993us/step - loss: 2.2713 - accuracy: 0.1879 - val_loss: 2.3574 - val_accuracy: 0.1682\n",
      "Epoch 445/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3616 - accuracy: 0.1545 - val_loss: 2.3029 - val_accuracy: 0.1409\n",
      "Epoch 446/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3565 - accuracy: 0.1424 - val_loss: 2.3023 - val_accuracy: 0.1500\n",
      "Epoch 447/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2798 - accuracy: 0.2015 - val_loss: 2.2870 - val_accuracy: 0.2136\n",
      "Epoch 448/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3977 - accuracy: 0.1439 - val_loss: 2.4459 - val_accuracy: 0.1364\n",
      "Epoch 449/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3046 - accuracy: 0.1742 - val_loss: 2.2239 - val_accuracy: 0.2500\n",
      "Epoch 450/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2496 - accuracy: 0.1697 - val_loss: 2.3337 - val_accuracy: 0.1182\n",
      "Epoch 451/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3448 - accuracy: 0.1439 - val_loss: 2.3189 - val_accuracy: 0.1773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 452/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3002 - accuracy: 0.1742 - val_loss: 2.2798 - val_accuracy: 0.1500\n",
      "Epoch 453/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2389 - accuracy: 0.1758 - val_loss: 2.2219 - val_accuracy: 0.1955\n",
      "Epoch 454/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2423 - accuracy: 0.2030 - val_loss: 2.3157 - val_accuracy: 0.2000\n",
      "Epoch 455/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2861 - accuracy: 0.1606 - val_loss: 2.5804 - val_accuracy: 0.0727\n",
      "Epoch 456/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3227 - accuracy: 0.1333 - val_loss: 2.2321 - val_accuracy: 0.2273\n",
      "Epoch 457/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3111 - accuracy: 0.1712 - val_loss: 2.3988 - val_accuracy: 0.1955\n",
      "Epoch 458/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4104 - accuracy: 0.1212 - val_loss: 2.2944 - val_accuracy: 0.1682\n",
      "Epoch 459/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2976 - accuracy: 0.1530 - val_loss: 2.2440 - val_accuracy: 0.1500\n",
      "Epoch 460/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3598 - accuracy: 0.1576 - val_loss: 2.4144 - val_accuracy: 0.1273\n",
      "Epoch 461/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4715 - accuracy: 0.1273 - val_loss: 2.3508 - val_accuracy: 0.1545\n",
      "Epoch 462/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3085 - accuracy: 0.1591 - val_loss: 2.7516 - val_accuracy: 0.0727\n",
      "Epoch 463/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4073 - accuracy: 0.1500 - val_loss: 2.2770 - val_accuracy: 0.1227\n",
      "Epoch 464/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2753 - accuracy: 0.1758 - val_loss: 2.2899 - val_accuracy: 0.2045\n",
      "Epoch 465/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2590 - accuracy: 0.1818 - val_loss: 2.2279 - val_accuracy: 0.1727\n",
      "Epoch 466/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2492 - accuracy: 0.1682 - val_loss: 2.3203 - val_accuracy: 0.0773\n",
      "Epoch 467/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2792 - accuracy: 0.1545 - val_loss: 2.2592 - val_accuracy: 0.1455\n",
      "Epoch 468/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2614 - accuracy: 0.1894 - val_loss: 2.2696 - val_accuracy: 0.1545\n",
      "Epoch 469/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3025 - accuracy: 0.1500 - val_loss: 2.3327 - val_accuracy: 0.1500\n",
      "Epoch 470/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2719 - accuracy: 0.1561 - val_loss: 2.2224 - val_accuracy: 0.1682\n",
      "Epoch 471/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2639 - accuracy: 0.1758 - val_loss: 2.2247 - val_accuracy: 0.1682\n",
      "Epoch 472/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2810 - accuracy: 0.1879 - val_loss: 2.2338 - val_accuracy: 0.1500\n",
      "Epoch 473/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2720 - accuracy: 0.1727 - val_loss: 2.4890 - val_accuracy: 0.0636\n",
      "Epoch 474/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2712 - accuracy: 0.1712 - val_loss: 2.3244 - val_accuracy: 0.1545\n",
      "Epoch 475/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2548 - accuracy: 0.1682 - val_loss: 2.4215 - val_accuracy: 0.0864\n",
      "Epoch 476/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2522 - accuracy: 0.1985 - val_loss: 2.2703 - val_accuracy: 0.1727\n",
      "Epoch 477/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3076 - accuracy: 0.1682 - val_loss: 2.1845 - val_accuracy: 0.2273\n",
      "Epoch 478/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2917 - accuracy: 0.1561 - val_loss: 2.3963 - val_accuracy: 0.1182\n",
      "Epoch 479/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3073 - accuracy: 0.1576 - val_loss: 2.3020 - val_accuracy: 0.1409\n",
      "Epoch 480/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4862 - accuracy: 0.1288 - val_loss: 2.5316 - val_accuracy: 0.1091\n",
      "Epoch 481/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3386 - accuracy: 0.1955 - val_loss: 2.1901 - val_accuracy: 0.2364\n",
      "Epoch 482/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2720 - accuracy: 0.1515 - val_loss: 2.2882 - val_accuracy: 0.1273\n",
      "Epoch 483/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2708 - accuracy: 0.1606 - val_loss: 2.4019 - val_accuracy: 0.1364\n",
      "Epoch 484/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2549 - accuracy: 0.1970 - val_loss: 2.2327 - val_accuracy: 0.1545\n",
      "Epoch 485/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2316 - accuracy: 0.1682 - val_loss: 2.2703 - val_accuracy: 0.1591\n",
      "Epoch 486/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2265 - accuracy: 0.1652 - val_loss: 2.2605 - val_accuracy: 0.2136\n",
      "Epoch 487/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2754 - accuracy: 0.1576 - val_loss: 2.2499 - val_accuracy: 0.2591\n",
      "Epoch 488/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2154 - accuracy: 0.2076 - val_loss: 2.2401 - val_accuracy: 0.1091\n",
      "Epoch 489/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3097 - accuracy: 0.1530 - val_loss: 2.2267 - val_accuracy: 0.1727\n",
      "Epoch 490/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3036 - accuracy: 0.1515 - val_loss: 2.3073 - val_accuracy: 0.1045\n",
      "Epoch 491/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3367 - accuracy: 0.1439 - val_loss: 2.2354 - val_accuracy: 0.1273\n",
      "Epoch 492/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2289 - accuracy: 0.1848 - val_loss: 2.1562 - val_accuracy: 0.1591\n",
      "Epoch 493/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2332 - accuracy: 0.2030 - val_loss: 2.4748 - val_accuracy: 0.1000\n",
      "Epoch 494/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2142 - accuracy: 0.1894 - val_loss: 2.1493 - val_accuracy: 0.2182\n",
      "Epoch 495/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2379 - accuracy: 0.1773 - val_loss: 2.1766 - val_accuracy: 0.1818\n",
      "Epoch 496/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2676 - accuracy: 0.1561 - val_loss: 2.1899 - val_accuracy: 0.2364\n",
      "Epoch 497/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2648 - accuracy: 0.1621 - val_loss: 2.2011 - val_accuracy: 0.2727\n",
      "Epoch 498/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2386 - accuracy: 0.1682 - val_loss: 2.1844 - val_accuracy: 0.2636\n",
      "Epoch 499/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4386 - accuracy: 0.1515 - val_loss: 2.2943 - val_accuracy: 0.2045\n",
      "Epoch 500/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3301 - accuracy: 0.1636 - val_loss: 2.1783 - val_accuracy: 0.2455\n",
      "Epoch 501/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2856 - accuracy: 0.1667 - val_loss: 2.2315 - val_accuracy: 0.1000\n",
      "Epoch 502/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2334 - accuracy: 0.1758 - val_loss: 2.2296 - val_accuracy: 0.1409\n",
      "Epoch 503/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2348 - accuracy: 0.2045 - val_loss: 2.2661 - val_accuracy: 0.1318\n",
      "Epoch 504/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2610 - accuracy: 0.1939 - val_loss: 2.2130 - val_accuracy: 0.1318\n",
      "Epoch 505/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3278 - accuracy: 0.1515 - val_loss: 2.2473 - val_accuracy: 0.2545\n",
      "Epoch 506/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3094 - accuracy: 0.2000 - val_loss: 2.2103 - val_accuracy: 0.2864\n",
      "Epoch 507/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2886 - accuracy: 0.1833 - val_loss: 2.5138 - val_accuracy: 0.1591\n",
      "Epoch 508/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4768 - accuracy: 0.1394 - val_loss: 2.2881 - val_accuracy: 0.1545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 509/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2613 - accuracy: 0.1667 - val_loss: 2.4207 - val_accuracy: 0.1591\n",
      "Epoch 510/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3666 - accuracy: 0.1273 - val_loss: 2.5225 - val_accuracy: 0.1136\n",
      "Epoch 511/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4528 - accuracy: 0.1591 - val_loss: 2.4081 - val_accuracy: 0.1273\n",
      "Epoch 512/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2877 - accuracy: 0.1606 - val_loss: 2.3098 - val_accuracy: 0.1545\n",
      "Epoch 513/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2996 - accuracy: 0.1652 - val_loss: 2.3198 - val_accuracy: 0.1364\n",
      "Epoch 514/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3032 - accuracy: 0.1652 - val_loss: 2.4062 - val_accuracy: 0.1364\n",
      "Epoch 515/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2458 - accuracy: 0.1955 - val_loss: 2.3219 - val_accuracy: 0.1455\n",
      "Epoch 516/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2431 - accuracy: 0.1894 - val_loss: 2.1772 - val_accuracy: 0.2273\n",
      "Epoch 517/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2308 - accuracy: 0.1712 - val_loss: 2.1666 - val_accuracy: 0.2182\n",
      "Epoch 518/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2316 - accuracy: 0.1682 - val_loss: 2.2316 - val_accuracy: 0.1364\n",
      "Epoch 519/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2530 - accuracy: 0.1621 - val_loss: 2.1412 - val_accuracy: 0.2227\n",
      "Epoch 520/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3335 - accuracy: 0.1561 - val_loss: 2.1915 - val_accuracy: 0.1818\n",
      "Epoch 521/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3563 - accuracy: 0.1439 - val_loss: 2.1876 - val_accuracy: 0.1591\n",
      "Epoch 522/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2634 - accuracy: 0.1394 - val_loss: 2.1760 - val_accuracy: 0.2909\n",
      "Epoch 523/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2925 - accuracy: 0.1788 - val_loss: 2.8006 - val_accuracy: 0.0773\n",
      "Epoch 524/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3463 - accuracy: 0.1545 - val_loss: 2.3108 - val_accuracy: 0.1818\n",
      "Epoch 525/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2509 - accuracy: 0.1515 - val_loss: 2.2581 - val_accuracy: 0.1318\n",
      "Epoch 526/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2113 - accuracy: 0.1788 - val_loss: 2.2422 - val_accuracy: 0.1955\n",
      "Epoch 527/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2615 - accuracy: 0.1621 - val_loss: 2.1381 - val_accuracy: 0.2409\n",
      "Epoch 528/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2138 - accuracy: 0.1818 - val_loss: 2.1611 - val_accuracy: 0.1727\n",
      "Epoch 529/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1667 - accuracy: 0.2121 - val_loss: 2.2328 - val_accuracy: 0.2227\n",
      "Epoch 530/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1749 - accuracy: 0.1955 - val_loss: 2.1757 - val_accuracy: 0.1818\n",
      "Epoch 531/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2190 - accuracy: 0.1758 - val_loss: 2.1850 - val_accuracy: 0.1909\n",
      "Epoch 532/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1570 - accuracy: 0.2136 - val_loss: 2.2573 - val_accuracy: 0.2227\n",
      "Epoch 533/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3185 - accuracy: 0.1500 - val_loss: 2.4157 - val_accuracy: 0.1318\n",
      "Epoch 534/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3540 - accuracy: 0.1697 - val_loss: 2.2854 - val_accuracy: 0.1909\n",
      "Epoch 535/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2243 - accuracy: 0.1985 - val_loss: 2.1294 - val_accuracy: 0.2136\n",
      "Epoch 536/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2189 - accuracy: 0.1879 - val_loss: 2.1174 - val_accuracy: 0.2045\n",
      "Epoch 537/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2173 - accuracy: 0.2091 - val_loss: 2.2530 - val_accuracy: 0.1045\n",
      "Epoch 538/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1994 - accuracy: 0.1909 - val_loss: 2.2798 - val_accuracy: 0.1545\n",
      "Epoch 539/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1980 - accuracy: 0.2091 - val_loss: 2.1283 - val_accuracy: 0.2182\n",
      "Epoch 540/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2246 - accuracy: 0.1833 - val_loss: 2.4535 - val_accuracy: 0.1182\n",
      "Epoch 541/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3070 - accuracy: 0.1697 - val_loss: 2.3498 - val_accuracy: 0.2045\n",
      "Epoch 542/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2110 - accuracy: 0.2015 - val_loss: 2.3546 - val_accuracy: 0.1500\n",
      "Epoch 543/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2572 - accuracy: 0.1727 - val_loss: 2.2561 - val_accuracy: 0.1273\n",
      "Epoch 544/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2813 - accuracy: 0.1758 - val_loss: 2.2753 - val_accuracy: 0.1545\n",
      "Epoch 545/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2232 - accuracy: 0.1955 - val_loss: 2.3509 - val_accuracy: 0.1364\n",
      "Epoch 546/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2786 - accuracy: 0.1727 - val_loss: 2.3524 - val_accuracy: 0.1182\n",
      "Epoch 547/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3253 - accuracy: 0.1379 - val_loss: 2.1519 - val_accuracy: 0.2182\n",
      "Epoch 548/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2735 - accuracy: 0.1788 - val_loss: 2.2138 - val_accuracy: 0.1636\n",
      "Epoch 549/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2974 - accuracy: 0.1545 - val_loss: 2.1870 - val_accuracy: 0.1909\n",
      "Epoch 550/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2545 - accuracy: 0.1636 - val_loss: 2.1526 - val_accuracy: 0.2182\n",
      "Epoch 551/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2243 - accuracy: 0.1803 - val_loss: 2.3122 - val_accuracy: 0.1773\n",
      "Epoch 552/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2555 - accuracy: 0.1773 - val_loss: 2.1582 - val_accuracy: 0.1727\n",
      "Epoch 553/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2085 - accuracy: 0.1621 - val_loss: 2.2480 - val_accuracy: 0.1636\n",
      "Epoch 554/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1725 - accuracy: 0.2106 - val_loss: 2.1900 - val_accuracy: 0.1409\n",
      "Epoch 555/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1357 - accuracy: 0.2045 - val_loss: 2.1049 - val_accuracy: 0.2773\n",
      "Epoch 556/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1885 - accuracy: 0.1909 - val_loss: 2.2560 - val_accuracy: 0.1955\n",
      "Epoch 557/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1803 - accuracy: 0.2091 - val_loss: 2.2478 - val_accuracy: 0.1727\n",
      "Epoch 558/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1884 - accuracy: 0.1924 - val_loss: 2.2359 - val_accuracy: 0.1136\n",
      "Epoch 559/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1536 - accuracy: 0.1985 - val_loss: 2.1175 - val_accuracy: 0.1455\n",
      "Epoch 560/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2259 - accuracy: 0.1864 - val_loss: 2.5436 - val_accuracy: 0.0818\n",
      "Epoch 561/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.3289 - accuracy: 0.1712 - val_loss: 2.1502 - val_accuracy: 0.1818\n",
      "Epoch 562/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2106 - accuracy: 0.2015 - val_loss: 2.3960 - val_accuracy: 0.1227\n",
      "Epoch 563/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2162 - accuracy: 0.1864 - val_loss: 2.2728 - val_accuracy: 0.1227\n",
      "Epoch 564/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2980 - accuracy: 0.1455 - val_loss: 2.2611 - val_accuracy: 0.1364\n",
      "Epoch 565/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2809 - accuracy: 0.1848 - val_loss: 2.2095 - val_accuracy: 0.1364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 566/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1838 - accuracy: 0.1833 - val_loss: 2.2176 - val_accuracy: 0.2227\n",
      "Epoch 567/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1548 - accuracy: 0.2045 - val_loss: 2.1805 - val_accuracy: 0.1636\n",
      "Epoch 568/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1730 - accuracy: 0.1742 - val_loss: 2.4096 - val_accuracy: 0.0682\n",
      "Epoch 569/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2291 - accuracy: 0.1652 - val_loss: 2.1066 - val_accuracy: 0.2955\n",
      "Epoch 570/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1775 - accuracy: 0.2076 - val_loss: 2.1301 - val_accuracy: 0.1773\n",
      "Epoch 571/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1187 - accuracy: 0.2076 - val_loss: 2.0904 - val_accuracy: 0.2000\n",
      "Epoch 572/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1616 - accuracy: 0.2061 - val_loss: 2.1380 - val_accuracy: 0.1636\n",
      "Epoch 573/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1431 - accuracy: 0.2000 - val_loss: 2.1081 - val_accuracy: 0.1500\n",
      "Epoch 574/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1121 - accuracy: 0.2439 - val_loss: 2.1857 - val_accuracy: 0.1773\n",
      "Epoch 575/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1389 - accuracy: 0.1985 - val_loss: 2.2547 - val_accuracy: 0.1182\n",
      "Epoch 576/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1621 - accuracy: 0.2439 - val_loss: 2.2090 - val_accuracy: 0.1864\n",
      "Epoch 577/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1429 - accuracy: 0.1985 - val_loss: 2.0818 - val_accuracy: 0.2318\n",
      "Epoch 578/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1792 - accuracy: 0.1758 - val_loss: 2.2904 - val_accuracy: 0.1318\n",
      "Epoch 579/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.4072 - accuracy: 0.1379 - val_loss: 2.2583 - val_accuracy: 0.1591\n",
      "Epoch 580/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1775 - accuracy: 0.1909 - val_loss: 2.2215 - val_accuracy: 0.1500\n",
      "Epoch 581/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2413 - accuracy: 0.1682 - val_loss: 2.2520 - val_accuracy: 0.1682\n",
      "Epoch 582/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1593 - accuracy: 0.1848 - val_loss: 2.1054 - val_accuracy: 0.2273\n",
      "Epoch 583/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1552 - accuracy: 0.1970 - val_loss: 2.1863 - val_accuracy: 0.1773\n",
      "Epoch 584/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1308 - accuracy: 0.1894 - val_loss: 2.1163 - val_accuracy: 0.2136\n",
      "Epoch 585/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1656 - accuracy: 0.1742 - val_loss: 2.1265 - val_accuracy: 0.1955\n",
      "Epoch 586/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1568 - accuracy: 0.1894 - val_loss: 2.1847 - val_accuracy: 0.1773\n",
      "Epoch 587/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2051 - accuracy: 0.1742 - val_loss: 2.1349 - val_accuracy: 0.2318\n",
      "Epoch 588/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1314 - accuracy: 0.2136 - val_loss: 2.1556 - val_accuracy: 0.2318\n",
      "Epoch 589/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1228 - accuracy: 0.2439 - val_loss: 2.0783 - val_accuracy: 0.2591\n",
      "Epoch 590/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1858 - accuracy: 0.1879 - val_loss: 2.1086 - val_accuracy: 0.1500\n",
      "Epoch 591/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1400 - accuracy: 0.1939 - val_loss: 2.1277 - val_accuracy: 0.2773\n",
      "Epoch 592/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1126 - accuracy: 0.2182 - val_loss: 2.1524 - val_accuracy: 0.1545\n",
      "Epoch 593/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2611 - accuracy: 0.1591 - val_loss: 2.2002 - val_accuracy: 0.1636\n",
      "Epoch 594/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2356 - accuracy: 0.1848 - val_loss: 2.0633 - val_accuracy: 0.3364\n",
      "Epoch 595/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1195 - accuracy: 0.1955 - val_loss: 2.0918 - val_accuracy: 0.2091\n",
      "Epoch 596/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1851 - accuracy: 0.1652 - val_loss: 2.0842 - val_accuracy: 0.1909\n",
      "Epoch 597/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2040 - accuracy: 0.1697 - val_loss: 2.5227 - val_accuracy: 0.1318\n",
      "Epoch 598/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2229 - accuracy: 0.1455 - val_loss: 2.4076 - val_accuracy: 0.1091\n",
      "Epoch 599/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2385 - accuracy: 0.1667 - val_loss: 2.2191 - val_accuracy: 0.1636\n",
      "Epoch 600/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1573 - accuracy: 0.1833 - val_loss: 2.2359 - val_accuracy: 0.1227\n",
      "Epoch 601/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1509 - accuracy: 0.1939 - val_loss: 2.2525 - val_accuracy: 0.2000\n",
      "Epoch 602/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1947 - accuracy: 0.1727 - val_loss: 2.1264 - val_accuracy: 0.2045\n",
      "Epoch 603/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2016 - accuracy: 0.1500 - val_loss: 2.0858 - val_accuracy: 0.1773\n",
      "Epoch 604/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1095 - accuracy: 0.2076 - val_loss: 2.1001 - val_accuracy: 0.2182\n",
      "Epoch 605/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1382 - accuracy: 0.1864 - val_loss: 2.1011 - val_accuracy: 0.2500\n",
      "Epoch 606/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1536 - accuracy: 0.1879 - val_loss: 2.0980 - val_accuracy: 0.2409\n",
      "Epoch 607/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1282 - accuracy: 0.1939 - val_loss: 2.1414 - val_accuracy: 0.2409\n",
      "Epoch 608/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1856 - accuracy: 0.1833 - val_loss: 2.2763 - val_accuracy: 0.1773\n",
      "Epoch 609/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1567 - accuracy: 0.1818 - val_loss: 2.1960 - val_accuracy: 0.1818\n",
      "Epoch 610/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0994 - accuracy: 0.2091 - val_loss: 2.0988 - val_accuracy: 0.2000\n",
      "Epoch 611/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1057 - accuracy: 0.2333 - val_loss: 2.0683 - val_accuracy: 0.2909\n",
      "Epoch 612/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0352 - accuracy: 0.2561 - val_loss: 2.1266 - val_accuracy: 0.1545\n",
      "Epoch 613/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0977 - accuracy: 0.2121 - val_loss: 2.0773 - val_accuracy: 0.1500\n",
      "Epoch 614/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.2345 - accuracy: 0.1652 - val_loss: 2.0247 - val_accuracy: 0.2409\n",
      "Epoch 615/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1395 - accuracy: 0.1652 - val_loss: 2.0869 - val_accuracy: 0.2500\n",
      "Epoch 616/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0817 - accuracy: 0.2182 - val_loss: 2.1420 - val_accuracy: 0.1091\n",
      "Epoch 617/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0755 - accuracy: 0.1879 - val_loss: 2.0560 - val_accuracy: 0.2727\n",
      "Epoch 618/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0479 - accuracy: 0.2470 - val_loss: 2.1437 - val_accuracy: 0.1455\n",
      "Epoch 619/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1560 - accuracy: 0.1894 - val_loss: 2.0786 - val_accuracy: 0.2318\n",
      "Epoch 620/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0877 - accuracy: 0.1924 - val_loss: 2.1873 - val_accuracy: 0.2545\n",
      "Epoch 621/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1265 - accuracy: 0.1742 - val_loss: 2.1195 - val_accuracy: 0.1682\n",
      "Epoch 622/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0831 - accuracy: 0.2121 - val_loss: 2.0175 - val_accuracy: 0.2227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 623/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0684 - accuracy: 0.2409 - val_loss: 2.1228 - val_accuracy: 0.1682\n",
      "Epoch 624/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1386 - accuracy: 0.1909 - val_loss: 2.1071 - val_accuracy: 0.2227\n",
      "Epoch 625/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1390 - accuracy: 0.1803 - val_loss: 2.2688 - val_accuracy: 0.1773\n",
      "Epoch 626/1000\n",
      "21/21 [==============================] - 0s 986us/step - loss: 2.1815 - accuracy: 0.1864 - val_loss: 2.0542 - val_accuracy: 0.1682\n",
      "Epoch 627/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0628 - accuracy: 0.2167 - val_loss: 2.1456 - val_accuracy: 0.1773\n",
      "Epoch 628/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0957 - accuracy: 0.1758 - val_loss: 2.0607 - val_accuracy: 0.2773\n",
      "Epoch 629/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0580 - accuracy: 0.2258 - val_loss: 2.1430 - val_accuracy: 0.1864\n",
      "Epoch 630/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0758 - accuracy: 0.2136 - val_loss: 2.0697 - val_accuracy: 0.1545\n",
      "Epoch 631/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0412 - accuracy: 0.2364 - val_loss: 2.0193 - val_accuracy: 0.2273\n",
      "Epoch 632/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0109 - accuracy: 0.2515 - val_loss: 2.1144 - val_accuracy: 0.2136\n",
      "Epoch 633/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0862 - accuracy: 0.2273 - val_loss: 2.0108 - val_accuracy: 0.2000\n",
      "Epoch 634/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0391 - accuracy: 0.2227 - val_loss: 1.9970 - val_accuracy: 0.2000\n",
      "Epoch 635/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1038 - accuracy: 0.2121 - val_loss: 2.2541 - val_accuracy: 0.1000\n",
      "Epoch 636/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1289 - accuracy: 0.1758 - val_loss: 2.1915 - val_accuracy: 0.1182\n",
      "Epoch 637/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.1818 - accuracy: 0.1515 - val_loss: 2.0675 - val_accuracy: 0.2045\n",
      "Epoch 638/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0535 - accuracy: 0.2136 - val_loss: 2.0258 - val_accuracy: 0.1773\n",
      "Epoch 639/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0805 - accuracy: 0.1788 - val_loss: 2.0719 - val_accuracy: 0.2136\n",
      "Epoch 640/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0182 - accuracy: 0.2167 - val_loss: 2.0270 - val_accuracy: 0.2682\n",
      "Epoch 641/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0182 - accuracy: 0.2333 - val_loss: 2.0332 - val_accuracy: 0.2091\n",
      "Epoch 642/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0253 - accuracy: 0.2394 - val_loss: 2.0257 - val_accuracy: 0.2045\n",
      "Epoch 643/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0189 - accuracy: 0.2273 - val_loss: 2.0709 - val_accuracy: 0.2409\n",
      "Epoch 644/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0190 - accuracy: 0.2364 - val_loss: 1.9702 - val_accuracy: 0.1864\n",
      "Epoch 645/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9754 - accuracy: 0.2424 - val_loss: 1.9310 - val_accuracy: 0.2818\n",
      "Epoch 646/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0303 - accuracy: 0.2061 - val_loss: 2.0821 - val_accuracy: 0.1318\n",
      "Epoch 647/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0087 - accuracy: 0.2333 - val_loss: 1.9465 - val_accuracy: 0.2864\n",
      "Epoch 648/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9961 - accuracy: 0.2439 - val_loss: 1.9948 - val_accuracy: 0.2545\n",
      "Epoch 649/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0524 - accuracy: 0.1879 - val_loss: 2.0688 - val_accuracy: 0.2318\n",
      "Epoch 650/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0495 - accuracy: 0.2061 - val_loss: 1.9912 - val_accuracy: 0.2409\n",
      "Epoch 651/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0780 - accuracy: 0.1939 - val_loss: 1.9098 - val_accuracy: 0.2364\n",
      "Epoch 652/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9697 - accuracy: 0.2545 - val_loss: 1.9414 - val_accuracy: 0.2682\n",
      "Epoch 653/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9732 - accuracy: 0.2379 - val_loss: 1.9419 - val_accuracy: 0.2227\n",
      "Epoch 654/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9993 - accuracy: 0.2182 - val_loss: 1.9520 - val_accuracy: 0.2364\n",
      "Epoch 655/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9908 - accuracy: 0.2136 - val_loss: 1.9835 - val_accuracy: 0.2045\n",
      "Epoch 656/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9631 - accuracy: 0.2515 - val_loss: 1.9581 - val_accuracy: 0.2091\n",
      "Epoch 657/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9486 - accuracy: 0.2515 - val_loss: 1.9372 - val_accuracy: 0.2136\n",
      "Epoch 658/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9855 - accuracy: 0.2333 - val_loss: 2.0151 - val_accuracy: 0.2091\n",
      "Epoch 659/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9848 - accuracy: 0.2318 - val_loss: 1.9539 - val_accuracy: 0.2864\n",
      "Epoch 660/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9982 - accuracy: 0.2318 - val_loss: 2.1157 - val_accuracy: 0.1318\n",
      "Epoch 661/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9945 - accuracy: 0.2273 - val_loss: 2.0202 - val_accuracy: 0.2273\n",
      "Epoch 662/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9324 - accuracy: 0.2576 - val_loss: 2.0619 - val_accuracy: 0.2182\n",
      "Epoch 663/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9858 - accuracy: 0.2152 - val_loss: 1.9458 - val_accuracy: 0.2136\n",
      "Epoch 664/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9547 - accuracy: 0.2470 - val_loss: 1.9174 - val_accuracy: 0.1955\n",
      "Epoch 665/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0257 - accuracy: 0.2121 - val_loss: 1.9963 - val_accuracy: 0.2227\n",
      "Epoch 666/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9376 - accuracy: 0.2545 - val_loss: 1.9461 - val_accuracy: 0.1636\n",
      "Epoch 667/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9753 - accuracy: 0.2015 - val_loss: 1.8969 - val_accuracy: 0.2000\n",
      "Epoch 668/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9446 - accuracy: 0.2288 - val_loss: 1.9408 - val_accuracy: 0.1909\n",
      "Epoch 669/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9475 - accuracy: 0.2439 - val_loss: 1.9791 - val_accuracy: 0.2864\n",
      "Epoch 670/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9470 - accuracy: 0.2439 - val_loss: 1.9410 - val_accuracy: 0.2000\n",
      "Epoch 671/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9885 - accuracy: 0.2242 - val_loss: 1.9038 - val_accuracy: 0.2318\n",
      "Epoch 672/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9200 - accuracy: 0.2394 - val_loss: 1.9372 - val_accuracy: 0.2136\n",
      "Epoch 673/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9500 - accuracy: 0.2258 - val_loss: 1.8852 - val_accuracy: 0.2909\n",
      "Epoch 674/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8832 - accuracy: 0.3030 - val_loss: 1.9188 - val_accuracy: 0.2591\n",
      "Epoch 675/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9320 - accuracy: 0.2485 - val_loss: 1.9687 - val_accuracy: 0.1727\n",
      "Epoch 676/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9264 - accuracy: 0.2424 - val_loss: 1.8958 - val_accuracy: 0.2955\n",
      "Epoch 677/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9009 - accuracy: 0.2636 - val_loss: 1.9156 - val_accuracy: 0.3000\n",
      "Epoch 678/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9148 - accuracy: 0.2500 - val_loss: 2.2652 - val_accuracy: 0.1000\n",
      "Epoch 679/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0155 - accuracy: 0.1924 - val_loss: 2.0212 - val_accuracy: 0.1591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 680/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0034 - accuracy: 0.2015 - val_loss: 1.8470 - val_accuracy: 0.3318\n",
      "Epoch 681/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9204 - accuracy: 0.2227 - val_loss: 1.8715 - val_accuracy: 0.2682\n",
      "Epoch 682/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8759 - accuracy: 0.2727 - val_loss: 1.8730 - val_accuracy: 0.3000\n",
      "Epoch 683/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8883 - accuracy: 0.2682 - val_loss: 1.8948 - val_accuracy: 0.2182\n",
      "Epoch 684/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9846 - accuracy: 0.2182 - val_loss: 2.0433 - val_accuracy: 0.1864\n",
      "Epoch 685/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9424 - accuracy: 0.2318 - val_loss: 1.9081 - val_accuracy: 0.3045\n",
      "Epoch 686/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9159 - accuracy: 0.2545 - val_loss: 1.8839 - val_accuracy: 0.2227\n",
      "Epoch 687/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9420 - accuracy: 0.2364 - val_loss: 1.8992 - val_accuracy: 0.2182\n",
      "Epoch 688/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9158 - accuracy: 0.2379 - val_loss: 1.8714 - val_accuracy: 0.2273\n",
      "Epoch 689/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9150 - accuracy: 0.2697 - val_loss: 2.0178 - val_accuracy: 0.1818\n",
      "Epoch 690/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9576 - accuracy: 0.2212 - val_loss: 2.0695 - val_accuracy: 0.1727\n",
      "Epoch 691/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9273 - accuracy: 0.2424 - val_loss: 1.8521 - val_accuracy: 0.2591\n",
      "Epoch 692/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8825 - accuracy: 0.2561 - val_loss: 1.8219 - val_accuracy: 0.2864\n",
      "Epoch 693/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8590 - accuracy: 0.2561 - val_loss: 1.8031 - val_accuracy: 0.3909\n",
      "Epoch 694/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8852 - accuracy: 0.2576 - val_loss: 1.8917 - val_accuracy: 0.2591\n",
      "Epoch 695/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9220 - accuracy: 0.2606 - val_loss: 1.8092 - val_accuracy: 0.2636\n",
      "Epoch 696/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8370 - accuracy: 0.2591 - val_loss: 1.8800 - val_accuracy: 0.2455\n",
      "Epoch 697/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8807 - accuracy: 0.2439 - val_loss: 1.8166 - val_accuracy: 0.2864\n",
      "Epoch 698/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8403 - accuracy: 0.2712 - val_loss: 1.9998 - val_accuracy: 0.2000\n",
      "Epoch 699/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8950 - accuracy: 0.2545 - val_loss: 1.9484 - val_accuracy: 0.1818\n",
      "Epoch 700/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8682 - accuracy: 0.2606 - val_loss: 1.8299 - val_accuracy: 0.2136\n",
      "Epoch 701/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8259 - accuracy: 0.2833 - val_loss: 1.8524 - val_accuracy: 0.3091\n",
      "Epoch 702/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8519 - accuracy: 0.2273 - val_loss: 1.8372 - val_accuracy: 0.2818\n",
      "Epoch 703/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8609 - accuracy: 0.2667 - val_loss: 1.8096 - val_accuracy: 0.2409\n",
      "Epoch 704/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8626 - accuracy: 0.2955 - val_loss: 1.8729 - val_accuracy: 0.2227\n",
      "Epoch 705/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9586 - accuracy: 0.1955 - val_loss: 1.8366 - val_accuracy: 0.2318\n",
      "Epoch 706/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9466 - accuracy: 0.2167 - val_loss: 1.9938 - val_accuracy: 0.1364\n",
      "Epoch 707/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8545 - accuracy: 0.2515 - val_loss: 1.8325 - val_accuracy: 0.2364\n",
      "Epoch 708/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9106 - accuracy: 0.2500 - val_loss: 1.8049 - val_accuracy: 0.3000\n",
      "Epoch 709/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9817 - accuracy: 0.2121 - val_loss: 1.8838 - val_accuracy: 0.2136\n",
      "Epoch 710/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9635 - accuracy: 0.2182 - val_loss: 1.8458 - val_accuracy: 0.2227\n",
      "Epoch 711/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8177 - accuracy: 0.2667 - val_loss: 1.8476 - val_accuracy: 0.2227\n",
      "Epoch 712/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8076 - accuracy: 0.2833 - val_loss: 1.7907 - val_accuracy: 0.2682\n",
      "Epoch 713/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9005 - accuracy: 0.2303 - val_loss: 1.7994 - val_accuracy: 0.2955\n",
      "Epoch 714/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8393 - accuracy: 0.2621 - val_loss: 1.9002 - val_accuracy: 0.2227\n",
      "Epoch 715/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8874 - accuracy: 0.2394 - val_loss: 1.8348 - val_accuracy: 0.1955\n",
      "Epoch 716/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8680 - accuracy: 0.2455 - val_loss: 1.8148 - val_accuracy: 0.2636\n",
      "Epoch 717/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8395 - accuracy: 0.2682 - val_loss: 1.8053 - val_accuracy: 0.2500\n",
      "Epoch 718/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8661 - accuracy: 0.2455 - val_loss: 1.9549 - val_accuracy: 0.1182\n",
      "Epoch 719/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9147 - accuracy: 0.2242 - val_loss: 1.8193 - val_accuracy: 0.2409\n",
      "Epoch 720/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8377 - accuracy: 0.2591 - val_loss: 1.8375 - val_accuracy: 0.2682\n",
      "Epoch 721/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8132 - accuracy: 0.2652 - val_loss: 1.8810 - val_accuracy: 0.2045\n",
      "Epoch 722/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7894 - accuracy: 0.2879 - val_loss: 1.7478 - val_accuracy: 0.2955\n",
      "Epoch 723/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7927 - accuracy: 0.2848 - val_loss: 1.9130 - val_accuracy: 0.1773\n",
      "Epoch 724/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8694 - accuracy: 0.2348 - val_loss: 1.8980 - val_accuracy: 0.2682\n",
      "Epoch 725/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8018 - accuracy: 0.2682 - val_loss: 1.7669 - val_accuracy: 0.3273\n",
      "Epoch 726/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8545 - accuracy: 0.2803 - val_loss: 1.9593 - val_accuracy: 0.2136\n",
      "Epoch 727/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8058 - accuracy: 0.2894 - val_loss: 1.7479 - val_accuracy: 0.2727\n",
      "Epoch 728/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7811 - accuracy: 0.2955 - val_loss: 1.7951 - val_accuracy: 0.2455\n",
      "Epoch 729/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8314 - accuracy: 0.2727 - val_loss: 1.7568 - val_accuracy: 0.2682\n",
      "Epoch 730/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7740 - accuracy: 0.3045 - val_loss: 1.8079 - val_accuracy: 0.2909\n",
      "Epoch 731/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7646 - accuracy: 0.3015 - val_loss: 1.8044 - val_accuracy: 0.2045\n",
      "Epoch 732/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8305 - accuracy: 0.2621 - val_loss: 1.7347 - val_accuracy: 0.2636\n",
      "Epoch 733/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8297 - accuracy: 0.2455 - val_loss: 1.8314 - val_accuracy: 0.1955\n",
      "Epoch 734/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7867 - accuracy: 0.2788 - val_loss: 1.7228 - val_accuracy: 0.2727\n",
      "Epoch 735/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7415 - accuracy: 0.2924 - val_loss: 1.7277 - val_accuracy: 0.2864\n",
      "Epoch 736/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7637 - accuracy: 0.2985 - val_loss: 1.7260 - val_accuracy: 0.3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 737/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7803 - accuracy: 0.2788 - val_loss: 1.8993 - val_accuracy: 0.1682\n",
      "Epoch 738/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8013 - accuracy: 0.2591 - val_loss: 1.7905 - val_accuracy: 0.1955\n",
      "Epoch 739/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7658 - accuracy: 0.2924 - val_loss: 1.7677 - val_accuracy: 0.2682\n",
      "Epoch 740/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7333 - accuracy: 0.3182 - val_loss: 1.7255 - val_accuracy: 0.3591\n",
      "Epoch 741/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7477 - accuracy: 0.3182 - val_loss: 1.8616 - val_accuracy: 0.2273\n",
      "Epoch 742/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7405 - accuracy: 0.2985 - val_loss: 1.7162 - val_accuracy: 0.3318\n",
      "Epoch 743/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7297 - accuracy: 0.2864 - val_loss: 1.7865 - val_accuracy: 0.2045\n",
      "Epoch 744/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7348 - accuracy: 0.2803 - val_loss: 1.7078 - val_accuracy: 0.3727\n",
      "Epoch 745/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7427 - accuracy: 0.2848 - val_loss: 1.7097 - val_accuracy: 0.2727\n",
      "Epoch 746/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7820 - accuracy: 0.2742 - val_loss: 1.8263 - val_accuracy: 0.2045\n",
      "Epoch 747/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7347 - accuracy: 0.2985 - val_loss: 1.7303 - val_accuracy: 0.2409\n",
      "Epoch 748/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7755 - accuracy: 0.2470 - val_loss: 1.7118 - val_accuracy: 0.2682\n",
      "Epoch 749/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7575 - accuracy: 0.2894 - val_loss: 1.6712 - val_accuracy: 0.4091\n",
      "Epoch 750/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7418 - accuracy: 0.2864 - val_loss: 1.7926 - val_accuracy: 0.2364\n",
      "Epoch 751/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7469 - accuracy: 0.2682 - val_loss: 1.8388 - val_accuracy: 0.2045\n",
      "Epoch 752/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8538 - accuracy: 0.2409 - val_loss: 2.0759 - val_accuracy: 0.1773\n",
      "Epoch 753/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7907 - accuracy: 0.2909 - val_loss: 1.8813 - val_accuracy: 0.2500\n",
      "Epoch 754/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7412 - accuracy: 0.3273 - val_loss: 1.7209 - val_accuracy: 0.2682\n",
      "Epoch 755/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7156 - accuracy: 0.3030 - val_loss: 1.6822 - val_accuracy: 0.2818\n",
      "Epoch 756/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7017 - accuracy: 0.3152 - val_loss: 1.7111 - val_accuracy: 0.2818\n",
      "Epoch 757/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7045 - accuracy: 0.2894 - val_loss: 1.6656 - val_accuracy: 0.2818\n",
      "Epoch 758/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7543 - accuracy: 0.2864 - val_loss: 2.0611 - val_accuracy: 0.1227\n",
      "Epoch 759/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7601 - accuracy: 0.2606 - val_loss: 1.9811 - val_accuracy: 0.1909\n",
      "Epoch 760/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7236 - accuracy: 0.3061 - val_loss: 1.7355 - val_accuracy: 0.2727\n",
      "Epoch 761/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7007 - accuracy: 0.3091 - val_loss: 1.6767 - val_accuracy: 0.2364\n",
      "Epoch 762/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7710 - accuracy: 0.2409 - val_loss: 1.6730 - val_accuracy: 0.2773\n",
      "Epoch 763/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7291 - accuracy: 0.2742 - val_loss: 1.6800 - val_accuracy: 0.2955\n",
      "Epoch 764/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7083 - accuracy: 0.3045 - val_loss: 1.6830 - val_accuracy: 0.3318\n",
      "Epoch 765/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7273 - accuracy: 0.2894 - val_loss: 1.6953 - val_accuracy: 0.2409\n",
      "Epoch 766/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6996 - accuracy: 0.2924 - val_loss: 1.8234 - val_accuracy: 0.2045\n",
      "Epoch 767/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7077 - accuracy: 0.2818 - val_loss: 1.7876 - val_accuracy: 0.2273\n",
      "Epoch 768/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6919 - accuracy: 0.3303 - val_loss: 1.6537 - val_accuracy: 0.3409\n",
      "Epoch 769/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7653 - accuracy: 0.2485 - val_loss: 1.6771 - val_accuracy: 0.2682\n",
      "Epoch 770/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7111 - accuracy: 0.2955 - val_loss: 1.7191 - val_accuracy: 0.2545\n",
      "Epoch 771/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6952 - accuracy: 0.3273 - val_loss: 1.7562 - val_accuracy: 0.2455\n",
      "Epoch 772/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7417 - accuracy: 0.2515 - val_loss: 1.8244 - val_accuracy: 0.2636\n",
      "Epoch 773/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7014 - accuracy: 0.2924 - val_loss: 1.7501 - val_accuracy: 0.2273\n",
      "Epoch 774/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7130 - accuracy: 0.3076 - val_loss: 1.6570 - val_accuracy: 0.3227\n",
      "Epoch 775/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6857 - accuracy: 0.2939 - val_loss: 1.9514 - val_accuracy: 0.1500\n",
      "Epoch 776/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7257 - accuracy: 0.2894 - val_loss: 1.6905 - val_accuracy: 0.2727\n",
      "Epoch 777/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7907 - accuracy: 0.2485 - val_loss: 1.7973 - val_accuracy: 0.2909\n",
      "Epoch 778/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7092 - accuracy: 0.2788 - val_loss: 1.6488 - val_accuracy: 0.3091\n",
      "Epoch 779/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6868 - accuracy: 0.2864 - val_loss: 1.6683 - val_accuracy: 0.3091\n",
      "Epoch 780/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6977 - accuracy: 0.3076 - val_loss: 1.7880 - val_accuracy: 0.2273\n",
      "Epoch 781/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7647 - accuracy: 0.2606 - val_loss: 1.8426 - val_accuracy: 0.2773\n",
      "Epoch 782/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7135 - accuracy: 0.2758 - val_loss: 1.7176 - val_accuracy: 0.2091\n",
      "Epoch 783/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7384 - accuracy: 0.2833 - val_loss: 1.8867 - val_accuracy: 0.1591\n",
      "Epoch 784/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7344 - accuracy: 0.3106 - val_loss: 1.6715 - val_accuracy: 0.3136\n",
      "Epoch 785/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6622 - accuracy: 0.3076 - val_loss: 1.6518 - val_accuracy: 0.2727\n",
      "Epoch 786/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6507 - accuracy: 0.3045 - val_loss: 1.6429 - val_accuracy: 0.2455\n",
      "Epoch 787/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6648 - accuracy: 0.3091 - val_loss: 1.6457 - val_accuracy: 0.2864\n",
      "Epoch 788/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6392 - accuracy: 0.3515 - val_loss: 1.6317 - val_accuracy: 0.3136\n",
      "Epoch 789/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6430 - accuracy: 0.3455 - val_loss: 1.6402 - val_accuracy: 0.2318\n",
      "Epoch 790/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7410 - accuracy: 0.2409 - val_loss: 1.6369 - val_accuracy: 0.3136\n",
      "Epoch 791/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7828 - accuracy: 0.2636 - val_loss: 1.6233 - val_accuracy: 0.3000\n",
      "Epoch 792/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6731 - accuracy: 0.3000 - val_loss: 1.6253 - val_accuracy: 0.2409\n",
      "Epoch 793/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6750 - accuracy: 0.3121 - val_loss: 1.6225 - val_accuracy: 0.3591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 794/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6920 - accuracy: 0.3045 - val_loss: 1.6183 - val_accuracy: 0.2909\n",
      "Epoch 795/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6977 - accuracy: 0.3076 - val_loss: 2.4510 - val_accuracy: 0.0909\n",
      "Epoch 796/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8560 - accuracy: 0.2318 - val_loss: 1.6317 - val_accuracy: 0.3364\n",
      "Epoch 797/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6716 - accuracy: 0.2909 - val_loss: 1.6181 - val_accuracy: 0.2864\n",
      "Epoch 798/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6390 - accuracy: 0.3106 - val_loss: 1.8263 - val_accuracy: 0.2591\n",
      "Epoch 799/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6455 - accuracy: 0.3258 - val_loss: 1.6706 - val_accuracy: 0.2818\n",
      "Epoch 800/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7035 - accuracy: 0.2909 - val_loss: 1.6021 - val_accuracy: 0.3182\n",
      "Epoch 801/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6844 - accuracy: 0.3212 - val_loss: 1.6478 - val_accuracy: 0.2273\n",
      "Epoch 802/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7387 - accuracy: 0.2545 - val_loss: 2.1098 - val_accuracy: 0.1500\n",
      "Epoch 803/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8534 - accuracy: 0.2439 - val_loss: 1.6331 - val_accuracy: 0.2591\n",
      "Epoch 804/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6309 - accuracy: 0.3485 - val_loss: 1.6226 - val_accuracy: 0.3455\n",
      "Epoch 805/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6715 - accuracy: 0.3424 - val_loss: 1.6425 - val_accuracy: 0.2818\n",
      "Epoch 806/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7824 - accuracy: 0.2530 - val_loss: 1.6090 - val_accuracy: 0.2864\n",
      "Epoch 807/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6667 - accuracy: 0.2894 - val_loss: 1.6014 - val_accuracy: 0.2636\n",
      "Epoch 808/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6600 - accuracy: 0.3258 - val_loss: 1.6077 - val_accuracy: 0.2773\n",
      "Epoch 809/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6821 - accuracy: 0.3015 - val_loss: 1.7679 - val_accuracy: 0.2091\n",
      "Epoch 810/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7205 - accuracy: 0.2667 - val_loss: 1.6192 - val_accuracy: 0.3045\n",
      "Epoch 811/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6334 - accuracy: 0.3015 - val_loss: 1.8231 - val_accuracy: 0.2727\n",
      "Epoch 812/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8079 - accuracy: 0.2485 - val_loss: 1.6631 - val_accuracy: 0.3091\n",
      "Epoch 813/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6454 - accuracy: 0.3045 - val_loss: 1.6310 - val_accuracy: 0.2682\n",
      "Epoch 814/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7225 - accuracy: 0.2818 - val_loss: 1.6221 - val_accuracy: 0.3091\n",
      "Epoch 815/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6420 - accuracy: 0.3333 - val_loss: 1.6023 - val_accuracy: 0.3227\n",
      "Epoch 816/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7223 - accuracy: 0.2545 - val_loss: 1.6160 - val_accuracy: 0.3227\n",
      "Epoch 817/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6653 - accuracy: 0.3030 - val_loss: 1.6198 - val_accuracy: 0.2500\n",
      "Epoch 818/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6221 - accuracy: 0.3318 - val_loss: 1.5877 - val_accuracy: 0.2773\n",
      "Epoch 819/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6409 - accuracy: 0.3091 - val_loss: 1.6966 - val_accuracy: 0.2364\n",
      "Epoch 820/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6174 - accuracy: 0.3561 - val_loss: 1.5983 - val_accuracy: 0.3227\n",
      "Epoch 821/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6249 - accuracy: 0.3091 - val_loss: 1.5897 - val_accuracy: 0.3818\n",
      "Epoch 822/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6026 - accuracy: 0.3515 - val_loss: 1.6709 - val_accuracy: 0.2273\n",
      "Epoch 823/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.8000 - accuracy: 0.2576 - val_loss: 1.6899 - val_accuracy: 0.2591\n",
      "Epoch 824/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7054 - accuracy: 0.2758 - val_loss: 1.6171 - val_accuracy: 0.2182\n",
      "Epoch 825/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7113 - accuracy: 0.2727 - val_loss: 1.7953 - val_accuracy: 0.2591\n",
      "Epoch 826/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7446 - accuracy: 0.2606 - val_loss: 1.6965 - val_accuracy: 0.2273\n",
      "Epoch 827/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6527 - accuracy: 0.2818 - val_loss: 1.5802 - val_accuracy: 0.2955\n",
      "Epoch 828/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6400 - accuracy: 0.3212 - val_loss: 1.8228 - val_accuracy: 0.3091\n",
      "Epoch 829/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7910 - accuracy: 0.2712 - val_loss: 1.8583 - val_accuracy: 0.3000\n",
      "Epoch 830/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6487 - accuracy: 0.3379 - val_loss: 1.6489 - val_accuracy: 0.2227\n",
      "Epoch 831/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6515 - accuracy: 0.3061 - val_loss: 1.7364 - val_accuracy: 0.2045\n",
      "Epoch 832/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6193 - accuracy: 0.2985 - val_loss: 1.6169 - val_accuracy: 0.2545\n",
      "Epoch 833/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7439 - accuracy: 0.2773 - val_loss: 1.6284 - val_accuracy: 0.2227\n",
      "Epoch 834/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6098 - accuracy: 0.3076 - val_loss: 1.8051 - val_accuracy: 0.2227\n",
      "Epoch 835/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6268 - accuracy: 0.3333 - val_loss: 1.7216 - val_accuracy: 0.3227\n",
      "Epoch 836/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6180 - accuracy: 0.3258 - val_loss: 1.6093 - val_accuracy: 0.2500\n",
      "Epoch 837/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5941 - accuracy: 0.3273 - val_loss: 1.5773 - val_accuracy: 0.2955\n",
      "Epoch 838/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5804 - accuracy: 0.3500 - val_loss: 1.8305 - val_accuracy: 0.2182\n",
      "Epoch 839/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6483 - accuracy: 0.2970 - val_loss: 1.6116 - val_accuracy: 0.2364\n",
      "Epoch 840/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6181 - accuracy: 0.3470 - val_loss: 1.6454 - val_accuracy: 0.2545\n",
      "Epoch 841/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6162 - accuracy: 0.3091 - val_loss: 1.5726 - val_accuracy: 0.3227\n",
      "Epoch 842/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6001 - accuracy: 0.3303 - val_loss: 1.5804 - val_accuracy: 0.2636\n",
      "Epoch 843/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7893 - accuracy: 0.2500 - val_loss: 2.0474 - val_accuracy: 0.1455\n",
      "Epoch 844/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6979 - accuracy: 0.2758 - val_loss: 1.6899 - val_accuracy: 0.2136\n",
      "Epoch 845/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6145 - accuracy: 0.3409 - val_loss: 1.5897 - val_accuracy: 0.2955\n",
      "Epoch 846/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6246 - accuracy: 0.3561 - val_loss: 1.7789 - val_accuracy: 0.2136\n",
      "Epoch 847/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6476 - accuracy: 0.3045 - val_loss: 1.5705 - val_accuracy: 0.2682\n",
      "Epoch 848/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5900 - accuracy: 0.3258 - val_loss: 1.6138 - val_accuracy: 0.3364\n",
      "Epoch 849/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5973 - accuracy: 0.3394 - val_loss: 1.5742 - val_accuracy: 0.2864\n",
      "Epoch 850/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6365 - accuracy: 0.3364 - val_loss: 1.7329 - val_accuracy: 0.1864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 851/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6906 - accuracy: 0.2773 - val_loss: 1.6818 - val_accuracy: 0.2273\n",
      "Epoch 852/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5910 - accuracy: 0.3318 - val_loss: 1.5573 - val_accuracy: 0.2818\n",
      "Epoch 853/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5945 - accuracy: 0.3409 - val_loss: 1.6578 - val_accuracy: 0.2318\n",
      "Epoch 854/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6084 - accuracy: 0.3197 - val_loss: 1.5546 - val_accuracy: 0.2818\n",
      "Epoch 855/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6559 - accuracy: 0.2909 - val_loss: 1.5556 - val_accuracy: 0.3500\n",
      "Epoch 856/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6805 - accuracy: 0.3136 - val_loss: 1.5577 - val_accuracy: 0.3500\n",
      "Epoch 857/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5894 - accuracy: 0.3364 - val_loss: 1.5567 - val_accuracy: 0.3000\n",
      "Epoch 858/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5649 - accuracy: 0.3621 - val_loss: 1.5472 - val_accuracy: 0.3136\n",
      "Epoch 859/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5470 - accuracy: 0.3515 - val_loss: 1.5472 - val_accuracy: 0.2864\n",
      "Epoch 860/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5533 - accuracy: 0.3576 - val_loss: 1.5381 - val_accuracy: 0.2727\n",
      "Epoch 861/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6923 - accuracy: 0.2727 - val_loss: 1.5628 - val_accuracy: 0.2773\n",
      "Epoch 862/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5851 - accuracy: 0.3485 - val_loss: 1.5552 - val_accuracy: 0.2545\n",
      "Epoch 863/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6114 - accuracy: 0.3152 - val_loss: 1.5491 - val_accuracy: 0.3182\n",
      "Epoch 864/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5508 - accuracy: 0.3636 - val_loss: 1.5532 - val_accuracy: 0.2455\n",
      "Epoch 865/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6304 - accuracy: 0.2924 - val_loss: 1.6599 - val_accuracy: 0.2409\n",
      "Epoch 866/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6132 - accuracy: 0.3106 - val_loss: 1.5431 - val_accuracy: 0.3136\n",
      "Epoch 867/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6045 - accuracy: 0.3106 - val_loss: 1.6417 - val_accuracy: 0.2000\n",
      "Epoch 868/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5861 - accuracy: 0.3273 - val_loss: 1.6065 - val_accuracy: 0.2591\n",
      "Epoch 869/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5708 - accuracy: 0.3470 - val_loss: 1.5381 - val_accuracy: 0.3227\n",
      "Epoch 870/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6196 - accuracy: 0.3242 - val_loss: 1.7799 - val_accuracy: 0.2182\n",
      "Epoch 871/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6156 - accuracy: 0.2985 - val_loss: 1.6464 - val_accuracy: 0.2091\n",
      "Epoch 872/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6165 - accuracy: 0.3061 - val_loss: 1.5307 - val_accuracy: 0.2682\n",
      "Epoch 873/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5969 - accuracy: 0.3212 - val_loss: 1.5361 - val_accuracy: 0.2773\n",
      "Epoch 874/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7834 - accuracy: 0.2803 - val_loss: 1.5921 - val_accuracy: 0.2318\n",
      "Epoch 875/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6656 - accuracy: 0.3197 - val_loss: 1.5454 - val_accuracy: 0.2909\n",
      "Epoch 876/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6021 - accuracy: 0.3258 - val_loss: 1.5400 - val_accuracy: 0.2818\n",
      "Epoch 877/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6585 - accuracy: 0.3318 - val_loss: 1.8442 - val_accuracy: 0.2045\n",
      "Epoch 878/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7004 - accuracy: 0.2727 - val_loss: 1.6125 - val_accuracy: 0.2455\n",
      "Epoch 879/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6676 - accuracy: 0.2758 - val_loss: 1.5845 - val_accuracy: 0.2182\n",
      "Epoch 880/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5817 - accuracy: 0.2985 - val_loss: 1.5296 - val_accuracy: 0.3591\n",
      "Epoch 881/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5564 - accuracy: 0.3515 - val_loss: 1.5339 - val_accuracy: 0.2864\n",
      "Epoch 882/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5480 - accuracy: 0.3273 - val_loss: 1.5816 - val_accuracy: 0.2318\n",
      "Epoch 883/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5893 - accuracy: 0.3485 - val_loss: 1.5505 - val_accuracy: 0.2545\n",
      "Epoch 884/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6156 - accuracy: 0.2939 - val_loss: 1.5625 - val_accuracy: 0.2818\n",
      "Epoch 885/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5389 - accuracy: 0.3561 - val_loss: 1.7808 - val_accuracy: 0.1727\n",
      "Epoch 886/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5980 - accuracy: 0.3030 - val_loss: 1.5813 - val_accuracy: 0.2727\n",
      "Epoch 887/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5509 - accuracy: 0.3591 - val_loss: 1.6219 - val_accuracy: 0.2818\n",
      "Epoch 888/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7620 - accuracy: 0.2697 - val_loss: 2.4843 - val_accuracy: 0.1045\n",
      "Epoch 889/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 2.0545 - accuracy: 0.2136 - val_loss: 1.8113 - val_accuracy: 0.1636\n",
      "Epoch 890/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6470 - accuracy: 0.2894 - val_loss: 1.5634 - val_accuracy: 0.2500\n",
      "Epoch 891/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5734 - accuracy: 0.3485 - val_loss: 1.5509 - val_accuracy: 0.2591\n",
      "Epoch 892/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5701 - accuracy: 0.3606 - val_loss: 1.5194 - val_accuracy: 0.2455\n",
      "Epoch 893/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6485 - accuracy: 0.3000 - val_loss: 2.0201 - val_accuracy: 0.2682\n",
      "Epoch 894/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5551 - accuracy: 0.3545 - val_loss: 1.5296 - val_accuracy: 0.3182\n",
      "Epoch 895/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5420 - accuracy: 0.3364 - val_loss: 1.5503 - val_accuracy: 0.3545\n",
      "Epoch 896/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5692 - accuracy: 0.3545 - val_loss: 1.6002 - val_accuracy: 0.2773\n",
      "Epoch 897/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5581 - accuracy: 0.3439 - val_loss: 1.5158 - val_accuracy: 0.3818\n",
      "Epoch 898/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5656 - accuracy: 0.3545 - val_loss: 1.5266 - val_accuracy: 0.3182\n",
      "Epoch 899/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5427 - accuracy: 0.3712 - val_loss: 1.6382 - val_accuracy: 0.2364\n",
      "Epoch 900/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5898 - accuracy: 0.3455 - val_loss: 1.6417 - val_accuracy: 0.2818\n",
      "Epoch 901/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5670 - accuracy: 0.3000 - val_loss: 1.5263 - val_accuracy: 0.3682\n",
      "Epoch 902/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6550 - accuracy: 0.3045 - val_loss: 1.8036 - val_accuracy: 0.1818\n",
      "Epoch 903/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5950 - accuracy: 0.3424 - val_loss: 1.5850 - val_accuracy: 0.2636\n",
      "Epoch 904/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5551 - accuracy: 0.3409 - val_loss: 1.5180 - val_accuracy: 0.2955\n",
      "Epoch 905/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6944 - accuracy: 0.2848 - val_loss: 1.6765 - val_accuracy: 0.2273\n",
      "Epoch 906/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5571 - accuracy: 0.3561 - val_loss: 1.5256 - val_accuracy: 0.3273\n",
      "Epoch 907/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6099 - accuracy: 0.3242 - val_loss: 1.5331 - val_accuracy: 0.2773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 908/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6103 - accuracy: 0.3409 - val_loss: 1.5861 - val_accuracy: 0.2273\n",
      "Epoch 909/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5776 - accuracy: 0.3576 - val_loss: 1.8909 - val_accuracy: 0.2636\n",
      "Epoch 910/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6128 - accuracy: 0.3121 - val_loss: 1.6024 - val_accuracy: 0.2227\n",
      "Epoch 911/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5932 - accuracy: 0.3076 - val_loss: 1.8015 - val_accuracy: 0.1818\n",
      "Epoch 912/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6171 - accuracy: 0.3061 - val_loss: 1.6950 - val_accuracy: 0.3318\n",
      "Epoch 913/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5779 - accuracy: 0.3424 - val_loss: 1.6593 - val_accuracy: 0.3591\n",
      "Epoch 914/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6209 - accuracy: 0.2833 - val_loss: 1.5588 - val_accuracy: 0.2727\n",
      "Epoch 915/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5353 - accuracy: 0.3636 - val_loss: 1.5787 - val_accuracy: 0.3091\n",
      "Epoch 916/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5432 - accuracy: 0.3500 - val_loss: 1.5043 - val_accuracy: 0.3045\n",
      "Epoch 917/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5090 - accuracy: 0.3636 - val_loss: 1.5704 - val_accuracy: 0.4091\n",
      "Epoch 918/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6307 - accuracy: 0.3182 - val_loss: 1.5213 - val_accuracy: 0.2682\n",
      "Epoch 919/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6996 - accuracy: 0.2439 - val_loss: 2.2189 - val_accuracy: 0.0955\n",
      "Epoch 920/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.9188 - accuracy: 0.2364 - val_loss: 1.5173 - val_accuracy: 0.3545\n",
      "Epoch 921/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5732 - accuracy: 0.3515 - val_loss: 1.5061 - val_accuracy: 0.3182\n",
      "Epoch 922/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5886 - accuracy: 0.3227 - val_loss: 1.6059 - val_accuracy: 0.2182\n",
      "Epoch 923/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5269 - accuracy: 0.3682 - val_loss: 1.5571 - val_accuracy: 0.3227\n",
      "Epoch 924/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5775 - accuracy: 0.3379 - val_loss: 1.5439 - val_accuracy: 0.2682\n",
      "Epoch 925/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5847 - accuracy: 0.3455 - val_loss: 1.5312 - val_accuracy: 0.2545\n",
      "Epoch 926/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5802 - accuracy: 0.3318 - val_loss: 1.5964 - val_accuracy: 0.2227\n",
      "Epoch 927/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5812 - accuracy: 0.3121 - val_loss: 1.5028 - val_accuracy: 0.3318\n",
      "Epoch 928/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6864 - accuracy: 0.2788 - val_loss: 1.5484 - val_accuracy: 0.2227\n",
      "Epoch 929/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5982 - accuracy: 0.3212 - val_loss: 1.5550 - val_accuracy: 0.2455\n",
      "Epoch 930/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6054 - accuracy: 0.2833 - val_loss: 1.5032 - val_accuracy: 0.2682\n",
      "Epoch 931/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6568 - accuracy: 0.2970 - val_loss: 1.6912 - val_accuracy: 0.3136\n",
      "Epoch 932/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6640 - accuracy: 0.3000 - val_loss: 1.5387 - val_accuracy: 0.3409\n",
      "Epoch 933/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5622 - accuracy: 0.3530 - val_loss: 1.5005 - val_accuracy: 0.3318\n",
      "Epoch 934/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5403 - accuracy: 0.3364 - val_loss: 1.5931 - val_accuracy: 0.3000\n",
      "Epoch 935/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5696 - accuracy: 0.3121 - val_loss: 1.8040 - val_accuracy: 0.3227\n",
      "Epoch 936/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7205 - accuracy: 0.2576 - val_loss: 1.4964 - val_accuracy: 0.2773\n",
      "Epoch 937/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6668 - accuracy: 0.2727 - val_loss: 1.7305 - val_accuracy: 0.3636\n",
      "Epoch 938/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6625 - accuracy: 0.3152 - val_loss: 1.5060 - val_accuracy: 0.3455\n",
      "Epoch 939/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5902 - accuracy: 0.3152 - val_loss: 1.5402 - val_accuracy: 0.2818\n",
      "Epoch 940/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5186 - accuracy: 0.3409 - val_loss: 1.5318 - val_accuracy: 0.3045\n",
      "Epoch 941/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5986 - accuracy: 0.3242 - val_loss: 1.8912 - val_accuracy: 0.2591\n",
      "Epoch 942/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6761 - accuracy: 0.2606 - val_loss: 1.6070 - val_accuracy: 0.2455\n",
      "Epoch 943/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5497 - accuracy: 0.3364 - val_loss: 1.5057 - val_accuracy: 0.3045\n",
      "Epoch 944/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5323 - accuracy: 0.3273 - val_loss: 1.5146 - val_accuracy: 0.2818\n",
      "Epoch 945/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5237 - accuracy: 0.3545 - val_loss: 1.4857 - val_accuracy: 0.3273\n",
      "Epoch 946/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5032 - accuracy: 0.3606 - val_loss: 1.5126 - val_accuracy: 0.3000\n",
      "Epoch 947/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5248 - accuracy: 0.3470 - val_loss: 1.5421 - val_accuracy: 0.3227\n",
      "Epoch 948/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5177 - accuracy: 0.3333 - val_loss: 1.4959 - val_accuracy: 0.3182\n",
      "Epoch 949/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5290 - accuracy: 0.3439 - val_loss: 1.5316 - val_accuracy: 0.2818\n",
      "Epoch 950/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5732 - accuracy: 0.3303 - val_loss: 1.5063 - val_accuracy: 0.3091\n",
      "Epoch 951/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5380 - accuracy: 0.3318 - val_loss: 1.6506 - val_accuracy: 0.2182\n",
      "Epoch 952/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5182 - accuracy: 0.3500 - val_loss: 1.6434 - val_accuracy: 0.3364\n",
      "Epoch 953/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5658 - accuracy: 0.3439 - val_loss: 1.4902 - val_accuracy: 0.3318\n",
      "Epoch 954/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5858 - accuracy: 0.3227 - val_loss: 1.5566 - val_accuracy: 0.2227\n",
      "Epoch 955/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5558 - accuracy: 0.3152 - val_loss: 1.4818 - val_accuracy: 0.2909\n",
      "Epoch 956/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5277 - accuracy: 0.3394 - val_loss: 1.6018 - val_accuracy: 0.2318\n",
      "Epoch 957/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5699 - accuracy: 0.3424 - val_loss: 1.5327 - val_accuracy: 0.3000\n",
      "Epoch 958/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5251 - accuracy: 0.3409 - val_loss: 1.5078 - val_accuracy: 0.4500\n",
      "Epoch 959/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5963 - accuracy: 0.3485 - val_loss: 1.5545 - val_accuracy: 0.2500\n",
      "Epoch 960/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5831 - accuracy: 0.3288 - val_loss: 1.8651 - val_accuracy: 0.2455\n",
      "Epoch 961/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7659 - accuracy: 0.2697 - val_loss: 2.0636 - val_accuracy: 0.1318\n",
      "Epoch 962/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6924 - accuracy: 0.2985 - val_loss: 1.5877 - val_accuracy: 0.2318\n",
      "Epoch 963/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5317 - accuracy: 0.3576 - val_loss: 1.4927 - val_accuracy: 0.3091\n",
      "Epoch 964/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6248 - accuracy: 0.2955 - val_loss: 1.5017 - val_accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 965/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5583 - accuracy: 0.3318 - val_loss: 1.6360 - val_accuracy: 0.1955\n",
      "Epoch 966/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5078 - accuracy: 0.3500 - val_loss: 1.5494 - val_accuracy: 0.2182\n",
      "Epoch 967/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5336 - accuracy: 0.3136 - val_loss: 1.4997 - val_accuracy: 0.2864\n",
      "Epoch 968/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5197 - accuracy: 0.3485 - val_loss: 1.4903 - val_accuracy: 0.3136\n",
      "Epoch 969/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.4932 - accuracy: 0.3652 - val_loss: 1.4891 - val_accuracy: 0.2727\n",
      "Epoch 970/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7503 - accuracy: 0.2712 - val_loss: 1.5018 - val_accuracy: 0.3727\n",
      "Epoch 971/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5864 - accuracy: 0.3121 - val_loss: 1.6054 - val_accuracy: 0.2227\n",
      "Epoch 972/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6418 - accuracy: 0.2909 - val_loss: 1.8552 - val_accuracy: 0.2545\n",
      "Epoch 973/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5956 - accuracy: 0.3045 - val_loss: 1.4981 - val_accuracy: 0.3182\n",
      "Epoch 974/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5280 - accuracy: 0.3712 - val_loss: 1.7642 - val_accuracy: 0.3227\n",
      "Epoch 975/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6283 - accuracy: 0.3091 - val_loss: 1.4993 - val_accuracy: 0.2909\n",
      "Epoch 976/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.4917 - accuracy: 0.3894 - val_loss: 1.4802 - val_accuracy: 0.3182\n",
      "Epoch 977/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.4859 - accuracy: 0.3758 - val_loss: 1.5336 - val_accuracy: 0.2591\n",
      "Epoch 978/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5035 - accuracy: 0.3712 - val_loss: 1.4979 - val_accuracy: 0.2727\n",
      "Epoch 979/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5243 - accuracy: 0.3788 - val_loss: 1.5821 - val_accuracy: 0.2136\n",
      "Epoch 980/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5467 - accuracy: 0.3394 - val_loss: 1.8109 - val_accuracy: 0.3045\n",
      "Epoch 981/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6393 - accuracy: 0.3030 - val_loss: 1.5183 - val_accuracy: 0.3273\n",
      "Epoch 982/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5424 - accuracy: 0.3500 - val_loss: 1.5014 - val_accuracy: 0.2818\n",
      "Epoch 983/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5629 - accuracy: 0.3379 - val_loss: 1.5711 - val_accuracy: 0.3864\n",
      "Epoch 984/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6968 - accuracy: 0.2924 - val_loss: 1.7409 - val_accuracy: 0.2000\n",
      "Epoch 985/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5906 - accuracy: 0.3288 - val_loss: 1.6179 - val_accuracy: 0.2318\n",
      "Epoch 986/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5491 - accuracy: 0.3318 - val_loss: 1.4919 - val_accuracy: 0.2818\n",
      "Epoch 987/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5579 - accuracy: 0.3273 - val_loss: 1.4847 - val_accuracy: 0.3864\n",
      "Epoch 988/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6053 - accuracy: 0.2970 - val_loss: 1.5243 - val_accuracy: 0.2773\n",
      "Epoch 989/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7097 - accuracy: 0.2803 - val_loss: 1.6332 - val_accuracy: 0.2273\n",
      "Epoch 990/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5782 - accuracy: 0.3030 - val_loss: 1.5997 - val_accuracy: 0.2227\n",
      "Epoch 991/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5237 - accuracy: 0.3288 - val_loss: 1.5379 - val_accuracy: 0.2500\n",
      "Epoch 992/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5627 - accuracy: 0.3273 - val_loss: 1.7491 - val_accuracy: 0.2045\n",
      "Epoch 993/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.6468 - accuracy: 0.2818 - val_loss: 1.6567 - val_accuracy: 0.3364\n",
      "Epoch 994/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.7342 - accuracy: 0.2697 - val_loss: 1.6076 - val_accuracy: 0.3000\n",
      "Epoch 995/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5109 - accuracy: 0.3803 - val_loss: 1.4737 - val_accuracy: 0.3136\n",
      "Epoch 996/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.4987 - accuracy: 0.3591 - val_loss: 1.4753 - val_accuracy: 0.3955\n",
      "Epoch 997/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.4741 - accuracy: 0.3894 - val_loss: 1.5312 - val_accuracy: 0.2636\n",
      "Epoch 998/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5508 - accuracy: 0.3197 - val_loss: 1.4918 - val_accuracy: 0.2818\n",
      "Epoch 999/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.4750 - accuracy: 0.3652 - val_loss: 1.5058 - val_accuracy: 0.4045\n",
      "Epoch 1000/1000\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5887 - accuracy: 0.3530 - val_loss: 1.6062 - val_accuracy: 0.3591\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.10      0.17        20\n",
      "           1       0.53      1.00      0.69        20\n",
      "           2       0.23      0.15      0.18        20\n",
      "           3       0.74      0.70      0.72        20\n",
      "           4       0.50      0.30      0.37        20\n",
      "           5       0.00      0.00      0.00        20\n",
      "           6       0.30      1.00      0.46        20\n",
      "           7       0.33      0.25      0.29        20\n",
      "           8       0.10      0.10      0.10        20\n",
      "           9       0.06      0.10      0.08        20\n",
      "          10       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.34       220\n",
      "   macro avg       0.30      0.34      0.28       220\n",
      "weighted avg       0.30      0.34      0.28       220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model = getNetwork()\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.25)\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "report = classification_report(y_test, pred)\n",
    "classification_report_csv(report, \"NN\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Models in C code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp_dg7cwsd/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Neural network with TinyMLGen\n",
    "with open(tasks[choosenIndex] + '/exportedModels/' + 'NNmodel.h', 'w') as f:\n",
    "    f.write(tiny.port(model, optimize=False))\n",
    "\n",
    "# Classifiers with MicroMLGen\n",
    "for name, model in models:\n",
    "    prepath = tasks[choosenIndex] + '/exportedModels/'\n",
    "    path = prepath + name + '.h'\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(port(model, optimize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione Inferance Rate medio (|X_test| = 50/50/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = ['InfTime', 'InfTimeS3', 'InfTimeS6']\n",
    "elem = columns[choosenIndex]\n",
    "csv = read_csv(\"InfTimeReport.csv\")\n",
    "g = sbs.barplot(x=csv['Algoritmo'], y=csv[elem])\n",
    "g.set_yscale(\"log\")\n",
    "plt.ylabel(columns[choosenIndex])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoria occupata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = ['MemOccupata', 'MemOccupataS3', 'MemOccupataS6']\n",
    "elem = columns[choosenIndex]\n",
    "csv = read_csv(\"MemOccupationReport.csv\")\n",
    "g = sbs.barplot(x=csv['Algoritmo'], y=csv[elem])\n",
    "plt.ylabel(elem)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
