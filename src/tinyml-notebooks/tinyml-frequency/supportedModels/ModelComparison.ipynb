{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Comparison for TinyML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "import pickle\n",
    "\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,  classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold,StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dense, Input, concatenate, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from micromlgen import port\n",
    "import tinymlgen as tiny\n",
    "\n",
    "import warnings\n",
    "import sys\n",
    "import seaborn as sbs\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change 'chosenIndex' to change the chosen Test (s/s3/s6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataXPath = ['../data/X.pkl', '../data/XS3.pkl', '../data/XS6.pkl']\n",
    "dataYPath = ['../data/y.pkl', '../data/yS3.pkl', '../data/yS6.pkl']\n",
    "choosenIndex = 2\n",
    "\n",
    "with open(dataXPath[choosenIndex], 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open(dataYPath[choosenIndex], 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3  3\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  6  6  6  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  9  9  9  9  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n"
     ]
    }
   ],
   "source": [
    "# Convert the labels in values like 0...n for the NN tests\n",
    "\n",
    "labels = []\n",
    "uniques = list(np.unique(y))\n",
    "\n",
    "[labels.append(uniques.index(el)) for el in y]\n",
    "\n",
    "y = np.array(labels)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Spotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 42\n",
    "scoring = 'f1_macro'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "\n",
    "#models.append(('XGB', XGBClassifier(random_state=seed)))\n",
    "models.append(('GNB', GaussianNB(var_smoothing=2e-9)))\n",
    "models.append(('LR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression(random_state=seed))])))\n",
    "models.append(('CART' , DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('SVC' , Pipeline([('Scaler', StandardScaler()), ('SVC', SVC(gamma=0.05, random_state=seed))])))\n",
    "models.append(('RF', RandomForestClassifier(random_state=seed, n_estimators = 50)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB - 0,53 0,04\n",
      "LR - 0,49 0,06\n",
      "CART - 0,55 0,05\n",
      "SVC - 0,47 0,05\n",
      "RF - 0,64 0,04\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    # Dividere dati in n = num_folds\n",
    "    kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = np.array([])\n",
    "    for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "        X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "        X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "        model.fit(X_cross_train, y_cross_train)  \n",
    "        y_pred = model.predict(X_cross_test)\n",
    "        f1s = f1_score(y_cross_test, y_pred, average=\"weighted\")\n",
    "        cv_results = np.append(cv_results, [f1s])\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #msg = \"%s - %f - %f\" % (name, cv_results.mean(), cv_results.std())\n",
    "    msg = \"{} - {:.2f} {:.2f}\".format(name, cv_results.mean(), cv_results.std()).replace('.', ',')\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "fig.suptitle('Algorithms Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione dei modelli sul Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model GNB: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85        20\n",
      "           1       1.00      0.90      0.95        20\n",
      "           2       0.18      0.10      0.13        20\n",
      "           3       0.88      0.70      0.78        20\n",
      "           4       0.37      0.50      0.43        20\n",
      "           5       0.00      0.00      0.00        20\n",
      "           6       0.79      0.95      0.86        20\n",
      "           7       0.47      0.70      0.56        20\n",
      "           8       0.54      0.70      0.61        20\n",
      "           9       0.40      0.20      0.27        20\n",
      "          10       0.50      0.75      0.60        20\n",
      "\n",
      "    accuracy                           0.59       220\n",
      "   macro avg       0.53      0.59      0.55       220\n",
      "weighted avg       0.53      0.59      0.55       220\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model LR: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.35      0.44        20\n",
      "           1       0.62      1.00      0.77        20\n",
      "           2       0.43      0.30      0.35        20\n",
      "           3       0.82      0.70      0.76        20\n",
      "           4       0.37      0.55      0.44        20\n",
      "           5       0.11      0.05      0.07        20\n",
      "           6       0.83      1.00      0.91        20\n",
      "           7       0.53      0.45      0.49        20\n",
      "           8       0.48      0.60      0.53        20\n",
      "           9       0.40      0.30      0.34        20\n",
      "          10       0.44      0.55      0.49        20\n",
      "\n",
      "    accuracy                           0.53       220\n",
      "   macro avg       0.51      0.53      0.51       220\n",
      "weighted avg       0.51      0.53      0.51       220\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model CART: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.55      0.65        20\n",
      "           1       0.86      0.95      0.90        20\n",
      "           2       0.22      0.20      0.21        20\n",
      "           3       0.68      0.85      0.76        20\n",
      "           4       0.25      0.15      0.19        20\n",
      "           5       0.24      0.25      0.24        20\n",
      "           6       0.80      0.80      0.80        20\n",
      "           7       0.46      0.60      0.52        20\n",
      "           8       0.41      0.35      0.38        20\n",
      "           9       0.43      0.50      0.47        20\n",
      "          10       0.50      0.55      0.52        20\n",
      "\n",
      "    accuracy                           0.52       220\n",
      "   macro avg       0.51      0.52      0.51       220\n",
      "weighted avg       0.51      0.52      0.51       220\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model SVC: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.35      0.41        20\n",
      "           1       0.57      1.00      0.73        20\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       1.00      0.60      0.75        20\n",
      "           4       0.39      0.70      0.50        20\n",
      "           5       0.00      0.00      0.00        20\n",
      "           6       0.80      1.00      0.89        20\n",
      "           7       0.45      0.75      0.57        20\n",
      "           8       0.54      0.70      0.61        20\n",
      "           9       0.50      0.10      0.17        20\n",
      "          10       0.49      0.85      0.62        20\n",
      "\n",
      "    accuracy                           0.55       220\n",
      "   macro avg       0.48      0.55      0.48       220\n",
      "weighted avg       0.48      0.55      0.48       220\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model RF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91        20\n",
      "           1       1.00      0.95      0.97        20\n",
      "           2       0.50      0.35      0.41        20\n",
      "           3       1.00      0.85      0.92        20\n",
      "           4       0.55      0.30      0.39        20\n",
      "           5       0.43      0.45      0.44        20\n",
      "           6       1.00      0.90      0.95        20\n",
      "           7       0.56      0.70      0.62        20\n",
      "           8       0.52      0.55      0.54        20\n",
      "           9       0.50      0.60      0.55        20\n",
      "          10       0.42      0.55      0.48        20\n",
      "\n",
      "    accuracy                           0.65       220\n",
      "   macro avg       0.66      0.65      0.65       220\n",
      "weighted avg       0.66      0.65      0.65       220\n",
      "\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tasks = ['S', 'S3', 'S6']\n",
    "def classification_report_csv(report, model_name):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    index = 0\n",
    "    row = lines[-4].split('    ')\n",
    "    accuracy = row[-2]\n",
    "    for line in lines[2:-5]:\n",
    "        row = {}\n",
    "        row_data = line.split('      ')\n",
    "        row['class'] = uniques[index]\n",
    "        row['precision'] = float(row_data[2]) \n",
    "        row['recall'] = float(row_data[3]) \n",
    "        row['f1_score'] = float(row_data[4])\n",
    "        row['accuracy'] = accuracy\n",
    "        report_data.append(row)\n",
    "        index += 1\n",
    "    dataframe = pd.DataFrame.from_dict(report_data)\n",
    "    dataframe.to_csv(tasks[choosenIndex]+ '/'+'classification_report' + model_name +  '.csv', index = False)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "for name, model in models:\n",
    "    model.fit(X_train,  y_train)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    print(f\"Model {name}: \")\n",
    "    report = classification_report(y_test, pred_test)\n",
    "    print(report)\n",
    "    classification_report_csv(report, name)\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione Inferance Rate medio (|X_test| = 50/50/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = ['InfTime', 'InfTimeS3', 'InfTimeS6']\n",
    "for c in columns:\n",
    "    csv = read_csv(\"InfTimeReport.csv\")\n",
    "    g = sbs.barplot(x=csv['Algoritmo'], y=csv[c])\n",
    "    g.set_yscale(\"log\")\n",
    "    plt.ylabel(c)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoria occupata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEGCAYAAABYV4NmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAejElEQVR4nO3dfZQdVZ3u8e8zQRBETJBMjAQMYnRWQI0QMS5fQFEMOBpQZBK9Eh00OhBfRmeuoHMv+MIaHEXWoAgTJUPiRQIjIhlvFCMiL45BGomBgJgmyJDcAC1BUBEQeO4ftdtUmtOdk6TrnKTzfNaq1VW/2rtq18nLr2vXPrtkm4iIiOH2F91uQEREjExJMBER0YgkmIiIaEQSTERENCIJJiIiGrFTtxuwrdhrr708ceLEbjcjImK7cuONN/7G9thW+5JgiokTJ9LT09PtZkREbFck3TXYvnSRRUREI5JgIiKiEUkwERHRiCSYiIhoRBJMREQ0IgkmIiIakQQTERGNSIKJiIhGJMFEREQj8k3+iNhqV7/20G43Ydgdes3V3W7Cdq+xOxhJ+0i6StKtklZK+kiJ7ylpqaRV5eeYEpeksyX1Sloh6aDasWaX8qskza7FD5Z0c6lztiQNdY6IiOicJrvIHgc+bnsyMA04SdJk4GTgStuTgCvLNsCRwKSyzAHOhSpZAKcCrwAOAU6tJYxzgffX6k0v8cHOERERHdJYgrG9zvbPy/rvgNuAvYEZwIJSbAFwdFmfASx0ZRkwWtJ44E3AUtvrbT8ALAWml3172F5m28DCAcdqdY6IiOiQjjzklzQReBlwPTDO9rqy6x5gXFnfG7i7Vm1NiQ0VX9MizhDnGNiuOZJ6JPX09fVtwZVFRMRgGk8wknYHLgU+avuh+r5y5+Emzz/UOWzPsz3V9tSxY1u+ziAiIrZQowlG0tOoksuFtr9dwveW7i3Kz/tKfC2wT636hBIbKj6hRXyoc0RERIc0OYpMwPnAbba/VNu1GOgfCTYbuLwWP76MJpsGPFi6ua4AjpA0pjzcPwK4oux7SNK0cq7jBxyr1TkiIqJDmvwezKuAdwM3S1peYp8EzgAukXQCcBdwXNm3BDgK6AUeBt4LYHu9pM8CN5Ryn7G9vqyfCFwA7Ap8rywMcY6IiOiQxhKM7esADbL78BblDZw0yLHmA/NbxHuAA1vE7291joiI6JxMFRMREY1IgomIiEYkwURERCOSYCIiohFJMBER0YgkmIiIaEQSTERENCIJJiIiGpEEExERjUiCiYiIRiTBREREI5JgIiKiEUkwERHRiCSYiIhoRBJMREQ0IgkmIiIa0eQrk+dLuk/SLbXYxZKWl+XX/W+6lDRR0h9r+86r1TlY0s2SeiWdXV6PjKQ9JS2VtKr8HFPiKuV6Ja2QdFBT1xgREYNr8g7mAmB6PWD7b2xPsT0FuBT4dm33Hf37bH+wFj8XeD8wqSz9xzwZuNL2JODKsg1wZK3snFI/IiI6rLEEY/saYH2rfeUu5DjgoqGOIWk8sIftZeWVyguBo8vuGcCCsr5gQHyhK8uA0eU4ERHRQd16BvMa4F7bq2qx/STdJOlqSa8psb2BNbUya0oMYJztdWX9HmBcrc7dg9TZiKQ5knok9fT19W3F5URExEDdSjCz2PjuZR2wr+2XAR8Dvilpj3YPVu5uvLmNsD3P9lTbU8eOHbu51SMiYgg7dfqEknYC3gYc3B+z/SjwaFm/UdIdwAuBtcCEWvUJJQZwr6TxtteVLrD7SnwtsM8gdSIiokO6cQfzBuCXtv/c9SVprKRRZf35VA/oV5cusIckTSvPbY4HLi/VFgOzy/rsAfHjy2iyacCDta60iIjokCaHKV8E/BR4kaQ1kk4ou2by1If7rwVWlGHL3wI+aLt/gMCJwNeBXuAO4HslfgbwRkmrqJLWGSW+BFhdyn+t1I+IiA5rrIvM9qxB4u9pEbuUathyq/I9wIEt4vcDh7eIGzhpM5sbERHDLN/kj4iIRiTBREREI5JgIiKiEUkwERHRiCSYiIhoRBJMREQ0IgkmIiIakQQTERGNSIKJiIhGJMFEREQjkmAiIqIRSTAREdGIJJiIiGhEEkxERDQiCSYiIhqRBBMREY1IgomIiEY0+crk+ZLuk3RLLXaapLWSlpflqNq+UyT1Srpd0ptq8ekl1ivp5Fp8P0nXl/jFknYu8V3Kdm/ZP7Gpa4yIiME1eQdzATC9Rfws21PKsgRA0mRgJnBAqfNVSaMkjQLOAY4EJgOzSlmAz5djvQB4ADihxE8AHijxs0q5iIjosMYSjO1rgPVtFp8BLLL9qO07gV7gkLL02l5t+zFgETBDkoDXA98q9RcAR9eOtaCsfws4vJSPiIgO6sYzmLmSVpQutDEltjdwd63MmhIbLP5s4Le2Hx8Q3+hYZf+DpfxTSJojqUdST19f39ZfWURE/FmnE8y5wP7AFGAdcGaHz78R2/NsT7U9dezYsd1sSkTEiNPRBGP7XttP2H4S+BpVFxjAWmCfWtEJJTZY/H5gtKSdBsQ3OlbZ/6xSPiIiOqijCUbS+NrmMUD/CLPFwMwyAmw/YBLwM+AGYFIZMbYz1UCAxbYNXAUcW+rPBi6vHWt2WT8W+FEpHxERHbTTpotsGUkXAYcBe0laA5wKHCZpCmDg18AHAGyvlHQJcCvwOHCS7SfKceYCVwCjgPm2V5ZTfAJYJOlzwE3A+SV+PvANSb1UgwxmNnWNERExuMYSjO1ZLcLnt4j1lz8dOL1FfAmwpEV8NRu62OrxR4B3bFZjIyJi2OWb/BER0YgkmIiIaEQSTERENCIJJiIiGpEEExERjWgrwUiaJukGSb+X9JikJyQ91HTjIiJi+9XuHcxXgFnAKmBX4H1UsxxHRES01HYXme1eYFSZ6uXfaT0Vf0REBND+Fy0fLlO1LJf0L1QTVeb5TUREDKrdJPHuUnYu8AeqySTf1lSjIiJi+9dugjna9iO2H7L9adsfA/66yYZFRMT2rd0EM7tF7D3D2I6IiBhhhnwGI2kW8E5gP0mLa7ueSfuvQ46IiB3Qph7y/xfVA/292Pjtk78DVjTVqIiI2P4NmWBs3wXcBbyyM82JiIiRIt/kj4iIRjT2TX5J8yXdJ+mWWuwLkn4paYWkyySNLvGJkv4oaXlZzqvVOVjSzZJ6JZ0tSSW+p6SlklaVn2NKXKVcbznPQZvxeURExDBp8pv8F7QosxQ40PZLgF8Bp9T23WF7Slk+WIufC7wfmFSW/mOeDFxpexJwZdkGOLJWdk6pHxERHdZugtnom/yS/n5TdW1fw4CRZrZ/YPvxsrkMmDDUMSSNB/awvcy2gYXA0WX3DGBBWV8wIL7QlWXA6HKciIjooK35Jv/bt/Lcfwt8r7a9n6SbJF0t6TUltjewplZmTYkBjLO9rqzfA4yr1bl7kDobkTRHUo+knr6+vq24lIiIGKituchs31XuYCYC3wZut/3Ylp5U0qeAx4ELS2gdsK/t+yUdDHxH0gHtHs+2JXlz22F7HjAPYOrUqZtdPyIiBtdWgpH0ZuA84A5AVHcbH7D9vaFrtjzWe6immTm8dHth+1Hg0bJ+o6Q7gBcCa9m4G21CiQHcK2m87XWlC+y+El9LdYfVqk5ERHRIu11kZwKvs32Y7UOB1wFnbe7JJE0H/ifwVtsP1+JjJY0q68+nekC/unSBPVSGSQs4Hri8VFvMhilsZg+IH19Gk00DHqx1pUVERIe0O13/78oosn6rqb7NPyhJFwGHAXtJWgOcSjVqbBdgaRltvKyMGHst8BlJfwKeBD5ou3+AwIlUI9J2pXpm03/XdAZwiaQTqL4MelyJLwGOAnqBh4H3tnmNERExjNpNMD2SlgCXAAbeAdwg6W0Atr89sILtWS2Oc36rg9u+FLh0kH09wIEt4vcDh7eIGzhp0CuJiIiOaDfBPB24Fzi0bPdR3VG8hSrhPCXBRETEjq3dUWTpZoqIiM3S7iiyf6e6U9mI7b8d9hZFRMSI0G4X2Xdr608HjgH+3/A3JyIiRop2u8g2egBfRohd10iLIiJiRGh7sssBJgF/OZwNiYiIkaXdZzC/Y+NnMPcAn2ikRRERMSK020X2zKYbEhERI0u7b7Q8RtKzatujJR3dWKsiImK71+4zmFNtP9i/Yfu3VFO/REREtNRugmlVrt0hzhERsQNqN8H0SPqSpP3L8iXgxiYbFhER27d2E8yHgMeAi4FFwCNkQsmIiBhCu6PI/gCc3HBbIiJiBGl3FNlSSaNr22MkXdFYqyIiYrvXbhfZXmXkGAC2HyDf5I+IiCG0m2CelLRv/4ak59FiduWBJM2XdJ+kW2qxPcsd0aryc0yJS9LZknolrZB0UK3O7FJ+laTZtfjBkm4udc4ur1Ue9BwREdE57SaYTwHXSfqGpP8DXEP1+uNNuQCYPiB2MnCl7UnAlWx4tnMk1Rxnk4A5wLlQJQuq79y8AjgEOLWWMM4F3l+rN30T54iIiA5pK8HY/j5wEBtGkR1se5PPYGxfA6wfEJ4BLCjrC4Cja/GFriwDRksaD7wJWGp7femaWwpML/v2sL2svCZ54YBjtTpHRER0yCZHkUnaGXgXcEAJrQR+txXnHGd7XVm/BxhX1vcG7q6VW1NiQ8XXtIgPdY6NSJpDdbfEvvvu26pIRERsoSHvYCRNBm4FDgP+uyyHASvLvq1S7jw2+SynqXPYnmd7qu2pY8eObbIZERE7nE3dwXwZ+DvbS+tBSW8AzgFetwXnvFfSeNvrSjfXfSW+FtinVm5Cia2lSmr1+I9LfEKL8kOdIyIiOmRTz2D2HphcAGz/EHjOFp5zMdA/Emw2cHktfnwZTTYNeLB0c10BHFG+ezMGOAK4oux7SNK0Mnrs+AHHanWOiIjokE3dwfyFpF1sP1oPSnp6G3X7X618GLCXpDVUo8HOAC6RdAJwF3BcKb4EOAroBR4G3gtge72kzwI3lHKfsd0/cOBEqpFquwLfKwtDnCMiIjpkU0liIXCppJNs3wUgaSJwNvCNTR3c9qxBdh3eoqwZZH4z2/OB+S3iPcCBLeL3tzpHRER0zpAJxvbnJM0FrpW0GyDg98AXbX+5Ew3cFhz8jwu73YRhd+MXju92EyJihNtkN5ftrwBfkfTMsr01Q5QjRoxXfflV3W7CsPvJh37S7SZs977y8f/sdhOG3dwz37JF9dqaTblMdHk8MFHSn+vY/vAWnTUiIka8dt9KuQRYBtwMPNlccyIiYqRoN8E83fbHGm1JbBf++zMv7nYTht2+//vmbjchYkRqd7LLb0h6v6TxZabiPcsklBERES21ewfzGPAFqlmV+6ddMfD8JhoVERHbv3YTzMeBF9j+TZONiYiIkaPdLrL+b9dHRES0pd07mD8AyyVdBfx52pgMU46IiMG0m2C+U5aIiIi2tJVgbC+QtCuwr+3bG25TRESMAG09g5H0FmA58P2yPUXS4gbbFRER27l2H/KfBhwC/BbA9nIyRDkiIobQboL5k+0HB8QyZUxERAyq3Yf8KyW9ExglaRLwYeC/mmtWRERs79q9g/kQcADVEOWLgIeAjzbUpoiIGAHaSjC2H7b9Kdsvtz21rD+yJSeU9CJJy2vLQ5I+Kuk0SWtr8aNqdU6R1CvpdklvqsWnl1ivpJNr8f0kXV/iF0vaeUvaGhERW27ILrJNjRSz/dbNPWEZ5jylHH8UsBa4DHgvcJbtLw5ow2RgJtUd1HOBH0p6Ydl9DvBGYA1wg6TFtm8FPl+OtUjSecAJwLmb29aIiNhym3oG80rgbqpuseupXpk8nA4H7rB9lzTooWcAi2w/CtwpqZdqRBtAr+3VAJIWATMk3Qa8HnhnKbOAahRcEkxERAdtqovsOcAngQOBf6W6W/iN7attXz0M559Jlbz6zZW0QtJ8SWNKbG+qJNdvTYkNFn828Fvbjw+IP4WkOZJ6JPX09fVt/dVERMSfDZlgbD9h+/u2ZwPTqCa9/LGkuVt74vJc5K3Af5TQucD+VN1n64Azt/Ycm2J7XnmmNHXs2LFNny4iYoeyyWHKknYB3gzMAiYCZ1M9M9laRwI/t30vQP/Pcs6vAd8tm2uBfWr1JpQYg8TvB0ZL2qncxdTLR0REhwx5ByNpIfBT4CDg02UU2WdtD8d/2LOodY9JGl/bdwxwS1lfDMyUtIuk/YBJwM+AG4BJZcTYzlTdbYttG7gKOLbUnw1cPgztjYiIzbCpO5j/QTVV/0eAD9cexAuw7T225KSSnkH1POcDtfC/SJpC9abMX/fvs71S0iXArcDjwEm2nyjHmQtcAYwC5tteWY71CWCRpM8BNwHnb0k7IyJiyw2ZYGy3+0XMzWL7D1QP4+uxdw9R/nTg9BbxJcCSFvHVbBhpFhERXdBIAomIiEiCiYiIRiTBREREI5JgIiKiEUkwERHRiCSYiIhoRBJMREQ0IgkmIiIakQQTERGNSIKJiIhGJMFEREQjkmAiIqIRSTAREdGIJJiIiGhEEkxERDQiCSYiIhrRtQQj6deSbpa0XFJPie0paamkVeXnmBKXpLMl9UpaIemg2nFml/KrJM2uxQ8ux+8tdfXUVkRERFO6fQfzOttTbE8t2ycDV9qeBFxZtgGOBCaVZQ5wLlQJCTgVeAXVGyxP7U9Kpcz7a/WmN385ERHRr9sJZqAZwIKyvgA4uhZf6MoyYLSk8cCbgKW219t+AFgKTC/79rC9zLaBhbVjRUREB3QzwRj4gaQbJc0psXG215X1e4BxZX1v4O5a3TUlNlR8TYv4RiTNkdQjqaevr29rryciImp26uK5X217raS/BJZK+mV9p21LcpMNsD0PmAcwderURs8VEbGj6dodjO215ed9wGVUz1DuLd1blJ/3leJrgX1q1SeU2FDxCS3iERHRIV1JMJKeIemZ/evAEcAtwGKgfyTYbODysr4YOL6MJpsGPFi60q4AjpA0pjzcPwK4oux7SNK0Mnrs+NqxIiKiA7rVRTYOuKyMHN4J+Kbt70u6AbhE0gnAXcBxpfwS4CigF3gYeC+A7fWSPgvcUMp9xvb6sn4icAGwK/C9skRERId0JcHYXg28tEX8fuDwFnEDJw1yrPnA/BbxHuDArW5sRERskW1tmHJERIwQSTAREdGIJJiIiGhEEkxERDQiCSYiIhqRBBMREY1IgomIiEYkwURERCOSYCIiohFJMBER0YgkmIiIaEQSTERENCIJJiIiGpEEExERjUiCiYiIRiTBREREIzqeYCTtI+kqSbdKWinpIyV+mqS1kpaX5ahanVMk9Uq6XdKbavHpJdYr6eRafD9J15f4xZJ27uxVRkREN+5gHgc+bnsyMA04SdLksu8s21PKsgSg7JsJHABMB74qaZSkUcA5wJHAZGBW7TifL8d6AfAAcEKnLi4iIiodTzC219n+eVn/HXAbsPcQVWYAi2w/avtOoBc4pCy9tlfbfgxYBMyQJOD1wLdK/QXA0Y1cTEREDKqrz2AkTQReBlxfQnMlrZA0X9KYEtsbuLtWbU2JDRZ/NvBb248PiLc6/xxJPZJ6+vr6huOSIiKi6FqCkbQ7cCnwUdsPAecC+wNTgHXAmU23wfY821NtTx07dmzTp4uI2KHs1I2TSnoaVXK50Pa3AWzfW9v/NeC7ZXMtsE+t+oQSY5D4/cBoSTuVu5h6+YiI6JBujCITcD5wm+0v1eLja8WOAW4p64uBmZJ2kbQfMAn4GXADMKmMGNuZaiDAYtsGrgKOLfVnA5c3eU0REfFU3biDeRXwbuBmSctL7JNUo8CmAAZ+DXwAwPZKSZcAt1KNQDvJ9hMAkuYCVwCjgPm2V5bjfQJYJOlzwE1UCS0iIjqo4wnG9nWAWuxaMkSd04HTW8SXtKpnezXVKLOIiOiSfJM/IiIakQQTERGNSIKJiIhGJMFEREQjkmAiIqIRSTAREdGIJJiIiGhEEkxERDQiCSYiIhqRBBMREY1IgomIiEYkwURERCOSYCIiohFJMBER0YgkmIiIaEQSTERENCIJJiIiGjFiE4yk6ZJul9Qr6eRutyciYkczIhOMpFHAOcCRwGRglqTJ3W1VRMSOZUQmGOAQoNf2atuPAYuAGV1uU0TEDkW2u92GYSfpWGC67feV7XcDr7A9d0C5OcCcsvki4PaONrS1vYDfdLsR24h8FpV8Dhvks9hgW/ksnmd7bKsdO3W6JdsS2/OAed1uR52kHttTu92ObUE+i0o+hw3yWWywPXwWI7WLbC2wT217QolFRESHjNQEcwMwSdJ+knYGZgKLu9ymiIgdyojsIrP9uKS5wBXAKGC+7ZVdbla7tqkuuy7LZ1HJ57BBPosNtvnPYkQ+5I+IiO4bqV1kERHRZUkwERHRiCSYLpH0hKTlkm6R9J+SRpf4REl/LPv6l5273NxhIek5khZJukPSjZKWSHph2fdRSY9Ielat/GGSHiyfwS8lfVHSi2ufy3pJd5b1H3bvyoaPpN+3iJ0maW25zlslzepG2zpB0qckrZS0olzvqZL+eUCZKZJuK+u7S/q32t+pH0t6RXdaP3wkjZP0TUmry3X9VNIx5d+EJb2lVva7kg4r6z8uU2Qtl3Rb+a5f1yTBdM8fbU+xfSCwHjiptu+Osq9/eaxLbRw2kgRcBvzY9v62DwZOAcaVIrOoRv+9bUDVa21PAV4G/DWwR//nQjUy8B/L9hs6cBnddFa55hnAv0l6WpfbM+wkvZLqz/gg2y8B3gBcBfzNgKIzgYvK+tep/v1MKn+n3kv1BcTtVvm38h3gGtvPL9c1k+rrFgBrgE8NcYh3lb8rrwI+381fUJNgtg0/BfbudiMa9jrgT7bP6w/Y/oXtayXtD+wO/BNVonkK238EljPyP6ch2V4FPAyM6XZbGjAe+I3tRwFs/8b2NcADA+5KjgMuKn9vXgH8k+0nS507bf/fTjd8mL0eeGzAv5W7bH+5bP4CeFDSGzdxnN2BPwBPNNPMTUuC6bIyMefhbPw9nf1r3UDndKlpw+1A4MZB9s2kmi/uWuBFksYNLCBpDDAJuKaxFm4HJB0ErLJ9X7fb0oAfAPtI+pWkr0o6tMQvovo7gqRpwPqSaA8Altvu2n+gDTkA+PkmypxO9QtZKxdKWkE19dVnu/n5JMF0z66SlgP3UHUTLa3tq3eRndSy9sgyC1hUfgu9FHhHbd9rJP2CaiaGK2zf040GbgP+XtJK4Hqq/1xGHNu/Bw6mmh+wD7hY0nuAi4FjJf0FG3eP7RAknSPpF5Ju6I+VOzskvbpFlXeVLsZ9gX+Q9LwONfUpkmC654+ln/R5gNj4GcxItJLqP4+NSHox1Z3JUkm/pvoPpN5Ndq3tl1L9VneCpCnNN3WbdJbtA4C3A+dLenq3G9QE20/Y/rHtU4G5wNtt3w3cCRxKdf0Xl+IrgZeWXoCRZCVwUP9G+SXzcGDghJJD3cVgu4/qTqhrgx6SYLrM9sPAh4GPSxqRMysUPwJ2qY9qkfQS4GzgNNsTy/Jc4LkDf+uyfSdwBvCJTjZ6W2N7MdADzO52W4abpBdJmlQLTQHuKusXAWcBq22vAbB9B9Vn8enyYLx/FOabO9fqRvwIeLqkv6vFdhtYyPYPqJ7FvaTVQSTtRjU45o4mGtmOJJhtgO2bgBUM8oB7JHA1ZcQxwBvKkNKVwD8Dh1GNLqu7jNLnPsB5wGslTWywqd22m6Q1teVjLcp8BvhY6TIaSXYHFpSh2CuoXhZ4Wtn3H1R3sQO7x95H1cXcK+kW4AJgu34+Vf6tHA0cWobh/wxYQOtfrk5n44l9oXoGs5zqmecFtgd79tm4TBUTERGNGGm/AUVExDYiCSYiIhqRBBMREY1IgomIiEYkwURERCOSYCKGiaSjy0y3f1W2J5ahs8N1/K9LmlzWPzlcx41oShJMxPCZBVxHA99nkjTK9vts31pCSTCxzUuCiRgGknYHXg2cQIsviUraTdIl5UuEl0m6XtLUsm+WpJtVvRvo87U6v5d0ZpmL7ZXlXR9TJZ1BmctO0oXlTumXki4oE0VeKOkNkn4iaZWkQ8rx9pT0HVXvWllWZlKIaEwSTMTwmAF83/avgPslDZx37UTgAduTgf9FmZdN0nOBz1NN0T4FeLmko0udZwDX236p7ev6D2T7ZDa8T+hdJfwC4Ezgr8ryTqqE9w9suNv5NHBTmQjxk8DCYbr2iJaSYCKGxyyqVw5Qfg7sJnt1/37bt1BNDQTwcqqXsPXZfhy4EHht2fcE1ezS7bjT9s1lRuqVwJVlypGbgYm1NnyjtOFHwLMl7dH2FUZsppE8uWJER0jak+oO5MWSDIwCDGztu3we2Yx3eTxaW3+ytv0k+XceXZI7mIitdyzwDdvPKzNC70M1vXx9EsKfUL2JkTIS7MUl/jOqSQ33KtPOzwKubuOcf9qC1yZfC7yrtOEwqrdHPrSZx4hoWxJMxNabxVNnhL4UOKW2/VVgrKRbgc9RdWM9aHsdcDLVu+d/Adxo+/I2zjkPWCHpws1o52nAwWWm4jMYgVP+x7YlsylHdEC5O3ma7UfKu+R/CLzI9mNdblpEY9I3G9EZuwFXlW4tAScmucRIlzuYiIhoRJ7BREREI5JgIiKiEUkwERHRiCSYiIhoRBJMREQ04v8DE8wmBZD1BeIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = ['MemOccupata', 'MemOccupataS3', 'MemOccupataS6']\n",
    "for c in columns:   \n",
    "    csv = read_csv(\"MemOccupationReport.csv\")\n",
    "    g = sbs.barplot(x=csv['Algoritmo'], y=csv[c])\n",
    "    plt.ylabel(c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNetwork(): \n",
    "    n = 100\n",
    "    model = Sequential(name=\"Sequential-NN\")\n",
    "    model.add(layers.Dense(X.shape[1], activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(layers.Dense(np.unique(y).size * n, activation='relu'))\n",
    "    model.add(layers.Dense(np.unique(y).size, activation='softmax'))\n",
    "    opt = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 600)               19800     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 3606      \n",
      "=================================================================\n",
      "Total params: 24,462\n",
      "Trainable params: 24,462\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "54/54 [==============================] - 0s 540us/step - loss: 1.2375 - accuracy: 0.3796\n",
      "Epoch 2/500\n",
      "54/54 [==============================] - 0s 565us/step - loss: 0.9130 - accuracy: 0.5069\n",
      "Epoch 3/500\n",
      "54/54 [==============================] - 0s 543us/step - loss: 0.6900 - accuracy: 0.6829\n",
      "Epoch 4/500\n",
      "54/54 [==============================] - 0s 528us/step - loss: 0.6079 - accuracy: 0.7407\n",
      "Epoch 5/500\n",
      "54/54 [==============================] - 0s 524us/step - loss: 0.5115 - accuracy: 0.7917\n",
      "Epoch 6/500\n",
      "54/54 [==============================] - 0s 529us/step - loss: 0.4121 - accuracy: 0.8843\n",
      "Epoch 7/500\n",
      "54/54 [==============================] - 0s 548us/step - loss: 0.3575 - accuracy: 0.8912\n",
      "Epoch 8/500\n",
      "54/54 [==============================] - 0s 573us/step - loss: 0.3203 - accuracy: 0.8634\n",
      "Epoch 9/500\n",
      "54/54 [==============================] - 0s 510us/step - loss: 0.2797 - accuracy: 0.9097\n",
      "Epoch 10/500\n",
      "54/54 [==============================] - 0s 525us/step - loss: 0.2987 - accuracy: 0.8727\n",
      "Epoch 11/500\n",
      "54/54 [==============================] - 0s 516us/step - loss: 0.2475 - accuracy: 0.9005\n",
      "Epoch 12/500\n",
      "54/54 [==============================] - 0s 546us/step - loss: 0.2526 - accuracy: 0.8912\n",
      "Epoch 13/500\n",
      "54/54 [==============================] - 0s 529us/step - loss: 0.2584 - accuracy: 0.8866\n",
      "Epoch 14/500\n",
      "54/54 [==============================] - 0s 525us/step - loss: 0.2221 - accuracy: 0.9005\n",
      "Epoch 15/500\n",
      "54/54 [==============================] - 0s 520us/step - loss: 0.2195 - accuracy: 0.8958\n",
      "Epoch 16/500\n",
      "54/54 [==============================] - 0s 529us/step - loss: 0.2160 - accuracy: 0.8935\n",
      "Epoch 17/500\n",
      "54/54 [==============================] - 0s 528us/step - loss: 0.2113 - accuracy: 0.8796\n",
      "Epoch 18/500\n",
      "54/54 [==============================] - 0s 538us/step - loss: 0.2010 - accuracy: 0.8912\n",
      "Epoch 19/500\n",
      "54/54 [==============================] - 0s 550us/step - loss: 0.2357 - accuracy: 0.8681\n",
      "Epoch 20/500\n",
      "54/54 [==============================] - 0s 563us/step - loss: 0.2011 - accuracy: 0.8889\n",
      "Epoch 21/500\n",
      "54/54 [==============================] - 0s 526us/step - loss: 0.2131 - accuracy: 0.8889\n",
      "Epoch 22/500\n",
      "54/54 [==============================] - 0s 542us/step - loss: 0.2141 - accuracy: 0.8843\n",
      "Epoch 23/500\n",
      "54/54 [==============================] - 0s 532us/step - loss: 0.2074 - accuracy: 0.8958\n",
      "Epoch 24/500\n",
      "54/54 [==============================] - 0s 525us/step - loss: 0.1951 - accuracy: 0.8843\n",
      "Epoch 25/500\n",
      "54/54 [==============================] - 0s 546us/step - loss: 0.1889 - accuracy: 0.8843\n",
      "Epoch 26/500\n",
      "54/54 [==============================] - 0s 529us/step - loss: 0.1827 - accuracy: 0.9028\n",
      "Epoch 27/500\n",
      "54/54 [==============================] - 0s 559us/step - loss: 0.1960 - accuracy: 0.9051\n",
      "Epoch 28/500\n",
      "54/54 [==============================] - 0s 531us/step - loss: 0.1896 - accuracy: 0.8935\n",
      "Epoch 29/500\n",
      "54/54 [==============================] - 0s 526us/step - loss: 0.1775 - accuracy: 0.8935\n",
      "Epoch 30/500\n",
      "54/54 [==============================] - 0s 517us/step - loss: 0.1883 - accuracy: 0.8935\n",
      "Epoch 31/500\n",
      "54/54 [==============================] - 0s 521us/step - loss: 0.1983 - accuracy: 0.9120\n",
      "Epoch 32/500\n",
      "54/54 [==============================] - 0s 547us/step - loss: 0.1903 - accuracy: 0.8981\n",
      "Epoch 33/500\n",
      "54/54 [==============================] - 0s 521us/step - loss: 0.1902 - accuracy: 0.8912\n",
      "Epoch 34/500\n",
      "54/54 [==============================] - 0s 584us/step - loss: 0.1980 - accuracy: 0.8750\n",
      "Epoch 35/500\n",
      "54/54 [==============================] - 0s 528us/step - loss: 0.1943 - accuracy: 0.8935\n",
      "Epoch 36/500\n",
      "54/54 [==============================] - 0s 513us/step - loss: 0.2262 - accuracy: 0.8704\n",
      "Epoch 37/500\n",
      "54/54 [==============================] - 0s 537us/step - loss: 0.1969 - accuracy: 0.8843\n",
      "Epoch 38/500\n",
      "54/54 [==============================] - 0s 519us/step - loss: 0.1749 - accuracy: 0.9167\n",
      "Epoch 39/500\n",
      "54/54 [==============================] - 0s 516us/step - loss: 0.1937 - accuracy: 0.8704\n",
      "Epoch 40/500\n",
      "54/54 [==============================] - 0s 534us/step - loss: 0.2062 - accuracy: 0.9028\n",
      "Epoch 41/500\n",
      "54/54 [==============================] - 0s 548us/step - loss: 0.1948 - accuracy: 0.8912\n",
      "Epoch 42/500\n",
      "54/54 [==============================] - 0s 586us/step - loss: 0.1965 - accuracy: 0.8727\n",
      "Epoch 43/500\n",
      "54/54 [==============================] - 0s 583us/step - loss: 0.2055 - accuracy: 0.8889\n",
      "Epoch 44/500\n",
      "54/54 [==============================] - 0s 578us/step - loss: 0.1819 - accuracy: 0.9005\n",
      "Epoch 45/500\n",
      "54/54 [==============================] - 0s 568us/step - loss: 0.1856 - accuracy: 0.8819\n",
      "Epoch 46/500\n",
      "54/54 [==============================] - 0s 554us/step - loss: 0.2340 - accuracy: 0.8889\n",
      "Epoch 47/500\n",
      "54/54 [==============================] - 0s 561us/step - loss: 0.1716 - accuracy: 0.9028\n",
      "Epoch 48/500\n",
      "54/54 [==============================] - 0s 575us/step - loss: 0.1952 - accuracy: 0.9028\n",
      "Epoch 49/500\n",
      "54/54 [==============================] - 0s 556us/step - loss: 0.2239 - accuracy: 0.8912\n",
      "Epoch 50/500\n",
      "54/54 [==============================] - 0s 594us/step - loss: 0.1878 - accuracy: 0.8866\n",
      "Epoch 51/500\n",
      "54/54 [==============================] - 0s 543us/step - loss: 0.1849 - accuracy: 0.8704\n",
      "Epoch 52/500\n",
      "54/54 [==============================] - 0s 575us/step - loss: 0.1847 - accuracy: 0.8866\n",
      "Epoch 53/500\n",
      "54/54 [==============================] - 0s 528us/step - loss: 0.1807 - accuracy: 0.8958\n",
      "Epoch 54/500\n",
      "54/54 [==============================] - 0s 529us/step - loss: 0.2002 - accuracy: 0.8889\n",
      "Epoch 55/500\n",
      "54/54 [==============================] - 0s 550us/step - loss: 0.1780 - accuracy: 0.8889\n",
      "Epoch 56/500\n",
      "54/54 [==============================] - 0s 549us/step - loss: 0.1883 - accuracy: 0.8866\n",
      "Epoch 57/500\n",
      "54/54 [==============================] - 0s 569us/step - loss: 0.1958 - accuracy: 0.8843\n",
      "Epoch 58/500\n",
      "54/54 [==============================] - 0s 565us/step - loss: 0.1783 - accuracy: 0.8819\n",
      "Epoch 59/500\n",
      "54/54 [==============================] - 0s 534us/step - loss: 0.1882 - accuracy: 0.9005\n",
      "Epoch 60/500\n",
      "54/54 [==============================] - 0s 535us/step - loss: 0.1922 - accuracy: 0.8704\n",
      "Epoch 61/500\n",
      "54/54 [==============================] - 0s 523us/step - loss: 0.1808 - accuracy: 0.8889\n",
      "Epoch 62/500\n",
      "54/54 [==============================] - 0s 563us/step - loss: 0.1736 - accuracy: 0.8750\n",
      "Epoch 63/500\n",
      "54/54 [==============================] - 0s 557us/step - loss: 0.1905 - accuracy: 0.9097\n",
      "Epoch 64/500\n",
      "54/54 [==============================] - 0s 581us/step - loss: 0.1877 - accuracy: 0.8866\n",
      "Epoch 65/500\n",
      "54/54 [==============================] - 0s 583us/step - loss: 0.1957 - accuracy: 0.9051\n",
      "Epoch 66/500\n",
      "54/54 [==============================] - 0s 596us/step - loss: 0.1823 - accuracy: 0.8981\n",
      "Epoch 67/500\n",
      "54/54 [==============================] - 0s 540us/step - loss: 0.1881 - accuracy: 0.8704\n",
      "Epoch 68/500\n",
      "54/54 [==============================] - 0s 520us/step - loss: 0.2141 - accuracy: 0.8935\n",
      "Epoch 69/500\n",
      "54/54 [==============================] - 0s 545us/step - loss: 0.1863 - accuracy: 0.9028\n",
      "Epoch 70/500\n",
      "54/54 [==============================] - 0s 541us/step - loss: 0.1805 - accuracy: 0.9144\n",
      "Epoch 71/500\n",
      "54/54 [==============================] - 0s 696us/step - loss: 0.1706 - accuracy: 0.8866\n",
      "Epoch 72/500\n",
      "54/54 [==============================] - 0s 573us/step - loss: 0.1843 - accuracy: 0.8773\n",
      "Epoch 73/500\n",
      "54/54 [==============================] - 0s 570us/step - loss: 0.1764 - accuracy: 0.8935\n",
      "Epoch 74/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 584us/step - loss: 0.1773 - accuracy: 0.8843\n",
      "Epoch 75/500\n",
      "54/54 [==============================] - 0s 574us/step - loss: 0.1791 - accuracy: 0.8727\n",
      "Epoch 76/500\n",
      "54/54 [==============================] - 0s 563us/step - loss: 0.1725 - accuracy: 0.9074\n",
      "Epoch 77/500\n",
      "54/54 [==============================] - 0s 557us/step - loss: 0.1852 - accuracy: 0.8935\n",
      "Epoch 78/500\n",
      "54/54 [==============================] - 0s 538us/step - loss: 0.1737 - accuracy: 0.8958\n",
      "Epoch 79/500\n",
      "54/54 [==============================] - 0s 538us/step - loss: 0.1761 - accuracy: 0.8935\n",
      "Epoch 80/500\n",
      "54/54 [==============================] - 0s 595us/step - loss: 0.1801 - accuracy: 0.8958\n",
      "Epoch 81/500\n",
      "54/54 [==============================] - 0s 561us/step - loss: 0.1775 - accuracy: 0.8912\n",
      "Epoch 82/500\n",
      "54/54 [==============================] - 0s 568us/step - loss: 0.1730 - accuracy: 0.9167\n",
      "Epoch 83/500\n",
      "54/54 [==============================] - 0s 582us/step - loss: 0.1712 - accuracy: 0.9028\n",
      "Epoch 84/500\n",
      "54/54 [==============================] - 0s 569us/step - loss: 0.1700 - accuracy: 0.8958\n",
      "Epoch 85/500\n",
      "54/54 [==============================] - 0s 552us/step - loss: 0.1826 - accuracy: 0.8981\n",
      "Epoch 86/500\n",
      "54/54 [==============================] - 0s 576us/step - loss: 0.1713 - accuracy: 0.8935\n",
      "Epoch 87/500\n",
      "54/54 [==============================] - 0s 585us/step - loss: 0.1918 - accuracy: 0.8889\n",
      "Epoch 88/500\n",
      "54/54 [==============================] - 0s 529us/step - loss: 0.1771 - accuracy: 0.8796\n",
      "Epoch 89/500\n",
      "54/54 [==============================] - 0s 566us/step - loss: 0.1708 - accuracy: 0.9120\n",
      "Epoch 90/500\n",
      "54/54 [==============================] - 0s 544us/step - loss: 0.1785 - accuracy: 0.8843\n",
      "Epoch 91/500\n",
      "54/54 [==============================] - 0s 577us/step - loss: 0.1606 - accuracy: 0.9074\n",
      "Epoch 92/500\n",
      "54/54 [==============================] - 0s 555us/step - loss: 0.1994 - accuracy: 0.8819\n",
      "Epoch 93/500\n",
      "54/54 [==============================] - 0s 581us/step - loss: 0.1881 - accuracy: 0.8773\n",
      "Epoch 94/500\n",
      "54/54 [==============================] - 0s 624us/step - loss: 0.1784 - accuracy: 0.8866\n",
      "Epoch 95/500\n",
      "54/54 [==============================] - 0s 588us/step - loss: 0.1681 - accuracy: 0.9074\n",
      "Epoch 96/500\n",
      "54/54 [==============================] - 0s 543us/step - loss: 0.1878 - accuracy: 0.8912\n",
      "Epoch 97/500\n",
      "54/54 [==============================] - 0s 538us/step - loss: 0.1753 - accuracy: 0.8704\n",
      "Epoch 98/500\n",
      "54/54 [==============================] - 0s 566us/step - loss: 0.1776 - accuracy: 0.8889\n",
      "Epoch 99/500\n",
      "54/54 [==============================] - 0s 542us/step - loss: 0.1624 - accuracy: 0.8935\n",
      "Epoch 100/500\n",
      "54/54 [==============================] - 0s 587us/step - loss: 0.1859 - accuracy: 0.8796\n",
      "Epoch 101/500\n",
      "54/54 [==============================] - 0s 588us/step - loss: 0.1914 - accuracy: 0.8912\n",
      "Epoch 102/500\n",
      "54/54 [==============================] - 0s 563us/step - loss: 0.1691 - accuracy: 0.8866\n",
      "Epoch 103/500\n",
      "54/54 [==============================] - 0s 579us/step - loss: 0.1800 - accuracy: 0.8981\n",
      "Epoch 104/500\n",
      "54/54 [==============================] - 0s 599us/step - loss: 0.1792 - accuracy: 0.8912\n",
      "Epoch 105/500\n",
      "54/54 [==============================] - 0s 580us/step - loss: 0.1671 - accuracy: 0.8773\n",
      "Epoch 106/500\n",
      "54/54 [==============================] - 0s 583us/step - loss: 0.2028 - accuracy: 0.8958\n",
      "Epoch 107/500\n",
      "54/54 [==============================] - 0s 559us/step - loss: 0.1873 - accuracy: 0.8843\n",
      "Epoch 108/500\n",
      "54/54 [==============================] - 0s 552us/step - loss: 0.1718 - accuracy: 0.8935\n",
      "Epoch 109/500\n",
      "54/54 [==============================] - 0s 542us/step - loss: 0.1794 - accuracy: 0.8958\n",
      "Epoch 110/500\n",
      "54/54 [==============================] - 0s 566us/step - loss: 0.1846 - accuracy: 0.9144\n",
      "Epoch 111/500\n",
      "54/54 [==============================] - 0s 512us/step - loss: 0.1786 - accuracy: 0.9167\n",
      "Epoch 112/500\n",
      "54/54 [==============================] - 0s 528us/step - loss: 0.1574 - accuracy: 0.8912\n",
      "Epoch 113/500\n",
      "54/54 [==============================] - 0s 514us/step - loss: 0.1739 - accuracy: 0.9074\n",
      "Epoch 114/500\n",
      "54/54 [==============================] - 0s 510us/step - loss: 0.1834 - accuracy: 0.9167\n",
      "Epoch 115/500\n",
      "54/54 [==============================] - 0s 528us/step - loss: 0.1915 - accuracy: 0.9097\n",
      "Epoch 116/500\n",
      "54/54 [==============================] - 0s 529us/step - loss: 0.1664 - accuracy: 0.9028\n",
      "Epoch 117/500\n",
      "54/54 [==============================] - 0s 559us/step - loss: 0.1601 - accuracy: 0.9028\n",
      "Epoch 118/500\n",
      "54/54 [==============================] - 0s 547us/step - loss: 0.1592 - accuracy: 0.9028\n",
      "Epoch 119/500\n",
      "54/54 [==============================] - 0s 591us/step - loss: 0.1713 - accuracy: 0.9005\n",
      "Epoch 120/500\n",
      "54/54 [==============================] - 0s 613us/step - loss: 0.1659 - accuracy: 0.9097\n",
      "Epoch 121/500\n",
      "54/54 [==============================] - 0s 567us/step - loss: 0.1942 - accuracy: 0.8866\n",
      "Epoch 122/500\n",
      "54/54 [==============================] - 0s 551us/step - loss: 0.1685 - accuracy: 0.8889\n",
      "Epoch 123/500\n",
      "54/54 [==============================] - 0s 531us/step - loss: 0.1620 - accuracy: 0.8889\n",
      "Epoch 124/500\n",
      "54/54 [==============================] - 0s 574us/step - loss: 0.1664 - accuracy: 0.9051\n",
      "Epoch 125/500\n",
      "54/54 [==============================] - 0s 568us/step - loss: 0.1646 - accuracy: 0.8889\n",
      "Epoch 126/500\n",
      "54/54 [==============================] - 0s 525us/step - loss: 0.1687 - accuracy: 0.8981\n",
      "Epoch 127/500\n",
      "54/54 [==============================] - 0s 547us/step - loss: 0.1649 - accuracy: 0.8889\n",
      "Epoch 128/500\n",
      "54/54 [==============================] - 0s 552us/step - loss: 0.1706 - accuracy: 0.8981\n",
      "Epoch 129/500\n",
      "54/54 [==============================] - 0s 591us/step - loss: 0.1542 - accuracy: 0.9144\n",
      "Epoch 130/500\n",
      "54/54 [==============================] - 0s 563us/step - loss: 0.1581 - accuracy: 0.9005\n",
      "Epoch 131/500\n",
      "54/54 [==============================] - 0s 577us/step - loss: 0.1646 - accuracy: 0.9005\n",
      "Epoch 132/500\n",
      "54/54 [==============================] - 0s 567us/step - loss: 0.1708 - accuracy: 0.9028\n",
      "Epoch 133/500\n",
      "54/54 [==============================] - 0s 465us/step - loss: 0.1706 - accuracy: 0.9005\n",
      "Epoch 134/500\n",
      "54/54 [==============================] - 0s 527us/step - loss: 0.1648 - accuracy: 0.9051\n",
      "Epoch 135/500\n",
      "54/54 [==============================] - 0s 512us/step - loss: 0.1537 - accuracy: 0.9259\n",
      "Epoch 136/500\n",
      "54/54 [==============================] - 0s 516us/step - loss: 0.1551 - accuracy: 0.9144\n",
      "Epoch 137/500\n",
      "54/54 [==============================] - 0s 580us/step - loss: 0.1595 - accuracy: 0.9259\n",
      "Epoch 138/500\n",
      "54/54 [==============================] - 0s 575us/step - loss: 0.1824 - accuracy: 0.8819\n",
      "Epoch 139/500\n",
      "54/54 [==============================] - 0s 569us/step - loss: 0.1629 - accuracy: 0.9120\n",
      "Epoch 140/500\n",
      "54/54 [==============================] - 0s 630us/step - loss: 0.1562 - accuracy: 0.9236\n",
      "Epoch 141/500\n",
      "54/54 [==============================] - 0s 517us/step - loss: 0.1738 - accuracy: 0.8912\n",
      "Epoch 142/500\n",
      "54/54 [==============================] - 0s 529us/step - loss: 0.1536 - accuracy: 0.9120\n",
      "Epoch 143/500\n",
      "54/54 [==============================] - 0s 564us/step - loss: 0.1529 - accuracy: 0.9190\n",
      "Epoch 144/500\n",
      "54/54 [==============================] - 0s 549us/step - loss: 0.1483 - accuracy: 0.9120\n",
      "Epoch 145/500\n",
      "54/54 [==============================] - 0s 574us/step - loss: 0.1487 - accuracy: 0.9097\n",
      "Epoch 146/500\n",
      "54/54 [==============================] - 0s 562us/step - loss: 0.1687 - accuracy: 0.8981\n",
      "Epoch 147/500\n",
      "54/54 [==============================] - 0s 570us/step - loss: 0.1555 - accuracy: 0.9051\n",
      "Epoch 148/500\n",
      "54/54 [==============================] - 0s 601us/step - loss: 0.1701 - accuracy: 0.9120\n",
      "Epoch 149/500\n",
      "54/54 [==============================] - 0s 553us/step - loss: 0.1460 - accuracy: 0.9213\n",
      "Epoch 150/500\n",
      "54/54 [==============================] - 0s 569us/step - loss: 0.1830 - accuracy: 0.8912\n",
      "Epoch 151/500\n",
      "54/54 [==============================] - 0s 534us/step - loss: 0.1764 - accuracy: 0.8819\n",
      "Epoch 152/500\n",
      "54/54 [==============================] - 0s 557us/step - loss: 0.1713 - accuracy: 0.8889\n",
      "Epoch 153/500\n",
      "54/54 [==============================] - 0s 577us/step - loss: 0.1553 - accuracy: 0.9074\n",
      "Epoch 154/500\n",
      "54/54 [==============================] - 0s 580us/step - loss: 0.1656 - accuracy: 0.9051\n",
      "Epoch 155/500\n",
      "54/54 [==============================] - 0s 542us/step - loss: 0.1636 - accuracy: 0.9190\n",
      "Epoch 156/500\n",
      "54/54 [==============================] - 0s 565us/step - loss: 0.1876 - accuracy: 0.8958\n",
      "Epoch 157/500\n",
      "54/54 [==============================] - 0s 562us/step - loss: 0.1570 - accuracy: 0.9213\n",
      "Epoch 158/500\n",
      "54/54 [==============================] - 0s 576us/step - loss: 0.1555 - accuracy: 0.9120\n",
      "Epoch 159/500\n",
      "54/54 [==============================] - 0s 580us/step - loss: 0.1543 - accuracy: 0.9051\n",
      "Epoch 160/500\n",
      "54/54 [==============================] - 0s 552us/step - loss: 0.1435 - accuracy: 0.9306\n",
      "Epoch 161/500\n",
      "54/54 [==============================] - 0s 557us/step - loss: 0.1546 - accuracy: 0.9097\n",
      "Epoch 162/500\n",
      "54/54 [==============================] - 0s 527us/step - loss: 0.1631 - accuracy: 0.9097\n",
      "Epoch 163/500\n",
      "54/54 [==============================] - 0s 531us/step - loss: 0.1531 - accuracy: 0.9190\n",
      "Epoch 164/500\n",
      "54/54 [==============================] - 0s 540us/step - loss: 0.1533 - accuracy: 0.9282\n",
      "Epoch 165/500\n",
      "54/54 [==============================] - 0s 550us/step - loss: 0.1432 - accuracy: 0.9375\n",
      "Epoch 166/500\n",
      "54/54 [==============================] - 0s 523us/step - loss: 0.1630 - accuracy: 0.9190\n",
      "Epoch 167/500\n",
      "54/54 [==============================] - 0s 522us/step - loss: 0.1863 - accuracy: 0.9005\n",
      "Epoch 168/500\n",
      "54/54 [==============================] - 0s 537us/step - loss: 0.1604 - accuracy: 0.9074\n",
      "Epoch 169/500\n",
      "54/54 [==============================] - 0s 561us/step - loss: 0.1789 - accuracy: 0.9051\n",
      "Epoch 170/500\n",
      "54/54 [==============================] - 0s 593us/step - loss: 0.1832 - accuracy: 0.9028\n",
      "Epoch 171/500\n",
      "54/54 [==============================] - 0s 541us/step - loss: 0.1500 - accuracy: 0.9028\n",
      "Epoch 172/500\n",
      "54/54 [==============================] - 0s 558us/step - loss: 0.1557 - accuracy: 0.9167\n",
      "Epoch 173/500\n",
      "54/54 [==============================] - 0s 602us/step - loss: 0.1531 - accuracy: 0.9213\n",
      "Epoch 174/500\n",
      "54/54 [==============================] - 0s 600us/step - loss: 0.1545 - accuracy: 0.9259\n",
      "Epoch 175/500\n",
      "54/54 [==============================] - 0s 575us/step - loss: 0.1525 - accuracy: 0.9028\n",
      "Epoch 176/500\n",
      "54/54 [==============================] - 0s 581us/step - loss: 0.1536 - accuracy: 0.9329\n",
      "Epoch 177/500\n",
      "54/54 [==============================] - 0s 535us/step - loss: 0.1461 - accuracy: 0.9306\n",
      "Epoch 178/500\n",
      "54/54 [==============================] - 0s 578us/step - loss: 0.1552 - accuracy: 0.9074\n",
      "Epoch 179/500\n",
      "54/54 [==============================] - 0s 565us/step - loss: 0.1518 - accuracy: 0.9259\n",
      "Epoch 180/500\n",
      "54/54 [==============================] - 0s 538us/step - loss: 0.1416 - accuracy: 0.9259\n",
      "Epoch 181/500\n",
      "54/54 [==============================] - 0s 496us/step - loss: 0.1582 - accuracy: 0.9120\n",
      "Epoch 182/500\n",
      "54/54 [==============================] - 0s 524us/step - loss: 0.1480 - accuracy: 0.8912\n",
      "Epoch 183/500\n",
      "54/54 [==============================] - 0s 532us/step - loss: 0.1490 - accuracy: 0.9306\n",
      "Epoch 184/500\n",
      "54/54 [==============================] - 0s 578us/step - loss: 0.1389 - accuracy: 0.9306\n",
      "Epoch 185/500\n",
      "54/54 [==============================] - 0s 600us/step - loss: 0.2096 - accuracy: 0.9120\n",
      "Epoch 186/500\n",
      "54/54 [==============================] - 0s 517us/step - loss: 0.2046 - accuracy: 0.8773\n",
      "Epoch 187/500\n",
      "54/54 [==============================] - 0s 515us/step - loss: 0.1511 - accuracy: 0.9213\n",
      "Epoch 188/500\n",
      "54/54 [==============================] - 0s 526us/step - loss: 0.1417 - accuracy: 0.9352\n",
      "Epoch 189/500\n",
      "54/54 [==============================] - 0s 550us/step - loss: 0.1416 - accuracy: 0.9352\n",
      "Epoch 190/500\n",
      "54/54 [==============================] - 0s 509us/step - loss: 0.1389 - accuracy: 0.9375\n",
      "Epoch 191/500\n",
      " 1/54 [..............................] - ETA: 0s - loss: 0.3713 - accuracy: 0.7500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-695b788237c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mX_cross_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cross_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cross_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cross_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cross_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpredictions_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1095\u001b[0m                 \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m-> 1097\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# when verbose != 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_begin_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mhook_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'on_{mode}_batch_begin'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhook_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m    635\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m       expand_composites=expand_composites)\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   \"\"\"\n\u001b[0;32m--> 570\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sakka/anaconda3/envs/ts/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue_str\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue_str\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m     raise TypeError(\n\u001b[1;32m    507\u001b[0m         \u001b[0;34m\"Attempted to pack value:\\n  {}\\ninto a sequence, but found \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "BATCH_SIZE = 8\n",
    "num_folds = 10\n",
    "\n",
    "\n",
    "kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "cv_results = np.array([])\n",
    "for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "    X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "    X_cross_train = scaler.fit_transform(X_cross_train)\n",
    "    X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "    X_cross_test = scaler.transform(X_cross_test)\n",
    "    model = getNetwork()\n",
    "    model.fit(X_cross_train, y_cross_train, epochs=EPOCHS, batch_size=BATCH_SIZE)  \n",
    "    y_pred = model.predict(X_cross_test)\n",
    "    predictions_categorical = np.argmax(y_pred, axis=1)\n",
    "    f1s = f1_score(y_cross_test, predictions_categorical, average=\"weighted\")\n",
    "    cv_results = np.append(cv_results, [f1s])\n",
    "\n",
    "print(f'Average score of Cross Validation: {cv_results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1100)              36300     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 11)                12111     \n",
      "=================================================================\n",
      "Total params: 49,467\n",
      "Trainable params: 49,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.8209 - accuracy: 0.2227 - val_loss: 1.6424 - val_accuracy: 0.2500\n",
      "Epoch 2/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 1.4755 - accuracy: 0.3727 - val_loss: 1.3442 - val_accuracy: 0.4500\n",
      "Epoch 3/1000\n",
      "83/83 [==============================] - 0s 619us/step - loss: 1.2806 - accuracy: 0.4061 - val_loss: 1.2855 - val_accuracy: 0.3591\n",
      "Epoch 4/1000\n",
      "83/83 [==============================] - 0s 635us/step - loss: 1.2148 - accuracy: 0.4439 - val_loss: 1.3875 - val_accuracy: 0.3636\n",
      "Epoch 5/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 1.1704 - accuracy: 0.4500 - val_loss: 1.2456 - val_accuracy: 0.4318\n",
      "Epoch 6/1000\n",
      "83/83 [==============================] - 0s 613us/step - loss: 1.1763 - accuracy: 0.4682 - val_loss: 1.1927 - val_accuracy: 0.3727\n",
      "Epoch 7/1000\n",
      "83/83 [==============================] - 0s 681us/step - loss: 1.1242 - accuracy: 0.4712 - val_loss: 1.1669 - val_accuracy: 0.4500\n",
      "Epoch 8/1000\n",
      "83/83 [==============================] - 0s 668us/step - loss: 1.1195 - accuracy: 0.4652 - val_loss: 1.2063 - val_accuracy: 0.3727\n",
      "Epoch 9/1000\n",
      "83/83 [==============================] - 0s 604us/step - loss: 1.0752 - accuracy: 0.5273 - val_loss: 1.2067 - val_accuracy: 0.4455\n",
      "Epoch 10/1000\n",
      "83/83 [==============================] - 0s 623us/step - loss: 1.0765 - accuracy: 0.5061 - val_loss: 1.1820 - val_accuracy: 0.4818\n",
      "Epoch 11/1000\n",
      "83/83 [==============================] - 0s 612us/step - loss: 1.0674 - accuracy: 0.5152 - val_loss: 1.1646 - val_accuracy: 0.4318\n",
      "Epoch 12/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 1.0465 - accuracy: 0.5136 - val_loss: 1.1521 - val_accuracy: 0.4045\n",
      "Epoch 13/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 1.0962 - accuracy: 0.4742 - val_loss: 1.1410 - val_accuracy: 0.4682\n",
      "Epoch 14/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 1.0534 - accuracy: 0.5136 - val_loss: 1.1171 - val_accuracy: 0.4955\n",
      "Epoch 15/1000\n",
      "83/83 [==============================] - 0s 612us/step - loss: 1.0478 - accuracy: 0.5061 - val_loss: 1.0943 - val_accuracy: 0.5045\n",
      "Epoch 16/1000\n",
      "83/83 [==============================] - 0s 641us/step - loss: 1.0326 - accuracy: 0.5000 - val_loss: 1.2277 - val_accuracy: 0.4182\n",
      "Epoch 17/1000\n",
      "83/83 [==============================] - 0s 635us/step - loss: 1.0657 - accuracy: 0.4879 - val_loss: 1.1067 - val_accuracy: 0.4818\n",
      "Epoch 18/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 1.0877 - accuracy: 0.4909 - val_loss: 1.1276 - val_accuracy: 0.4818\n",
      "Epoch 19/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 1.0242 - accuracy: 0.5303 - val_loss: 1.1216 - val_accuracy: 0.4455\n",
      "Epoch 20/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 1.0579 - accuracy: 0.5015 - val_loss: 1.1411 - val_accuracy: 0.4500\n",
      "Epoch 21/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 1.0166 - accuracy: 0.5394 - val_loss: 1.1341 - val_accuracy: 0.5091\n",
      "Epoch 22/1000\n",
      "83/83 [==============================] - 0s 646us/step - loss: 1.0238 - accuracy: 0.5258 - val_loss: 1.1100 - val_accuracy: 0.4955\n",
      "Epoch 23/1000\n",
      "83/83 [==============================] - 0s 615us/step - loss: 1.0254 - accuracy: 0.5500 - val_loss: 1.0837 - val_accuracy: 0.4955\n",
      "Epoch 24/1000\n",
      "83/83 [==============================] - 0s 644us/step - loss: 0.9984 - accuracy: 0.5561 - val_loss: 1.1472 - val_accuracy: 0.4273\n",
      "Epoch 25/1000\n",
      "83/83 [==============================] - 0s 612us/step - loss: 1.0107 - accuracy: 0.5364 - val_loss: 1.1333 - val_accuracy: 0.4727\n",
      "Epoch 26/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.9766 - accuracy: 0.5500 - val_loss: 1.1595 - val_accuracy: 0.5409\n",
      "Epoch 27/1000\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.9958 - accuracy: 0.5288 - val_loss: 1.0888 - val_accuracy: 0.5818\n",
      "Epoch 28/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 1.0000 - accuracy: 0.5303 - val_loss: 1.1299 - val_accuracy: 0.4409\n",
      "Epoch 29/1000\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.9725 - accuracy: 0.5576 - val_loss: 1.3748 - val_accuracy: 0.3864\n",
      "Epoch 30/1000\n",
      "83/83 [==============================] - 0s 626us/step - loss: 1.0001 - accuracy: 0.5470 - val_loss: 1.1278 - val_accuracy: 0.4000\n",
      "Epoch 31/1000\n",
      "83/83 [==============================] - 0s 668us/step - loss: 0.9594 - accuracy: 0.5621 - val_loss: 1.1099 - val_accuracy: 0.5364\n",
      "Epoch 32/1000\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.9694 - accuracy: 0.5530 - val_loss: 1.0946 - val_accuracy: 0.5364\n",
      "Epoch 33/1000\n",
      "83/83 [==============================] - 0s 668us/step - loss: 0.9863 - accuracy: 0.5470 - val_loss: 1.0790 - val_accuracy: 0.5364\n",
      "Epoch 34/1000\n",
      "83/83 [==============================] - 0s 664us/step - loss: 0.9524 - accuracy: 0.5833 - val_loss: 1.1098 - val_accuracy: 0.4864\n",
      "Epoch 35/1000\n",
      "83/83 [==============================] - 0s 669us/step - loss: 0.9620 - accuracy: 0.5667 - val_loss: 1.1124 - val_accuracy: 0.5273\n",
      "Epoch 36/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.9587 - accuracy: 0.5727 - val_loss: 1.2505 - val_accuracy: 0.4136\n",
      "Epoch 37/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.9270 - accuracy: 0.5636 - val_loss: 1.1140 - val_accuracy: 0.4818\n",
      "Epoch 38/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.9432 - accuracy: 0.5894 - val_loss: 1.1071 - val_accuracy: 0.5409\n",
      "Epoch 39/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.9221 - accuracy: 0.5894 - val_loss: 1.0896 - val_accuracy: 0.5591\n",
      "Epoch 40/1000\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.9162 - accuracy: 0.5864 - val_loss: 1.1131 - val_accuracy: 0.5773\n",
      "Epoch 41/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.9045 - accuracy: 0.6197 - val_loss: 1.0626 - val_accuracy: 0.5864\n",
      "Epoch 42/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.8980 - accuracy: 0.6288 - val_loss: 1.0978 - val_accuracy: 0.5545\n",
      "Epoch 43/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.8833 - accuracy: 0.6212 - val_loss: 1.1279 - val_accuracy: 0.4364\n",
      "Epoch 44/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.8753 - accuracy: 0.6303 - val_loss: 1.0628 - val_accuracy: 0.6273\n",
      "Epoch 45/1000\n",
      "83/83 [==============================] - 0s 667us/step - loss: 0.9123 - accuracy: 0.5909 - val_loss: 1.1365 - val_accuracy: 0.4818\n",
      "Epoch 46/1000\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.8808 - accuracy: 0.6136 - val_loss: 1.0557 - val_accuracy: 0.6136\n",
      "Epoch 47/1000\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.8828 - accuracy: 0.6167 - val_loss: 1.0751 - val_accuracy: 0.5864\n",
      "Epoch 48/1000\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.9038 - accuracy: 0.5848 - val_loss: 1.1174 - val_accuracy: 0.5500\n",
      "Epoch 49/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.8831 - accuracy: 0.6273 - val_loss: 1.0891 - val_accuracy: 0.5455\n",
      "Epoch 50/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.8400 - accuracy: 0.6439 - val_loss: 1.1166 - val_accuracy: 0.5045\n",
      "Epoch 51/1000\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.8600 - accuracy: 0.6273 - val_loss: 1.1047 - val_accuracy: 0.5455\n",
      "Epoch 52/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 647us/step - loss: 0.8372 - accuracy: 0.6485 - val_loss: 1.0991 - val_accuracy: 0.5773\n",
      "Epoch 53/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.8261 - accuracy: 0.6530 - val_loss: 1.0718 - val_accuracy: 0.5636\n",
      "Epoch 54/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.8391 - accuracy: 0.6455 - val_loss: 1.1462 - val_accuracy: 0.5682\n",
      "Epoch 55/1000\n",
      "83/83 [==============================] - 0s 651us/step - loss: 0.8282 - accuracy: 0.6561 - val_loss: 1.1305 - val_accuracy: 0.5000\n",
      "Epoch 56/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.8249 - accuracy: 0.6394 - val_loss: 1.0782 - val_accuracy: 0.6318\n",
      "Epoch 57/1000\n",
      "83/83 [==============================] - 0s 641us/step - loss: 0.8387 - accuracy: 0.6182 - val_loss: 1.1406 - val_accuracy: 0.5091\n",
      "Epoch 58/1000\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.7974 - accuracy: 0.6742 - val_loss: 1.0794 - val_accuracy: 0.5136\n",
      "Epoch 59/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.7858 - accuracy: 0.6530 - val_loss: 1.0835 - val_accuracy: 0.6182\n",
      "Epoch 60/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.8042 - accuracy: 0.6591 - val_loss: 1.0858 - val_accuracy: 0.5955\n",
      "Epoch 61/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.7990 - accuracy: 0.6636 - val_loss: 1.0741 - val_accuracy: 0.6273\n",
      "Epoch 62/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.7612 - accuracy: 0.7000 - val_loss: 1.1302 - val_accuracy: 0.4818\n",
      "Epoch 63/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.7944 - accuracy: 0.6500 - val_loss: 1.1305 - val_accuracy: 0.5318\n",
      "Epoch 64/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.7863 - accuracy: 0.6591 - val_loss: 1.0944 - val_accuracy: 0.5773\n",
      "Epoch 65/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.7859 - accuracy: 0.6742 - val_loss: 1.1078 - val_accuracy: 0.6091\n",
      "Epoch 66/1000\n",
      "83/83 [==============================] - 0s 639us/step - loss: 0.7510 - accuracy: 0.6970 - val_loss: 1.1215 - val_accuracy: 0.5545\n",
      "Epoch 67/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.7618 - accuracy: 0.6742 - val_loss: 1.0852 - val_accuracy: 0.5591\n",
      "Epoch 68/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.7306 - accuracy: 0.6924 - val_loss: 1.0502 - val_accuracy: 0.5955\n",
      "Epoch 69/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.7216 - accuracy: 0.7136 - val_loss: 1.2155 - val_accuracy: 0.4682\n",
      "Epoch 70/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.7522 - accuracy: 0.7000 - val_loss: 1.1569 - val_accuracy: 0.5500\n",
      "Epoch 71/1000\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.7353 - accuracy: 0.6955 - val_loss: 1.2352 - val_accuracy: 0.4955\n",
      "Epoch 72/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.7449 - accuracy: 0.6848 - val_loss: 1.2095 - val_accuracy: 0.5455\n",
      "Epoch 73/1000\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.7310 - accuracy: 0.7015 - val_loss: 1.2075 - val_accuracy: 0.5909\n",
      "Epoch 74/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.6976 - accuracy: 0.7242 - val_loss: 1.1623 - val_accuracy: 0.6000\n",
      "Epoch 75/1000\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.7225 - accuracy: 0.6894 - val_loss: 1.1702 - val_accuracy: 0.5045\n",
      "Epoch 76/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.6903 - accuracy: 0.7242 - val_loss: 1.1982 - val_accuracy: 0.5500\n",
      "Epoch 77/1000\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.7049 - accuracy: 0.7091 - val_loss: 1.1401 - val_accuracy: 0.6182\n",
      "Epoch 78/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.6860 - accuracy: 0.7197 - val_loss: 1.1108 - val_accuracy: 0.6273\n",
      "Epoch 79/1000\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.6770 - accuracy: 0.7394 - val_loss: 1.2135 - val_accuracy: 0.5682\n",
      "Epoch 80/1000\n",
      "83/83 [==============================] - 0s 644us/step - loss: 0.6665 - accuracy: 0.7333 - val_loss: 1.1663 - val_accuracy: 0.5727\n",
      "Epoch 81/1000\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.6549 - accuracy: 0.7485 - val_loss: 1.1645 - val_accuracy: 0.5227\n",
      "Epoch 82/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.6813 - accuracy: 0.7273 - val_loss: 1.1671 - val_accuracy: 0.5955\n",
      "Epoch 83/1000\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.7295 - accuracy: 0.6924 - val_loss: 1.1712 - val_accuracy: 0.5682\n",
      "Epoch 84/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.6691 - accuracy: 0.7288 - val_loss: 1.1607 - val_accuracy: 0.6136\n",
      "Epoch 85/1000\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.6435 - accuracy: 0.7318 - val_loss: 1.1060 - val_accuracy: 0.6227\n",
      "Epoch 86/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.6504 - accuracy: 0.7364 - val_loss: 1.1624 - val_accuracy: 0.6136\n",
      "Epoch 87/1000\n",
      "83/83 [==============================] - 0s 656us/step - loss: 0.6919 - accuracy: 0.7136 - val_loss: 1.2792 - val_accuracy: 0.5909\n",
      "Epoch 88/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.6593 - accuracy: 0.7258 - val_loss: 1.2052 - val_accuracy: 0.5727\n",
      "Epoch 89/1000\n",
      "83/83 [==============================] - 0s 695us/step - loss: 0.6441 - accuracy: 0.7379 - val_loss: 1.1894 - val_accuracy: 0.5955\n",
      "Epoch 90/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.6099 - accuracy: 0.7515 - val_loss: 1.3054 - val_accuracy: 0.6045\n",
      "Epoch 91/1000\n",
      "83/83 [==============================] - 0s 657us/step - loss: 0.6569 - accuracy: 0.7424 - val_loss: 1.2318 - val_accuracy: 0.5727\n",
      "Epoch 92/1000\n",
      "83/83 [==============================] - 0s 644us/step - loss: 0.6192 - accuracy: 0.7545 - val_loss: 1.1978 - val_accuracy: 0.6000\n",
      "Epoch 93/1000\n",
      "83/83 [==============================] - 0s 649us/step - loss: 0.6217 - accuracy: 0.7485 - val_loss: 1.1732 - val_accuracy: 0.6136\n",
      "Epoch 94/1000\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.6181 - accuracy: 0.7424 - val_loss: 1.2820 - val_accuracy: 0.6000\n",
      "Epoch 95/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.6016 - accuracy: 0.7636 - val_loss: 1.2963 - val_accuracy: 0.5955\n",
      "Epoch 96/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.6051 - accuracy: 0.7636 - val_loss: 1.2017 - val_accuracy: 0.6136\n",
      "Epoch 97/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.6439 - accuracy: 0.7409 - val_loss: 1.1866 - val_accuracy: 0.6227\n",
      "Epoch 98/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.5872 - accuracy: 0.7788 - val_loss: 1.2359 - val_accuracy: 0.6045\n",
      "Epoch 99/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.6056 - accuracy: 0.7621 - val_loss: 1.2844 - val_accuracy: 0.6136\n",
      "Epoch 100/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.5923 - accuracy: 0.7561 - val_loss: 1.1541 - val_accuracy: 0.6273\n",
      "Epoch 101/1000\n",
      "83/83 [==============================] - 0s 656us/step - loss: 0.5609 - accuracy: 0.7909 - val_loss: 1.2824 - val_accuracy: 0.5909\n",
      "Epoch 102/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.5747 - accuracy: 0.7773 - val_loss: 1.2254 - val_accuracy: 0.6409\n",
      "Epoch 103/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.5725 - accuracy: 0.7848 - val_loss: 1.3179 - val_accuracy: 0.6091\n",
      "Epoch 104/1000\n",
      "83/83 [==============================] - 0s 668us/step - loss: 0.5827 - accuracy: 0.7576 - val_loss: 1.2921 - val_accuracy: 0.5955\n",
      "Epoch 105/1000\n",
      "83/83 [==============================] - 0s 655us/step - loss: 0.5762 - accuracy: 0.7848 - val_loss: 1.3196 - val_accuracy: 0.6000\n",
      "Epoch 106/1000\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.6186 - accuracy: 0.7500 - val_loss: 1.3017 - val_accuracy: 0.5909\n",
      "Epoch 107/1000\n",
      "83/83 [==============================] - 0s 675us/step - loss: 0.5465 - accuracy: 0.7788 - val_loss: 1.3863 - val_accuracy: 0.5636\n",
      "Epoch 108/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 655us/step - loss: 0.5439 - accuracy: 0.7894 - val_loss: 1.3436 - val_accuracy: 0.5773\n",
      "Epoch 109/1000\n",
      "83/83 [==============================] - 0s 651us/step - loss: 0.5505 - accuracy: 0.7848 - val_loss: 1.3306 - val_accuracy: 0.6091\n",
      "Epoch 110/1000\n",
      "83/83 [==============================] - 0s 670us/step - loss: 0.5958 - accuracy: 0.7439 - val_loss: 1.4485 - val_accuracy: 0.5318\n",
      "Epoch 111/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.5578 - accuracy: 0.7727 - val_loss: 1.3807 - val_accuracy: 0.6182\n",
      "Epoch 112/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.5454 - accuracy: 0.7924 - val_loss: 1.3201 - val_accuracy: 0.6273\n",
      "Epoch 113/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.5354 - accuracy: 0.7818 - val_loss: 1.3362 - val_accuracy: 0.5727\n",
      "Epoch 114/1000\n",
      "83/83 [==============================] - 0s 652us/step - loss: 0.5514 - accuracy: 0.7727 - val_loss: 1.3901 - val_accuracy: 0.5955\n",
      "Epoch 115/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.5273 - accuracy: 0.7924 - val_loss: 1.3862 - val_accuracy: 0.5955\n",
      "Epoch 116/1000\n",
      "83/83 [==============================] - 0s 667us/step - loss: 0.5402 - accuracy: 0.7955 - val_loss: 1.2962 - val_accuracy: 0.6273\n",
      "Epoch 117/1000\n",
      "83/83 [==============================] - 0s 651us/step - loss: 0.5185 - accuracy: 0.8015 - val_loss: 1.3951 - val_accuracy: 0.6045\n",
      "Epoch 118/1000\n",
      "83/83 [==============================] - 0s 653us/step - loss: 0.5163 - accuracy: 0.8061 - val_loss: 1.4160 - val_accuracy: 0.6318\n",
      "Epoch 119/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.5077 - accuracy: 0.8045 - val_loss: 1.3362 - val_accuracy: 0.6318\n",
      "Epoch 120/1000\n",
      "83/83 [==============================] - 0s 653us/step - loss: 0.5222 - accuracy: 0.8015 - val_loss: 1.5357 - val_accuracy: 0.5773\n",
      "Epoch 121/1000\n",
      "83/83 [==============================] - 0s 641us/step - loss: 0.5003 - accuracy: 0.8076 - val_loss: 1.3345 - val_accuracy: 0.6455\n",
      "Epoch 122/1000\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.4908 - accuracy: 0.8045 - val_loss: 1.3669 - val_accuracy: 0.6273\n",
      "Epoch 123/1000\n",
      "83/83 [==============================] - 0s 652us/step - loss: 0.5190 - accuracy: 0.7848 - val_loss: 1.3540 - val_accuracy: 0.6318\n",
      "Epoch 124/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.4944 - accuracy: 0.8076 - val_loss: 1.3072 - val_accuracy: 0.6727\n",
      "Epoch 125/1000\n",
      "83/83 [==============================] - 0s 667us/step - loss: 0.4811 - accuracy: 0.8167 - val_loss: 1.3962 - val_accuracy: 0.6318\n",
      "Epoch 126/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.5278 - accuracy: 0.7955 - val_loss: 1.4726 - val_accuracy: 0.6000\n",
      "Epoch 127/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.4970 - accuracy: 0.8197 - val_loss: 1.4835 - val_accuracy: 0.5909\n",
      "Epoch 128/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.4957 - accuracy: 0.7985 - val_loss: 1.5746 - val_accuracy: 0.6045\n",
      "Epoch 129/1000\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.4835 - accuracy: 0.7955 - val_loss: 1.4931 - val_accuracy: 0.6136\n",
      "Epoch 130/1000\n",
      "83/83 [==============================] - 0s 654us/step - loss: 0.4759 - accuracy: 0.8061 - val_loss: 1.4478 - val_accuracy: 0.6136\n",
      "Epoch 131/1000\n",
      "83/83 [==============================] - 0s 667us/step - loss: 0.4990 - accuracy: 0.7909 - val_loss: 1.5261 - val_accuracy: 0.6045\n",
      "Epoch 132/1000\n",
      "83/83 [==============================] - 0s 639us/step - loss: 0.4734 - accuracy: 0.8227 - val_loss: 1.5738 - val_accuracy: 0.5727\n",
      "Epoch 133/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.4638 - accuracy: 0.8333 - val_loss: 1.4394 - val_accuracy: 0.6364\n",
      "Epoch 134/1000\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.6563 - accuracy: 0.7182 - val_loss: 1.4896 - val_accuracy: 0.6000\n",
      "Epoch 135/1000\n",
      "83/83 [==============================] - 0s 671us/step - loss: 0.5076 - accuracy: 0.7909 - val_loss: 1.4445 - val_accuracy: 0.6091\n",
      "Epoch 136/1000\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.4702 - accuracy: 0.8182 - val_loss: 1.5662 - val_accuracy: 0.6091\n",
      "Epoch 137/1000\n",
      "83/83 [==============================] - 0s 641us/step - loss: 0.4797 - accuracy: 0.8091 - val_loss: 1.6209 - val_accuracy: 0.5364\n",
      "Epoch 138/1000\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.4988 - accuracy: 0.8000 - val_loss: 1.4699 - val_accuracy: 0.6364\n",
      "Epoch 139/1000\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.4598 - accuracy: 0.8212 - val_loss: 1.4932 - val_accuracy: 0.6591\n",
      "Epoch 140/1000\n",
      "83/83 [==============================] - 0s 652us/step - loss: 0.5160 - accuracy: 0.7742 - val_loss: 1.5037 - val_accuracy: 0.6182\n",
      "Epoch 141/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.5024 - accuracy: 0.7864 - val_loss: 1.4799 - val_accuracy: 0.6318\n",
      "Epoch 142/1000\n",
      "83/83 [==============================] - 0s 648us/step - loss: 0.4049 - accuracy: 0.8470 - val_loss: 1.6213 - val_accuracy: 0.5955\n",
      "Epoch 143/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.4802 - accuracy: 0.8136 - val_loss: 1.6021 - val_accuracy: 0.6045\n",
      "Epoch 144/1000\n",
      "83/83 [==============================] - 0s 644us/step - loss: 0.5176 - accuracy: 0.8076 - val_loss: 1.6805 - val_accuracy: 0.5727\n",
      "Epoch 145/1000\n",
      "83/83 [==============================] - 0s 689us/step - loss: 0.4358 - accuracy: 0.8242 - val_loss: 1.4797 - val_accuracy: 0.6682\n",
      "Epoch 146/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.5020 - accuracy: 0.7955 - val_loss: 1.6827 - val_accuracy: 0.6227\n",
      "Epoch 147/1000\n",
      "83/83 [==============================] - 0s 704us/step - loss: 0.4416 - accuracy: 0.8182 - val_loss: 1.5645 - val_accuracy: 0.6045\n",
      "Epoch 148/1000\n",
      "83/83 [==============================] - 0s 672us/step - loss: 0.4268 - accuracy: 0.8318 - val_loss: 1.5184 - val_accuracy: 0.6227\n",
      "Epoch 149/1000\n",
      "83/83 [==============================] - 0s 661us/step - loss: 0.4641 - accuracy: 0.8000 - val_loss: 1.4956 - val_accuracy: 0.6500\n",
      "Epoch 150/1000\n",
      "83/83 [==============================] - 0s 664us/step - loss: 0.4496 - accuracy: 0.8136 - val_loss: 1.4733 - val_accuracy: 0.6727\n",
      "Epoch 151/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.4141 - accuracy: 0.8439 - val_loss: 1.5364 - val_accuracy: 0.6636\n",
      "Epoch 152/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.4750 - accuracy: 0.8076 - val_loss: 1.7079 - val_accuracy: 0.5455\n",
      "Epoch 153/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.4651 - accuracy: 0.8152 - val_loss: 1.5907 - val_accuracy: 0.5773\n",
      "Epoch 154/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.4305 - accuracy: 0.8333 - val_loss: 1.6000 - val_accuracy: 0.6682\n",
      "Epoch 155/1000\n",
      "83/83 [==============================] - 0s 658us/step - loss: 0.4207 - accuracy: 0.8333 - val_loss: 1.6521 - val_accuracy: 0.6682\n",
      "Epoch 156/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.4290 - accuracy: 0.8273 - val_loss: 1.6789 - val_accuracy: 0.6409\n",
      "Epoch 157/1000\n",
      "83/83 [==============================] - 0s 654us/step - loss: 0.4190 - accuracy: 0.8318 - val_loss: 1.6112 - val_accuracy: 0.6318\n",
      "Epoch 158/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.3826 - accuracy: 0.8439 - val_loss: 1.8144 - val_accuracy: 0.5273\n",
      "Epoch 159/1000\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.4692 - accuracy: 0.8045 - val_loss: 1.8646 - val_accuracy: 0.5909\n",
      "Epoch 160/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.4220 - accuracy: 0.8182 - val_loss: 1.8427 - val_accuracy: 0.6045\n",
      "Epoch 161/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.4142 - accuracy: 0.8303 - val_loss: 1.7363 - val_accuracy: 0.6273\n",
      "Epoch 162/1000\n",
      "83/83 [==============================] - 0s 658us/step - loss: 0.4176 - accuracy: 0.8455 - val_loss: 1.7197 - val_accuracy: 0.6409\n",
      "Epoch 163/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.4051 - accuracy: 0.8455 - val_loss: 1.7503 - val_accuracy: 0.6364\n",
      "Epoch 164/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 651us/step - loss: 0.4156 - accuracy: 0.8379 - val_loss: 1.6851 - val_accuracy: 0.6545\n",
      "Epoch 165/1000\n",
      "83/83 [==============================] - 0s 665us/step - loss: 0.3878 - accuracy: 0.8379 - val_loss: 1.6760 - val_accuracy: 0.6545\n",
      "Epoch 166/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.3743 - accuracy: 0.8636 - val_loss: 1.6744 - val_accuracy: 0.6318\n",
      "Epoch 167/1000\n",
      "83/83 [==============================] - 0s 655us/step - loss: 0.3395 - accuracy: 0.8758 - val_loss: 1.7696 - val_accuracy: 0.5773\n",
      "Epoch 168/1000\n",
      "83/83 [==============================] - 0s 655us/step - loss: 0.3916 - accuracy: 0.8485 - val_loss: 1.7539 - val_accuracy: 0.6500\n",
      "Epoch 169/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.3985 - accuracy: 0.8318 - val_loss: 1.9246 - val_accuracy: 0.5455\n",
      "Epoch 170/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.4128 - accuracy: 0.8348 - val_loss: 1.6707 - val_accuracy: 0.6818\n",
      "Epoch 171/1000\n",
      "83/83 [==============================] - 0s 655us/step - loss: 0.3677 - accuracy: 0.8742 - val_loss: 1.8250 - val_accuracy: 0.6409\n",
      "Epoch 172/1000\n",
      "83/83 [==============================] - 0s 652us/step - loss: 0.3973 - accuracy: 0.8576 - val_loss: 1.8537 - val_accuracy: 0.6364\n",
      "Epoch 173/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.4217 - accuracy: 0.8273 - val_loss: 1.6825 - val_accuracy: 0.6591\n",
      "Epoch 174/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.4330 - accuracy: 0.8273 - val_loss: 1.8308 - val_accuracy: 0.6500\n",
      "Epoch 175/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.3791 - accuracy: 0.8606 - val_loss: 1.7661 - val_accuracy: 0.6409\n",
      "Epoch 176/1000\n",
      "83/83 [==============================] - 0s 645us/step - loss: 0.4142 - accuracy: 0.8364 - val_loss: 1.7928 - val_accuracy: 0.6500\n",
      "Epoch 177/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.4069 - accuracy: 0.8303 - val_loss: 2.0180 - val_accuracy: 0.6091\n",
      "Epoch 178/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.4284 - accuracy: 0.8258 - val_loss: 1.8103 - val_accuracy: 0.6455\n",
      "Epoch 179/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.3835 - accuracy: 0.8515 - val_loss: 1.8969 - val_accuracy: 0.6136\n",
      "Epoch 180/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.3540 - accuracy: 0.8636 - val_loss: 1.7517 - val_accuracy: 0.6636\n",
      "Epoch 181/1000\n",
      "83/83 [==============================] - 0s 644us/step - loss: 0.3613 - accuracy: 0.8561 - val_loss: 1.7934 - val_accuracy: 0.6682\n",
      "Epoch 182/1000\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.3860 - accuracy: 0.8439 - val_loss: 1.9319 - val_accuracy: 0.6273\n",
      "Epoch 183/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.3861 - accuracy: 0.8561 - val_loss: 1.9419 - val_accuracy: 0.6545\n",
      "Epoch 184/1000\n",
      "83/83 [==============================] - 0s 653us/step - loss: 0.3739 - accuracy: 0.8439 - val_loss: 1.8446 - val_accuracy: 0.6318\n",
      "Epoch 185/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.3897 - accuracy: 0.8470 - val_loss: 1.8032 - val_accuracy: 0.6545\n",
      "Epoch 186/1000\n",
      "83/83 [==============================] - 0s 685us/step - loss: 0.4114 - accuracy: 0.8394 - val_loss: 2.0787 - val_accuracy: 0.5273\n",
      "Epoch 187/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.4082 - accuracy: 0.8379 - val_loss: 1.8338 - val_accuracy: 0.6500\n",
      "Epoch 188/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.3205 - accuracy: 0.8742 - val_loss: 1.9057 - val_accuracy: 0.6773\n",
      "Epoch 189/1000\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.3613 - accuracy: 0.8636 - val_loss: 1.8843 - val_accuracy: 0.6136\n",
      "Epoch 190/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.3342 - accuracy: 0.8697 - val_loss: 2.2023 - val_accuracy: 0.5955\n",
      "Epoch 191/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.3848 - accuracy: 0.8530 - val_loss: 1.8979 - val_accuracy: 0.6500\n",
      "Epoch 192/1000\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.3877 - accuracy: 0.8636 - val_loss: 1.8599 - val_accuracy: 0.6727\n",
      "Epoch 193/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.3333 - accuracy: 0.8818 - val_loss: 1.8980 - val_accuracy: 0.6773\n",
      "Epoch 194/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.4050 - accuracy: 0.8485 - val_loss: 1.8698 - val_accuracy: 0.6409\n",
      "Epoch 195/1000\n",
      "83/83 [==============================] - 0s 660us/step - loss: 0.3981 - accuracy: 0.8273 - val_loss: 1.8793 - val_accuracy: 0.6591\n",
      "Epoch 196/1000\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.3461 - accuracy: 0.8697 - val_loss: 1.9395 - val_accuracy: 0.6682\n",
      "Epoch 197/1000\n",
      "83/83 [==============================] - 0s 652us/step - loss: 0.3610 - accuracy: 0.8561 - val_loss: 2.1493 - val_accuracy: 0.6364\n",
      "Epoch 198/1000\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.4278 - accuracy: 0.8303 - val_loss: 1.9067 - val_accuracy: 0.6682\n",
      "Epoch 199/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.3900 - accuracy: 0.8455 - val_loss: 1.9766 - val_accuracy: 0.6227\n",
      "Epoch 200/1000\n",
      "83/83 [==============================] - 0s 652us/step - loss: 0.3292 - accuracy: 0.8864 - val_loss: 1.9145 - val_accuracy: 0.6591\n",
      "Epoch 201/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.3352 - accuracy: 0.8682 - val_loss: 1.8931 - val_accuracy: 0.6455\n",
      "Epoch 202/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.3816 - accuracy: 0.8485 - val_loss: 1.9101 - val_accuracy: 0.6591\n",
      "Epoch 203/1000\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.2962 - accuracy: 0.8818 - val_loss: 1.9509 - val_accuracy: 0.6727\n",
      "Epoch 204/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.3166 - accuracy: 0.8833 - val_loss: 2.0511 - val_accuracy: 0.6545\n",
      "Epoch 205/1000\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.3752 - accuracy: 0.8591 - val_loss: 1.9842 - val_accuracy: 0.6727\n",
      "Epoch 206/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.3364 - accuracy: 0.8667 - val_loss: 1.8898 - val_accuracy: 0.6909\n",
      "Epoch 207/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.3267 - accuracy: 0.8788 - val_loss: 2.0946 - val_accuracy: 0.6500\n",
      "Epoch 208/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.3232 - accuracy: 0.8758 - val_loss: 2.0953 - val_accuracy: 0.6136\n",
      "Epoch 209/1000\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.3347 - accuracy: 0.8652 - val_loss: 1.9828 - val_accuracy: 0.6727\n",
      "Epoch 210/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.3336 - accuracy: 0.8682 - val_loss: 2.0712 - val_accuracy: 0.6591\n",
      "Epoch 211/1000\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.2914 - accuracy: 0.8939 - val_loss: 1.8958 - val_accuracy: 0.6773\n",
      "Epoch 212/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.2954 - accuracy: 0.8833 - val_loss: 1.9831 - val_accuracy: 0.6500\n",
      "Epoch 213/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.3059 - accuracy: 0.8758 - val_loss: 2.1081 - val_accuracy: 0.6545\n",
      "Epoch 214/1000\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.3182 - accuracy: 0.8742 - val_loss: 2.0711 - val_accuracy: 0.6455\n",
      "Epoch 215/1000\n",
      "83/83 [==============================] - 0s 660us/step - loss: 0.4314 - accuracy: 0.8394 - val_loss: 2.3793 - val_accuracy: 0.6000\n",
      "Epoch 216/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.4311 - accuracy: 0.8273 - val_loss: 2.0635 - val_accuracy: 0.6455\n",
      "Epoch 217/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.2929 - accuracy: 0.8924 - val_loss: 1.9924 - val_accuracy: 0.6773\n",
      "Epoch 218/1000\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.3367 - accuracy: 0.8773 - val_loss: 2.1040 - val_accuracy: 0.6409\n",
      "Epoch 219/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.3097 - accuracy: 0.8833 - val_loss: 1.9777 - val_accuracy: 0.6591\n",
      "Epoch 220/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 607us/step - loss: 0.3357 - accuracy: 0.8621 - val_loss: 2.0685 - val_accuracy: 0.6545\n",
      "Epoch 221/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.3268 - accuracy: 0.8697 - val_loss: 2.1842 - val_accuracy: 0.6682\n",
      "Epoch 222/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.3014 - accuracy: 0.8803 - val_loss: 2.0934 - val_accuracy: 0.6818\n",
      "Epoch 223/1000\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.3012 - accuracy: 0.8955 - val_loss: 2.2225 - val_accuracy: 0.6409\n",
      "Epoch 224/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.3002 - accuracy: 0.8848 - val_loss: 2.3331 - val_accuracy: 0.6273\n",
      "Epoch 225/1000\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.2967 - accuracy: 0.8773 - val_loss: 2.3282 - val_accuracy: 0.5682\n",
      "Epoch 226/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.3328 - accuracy: 0.8636 - val_loss: 2.1176 - val_accuracy: 0.6864\n",
      "Epoch 227/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.2992 - accuracy: 0.8803 - val_loss: 2.1140 - val_accuracy: 0.6636\n",
      "Epoch 228/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.3083 - accuracy: 0.8848 - val_loss: 2.2800 - val_accuracy: 0.6091\n",
      "Epoch 229/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.3895 - accuracy: 0.8530 - val_loss: 2.2211 - val_accuracy: 0.6409\n",
      "Epoch 230/1000\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.3550 - accuracy: 0.8652 - val_loss: 2.3050 - val_accuracy: 0.6182\n",
      "Epoch 231/1000\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.3016 - accuracy: 0.8864 - val_loss: 2.2682 - val_accuracy: 0.6591\n",
      "Epoch 232/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.2652 - accuracy: 0.8939 - val_loss: 2.2685 - val_accuracy: 0.6409\n",
      "Epoch 233/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.4746 - accuracy: 0.8212 - val_loss: 2.3551 - val_accuracy: 0.6227\n",
      "Epoch 234/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.3557 - accuracy: 0.8697 - val_loss: 2.2580 - val_accuracy: 0.6273\n",
      "Epoch 235/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.2966 - accuracy: 0.8985 - val_loss: 2.1422 - val_accuracy: 0.6773\n",
      "Epoch 236/1000\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.2756 - accuracy: 0.9015 - val_loss: 2.4057 - val_accuracy: 0.5864\n",
      "Epoch 237/1000\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.2643 - accuracy: 0.9182 - val_loss: 2.3397 - val_accuracy: 0.6455\n",
      "Epoch 238/1000\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.3861 - accuracy: 0.8409 - val_loss: 2.3595 - val_accuracy: 0.6500\n",
      "Epoch 239/1000\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.3358 - accuracy: 0.8652 - val_loss: 2.1082 - val_accuracy: 0.6773\n",
      "Epoch 240/1000\n",
      "83/83 [==============================] - 0s 639us/step - loss: 0.2900 - accuracy: 0.8773 - val_loss: 2.2248 - val_accuracy: 0.6591\n",
      "Epoch 241/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.2860 - accuracy: 0.8879 - val_loss: 2.2711 - val_accuracy: 0.6182\n",
      "Epoch 242/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.2668 - accuracy: 0.9106 - val_loss: 2.1876 - val_accuracy: 0.6545\n",
      "Epoch 243/1000\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.3415 - accuracy: 0.8742 - val_loss: 2.3592 - val_accuracy: 0.6455\n",
      "Epoch 244/1000\n",
      "83/83 [==============================] - 0s 648us/step - loss: 0.3070 - accuracy: 0.8788 - val_loss: 2.2867 - val_accuracy: 0.6727\n",
      "Epoch 245/1000\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.4746 - accuracy: 0.8121 - val_loss: 2.4830 - val_accuracy: 0.5955\n",
      "Epoch 246/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.3157 - accuracy: 0.8788 - val_loss: 2.1841 - val_accuracy: 0.6500\n",
      "Epoch 247/1000\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.2718 - accuracy: 0.8939 - val_loss: 2.1744 - val_accuracy: 0.6818\n",
      "Epoch 248/1000\n",
      "83/83 [==============================] - 0s 653us/step - loss: 0.2894 - accuracy: 0.8848 - val_loss: 2.4136 - val_accuracy: 0.6727\n",
      "Epoch 249/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.3388 - accuracy: 0.8500 - val_loss: 2.3064 - val_accuracy: 0.6682\n",
      "Epoch 250/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.2710 - accuracy: 0.8879 - val_loss: 2.6034 - val_accuracy: 0.5227\n",
      "Epoch 251/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.2920 - accuracy: 0.8818 - val_loss: 2.5513 - val_accuracy: 0.6364\n",
      "Epoch 252/1000\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.3569 - accuracy: 0.8697 - val_loss: 2.2898 - val_accuracy: 0.6682\n",
      "Epoch 253/1000\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.2736 - accuracy: 0.8985 - val_loss: 2.1841 - val_accuracy: 0.7091\n",
      "Epoch 254/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.2784 - accuracy: 0.8970 - val_loss: 2.3248 - val_accuracy: 0.6773\n",
      "Epoch 255/1000\n",
      "83/83 [==============================] - 0s 682us/step - loss: 0.2943 - accuracy: 0.8894 - val_loss: 2.3508 - val_accuracy: 0.6727\n",
      "Epoch 256/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.3075 - accuracy: 0.8788 - val_loss: 2.5460 - val_accuracy: 0.5682\n",
      "Epoch 257/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.3181 - accuracy: 0.8727 - val_loss: 2.4279 - val_accuracy: 0.6500\n",
      "Epoch 258/1000\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.2403 - accuracy: 0.9030 - val_loss: 2.4921 - val_accuracy: 0.5864\n",
      "Epoch 259/1000\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.2816 - accuracy: 0.8909 - val_loss: 2.5409 - val_accuracy: 0.6409\n",
      "Epoch 260/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.3717 - accuracy: 0.8455 - val_loss: 2.4333 - val_accuracy: 0.6318\n",
      "Epoch 261/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.3221 - accuracy: 0.8742 - val_loss: 2.4521 - val_accuracy: 0.6318\n",
      "Epoch 262/1000\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.2658 - accuracy: 0.8939 - val_loss: 2.3396 - val_accuracy: 0.6909\n",
      "Epoch 263/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.2529 - accuracy: 0.9106 - val_loss: 2.3771 - val_accuracy: 0.6773\n",
      "Epoch 264/1000\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.2850 - accuracy: 0.8833 - val_loss: 2.3325 - val_accuracy: 0.6636\n",
      "Epoch 265/1000\n",
      "83/83 [==============================] - 0s 651us/step - loss: 0.2632 - accuracy: 0.8894 - val_loss: 2.7437 - val_accuracy: 0.6136\n",
      "Epoch 266/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.2672 - accuracy: 0.9015 - val_loss: 2.4436 - val_accuracy: 0.6273\n",
      "Epoch 267/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.2549 - accuracy: 0.9045 - val_loss: 2.3915 - val_accuracy: 0.6545\n",
      "Epoch 268/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.2679 - accuracy: 0.8909 - val_loss: 2.5402 - val_accuracy: 0.6182\n",
      "Epoch 269/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.2576 - accuracy: 0.8970 - val_loss: 2.4401 - val_accuracy: 0.6591\n",
      "Epoch 270/1000\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.2694 - accuracy: 0.8939 - val_loss: 2.5550 - val_accuracy: 0.6091\n",
      "Epoch 271/1000\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.2374 - accuracy: 0.9106 - val_loss: 2.4906 - val_accuracy: 0.6591\n",
      "Epoch 272/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.2694 - accuracy: 0.8939 - val_loss: 2.4952 - val_accuracy: 0.6318\n",
      "Epoch 273/1000\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.2770 - accuracy: 0.8879 - val_loss: 2.5066 - val_accuracy: 0.6636\n",
      "Epoch 274/1000\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.3237 - accuracy: 0.8667 - val_loss: 2.4801 - val_accuracy: 0.6227\n",
      "Epoch 275/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.2604 - accuracy: 0.9076 - val_loss: 2.5087 - val_accuracy: 0.6273\n",
      "Epoch 276/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 633us/step - loss: 0.2572 - accuracy: 0.8985 - val_loss: 2.6944 - val_accuracy: 0.6545\n",
      "Epoch 277/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.3261 - accuracy: 0.8773 - val_loss: 2.5783 - val_accuracy: 0.6364\n",
      "Epoch 278/1000\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.2411 - accuracy: 0.9106 - val_loss: 2.6067 - val_accuracy: 0.6409\n",
      "Epoch 279/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.2897 - accuracy: 0.8909 - val_loss: 2.4559 - val_accuracy: 0.6682\n",
      "Epoch 280/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.2901 - accuracy: 0.8924 - val_loss: 2.7018 - val_accuracy: 0.6136\n",
      "Epoch 281/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.2833 - accuracy: 0.8727 - val_loss: 2.4432 - val_accuracy: 0.6636\n",
      "Epoch 282/1000\n",
      "83/83 [==============================] - 0s 648us/step - loss: 0.3274 - accuracy: 0.8667 - val_loss: 2.6400 - val_accuracy: 0.6545\n",
      "Epoch 283/1000\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.3494 - accuracy: 0.8561 - val_loss: 2.3198 - val_accuracy: 0.6773\n",
      "Epoch 284/1000\n",
      "83/83 [==============================] - 0s 641us/step - loss: 0.2587 - accuracy: 0.9030 - val_loss: 2.4365 - val_accuracy: 0.6818\n",
      "Epoch 285/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.2908 - accuracy: 0.8833 - val_loss: 2.6347 - val_accuracy: 0.6364\n",
      "Epoch 286/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.2624 - accuracy: 0.8970 - val_loss: 2.4164 - val_accuracy: 0.7091\n",
      "Epoch 287/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.2309 - accuracy: 0.9091 - val_loss: 2.5268 - val_accuracy: 0.6227\n",
      "Epoch 288/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.2755 - accuracy: 0.8894 - val_loss: 2.7220 - val_accuracy: 0.5273\n",
      "Epoch 289/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.3600 - accuracy: 0.8561 - val_loss: 2.4518 - val_accuracy: 0.6818\n",
      "Epoch 290/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.2594 - accuracy: 0.9000 - val_loss: 2.7988 - val_accuracy: 0.6273\n",
      "Epoch 291/1000\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.2752 - accuracy: 0.8803 - val_loss: 2.5090 - val_accuracy: 0.6636\n",
      "Epoch 292/1000\n",
      "83/83 [==============================] - 0s 648us/step - loss: 0.2700 - accuracy: 0.8970 - val_loss: 2.7211 - val_accuracy: 0.6364\n",
      "Epoch 293/1000\n",
      "83/83 [==============================] - 0s 656us/step - loss: 0.2295 - accuracy: 0.9152 - val_loss: 2.5806 - val_accuracy: 0.6591\n",
      "Epoch 294/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.2343 - accuracy: 0.9061 - val_loss: 2.6909 - val_accuracy: 0.6318\n",
      "Epoch 295/1000\n",
      "83/83 [==============================] - 0s 651us/step - loss: 0.2767 - accuracy: 0.8864 - val_loss: 2.7121 - val_accuracy: 0.6455\n",
      "Epoch 296/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.2756 - accuracy: 0.8970 - val_loss: 2.7221 - val_accuracy: 0.6500\n",
      "Epoch 297/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.2390 - accuracy: 0.9076 - val_loss: 2.5491 - val_accuracy: 0.6773\n",
      "Epoch 298/1000\n",
      "83/83 [==============================] - 0s 668us/step - loss: 0.2455 - accuracy: 0.9000 - val_loss: 2.7400 - val_accuracy: 0.6500\n",
      "Epoch 299/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.2929 - accuracy: 0.8818 - val_loss: 2.8695 - val_accuracy: 0.5091\n",
      "Epoch 300/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.3734 - accuracy: 0.8485 - val_loss: 2.5600 - val_accuracy: 0.6682\n",
      "Epoch 301/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.2319 - accuracy: 0.9121 - val_loss: 2.5716 - val_accuracy: 0.6818\n",
      "Epoch 302/1000\n",
      "83/83 [==============================] - 0s 639us/step - loss: 0.2462 - accuracy: 0.9076 - val_loss: 2.7138 - val_accuracy: 0.6182\n",
      "Epoch 303/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.4168 - accuracy: 0.8561 - val_loss: 2.5351 - val_accuracy: 0.6318\n",
      "Epoch 304/1000\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.2793 - accuracy: 0.8833 - val_loss: 2.6404 - val_accuracy: 0.6455\n",
      "Epoch 305/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.2114 - accuracy: 0.9136 - val_loss: 2.8343 - val_accuracy: 0.6227\n",
      "Epoch 306/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.2638 - accuracy: 0.8939 - val_loss: 2.6238 - val_accuracy: 0.6636\n",
      "Epoch 307/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.1966 - accuracy: 0.9212 - val_loss: 2.7205 - val_accuracy: 0.6591\n",
      "Epoch 308/1000\n",
      "83/83 [==============================] - 0s 824us/step - loss: 0.2420 - accuracy: 0.8985 - val_loss: 2.8640 - val_accuracy: 0.6182\n",
      "Epoch 309/1000\n",
      "83/83 [==============================] - 0s 679us/step - loss: 0.2303 - accuracy: 0.9197 - val_loss: 2.8944 - val_accuracy: 0.6318\n",
      "Epoch 310/1000\n",
      "83/83 [==============================] - 0s 668us/step - loss: 0.2491 - accuracy: 0.9000 - val_loss: 2.6753 - val_accuracy: 0.6273\n",
      "Epoch 311/1000\n",
      "83/83 [==============================] - 0s 684us/step - loss: 0.2394 - accuracy: 0.9106 - val_loss: 2.8222 - val_accuracy: 0.6682\n",
      "Epoch 312/1000\n",
      "83/83 [==============================] - 0s 668us/step - loss: 0.2612 - accuracy: 0.9000 - val_loss: 2.9903 - val_accuracy: 0.6182\n",
      "Epoch 313/1000\n",
      "83/83 [==============================] - 0s 667us/step - loss: 0.2491 - accuracy: 0.9091 - val_loss: 2.6217 - val_accuracy: 0.7000\n",
      "Epoch 314/1000\n",
      "83/83 [==============================] - 0s 748us/step - loss: 0.2439 - accuracy: 0.9000 - val_loss: 2.7323 - val_accuracy: 0.6773\n",
      "Epoch 315/1000\n",
      "83/83 [==============================] - 0s 676us/step - loss: 0.2280 - accuracy: 0.9152 - val_loss: 2.7573 - val_accuracy: 0.6500\n",
      "Epoch 316/1000\n",
      "83/83 [==============================] - 0s 663us/step - loss: 0.2105 - accuracy: 0.9136 - val_loss: 2.8437 - val_accuracy: 0.6364\n",
      "Epoch 317/1000\n",
      "83/83 [==============================] - 0s 715us/step - loss: 0.2752 - accuracy: 0.8894 - val_loss: 2.7619 - val_accuracy: 0.6182\n",
      "Epoch 318/1000\n",
      "83/83 [==============================] - 0s 654us/step - loss: 0.2344 - accuracy: 0.9076 - val_loss: 2.8290 - val_accuracy: 0.6727\n",
      "Epoch 319/1000\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.3543 - accuracy: 0.8712 - val_loss: 2.8362 - val_accuracy: 0.6136\n",
      "Epoch 320/1000\n",
      "83/83 [==============================] - 0s 666us/step - loss: 0.2747 - accuracy: 0.9061 - val_loss: 2.6384 - val_accuracy: 0.6545\n",
      "Epoch 321/1000\n",
      "83/83 [==============================] - 0s 661us/step - loss: 0.2063 - accuracy: 0.9273 - val_loss: 2.7670 - val_accuracy: 0.6727\n",
      "Epoch 322/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.2755 - accuracy: 0.8985 - val_loss: 3.0923 - val_accuracy: 0.6273\n",
      "Epoch 323/1000\n",
      "83/83 [==============================] - 0s 686us/step - loss: 0.2546 - accuracy: 0.8879 - val_loss: 2.8927 - val_accuracy: 0.6591\n",
      "Epoch 324/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.2706 - accuracy: 0.8985 - val_loss: 2.8143 - val_accuracy: 0.6455\n",
      "Epoch 325/1000\n",
      "83/83 [==============================] - 0s 672us/step - loss: 0.2743 - accuracy: 0.8939 - val_loss: 2.9873 - val_accuracy: 0.6000\n",
      "Epoch 326/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.2864 - accuracy: 0.8773 - val_loss: 2.8234 - val_accuracy: 0.6455\n",
      "Epoch 327/1000\n",
      "83/83 [==============================] - 0s 649us/step - loss: 0.2240 - accuracy: 0.9121 - val_loss: 2.7680 - val_accuracy: 0.6545\n",
      "Epoch 328/1000\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.2129 - accuracy: 0.9212 - val_loss: 2.7119 - val_accuracy: 0.6727\n",
      "Epoch 329/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.2109 - accuracy: 0.9136 - val_loss: 3.0703 - val_accuracy: 0.6318\n",
      "Epoch 330/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.2660 - accuracy: 0.8939 - val_loss: 2.8417 - val_accuracy: 0.6409\n",
      "Epoch 331/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.2417 - accuracy: 0.9061 - val_loss: 2.7693 - val_accuracy: 0.6636\n",
      "Epoch 332/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 606us/step - loss: 0.2263 - accuracy: 0.9197 - val_loss: 2.9298 - val_accuracy: 0.6545\n",
      "Epoch 333/1000\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.2088 - accuracy: 0.9212 - val_loss: 2.8428 - val_accuracy: 0.6773\n",
      "Epoch 334/1000\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.2188 - accuracy: 0.9136 - val_loss: 3.0408 - val_accuracy: 0.6045\n",
      "Epoch 335/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.2954 - accuracy: 0.8727 - val_loss: 2.7717 - val_accuracy: 0.6545\n",
      "Epoch 336/1000\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.3127 - accuracy: 0.8803 - val_loss: 2.7999 - val_accuracy: 0.6409\n",
      "Epoch 337/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.1987 - accuracy: 0.9333 - val_loss: 2.7529 - val_accuracy: 0.6909\n",
      "Epoch 338/1000\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.1726 - accuracy: 0.9409 - val_loss: 3.1118 - val_accuracy: 0.6091\n",
      "Epoch 339/1000\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.2543 - accuracy: 0.9030 - val_loss: 3.0193 - val_accuracy: 0.6227\n",
      "Epoch 340/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.2605 - accuracy: 0.8848 - val_loss: 3.0058 - val_accuracy: 0.6364\n",
      "Epoch 341/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.1966 - accuracy: 0.9273 - val_loss: 2.8962 - val_accuracy: 0.6773\n",
      "Epoch 342/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.1815 - accuracy: 0.9318 - val_loss: 2.9667 - val_accuracy: 0.6636\n",
      "Epoch 343/1000\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.2917 - accuracy: 0.8833 - val_loss: 2.8204 - val_accuracy: 0.6273\n",
      "Epoch 344/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.1960 - accuracy: 0.9364 - val_loss: 2.9859 - val_accuracy: 0.6409\n",
      "Epoch 345/1000\n",
      "83/83 [==============================] - 0s 639us/step - loss: 0.1864 - accuracy: 0.9212 - val_loss: 2.9471 - val_accuracy: 0.6682\n",
      "Epoch 346/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.1868 - accuracy: 0.9348 - val_loss: 3.1125 - val_accuracy: 0.6182\n",
      "Epoch 347/1000\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.2520 - accuracy: 0.8970 - val_loss: 3.1569 - val_accuracy: 0.6182\n",
      "Epoch 348/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.1870 - accuracy: 0.9288 - val_loss: 3.0987 - val_accuracy: 0.6455\n",
      "Epoch 349/1000\n",
      "83/83 [==============================] - 0s 639us/step - loss: 0.2154 - accuracy: 0.9106 - val_loss: 3.0999 - val_accuracy: 0.6636\n",
      "Epoch 350/1000\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.1781 - accuracy: 0.9394 - val_loss: 3.1910 - val_accuracy: 0.6500\n",
      "Epoch 351/1000\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2516 - accuracy: 0.9106 - val_loss: 3.7595 - val_accuracy: 0.4955\n",
      "Epoch 352/1000\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.2205 - accuracy: 0.9182 - val_loss: 2.8493 - val_accuracy: 0.6409\n",
      "Epoch 353/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.2488 - accuracy: 0.9000 - val_loss: 2.7929 - val_accuracy: 0.6727\n",
      "Epoch 354/1000\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.2349 - accuracy: 0.9182 - val_loss: 3.0666 - val_accuracy: 0.6273\n",
      "Epoch 355/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.2057 - accuracy: 0.9182 - val_loss: 3.3579 - val_accuracy: 0.5364\n",
      "Epoch 356/1000\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.2422 - accuracy: 0.9030 - val_loss: 3.2269 - val_accuracy: 0.6091\n",
      "Epoch 357/1000\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2097 - accuracy: 0.9227 - val_loss: 2.9014 - val_accuracy: 0.6909\n",
      "Epoch 358/1000\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.1954 - accuracy: 0.9197 - val_loss: 2.9935 - val_accuracy: 0.6500\n",
      "Epoch 359/1000\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.2566 - accuracy: 0.9045 - val_loss: 3.0335 - val_accuracy: 0.6409\n",
      "Epoch 360/1000\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.2127 - accuracy: 0.9212 - val_loss: 3.0232 - val_accuracy: 0.6636\n",
      "Epoch 361/1000\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.1630 - accuracy: 0.9470 - val_loss: 2.9924 - val_accuracy: 0.6318\n",
      "Epoch 362/1000\n",
      "83/83 [==============================] - 0s 724us/step - loss: 0.2115 - accuracy: 0.9212 - val_loss: 2.8498 - val_accuracy: 0.6818\n",
      "Epoch 363/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.1734 - accuracy: 0.9318 - val_loss: 2.9469 - val_accuracy: 0.6818\n",
      "Epoch 364/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.1532 - accuracy: 0.9394 - val_loss: 2.8839 - val_accuracy: 0.6727\n",
      "Epoch 365/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.1600 - accuracy: 0.9439 - val_loss: 3.0301 - val_accuracy: 0.6545\n",
      "Epoch 366/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.1764 - accuracy: 0.9303 - val_loss: 2.9372 - val_accuracy: 0.6500\n",
      "Epoch 367/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.1964 - accuracy: 0.9364 - val_loss: 2.9320 - val_accuracy: 0.6682\n",
      "Epoch 368/1000\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.2038 - accuracy: 0.9318 - val_loss: 3.9728 - val_accuracy: 0.5364\n",
      "Epoch 369/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.4627 - accuracy: 0.8318 - val_loss: 2.9402 - val_accuracy: 0.6727\n",
      "Epoch 370/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.2348 - accuracy: 0.9030 - val_loss: 2.7897 - val_accuracy: 0.6864\n",
      "Epoch 371/1000\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.2260 - accuracy: 0.9091 - val_loss: 3.0822 - val_accuracy: 0.6409\n",
      "Epoch 372/1000\n",
      "83/83 [==============================] - 0s 710us/step - loss: 0.2042 - accuracy: 0.9303 - val_loss: 2.9089 - val_accuracy: 0.6636\n",
      "Epoch 373/1000\n",
      "83/83 [==============================] - 0s 662us/step - loss: 0.1836 - accuracy: 0.9348 - val_loss: 3.0365 - val_accuracy: 0.6636\n",
      "Epoch 374/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.1841 - accuracy: 0.9258 - val_loss: 3.0804 - val_accuracy: 0.6409\n",
      "Epoch 375/1000\n",
      "83/83 [==============================] - 0s 641us/step - loss: 0.2259 - accuracy: 0.9121 - val_loss: 3.1740 - val_accuracy: 0.6409\n",
      "Epoch 376/1000\n",
      "83/83 [==============================] - 0s 641us/step - loss: 0.2076 - accuracy: 0.9136 - val_loss: 3.2338 - val_accuracy: 0.6409\n",
      "Epoch 377/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.1906 - accuracy: 0.9348 - val_loss: 2.9072 - val_accuracy: 0.6773\n",
      "Epoch 378/1000\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.2256 - accuracy: 0.9106 - val_loss: 3.1081 - val_accuracy: 0.6409\n",
      "Epoch 379/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.3873 - accuracy: 0.8439 - val_loss: 2.9517 - val_accuracy: 0.6273\n",
      "Epoch 380/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 2.9485 - val_accuracy: 0.6500\n",
      "Epoch 381/1000\n",
      "83/83 [==============================] - 0s 648us/step - loss: 0.1751 - accuracy: 0.9394 - val_loss: 2.9838 - val_accuracy: 0.6591\n",
      "Epoch 382/1000\n",
      "83/83 [==============================] - 0s 715us/step - loss: 0.1872 - accuracy: 0.9273 - val_loss: 3.0279 - val_accuracy: 0.6455\n",
      "Epoch 383/1000\n",
      "83/83 [==============================] - 0s 727us/step - loss: 0.2300 - accuracy: 0.9106 - val_loss: 3.0483 - val_accuracy: 0.6591\n",
      "Epoch 384/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.1841 - accuracy: 0.9303 - val_loss: 3.0777 - val_accuracy: 0.6727\n",
      "Epoch 385/1000\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.1613 - accuracy: 0.9439 - val_loss: 3.1205 - val_accuracy: 0.6409\n",
      "Epoch 386/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.2188 - accuracy: 0.9076 - val_loss: 3.1035 - val_accuracy: 0.6727\n",
      "Epoch 387/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.1441 - accuracy: 0.9515 - val_loss: 2.9752 - val_accuracy: 0.6773\n",
      "Epoch 388/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 620us/step - loss: 0.2027 - accuracy: 0.9121 - val_loss: 3.2252 - val_accuracy: 0.6455\n",
      "Epoch 389/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.1997 - accuracy: 0.9242 - val_loss: 3.0099 - val_accuracy: 0.6818\n",
      "Epoch 390/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.1701 - accuracy: 0.9288 - val_loss: 3.0651 - val_accuracy: 0.6091\n",
      "Epoch 391/1000\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.1717 - accuracy: 0.9318 - val_loss: 3.1187 - val_accuracy: 0.6682\n",
      "Epoch 392/1000\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.1312 - accuracy: 0.9576 - val_loss: 3.2179 - val_accuracy: 0.6409\n",
      "Epoch 393/1000\n",
      "83/83 [==============================] - 0s 644us/step - loss: 0.1620 - accuracy: 0.9364 - val_loss: 3.1119 - val_accuracy: 0.6818\n",
      "Epoch 394/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.1551 - accuracy: 0.9394 - val_loss: 3.2190 - val_accuracy: 0.6227\n",
      "Epoch 395/1000\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.1836 - accuracy: 0.9333 - val_loss: 3.1537 - val_accuracy: 0.6773\n",
      "Epoch 396/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.1730 - accuracy: 0.9394 - val_loss: 3.1474 - val_accuracy: 0.6409\n",
      "Epoch 397/1000\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.3190 - accuracy: 0.8833 - val_loss: 3.1536 - val_accuracy: 0.6455\n",
      "Epoch 398/1000\n",
      "83/83 [==============================] - 0s 664us/step - loss: 0.1985 - accuracy: 0.9182 - val_loss: 2.9796 - val_accuracy: 0.6591\n",
      "Epoch 399/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.1632 - accuracy: 0.9455 - val_loss: 2.9754 - val_accuracy: 0.6682\n",
      "Epoch 400/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.3030 - accuracy: 0.8727 - val_loss: 3.0693 - val_accuracy: 0.6500\n",
      "Epoch 401/1000\n",
      "83/83 [==============================] - 0s 658us/step - loss: 0.1762 - accuracy: 0.9288 - val_loss: 2.9661 - val_accuracy: 0.6773\n",
      "Epoch 402/1000\n",
      "83/83 [==============================] - 0s 641us/step - loss: 0.1276 - accuracy: 0.9576 - val_loss: 3.1959 - val_accuracy: 0.5727\n",
      "Epoch 403/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.1677 - accuracy: 0.9318 - val_loss: 3.2680 - val_accuracy: 0.6500\n",
      "Epoch 404/1000\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.1463 - accuracy: 0.9561 - val_loss: 3.1717 - val_accuracy: 0.6636\n",
      "Epoch 405/1000\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.1722 - accuracy: 0.9303 - val_loss: 3.2538 - val_accuracy: 0.6273\n",
      "Epoch 406/1000\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.1790 - accuracy: 0.9273 - val_loss: 3.1917 - val_accuracy: 0.6500\n",
      "Epoch 407/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.1638 - accuracy: 0.9409 - val_loss: 3.1867 - val_accuracy: 0.6591\n",
      "Epoch 408/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.1488 - accuracy: 0.9439 - val_loss: 3.2380 - val_accuracy: 0.6455\n",
      "Epoch 409/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.2697 - accuracy: 0.8939 - val_loss: 3.6051 - val_accuracy: 0.6000\n",
      "Epoch 410/1000\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.2474 - accuracy: 0.9227 - val_loss: 3.0588 - val_accuracy: 0.6682\n",
      "Epoch 411/1000\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.1393 - accuracy: 0.9470 - val_loss: 3.0456 - val_accuracy: 0.6545\n",
      "Epoch 412/1000\n",
      "83/83 [==============================] - 0s 639us/step - loss: 0.1989 - accuracy: 0.9212 - val_loss: 2.9909 - val_accuracy: 0.6682\n",
      "Epoch 413/1000\n",
      "83/83 [==============================] - 0s 651us/step - loss: 0.1921 - accuracy: 0.9288 - val_loss: 3.0480 - val_accuracy: 0.6818\n",
      "Epoch 414/1000\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.1506 - accuracy: 0.9515 - val_loss: 3.1595 - val_accuracy: 0.6636\n",
      "Epoch 415/1000\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.2273 - accuracy: 0.9182 - val_loss: 3.2094 - val_accuracy: 0.6818\n",
      "Epoch 416/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.1789 - accuracy: 0.9424 - val_loss: 3.2757 - val_accuracy: 0.6455\n",
      "Epoch 417/1000\n",
      "83/83 [==============================] - 0s 670us/step - loss: 0.1642 - accuracy: 0.9348 - val_loss: 3.3932 - val_accuracy: 0.5500\n",
      "Epoch 418/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.2389 - accuracy: 0.9091 - val_loss: 3.1496 - val_accuracy: 0.6727\n",
      "Epoch 419/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.1633 - accuracy: 0.9409 - val_loss: 3.2713 - val_accuracy: 0.6682\n",
      "Epoch 420/1000\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.1594 - accuracy: 0.9364 - val_loss: 3.3418 - val_accuracy: 0.6455\n",
      "Epoch 421/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.1886 - accuracy: 0.9273 - val_loss: 3.3129 - val_accuracy: 0.6318\n",
      "Epoch 422/1000\n",
      "83/83 [==============================] - 0s 656us/step - loss: 0.1805 - accuracy: 0.9258 - val_loss: 3.2695 - val_accuracy: 0.6455\n",
      "Epoch 423/1000\n",
      "83/83 [==============================] - 0s 646us/step - loss: 0.1619 - accuracy: 0.9333 - val_loss: 3.6670 - val_accuracy: 0.6045\n",
      "Epoch 424/1000\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.2341 - accuracy: 0.9015 - val_loss: 3.0642 - val_accuracy: 0.6227\n",
      "Epoch 425/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.2294 - accuracy: 0.9030 - val_loss: 3.3583 - val_accuracy: 0.6364\n",
      "Epoch 426/1000\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.1827 - accuracy: 0.9318 - val_loss: 3.3561 - val_accuracy: 0.6364\n",
      "Epoch 427/1000\n",
      "83/83 [==============================] - 0s 662us/step - loss: 0.1642 - accuracy: 0.9409 - val_loss: 3.1872 - val_accuracy: 0.6636\n",
      "Epoch 428/1000\n",
      "83/83 [==============================] - 0s 652us/step - loss: 0.1699 - accuracy: 0.9364 - val_loss: 3.1528 - val_accuracy: 0.6727\n",
      "Epoch 429/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.2183 - accuracy: 0.9242 - val_loss: 3.1689 - val_accuracy: 0.6455\n",
      "Epoch 430/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.1694 - accuracy: 0.9364 - val_loss: 3.0294 - val_accuracy: 0.6500\n",
      "Epoch 431/1000\n",
      "83/83 [==============================] - 0s 712us/step - loss: 0.1541 - accuracy: 0.9500 - val_loss: 3.1881 - val_accuracy: 0.6636\n",
      "Epoch 432/1000\n",
      "83/83 [==============================] - 0s 760us/step - loss: 0.1889 - accuracy: 0.9258 - val_loss: 3.2012 - val_accuracy: 0.6409\n",
      "Epoch 433/1000\n",
      "83/83 [==============================] - 0s 680us/step - loss: 0.1728 - accuracy: 0.9394 - val_loss: 3.3193 - val_accuracy: 0.6500\n",
      "Epoch 434/1000\n",
      "83/83 [==============================] - 0s 686us/step - loss: 0.1931 - accuracy: 0.9121 - val_loss: 3.2239 - val_accuracy: 0.6182\n",
      "Epoch 435/1000\n",
      "83/83 [==============================] - 0s 690us/step - loss: 0.1691 - accuracy: 0.9288 - val_loss: 3.1464 - val_accuracy: 0.6818\n",
      "Epoch 436/1000\n",
      "83/83 [==============================] - 0s 657us/step - loss: 0.1797 - accuracy: 0.9242 - val_loss: 3.3648 - val_accuracy: 0.6455\n",
      "Epoch 437/1000\n",
      "83/83 [==============================] - 0s 660us/step - loss: 0.1823 - accuracy: 0.9318 - val_loss: 3.3540 - val_accuracy: 0.6318\n",
      "Epoch 438/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.1308 - accuracy: 0.9561 - val_loss: 3.1703 - val_accuracy: 0.6818\n",
      "Epoch 439/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.1363 - accuracy: 0.9500 - val_loss: 3.2619 - val_accuracy: 0.6682\n",
      "Epoch 440/1000\n",
      "83/83 [==============================] - 0s 699us/step - loss: 0.1127 - accuracy: 0.9561 - val_loss: 3.2881 - val_accuracy: 0.6409\n",
      "Epoch 441/1000\n",
      "83/83 [==============================] - 0s 693us/step - loss: 0.1089 - accuracy: 0.9621 - val_loss: 3.2985 - val_accuracy: 0.6455\n",
      "Epoch 442/1000\n",
      "83/83 [==============================] - 0s 654us/step - loss: 0.2166 - accuracy: 0.9152 - val_loss: 3.5385 - val_accuracy: 0.5636\n",
      "Epoch 443/1000\n",
      "83/83 [==============================] - 0s 655us/step - loss: 0.2403 - accuracy: 0.9045 - val_loss: 3.3467 - val_accuracy: 0.6091\n",
      "Epoch 444/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 759us/step - loss: 0.2664 - accuracy: 0.8985 - val_loss: 3.7788 - val_accuracy: 0.5955\n",
      "Epoch 445/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.5136 - accuracy: 0.8409 - val_loss: 3.1541 - val_accuracy: 0.6636\n",
      "Epoch 446/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.1663 - accuracy: 0.9485 - val_loss: 3.0709 - val_accuracy: 0.6773\n",
      "Epoch 447/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.1469 - accuracy: 0.9439 - val_loss: 3.0088 - val_accuracy: 0.6682\n",
      "Epoch 448/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.1114 - accuracy: 0.9606 - val_loss: 3.0503 - val_accuracy: 0.6909\n",
      "Epoch 449/1000\n",
      "83/83 [==============================] - 0s 655us/step - loss: 0.1467 - accuracy: 0.9439 - val_loss: 3.2140 - val_accuracy: 0.6636\n",
      "Epoch 450/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.1107 - accuracy: 0.9606 - val_loss: 3.1187 - val_accuracy: 0.6773\n",
      "Epoch 451/1000\n",
      "83/83 [==============================] - 0s 644us/step - loss: 0.1713 - accuracy: 0.9333 - val_loss: 3.3051 - val_accuracy: 0.6318\n",
      "Epoch 452/1000\n",
      "83/83 [==============================] - 0s 652us/step - loss: 0.2499 - accuracy: 0.9045 - val_loss: 3.3069 - val_accuracy: 0.6773\n",
      "Epoch 453/1000\n",
      "83/83 [==============================] - 0s 683us/step - loss: 0.1479 - accuracy: 0.9439 - val_loss: 3.1979 - val_accuracy: 0.6591\n",
      "Epoch 454/1000\n",
      "83/83 [==============================] - 0s 654us/step - loss: 0.1540 - accuracy: 0.9348 - val_loss: 3.3087 - val_accuracy: 0.6682\n",
      "Epoch 455/1000\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.1797 - accuracy: 0.9288 - val_loss: 3.3086 - val_accuracy: 0.6409\n",
      "Epoch 456/1000\n",
      "83/83 [==============================] - 0s 653us/step - loss: 0.1548 - accuracy: 0.9364 - val_loss: 3.3278 - val_accuracy: 0.6864\n",
      "Epoch 457/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.1170 - accuracy: 0.9591 - val_loss: 3.1359 - val_accuracy: 0.6727\n",
      "Epoch 458/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.1272 - accuracy: 0.9576 - val_loss: 3.3791 - val_accuracy: 0.6364\n",
      "Epoch 459/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.2737 - accuracy: 0.8985 - val_loss: 3.4463 - val_accuracy: 0.6364\n",
      "Epoch 460/1000\n",
      "83/83 [==============================] - 0s 646us/step - loss: 0.3369 - accuracy: 0.8712 - val_loss: 3.6288 - val_accuracy: 0.6318\n",
      "Epoch 461/1000\n",
      "83/83 [==============================] - 0s 658us/step - loss: 0.2228 - accuracy: 0.9106 - val_loss: 3.1890 - val_accuracy: 0.6545\n",
      "Epoch 462/1000\n",
      "83/83 [==============================] - 0s 696us/step - loss: 0.1386 - accuracy: 0.9500 - val_loss: 3.1978 - val_accuracy: 0.6773\n",
      "Epoch 463/1000\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.1824 - accuracy: 0.9303 - val_loss: 3.2073 - val_accuracy: 0.6682\n",
      "Epoch 464/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.1578 - accuracy: 0.9379 - val_loss: 3.3524 - val_accuracy: 0.6636\n",
      "Epoch 465/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.1130 - accuracy: 0.9636 - val_loss: 3.1721 - val_accuracy: 0.7136\n",
      "Epoch 466/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.1129 - accuracy: 0.9636 - val_loss: 3.4922 - val_accuracy: 0.6045\n",
      "Epoch 467/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.1591 - accuracy: 0.9424 - val_loss: 3.4727 - val_accuracy: 0.6455\n",
      "Epoch 468/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.1368 - accuracy: 0.9424 - val_loss: 3.3670 - val_accuracy: 0.6409\n",
      "Epoch 469/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.1762 - accuracy: 0.9364 - val_loss: 3.1546 - val_accuracy: 0.6955\n",
      "Epoch 470/1000\n",
      "83/83 [==============================] - 0s 673us/step - loss: 0.1088 - accuracy: 0.9652 - val_loss: 3.3709 - val_accuracy: 0.6864\n",
      "Epoch 471/1000\n",
      "83/83 [==============================] - 0s 696us/step - loss: 0.1080 - accuracy: 0.9561 - val_loss: 3.3385 - val_accuracy: 0.6682\n",
      "Epoch 472/1000\n",
      "83/83 [==============================] - 0s 672us/step - loss: 0.1476 - accuracy: 0.9439 - val_loss: 3.2149 - val_accuracy: 0.6864\n",
      "Epoch 473/1000\n",
      "83/83 [==============================] - 0s 679us/step - loss: 0.1848 - accuracy: 0.9273 - val_loss: 3.5793 - val_accuracy: 0.6000\n",
      "Epoch 474/1000\n",
      "83/83 [==============================] - 0s 663us/step - loss: 0.1669 - accuracy: 0.9409 - val_loss: 3.3626 - val_accuracy: 0.6682\n",
      "Epoch 475/1000\n",
      "83/83 [==============================] - 0s 667us/step - loss: 0.1201 - accuracy: 0.9621 - val_loss: 3.4647 - val_accuracy: 0.6455\n",
      "Epoch 476/1000\n",
      "83/83 [==============================] - 0s 662us/step - loss: 0.2078 - accuracy: 0.9197 - val_loss: 3.3753 - val_accuracy: 0.6409\n",
      "Epoch 477/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.2119 - accuracy: 0.9182 - val_loss: 3.5577 - val_accuracy: 0.6500\n",
      "Epoch 478/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.1874 - accuracy: 0.9348 - val_loss: 3.3268 - val_accuracy: 0.6682\n",
      "Epoch 479/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.1670 - accuracy: 0.9348 - val_loss: 3.4114 - val_accuracy: 0.6227\n",
      "Epoch 480/1000\n",
      "83/83 [==============================] - 0s 677us/step - loss: 0.1657 - accuracy: 0.9318 - val_loss: 3.1808 - val_accuracy: 0.6955\n",
      "Epoch 481/1000\n",
      "83/83 [==============================] - 0s 670us/step - loss: 0.1220 - accuracy: 0.9606 - val_loss: 3.2980 - val_accuracy: 0.6545\n",
      "Epoch 482/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.1254 - accuracy: 0.9500 - val_loss: 3.3883 - val_accuracy: 0.6545\n",
      "Epoch 483/1000\n",
      "83/83 [==============================] - 0s 662us/step - loss: 0.3139 - accuracy: 0.8924 - val_loss: 3.3195 - val_accuracy: 0.6682\n",
      "Epoch 484/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.1639 - accuracy: 0.9379 - val_loss: 3.4408 - val_accuracy: 0.6500\n",
      "Epoch 485/1000\n",
      "83/83 [==============================] - 0s 641us/step - loss: 0.1233 - accuracy: 0.9545 - val_loss: 3.3817 - val_accuracy: 0.6818\n",
      "Epoch 486/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.1327 - accuracy: 0.9545 - val_loss: 3.5661 - val_accuracy: 0.6364\n",
      "Epoch 487/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.1467 - accuracy: 0.9364 - val_loss: 3.1971 - val_accuracy: 0.6909\n",
      "Epoch 488/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.1110 - accuracy: 0.9621 - val_loss: 3.3980 - val_accuracy: 0.6909\n",
      "Epoch 489/1000\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.0935 - accuracy: 0.9682 - val_loss: 3.3784 - val_accuracy: 0.6773\n",
      "Epoch 490/1000\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.1686 - accuracy: 0.9379 - val_loss: 3.4409 - val_accuracy: 0.6409\n",
      "Epoch 491/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.2625 - accuracy: 0.9015 - val_loss: 3.2754 - val_accuracy: 0.6545\n",
      "Epoch 492/1000\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.1526 - accuracy: 0.9424 - val_loss: 3.4407 - val_accuracy: 0.6864\n",
      "Epoch 493/1000\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.1521 - accuracy: 0.9409 - val_loss: 3.5112 - val_accuracy: 0.6227\n",
      "Epoch 494/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.1835 - accuracy: 0.9212 - val_loss: 3.3352 - val_accuracy: 0.6545\n",
      "Epoch 495/1000\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.1706 - accuracy: 0.9364 - val_loss: 3.6012 - val_accuracy: 0.6273\n",
      "Epoch 496/1000\n",
      "83/83 [==============================] - 0s 639us/step - loss: 0.1423 - accuracy: 0.9470 - val_loss: 3.5833 - val_accuracy: 0.6545\n",
      "Epoch 497/1000\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.1147 - accuracy: 0.9652 - val_loss: 3.3621 - val_accuracy: 0.6727\n",
      "Epoch 498/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.1097 - accuracy: 0.9621 - val_loss: 3.6070 - val_accuracy: 0.6682\n",
      "Epoch 499/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.1688 - accuracy: 0.9364 - val_loss: 3.6067 - val_accuracy: 0.6500\n",
      "Epoch 500/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 808us/step - loss: 0.1349 - accuracy: 0.9409 - val_loss: 3.6023 - val_accuracy: 0.6636\n",
      "Epoch 501/1000\n",
      "83/83 [==============================] - 0s 641us/step - loss: 0.1168 - accuracy: 0.9485 - val_loss: 3.4785 - val_accuracy: 0.6591\n",
      "Epoch 502/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.1392 - accuracy: 0.9500 - val_loss: 3.3800 - val_accuracy: 0.6955\n",
      "Epoch 503/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.1162 - accuracy: 0.9530 - val_loss: 3.2744 - val_accuracy: 0.6455\n",
      "Epoch 504/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.1229 - accuracy: 0.9515 - val_loss: 3.4213 - val_accuracy: 0.6955\n",
      "Epoch 505/1000\n",
      "83/83 [==============================] - 0s 644us/step - loss: 0.1260 - accuracy: 0.9530 - val_loss: 3.5796 - val_accuracy: 0.6545\n",
      "Epoch 506/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.4053 - accuracy: 0.8409 - val_loss: 3.5193 - val_accuracy: 0.6045\n",
      "Epoch 507/1000\n",
      "83/83 [==============================] - 0s 669us/step - loss: 0.3037 - accuracy: 0.8758 - val_loss: 3.3918 - val_accuracy: 0.6273\n",
      "Epoch 508/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.1554 - accuracy: 0.9455 - val_loss: 3.2804 - val_accuracy: 0.6591\n",
      "Epoch 509/1000\n",
      "83/83 [==============================] - 0s 645us/step - loss: 0.1134 - accuracy: 0.9591 - val_loss: 3.2552 - val_accuracy: 0.6818\n",
      "Epoch 510/1000\n",
      "83/83 [==============================] - 0s 658us/step - loss: 0.1142 - accuracy: 0.9576 - val_loss: 3.4033 - val_accuracy: 0.6545\n",
      "Epoch 511/1000\n",
      "83/83 [==============================] - 0s 639us/step - loss: 0.1188 - accuracy: 0.9561 - val_loss: 3.4398 - val_accuracy: 0.6545\n",
      "Epoch 512/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.1678 - accuracy: 0.9424 - val_loss: 3.3426 - val_accuracy: 0.6773\n",
      "Epoch 513/1000\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.1161 - accuracy: 0.9561 - val_loss: 3.2992 - val_accuracy: 0.6591\n",
      "Epoch 514/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.1207 - accuracy: 0.9545 - val_loss: 3.1733 - val_accuracy: 0.6636\n",
      "Epoch 515/1000\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.1717 - accuracy: 0.9333 - val_loss: 3.6437 - val_accuracy: 0.6227\n",
      "Epoch 516/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.1729 - accuracy: 0.9500 - val_loss: 3.2946 - val_accuracy: 0.6591\n",
      "Epoch 517/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.1367 - accuracy: 0.9515 - val_loss: 3.3577 - val_accuracy: 0.6636\n",
      "Epoch 518/1000\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.1244 - accuracy: 0.9530 - val_loss: 3.1183 - val_accuracy: 0.6864\n",
      "Epoch 519/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.1335 - accuracy: 0.9455 - val_loss: 3.1797 - val_accuracy: 0.6773\n",
      "Epoch 520/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.1480 - accuracy: 0.9470 - val_loss: 3.3955 - val_accuracy: 0.6636\n",
      "Epoch 521/1000\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.1498 - accuracy: 0.9500 - val_loss: 3.5762 - val_accuracy: 0.6455\n",
      "Epoch 522/1000\n",
      "83/83 [==============================] - 0s 645us/step - loss: 0.1237 - accuracy: 0.9545 - val_loss: 3.6568 - val_accuracy: 0.6227\n",
      "Epoch 523/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.1799 - accuracy: 0.9273 - val_loss: 3.6794 - val_accuracy: 0.6091\n",
      "Epoch 524/1000\n",
      "83/83 [==============================] - 0s 663us/step - loss: 0.1466 - accuracy: 0.9424 - val_loss: 3.4700 - val_accuracy: 0.6500\n",
      "Epoch 525/1000\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.0948 - accuracy: 0.9636 - val_loss: 3.5914 - val_accuracy: 0.6636\n",
      "Epoch 526/1000\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.1413 - accuracy: 0.9439 - val_loss: 3.5547 - val_accuracy: 0.6409\n",
      "Epoch 527/1000\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.1319 - accuracy: 0.9500 - val_loss: 3.5117 - val_accuracy: 0.6364\n",
      "Epoch 528/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.1341 - accuracy: 0.9455 - val_loss: 3.6494 - val_accuracy: 0.6364\n",
      "Epoch 529/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.1207 - accuracy: 0.9561 - val_loss: 3.6131 - val_accuracy: 0.6455\n",
      "Epoch 530/1000\n",
      "83/83 [==============================] - 0s 652us/step - loss: 0.2068 - accuracy: 0.9348 - val_loss: 4.2502 - val_accuracy: 0.5318\n",
      "Epoch 531/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.2542 - accuracy: 0.8985 - val_loss: 3.6079 - val_accuracy: 0.6409\n",
      "Epoch 532/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.1349 - accuracy: 0.9485 - val_loss: 3.2536 - val_accuracy: 0.6864\n",
      "Epoch 533/1000\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.1267 - accuracy: 0.9561 - val_loss: 3.4264 - val_accuracy: 0.6909\n",
      "Epoch 534/1000\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.1134 - accuracy: 0.9576 - val_loss: 3.5611 - val_accuracy: 0.6636\n",
      "Epoch 535/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.1810 - accuracy: 0.9167 - val_loss: 3.4802 - val_accuracy: 0.6682\n",
      "Epoch 536/1000\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.1509 - accuracy: 0.9318 - val_loss: 3.4230 - val_accuracy: 0.6455\n",
      "Epoch 537/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.1118 - accuracy: 0.9636 - val_loss: 3.5788 - val_accuracy: 0.6500\n",
      "Epoch 538/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.0890 - accuracy: 0.9712 - val_loss: 3.5967 - val_accuracy: 0.6591\n",
      "Epoch 539/1000\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.0890 - accuracy: 0.9636 - val_loss: 3.4534 - val_accuracy: 0.6727\n",
      "Epoch 540/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.1615 - accuracy: 0.9409 - val_loss: 3.6259 - val_accuracy: 0.6455\n",
      "Epoch 541/1000\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.2348 - accuracy: 0.9197 - val_loss: 3.6784 - val_accuracy: 0.6136\n",
      "Epoch 542/1000\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.1144 - accuracy: 0.9561 - val_loss: 3.6815 - val_accuracy: 0.6227\n",
      "Epoch 543/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.1079 - accuracy: 0.9606 - val_loss: 3.5767 - val_accuracy: 0.6682\n",
      "Epoch 544/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.0951 - accuracy: 0.9727 - val_loss: 3.6498 - val_accuracy: 0.6455\n",
      "Epoch 545/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.1604 - accuracy: 0.9348 - val_loss: 3.5739 - val_accuracy: 0.6727\n",
      "Epoch 546/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.2039 - accuracy: 0.9227 - val_loss: 3.6956 - val_accuracy: 0.6545\n",
      "Epoch 547/1000\n",
      "83/83 [==============================] - 0s 670us/step - loss: 0.1683 - accuracy: 0.9303 - val_loss: 3.5907 - val_accuracy: 0.6227\n",
      "Epoch 548/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.1381 - accuracy: 0.9470 - val_loss: 3.6902 - val_accuracy: 0.6545\n",
      "Epoch 549/1000\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.1336 - accuracy: 0.9576 - val_loss: 3.5824 - val_accuracy: 0.6091\n",
      "Epoch 550/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.1624 - accuracy: 0.9424 - val_loss: 3.8209 - val_accuracy: 0.6273\n",
      "Epoch 551/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.3413 - accuracy: 0.8864 - val_loss: 3.4105 - val_accuracy: 0.6636\n",
      "Epoch 552/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.1343 - accuracy: 0.9455 - val_loss: 3.4445 - val_accuracy: 0.6545\n",
      "Epoch 553/1000\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.1030 - accuracy: 0.9636 - val_loss: 3.5624 - val_accuracy: 0.6818\n",
      "Epoch 554/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.1081 - accuracy: 0.9682 - val_loss: 3.5396 - val_accuracy: 0.6591\n",
      "Epoch 555/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.1010 - accuracy: 0.9636 - val_loss: 3.6480 - val_accuracy: 0.6773\n",
      "Epoch 556/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 618us/step - loss: 0.1190 - accuracy: 0.9515 - val_loss: 3.6143 - val_accuracy: 0.6818\n",
      "Epoch 557/1000\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.1635 - accuracy: 0.9364 - val_loss: 3.5916 - val_accuracy: 0.6636\n",
      "Epoch 558/1000\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.1557 - accuracy: 0.9348 - val_loss: 3.6582 - val_accuracy: 0.6682\n",
      "Epoch 559/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.0948 - accuracy: 0.9667 - val_loss: 3.5302 - val_accuracy: 0.6773\n",
      "Epoch 560/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.1191 - accuracy: 0.9545 - val_loss: 3.7262 - val_accuracy: 0.6682\n",
      "Epoch 561/1000\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.0715 - accuracy: 0.9758 - val_loss: 3.5780 - val_accuracy: 0.6727\n",
      "Epoch 562/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.0946 - accuracy: 0.9682 - val_loss: 3.7230 - val_accuracy: 0.6318\n",
      "Epoch 563/1000\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.0811 - accuracy: 0.9697 - val_loss: 3.5828 - val_accuracy: 0.6409\n",
      "Epoch 564/1000\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.0873 - accuracy: 0.9652 - val_loss: 3.6742 - val_accuracy: 0.6636\n",
      "Epoch 565/1000\n",
      "83/83 [==============================] - 0s 690us/step - loss: 0.0755 - accuracy: 0.9758 - val_loss: 3.7759 - val_accuracy: 0.6727\n",
      "Epoch 566/1000\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.0946 - accuracy: 0.9667 - val_loss: 3.7632 - val_accuracy: 0.6318\n",
      "Epoch 567/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.1869 - accuracy: 0.9364 - val_loss: 3.6052 - val_accuracy: 0.6636\n",
      "Epoch 568/1000\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.1596 - accuracy: 0.9364 - val_loss: 3.6318 - val_accuracy: 0.6864\n",
      "Epoch 569/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.1345 - accuracy: 0.9500 - val_loss: 3.7139 - val_accuracy: 0.6591\n",
      "Epoch 570/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.1731 - accuracy: 0.9394 - val_loss: 4.0344 - val_accuracy: 0.6364\n",
      "Epoch 571/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.1546 - accuracy: 0.9348 - val_loss: 3.7700 - val_accuracy: 0.6455\n",
      "Epoch 572/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.1188 - accuracy: 0.9530 - val_loss: 3.7657 - val_accuracy: 0.6545\n",
      "Epoch 573/1000\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.1246 - accuracy: 0.9545 - val_loss: 3.7626 - val_accuracy: 0.6409\n",
      "Epoch 574/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.2163 - accuracy: 0.9212 - val_loss: 3.7147 - val_accuracy: 0.6364\n",
      "Epoch 575/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.1397 - accuracy: 0.9439 - val_loss: 3.6546 - val_accuracy: 0.6227\n",
      "Epoch 576/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.1585 - accuracy: 0.9333 - val_loss: 3.7500 - val_accuracy: 0.6545\n",
      "Epoch 577/1000\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.1099 - accuracy: 0.9591 - val_loss: 3.7072 - val_accuracy: 0.6545\n",
      "Epoch 578/1000\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.0880 - accuracy: 0.9697 - val_loss: 3.7011 - val_accuracy: 0.6773\n",
      "Epoch 579/1000\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.1729 - accuracy: 0.9288 - val_loss: 3.8477 - val_accuracy: 0.6500\n",
      "Epoch 580/1000\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.1419 - accuracy: 0.9455 - val_loss: 3.8151 - val_accuracy: 0.6091\n",
      "Epoch 581/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.1200 - accuracy: 0.9500 - val_loss: 3.7523 - val_accuracy: 0.6273\n",
      "Epoch 582/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.1785 - accuracy: 0.9258 - val_loss: 3.8752 - val_accuracy: 0.6455\n",
      "Epoch 583/1000\n",
      "83/83 [==============================] - 0s 671us/step - loss: 0.1295 - accuracy: 0.9500 - val_loss: 3.6783 - val_accuracy: 0.6591\n",
      "Epoch 584/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.1051 - accuracy: 0.9621 - val_loss: 3.6495 - val_accuracy: 0.6864\n",
      "Epoch 585/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.0884 - accuracy: 0.9712 - val_loss: 3.6716 - val_accuracy: 0.6773\n",
      "Epoch 586/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.0868 - accuracy: 0.9712 - val_loss: 3.8487 - val_accuracy: 0.6182\n",
      "Epoch 587/1000\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.1199 - accuracy: 0.9606 - val_loss: 3.7261 - val_accuracy: 0.6682\n",
      "Epoch 588/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.1402 - accuracy: 0.9591 - val_loss: 3.8478 - val_accuracy: 0.6364\n",
      "Epoch 589/1000\n",
      "83/83 [==============================] - 0s 645us/step - loss: 0.0808 - accuracy: 0.9727 - val_loss: 3.4807 - val_accuracy: 0.6864\n",
      "Epoch 590/1000\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.0912 - accuracy: 0.9621 - val_loss: 3.8139 - val_accuracy: 0.6455\n",
      "Epoch 591/1000\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.2510 - accuracy: 0.9045 - val_loss: 3.8424 - val_accuracy: 0.6682\n",
      "Epoch 592/1000\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.1469 - accuracy: 0.9485 - val_loss: 3.6438 - val_accuracy: 0.6955\n",
      "Epoch 593/1000\n",
      "83/83 [==============================] - 0s 639us/step - loss: 0.0952 - accuracy: 0.9621 - val_loss: 3.6024 - val_accuracy: 0.6818\n",
      "Epoch 594/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.1003 - accuracy: 0.9561 - val_loss: 3.9484 - val_accuracy: 0.6182\n",
      "Epoch 595/1000\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.1740 - accuracy: 0.9348 - val_loss: 3.9782 - val_accuracy: 0.5955\n",
      "Epoch 596/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.2427 - accuracy: 0.9106 - val_loss: 3.7699 - val_accuracy: 0.6227\n",
      "Epoch 597/1000\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.1180 - accuracy: 0.9591 - val_loss: 3.6978 - val_accuracy: 0.6682\n",
      "Epoch 598/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.0950 - accuracy: 0.9652 - val_loss: 3.7741 - val_accuracy: 0.6636\n",
      "Epoch 599/1000\n",
      "83/83 [==============================] - 0s 704us/step - loss: 0.0626 - accuracy: 0.9818 - val_loss: 3.6102 - val_accuracy: 0.6864\n",
      "Epoch 600/1000\n",
      "83/83 [==============================] - 0s 671us/step - loss: 0.0691 - accuracy: 0.9758 - val_loss: 3.8580 - val_accuracy: 0.6591\n",
      "Epoch 601/1000\n",
      "83/83 [==============================] - 0s 654us/step - loss: 0.0892 - accuracy: 0.9667 - val_loss: 3.7765 - val_accuracy: 0.6682\n",
      "Epoch 602/1000\n",
      "83/83 [==============================] - 0s 655us/step - loss: 0.1504 - accuracy: 0.9348 - val_loss: 3.5623 - val_accuracy: 0.6773\n",
      "Epoch 603/1000\n",
      "83/83 [==============================] - 0s 660us/step - loss: 0.1138 - accuracy: 0.9530 - val_loss: 3.7211 - val_accuracy: 0.6273\n",
      "Epoch 604/1000\n",
      "83/83 [==============================] - 0s 652us/step - loss: 0.1006 - accuracy: 0.9667 - val_loss: 3.6740 - val_accuracy: 0.6909\n",
      "Epoch 605/1000\n",
      "83/83 [==============================] - 0s 680us/step - loss: 0.0950 - accuracy: 0.9606 - val_loss: 3.5953 - val_accuracy: 0.6955\n",
      "Epoch 606/1000\n",
      "83/83 [==============================] - 0s 666us/step - loss: 0.1459 - accuracy: 0.9470 - val_loss: 4.0388 - val_accuracy: 0.6364\n",
      "Epoch 607/1000\n",
      "83/83 [==============================] - 0s 672us/step - loss: 0.1405 - accuracy: 0.9409 - val_loss: 4.0217 - val_accuracy: 0.6364\n",
      "Epoch 608/1000\n",
      "83/83 [==============================] - 0s 662us/step - loss: 0.1030 - accuracy: 0.9636 - val_loss: 3.7053 - val_accuracy: 0.6773\n",
      "Epoch 609/1000\n",
      "83/83 [==============================] - 0s 654us/step - loss: 0.1179 - accuracy: 0.9545 - val_loss: 3.7152 - val_accuracy: 0.6636\n",
      "Epoch 610/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.0804 - accuracy: 0.9727 - val_loss: 3.7004 - val_accuracy: 0.6773\n",
      "Epoch 611/1000\n",
      "83/83 [==============================] - 0s 651us/step - loss: 0.0734 - accuracy: 0.9742 - val_loss: 3.8313 - val_accuracy: 0.6545\n",
      "Epoch 612/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 637us/step - loss: 0.0819 - accuracy: 0.9682 - val_loss: 3.6520 - val_accuracy: 0.6864\n",
      "Epoch 613/1000\n",
      "83/83 [==============================] - 0s 688us/step - loss: 0.0645 - accuracy: 0.9742 - val_loss: 3.7970 - val_accuracy: 0.6636\n",
      "Epoch 614/1000\n",
      "83/83 [==============================] - 0s 651us/step - loss: 0.2027 - accuracy: 0.9167 - val_loss: 3.7899 - val_accuracy: 0.6455\n",
      "Epoch 615/1000\n",
      "83/83 [==============================] - 0s 691us/step - loss: 0.1700 - accuracy: 0.9333 - val_loss: 3.9226 - val_accuracy: 0.6227\n",
      "Epoch 616/1000\n",
      "83/83 [==============================] - 0s 673us/step - loss: 0.2317 - accuracy: 0.9106 - val_loss: 3.7391 - val_accuracy: 0.6818\n",
      "Epoch 617/1000\n",
      "83/83 [==============================] - 0s 651us/step - loss: 0.1216 - accuracy: 0.9515 - val_loss: 3.6196 - val_accuracy: 0.6636\n",
      "Epoch 618/1000\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.0791 - accuracy: 0.9712 - val_loss: 3.7382 - val_accuracy: 0.6682\n",
      "Epoch 619/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.0862 - accuracy: 0.9652 - val_loss: 3.8830 - val_accuracy: 0.6318\n",
      "Epoch 620/1000\n",
      "83/83 [==============================] - 0s 662us/step - loss: 0.1085 - accuracy: 0.9561 - val_loss: 4.1486 - val_accuracy: 0.5636\n",
      "Epoch 621/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.4699 - accuracy: 0.8424 - val_loss: 3.8099 - val_accuracy: 0.6455\n",
      "Epoch 622/1000\n",
      "83/83 [==============================] - 0s 661us/step - loss: 0.2326 - accuracy: 0.9136 - val_loss: 3.9341 - val_accuracy: 0.6182\n",
      "Epoch 623/1000\n",
      "83/83 [==============================] - 0s 671us/step - loss: 0.1577 - accuracy: 0.9424 - val_loss: 3.6701 - val_accuracy: 0.6227\n",
      "Epoch 624/1000\n",
      "83/83 [==============================] - 0s 681us/step - loss: 0.0985 - accuracy: 0.9636 - val_loss: 3.7626 - val_accuracy: 0.6591\n",
      "Epoch 625/1000\n",
      "83/83 [==============================] - 0s 671us/step - loss: 0.1576 - accuracy: 0.9500 - val_loss: 3.7755 - val_accuracy: 0.6818\n",
      "Epoch 626/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.0935 - accuracy: 0.9652 - val_loss: 3.8614 - val_accuracy: 0.6455\n",
      "Epoch 627/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.0675 - accuracy: 0.9833 - val_loss: 3.7527 - val_accuracy: 0.6955\n",
      "Epoch 628/1000\n",
      "83/83 [==============================] - 0s 644us/step - loss: 0.0775 - accuracy: 0.9712 - val_loss: 3.7603 - val_accuracy: 0.6909\n",
      "Epoch 629/1000\n",
      "83/83 [==============================] - 0s 639us/step - loss: 0.0976 - accuracy: 0.9636 - val_loss: 4.1154 - val_accuracy: 0.6545\n",
      "Epoch 630/1000\n",
      "83/83 [==============================] - 0s 662us/step - loss: 0.0944 - accuracy: 0.9652 - val_loss: 3.9136 - val_accuracy: 0.6409\n",
      "Epoch 631/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.0967 - accuracy: 0.9561 - val_loss: 3.8262 - val_accuracy: 0.6591\n",
      "Epoch 632/1000\n",
      "83/83 [==============================] - 0s 662us/step - loss: 0.0747 - accuracy: 0.9727 - val_loss: 3.8998 - val_accuracy: 0.6682\n",
      "Epoch 633/1000\n",
      "83/83 [==============================] - 0s 652us/step - loss: 0.1386 - accuracy: 0.9439 - val_loss: 4.0242 - val_accuracy: 0.6636\n",
      "Epoch 634/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.0892 - accuracy: 0.9712 - val_loss: 3.7804 - val_accuracy: 0.6818\n",
      "Epoch 635/1000\n",
      "83/83 [==============================] - 0s 660us/step - loss: 0.0914 - accuracy: 0.9636 - val_loss: 3.9062 - val_accuracy: 0.6727\n",
      "Epoch 636/1000\n",
      "83/83 [==============================] - 0s 649us/step - loss: 0.1365 - accuracy: 0.9561 - val_loss: 4.2246 - val_accuracy: 0.6318\n",
      "Epoch 637/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.1313 - accuracy: 0.9455 - val_loss: 3.9056 - val_accuracy: 0.6682\n",
      "Epoch 638/1000\n",
      "83/83 [==============================] - 0s 641us/step - loss: 0.1006 - accuracy: 0.9606 - val_loss: 3.8988 - val_accuracy: 0.6364\n",
      "Epoch 639/1000\n",
      "83/83 [==============================] - 0s 672us/step - loss: 0.1030 - accuracy: 0.9591 - val_loss: 3.8033 - val_accuracy: 0.6682\n",
      "Epoch 640/1000\n",
      "83/83 [==============================] - 0s 685us/step - loss: 0.1030 - accuracy: 0.9545 - val_loss: 3.8722 - val_accuracy: 0.6136\n",
      "Epoch 641/1000\n",
      "83/83 [==============================] - 0s 648us/step - loss: 0.0896 - accuracy: 0.9682 - val_loss: 3.8447 - val_accuracy: 0.6818\n",
      "Epoch 642/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.1094 - accuracy: 0.9561 - val_loss: 3.7882 - val_accuracy: 0.6773\n",
      "Epoch 643/1000\n",
      "83/83 [==============================] - 0s 648us/step - loss: 0.0941 - accuracy: 0.9667 - val_loss: 4.0381 - val_accuracy: 0.6273\n",
      "Epoch 644/1000\n",
      "83/83 [==============================] - 0s 655us/step - loss: 0.1242 - accuracy: 0.9545 - val_loss: 3.8709 - val_accuracy: 0.6682\n",
      "Epoch 645/1000\n",
      "83/83 [==============================] - 0s 656us/step - loss: 0.1320 - accuracy: 0.9470 - val_loss: 4.0412 - val_accuracy: 0.6227\n",
      "Epoch 646/1000\n",
      "83/83 [==============================] - 0s 670us/step - loss: 0.2785 - accuracy: 0.9045 - val_loss: 3.8921 - val_accuracy: 0.6682\n",
      "Epoch 647/1000\n",
      "83/83 [==============================] - 0s 654us/step - loss: 0.1847 - accuracy: 0.9273 - val_loss: 4.0310 - val_accuracy: 0.6364\n",
      "Epoch 648/1000\n",
      "83/83 [==============================] - 0s 667us/step - loss: 0.0918 - accuracy: 0.9682 - val_loss: 3.9932 - val_accuracy: 0.6591\n",
      "Epoch 649/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.0671 - accuracy: 0.9697 - val_loss: 3.9060 - val_accuracy: 0.6591\n",
      "Epoch 650/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.0944 - accuracy: 0.9652 - val_loss: 4.0990 - val_accuracy: 0.6636\n",
      "Epoch 651/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.0900 - accuracy: 0.9727 - val_loss: 4.0786 - val_accuracy: 0.6636\n",
      "Epoch 652/1000\n",
      "83/83 [==============================] - 0s 691us/step - loss: 0.0912 - accuracy: 0.9697 - val_loss: 4.0229 - val_accuracy: 0.6500\n",
      "Epoch 653/1000\n",
      "83/83 [==============================] - 0s 655us/step - loss: 0.0850 - accuracy: 0.9682 - val_loss: 4.0727 - val_accuracy: 0.6682\n",
      "Epoch 654/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.0690 - accuracy: 0.9788 - val_loss: 3.8850 - val_accuracy: 0.6909\n",
      "Epoch 655/1000\n",
      "83/83 [==============================] - 0s 750us/step - loss: 0.0736 - accuracy: 0.9712 - val_loss: 4.1318 - val_accuracy: 0.6545\n",
      "Epoch 656/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.1365 - accuracy: 0.9470 - val_loss: 4.0686 - val_accuracy: 0.6364\n",
      "Epoch 657/1000\n",
      "83/83 [==============================] - 0s 653us/step - loss: 0.1773 - accuracy: 0.9364 - val_loss: 4.2186 - val_accuracy: 0.6682\n",
      "Epoch 658/1000\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.2179 - accuracy: 0.9091 - val_loss: 4.1774 - val_accuracy: 0.6273\n",
      "Epoch 659/1000\n",
      "83/83 [==============================] - 0s 657us/step - loss: 0.3373 - accuracy: 0.8742 - val_loss: 4.0573 - val_accuracy: 0.6273\n",
      "Epoch 660/1000\n",
      "83/83 [==============================] - 0s 681us/step - loss: 0.2159 - accuracy: 0.9167 - val_loss: 3.8077 - val_accuracy: 0.6500\n",
      "Epoch 661/1000\n",
      "83/83 [==============================] - 0s 660us/step - loss: 0.1106 - accuracy: 0.9591 - val_loss: 3.7528 - val_accuracy: 0.6682\n",
      "Epoch 662/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.0731 - accuracy: 0.9788 - val_loss: 3.9173 - val_accuracy: 0.6773\n",
      "Epoch 663/1000\n",
      "83/83 [==============================] - 0s 680us/step - loss: 0.1217 - accuracy: 0.9545 - val_loss: 4.0164 - val_accuracy: 0.6045\n",
      "Epoch 664/1000\n",
      "83/83 [==============================] - 0s 695us/step - loss: 0.2010 - accuracy: 0.9242 - val_loss: 3.9477 - val_accuracy: 0.6409\n",
      "Epoch 665/1000\n",
      "83/83 [==============================] - 0s 670us/step - loss: 0.1192 - accuracy: 0.9561 - val_loss: 4.0503 - val_accuracy: 0.6318\n",
      "Epoch 666/1000\n",
      "83/83 [==============================] - 0s 653us/step - loss: 0.1247 - accuracy: 0.9455 - val_loss: 3.9661 - val_accuracy: 0.6409\n",
      "Epoch 667/1000\n",
      "83/83 [==============================] - 0s 672us/step - loss: 0.0660 - accuracy: 0.9727 - val_loss: 3.8821 - val_accuracy: 0.6864\n",
      "Epoch 668/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 650us/step - loss: 0.0583 - accuracy: 0.9773 - val_loss: 3.9124 - val_accuracy: 0.6500\n",
      "Epoch 669/1000\n",
      "83/83 [==============================] - 0s 651us/step - loss: 0.1150 - accuracy: 0.9530 - val_loss: 4.1154 - val_accuracy: 0.6318\n",
      "Epoch 670/1000\n",
      "83/83 [==============================] - 0s 645us/step - loss: 0.1602 - accuracy: 0.9333 - val_loss: 3.9315 - val_accuracy: 0.6636\n",
      "Epoch 671/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.0827 - accuracy: 0.9636 - val_loss: 3.8357 - val_accuracy: 0.6591\n",
      "Epoch 672/1000\n",
      "83/83 [==============================] - 0s 660us/step - loss: 0.0782 - accuracy: 0.9758 - val_loss: 4.0167 - val_accuracy: 0.6318\n",
      "Epoch 673/1000\n",
      "83/83 [==============================] - 0s 660us/step - loss: 0.0802 - accuracy: 0.9727 - val_loss: 3.9842 - val_accuracy: 0.6273\n",
      "Epoch 674/1000\n",
      "83/83 [==============================] - 0s 653us/step - loss: 0.0904 - accuracy: 0.9697 - val_loss: 4.1780 - val_accuracy: 0.6273\n",
      "Epoch 675/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.2628 - accuracy: 0.9015 - val_loss: 3.8148 - val_accuracy: 0.6318\n",
      "Epoch 676/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.1326 - accuracy: 0.9470 - val_loss: 3.9197 - val_accuracy: 0.6409\n",
      "Epoch 677/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.0892 - accuracy: 0.9667 - val_loss: 3.8979 - val_accuracy: 0.6545\n",
      "Epoch 678/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.0690 - accuracy: 0.9833 - val_loss: 3.9875 - val_accuracy: 0.6636\n",
      "Epoch 679/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.0656 - accuracy: 0.9712 - val_loss: 3.9219 - val_accuracy: 0.6545\n",
      "Epoch 680/1000\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.1882 - accuracy: 0.9273 - val_loss: 4.0684 - val_accuracy: 0.6409\n",
      "Epoch 681/1000\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.0659 - accuracy: 0.9803 - val_loss: 4.0185 - val_accuracy: 0.6682\n",
      "Epoch 682/1000\n",
      "83/83 [==============================] - 0s 648us/step - loss: 0.0978 - accuracy: 0.9606 - val_loss: 4.0221 - val_accuracy: 0.6545\n",
      "Epoch 683/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.1120 - accuracy: 0.9606 - val_loss: 4.1249 - val_accuracy: 0.6318\n",
      "Epoch 684/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.1599 - accuracy: 0.9455 - val_loss: 4.0798 - val_accuracy: 0.6591\n",
      "Epoch 685/1000\n",
      "83/83 [==============================] - 0s 658us/step - loss: 0.2397 - accuracy: 0.9106 - val_loss: 3.8971 - val_accuracy: 0.6545\n",
      "Epoch 686/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.1619 - accuracy: 0.9333 - val_loss: 4.0454 - val_accuracy: 0.6273\n",
      "Epoch 687/1000\n",
      "83/83 [==============================] - 0s 673us/step - loss: 0.1321 - accuracy: 0.9485 - val_loss: 3.8527 - val_accuracy: 0.6682\n",
      "Epoch 688/1000\n",
      "83/83 [==============================] - 0s 663us/step - loss: 0.0756 - accuracy: 0.9727 - val_loss: 3.9516 - val_accuracy: 0.6500\n",
      "Epoch 689/1000\n",
      "83/83 [==============================] - 0s 661us/step - loss: 0.0842 - accuracy: 0.9652 - val_loss: 4.0279 - val_accuracy: 0.6545\n",
      "Epoch 690/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.2078 - accuracy: 0.9273 - val_loss: 3.9552 - val_accuracy: 0.6409\n",
      "Epoch 691/1000\n",
      "83/83 [==============================] - 0s 705us/step - loss: 0.0957 - accuracy: 0.9636 - val_loss: 3.8302 - val_accuracy: 0.6545\n",
      "Epoch 692/1000\n",
      "83/83 [==============================] - 0s 658us/step - loss: 0.0671 - accuracy: 0.9758 - val_loss: 3.9750 - val_accuracy: 0.6455\n",
      "Epoch 693/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.0632 - accuracy: 0.9758 - val_loss: 4.0363 - val_accuracy: 0.6818\n",
      "Epoch 694/1000\n",
      "83/83 [==============================] - 0s 641us/step - loss: 0.1083 - accuracy: 0.9591 - val_loss: 4.1692 - val_accuracy: 0.6409\n",
      "Epoch 695/1000\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.0747 - accuracy: 0.9727 - val_loss: 4.2322 - val_accuracy: 0.6273\n",
      "Epoch 696/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.1740 - accuracy: 0.9318 - val_loss: 4.4591 - val_accuracy: 0.6136\n",
      "Epoch 697/1000\n",
      "83/83 [==============================] - 0s 649us/step - loss: 0.2277 - accuracy: 0.9273 - val_loss: 3.9397 - val_accuracy: 0.6318\n",
      "Epoch 698/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.1376 - accuracy: 0.9500 - val_loss: 3.8952 - val_accuracy: 0.6955\n",
      "Epoch 699/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.1100 - accuracy: 0.9652 - val_loss: 3.9961 - val_accuracy: 0.6455\n",
      "Epoch 700/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.0622 - accuracy: 0.9788 - val_loss: 3.8844 - val_accuracy: 0.7000\n",
      "Epoch 701/1000\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.0685 - accuracy: 0.9712 - val_loss: 4.0009 - val_accuracy: 0.6318\n",
      "Epoch 702/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.1417 - accuracy: 0.9561 - val_loss: 4.2169 - val_accuracy: 0.5818\n",
      "Epoch 703/1000\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.2528 - accuracy: 0.9000 - val_loss: 3.6933 - val_accuracy: 0.6273\n",
      "Epoch 704/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.0972 - accuracy: 0.9636 - val_loss: 3.9318 - val_accuracy: 0.6591\n",
      "Epoch 705/1000\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.0905 - accuracy: 0.9667 - val_loss: 3.8979 - val_accuracy: 0.6682\n",
      "Epoch 706/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.1131 - accuracy: 0.9591 - val_loss: 4.0071 - val_accuracy: 0.6364\n",
      "Epoch 707/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.1178 - accuracy: 0.9561 - val_loss: 3.9591 - val_accuracy: 0.6727\n",
      "Epoch 708/1000\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.1162 - accuracy: 0.9424 - val_loss: 4.0019 - val_accuracy: 0.6545\n",
      "Epoch 709/1000\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.0873 - accuracy: 0.9576 - val_loss: 3.9690 - val_accuracy: 0.6364\n",
      "Epoch 710/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.0841 - accuracy: 0.9682 - val_loss: 4.0979 - val_accuracy: 0.6091\n",
      "Epoch 711/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.1406 - accuracy: 0.9576 - val_loss: 4.0133 - val_accuracy: 0.6136\n",
      "Epoch 712/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.1654 - accuracy: 0.9348 - val_loss: 3.8045 - val_accuracy: 0.6682\n",
      "Epoch 713/1000\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.1366 - accuracy: 0.9530 - val_loss: 4.4685 - val_accuracy: 0.5818\n",
      "Epoch 714/1000\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.2321 - accuracy: 0.9076 - val_loss: 3.7291 - val_accuracy: 0.6591\n",
      "Epoch 715/1000\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.1199 - accuracy: 0.9545 - val_loss: 3.9946 - val_accuracy: 0.6500\n",
      "Epoch 716/1000\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.0659 - accuracy: 0.9773 - val_loss: 4.1267 - val_accuracy: 0.6591\n",
      "Epoch 717/1000\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.0714 - accuracy: 0.9773 - val_loss: 4.0447 - val_accuracy: 0.6773\n",
      "Epoch 718/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.0703 - accuracy: 0.9773 - val_loss: 4.0862 - val_accuracy: 0.6500\n",
      "Epoch 719/1000\n",
      "83/83 [==============================] - 0s 667us/step - loss: 0.0583 - accuracy: 0.9803 - val_loss: 3.9747 - val_accuracy: 0.6727\n",
      "Epoch 720/1000\n",
      "83/83 [==============================] - 0s 658us/step - loss: 0.2069 - accuracy: 0.9197 - val_loss: 4.2856 - val_accuracy: 0.5227\n",
      "Epoch 721/1000\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.1441 - accuracy: 0.9439 - val_loss: 4.2126 - val_accuracy: 0.6500\n",
      "Epoch 722/1000\n",
      "83/83 [==============================] - 0s 652us/step - loss: 0.0871 - accuracy: 0.9697 - val_loss: 3.9810 - val_accuracy: 0.6364\n",
      "Epoch 723/1000\n",
      "83/83 [==============================] - 0s 652us/step - loss: 0.0579 - accuracy: 0.9788 - val_loss: 3.9992 - val_accuracy: 0.6636\n",
      "Epoch 724/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 679us/step - loss: 0.1409 - accuracy: 0.9455 - val_loss: 4.0851 - val_accuracy: 0.6318\n",
      "Epoch 725/1000\n",
      "83/83 [==============================] - 0s 663us/step - loss: 0.0912 - accuracy: 0.9712 - val_loss: 3.8803 - val_accuracy: 0.6545\n",
      "Epoch 726/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.0992 - accuracy: 0.9606 - val_loss: 4.0809 - val_accuracy: 0.6455\n",
      "Epoch 727/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.0645 - accuracy: 0.9788 - val_loss: 3.9814 - val_accuracy: 0.6727\n",
      "Epoch 728/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.1332 - accuracy: 0.9500 - val_loss: 4.4161 - val_accuracy: 0.6091\n",
      "Epoch 729/1000\n",
      "83/83 [==============================] - 0s 660us/step - loss: 0.1169 - accuracy: 0.9606 - val_loss: 4.1958 - val_accuracy: 0.6727\n",
      "Epoch 730/1000\n",
      "83/83 [==============================] - 0s 677us/step - loss: 0.0679 - accuracy: 0.9758 - val_loss: 4.1278 - val_accuracy: 0.6682\n",
      "Epoch 731/1000\n",
      "83/83 [==============================] - 0s 662us/step - loss: 0.0533 - accuracy: 0.9803 - val_loss: 3.9914 - val_accuracy: 0.6818\n",
      "Epoch 732/1000\n",
      "83/83 [==============================] - 0s 678us/step - loss: 0.0604 - accuracy: 0.9773 - val_loss: 4.2319 - val_accuracy: 0.6682\n",
      "Epoch 733/1000\n",
      "83/83 [==============================] - 0s 660us/step - loss: 0.0975 - accuracy: 0.9576 - val_loss: 4.1046 - val_accuracy: 0.6500\n",
      "Epoch 734/1000\n",
      "83/83 [==============================] - 0s 668us/step - loss: 0.1334 - accuracy: 0.9409 - val_loss: 4.1544 - val_accuracy: 0.6364\n",
      "Epoch 735/1000\n",
      "83/83 [==============================] - 0s 666us/step - loss: 0.1612 - accuracy: 0.9364 - val_loss: 4.1498 - val_accuracy: 0.6227\n",
      "Epoch 736/1000\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.2421 - accuracy: 0.9106 - val_loss: 4.2528 - val_accuracy: 0.6500\n",
      "Epoch 737/1000\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.1685 - accuracy: 0.9227 - val_loss: 3.8837 - val_accuracy: 0.6727\n",
      "Epoch 738/1000\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.0722 - accuracy: 0.9773 - val_loss: 3.9505 - val_accuracy: 0.6591\n",
      "Epoch 739/1000\n",
      "83/83 [==============================] - 0s 658us/step - loss: 0.0728 - accuracy: 0.9742 - val_loss: 4.0370 - val_accuracy: 0.6227\n",
      "Epoch 740/1000\n",
      "83/83 [==============================] - 0s 656us/step - loss: 0.0965 - accuracy: 0.9636 - val_loss: 3.8893 - val_accuracy: 0.6545\n",
      "Epoch 741/1000\n",
      "83/83 [==============================] - 0s 665us/step - loss: 0.0580 - accuracy: 0.9788 - val_loss: 3.8949 - val_accuracy: 0.6727\n",
      "Epoch 742/1000\n",
      "83/83 [==============================] - 0s 671us/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 3.9743 - val_accuracy: 0.6591\n",
      "Epoch 743/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.0713 - accuracy: 0.9788 - val_loss: 3.8787 - val_accuracy: 0.6591\n",
      "Epoch 744/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.1561 - accuracy: 0.9500 - val_loss: 4.1404 - val_accuracy: 0.6500\n",
      "Epoch 745/1000\n",
      "83/83 [==============================] - 0s 661us/step - loss: 0.1220 - accuracy: 0.9545 - val_loss: 3.8850 - val_accuracy: 0.6545\n",
      "Epoch 746/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.0622 - accuracy: 0.9803 - val_loss: 4.0590 - val_accuracy: 0.6545\n",
      "Epoch 747/1000\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.1035 - accuracy: 0.9545 - val_loss: 3.9425 - val_accuracy: 0.6227\n",
      "Epoch 748/1000\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.3239 - accuracy: 0.8848 - val_loss: 3.8554 - val_accuracy: 0.6909\n",
      "Epoch 749/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.2154 - accuracy: 0.9121 - val_loss: 3.7948 - val_accuracy: 0.6636\n",
      "Epoch 750/1000\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.0877 - accuracy: 0.9591 - val_loss: 3.7953 - val_accuracy: 0.6727\n",
      "Epoch 751/1000\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.0616 - accuracy: 0.9788 - val_loss: 4.0131 - val_accuracy: 0.6409\n",
      "Epoch 752/1000\n",
      "83/83 [==============================] - 0s 663us/step - loss: 0.0626 - accuracy: 0.9773 - val_loss: 4.1937 - val_accuracy: 0.6591\n",
      "Epoch 753/1000\n",
      "83/83 [==============================] - 0s 663us/step - loss: 0.0813 - accuracy: 0.9697 - val_loss: 3.9705 - val_accuracy: 0.6727\n",
      "Epoch 754/1000\n",
      "83/83 [==============================] - 0s 675us/step - loss: 0.1101 - accuracy: 0.9621 - val_loss: 4.3028 - val_accuracy: 0.6500\n",
      "Epoch 755/1000\n",
      "83/83 [==============================] - 0s 646us/step - loss: 0.0525 - accuracy: 0.9803 - val_loss: 4.0624 - val_accuracy: 0.6682\n",
      "Epoch 756/1000\n",
      "83/83 [==============================] - 0s 649us/step - loss: 0.0537 - accuracy: 0.9848 - val_loss: 4.0085 - val_accuracy: 0.6318\n",
      "Epoch 757/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.1272 - accuracy: 0.9500 - val_loss: 4.0772 - val_accuracy: 0.6182\n",
      "Epoch 758/1000\n",
      "83/83 [==============================] - 0s 649us/step - loss: 0.1717 - accuracy: 0.9303 - val_loss: 3.8054 - val_accuracy: 0.6318\n",
      "Epoch 759/1000\n",
      "83/83 [==============================] - 0s 706us/step - loss: 0.0737 - accuracy: 0.9758 - val_loss: 4.0019 - val_accuracy: 0.6455\n",
      "Epoch 760/1000\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.0851 - accuracy: 0.9682 - val_loss: 4.0048 - val_accuracy: 0.6364\n",
      "Epoch 761/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.0615 - accuracy: 0.9803 - val_loss: 4.0408 - val_accuracy: 0.6409\n",
      "Epoch 762/1000\n",
      "83/83 [==============================] - 0s 669us/step - loss: 0.0633 - accuracy: 0.9788 - val_loss: 4.0019 - val_accuracy: 0.6636\n",
      "Epoch 763/1000\n",
      "83/83 [==============================] - 0s 660us/step - loss: 0.1041 - accuracy: 0.9561 - val_loss: 3.8021 - val_accuracy: 0.6636\n",
      "Epoch 764/1000\n",
      "83/83 [==============================] - 0s 658us/step - loss: 0.1350 - accuracy: 0.9545 - val_loss: 4.2346 - val_accuracy: 0.6409\n",
      "Epoch 765/1000\n",
      "83/83 [==============================] - 0s 641us/step - loss: 0.0662 - accuracy: 0.9788 - val_loss: 4.1238 - val_accuracy: 0.6455\n",
      "Epoch 766/1000\n",
      "83/83 [==============================] - 0s 654us/step - loss: 0.0752 - accuracy: 0.9742 - val_loss: 4.1098 - val_accuracy: 0.6773\n",
      "Epoch 767/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.2823 - accuracy: 0.8939 - val_loss: 3.9429 - val_accuracy: 0.6091\n",
      "Epoch 768/1000\n",
      "83/83 [==============================] - 0s 649us/step - loss: 0.2194 - accuracy: 0.9197 - val_loss: 4.4411 - val_accuracy: 0.6227\n",
      "Epoch 769/1000\n",
      "83/83 [==============================] - 0s 644us/step - loss: 0.1309 - accuracy: 0.9545 - val_loss: 4.0529 - val_accuracy: 0.6409\n",
      "Epoch 770/1000\n",
      "83/83 [==============================] - 0s 663us/step - loss: 0.0775 - accuracy: 0.9742 - val_loss: 3.8365 - val_accuracy: 0.6909\n",
      "Epoch 771/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.0486 - accuracy: 0.9833 - val_loss: 4.0078 - val_accuracy: 0.6409\n",
      "Epoch 772/1000\n",
      "83/83 [==============================] - 0s 669us/step - loss: 0.0706 - accuracy: 0.9712 - val_loss: 4.0010 - val_accuracy: 0.6591\n",
      "Epoch 773/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.0833 - accuracy: 0.9667 - val_loss: 4.0570 - val_accuracy: 0.6318\n",
      "Epoch 774/1000\n",
      "83/83 [==============================] - 0s 649us/step - loss: 0.1071 - accuracy: 0.9530 - val_loss: 3.8786 - val_accuracy: 0.6591\n",
      "Epoch 775/1000\n",
      "83/83 [==============================] - 0s 672us/step - loss: 0.0753 - accuracy: 0.9712 - val_loss: 4.0026 - val_accuracy: 0.6409\n",
      "Epoch 776/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.0666 - accuracy: 0.9727 - val_loss: 4.1265 - val_accuracy: 0.6591\n",
      "Epoch 777/1000\n",
      "83/83 [==============================] - 0s 651us/step - loss: 0.1005 - accuracy: 0.9667 - val_loss: 4.3319 - val_accuracy: 0.6000\n",
      "Epoch 778/1000\n",
      "83/83 [==============================] - 0s 653us/step - loss: 0.1564 - accuracy: 0.9273 - val_loss: 3.9002 - val_accuracy: 0.6818\n",
      "Epoch 779/1000\n",
      "83/83 [==============================] - 0s 672us/step - loss: 0.1073 - accuracy: 0.9530 - val_loss: 4.1291 - val_accuracy: 0.6136\n",
      "Epoch 780/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 658us/step - loss: 0.1084 - accuracy: 0.9530 - val_loss: 4.1280 - val_accuracy: 0.6227\n",
      "Epoch 781/1000\n",
      "83/83 [==============================] - 0s 672us/step - loss: 0.1275 - accuracy: 0.9515 - val_loss: 4.0678 - val_accuracy: 0.5773\n",
      "Epoch 782/1000\n",
      "83/83 [==============================] - 0s 651us/step - loss: 0.0949 - accuracy: 0.9682 - val_loss: 4.0546 - val_accuracy: 0.5955\n",
      "Epoch 783/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.1147 - accuracy: 0.9515 - val_loss: 4.1728 - val_accuracy: 0.6227\n",
      "Epoch 784/1000\n",
      "83/83 [==============================] - 0s 648us/step - loss: 0.1087 - accuracy: 0.9576 - val_loss: 4.1522 - val_accuracy: 0.6500\n",
      "Epoch 785/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.1103 - accuracy: 0.9636 - val_loss: 3.8462 - val_accuracy: 0.6591\n",
      "Epoch 786/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.1153 - accuracy: 0.9652 - val_loss: 3.9072 - val_accuracy: 0.6364\n",
      "Epoch 787/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.0645 - accuracy: 0.9788 - val_loss: 4.0804 - val_accuracy: 0.6727\n",
      "Epoch 788/1000\n",
      "83/83 [==============================] - 0s 646us/step - loss: 0.0563 - accuracy: 0.9803 - val_loss: 3.9716 - val_accuracy: 0.6773\n",
      "Epoch 789/1000\n",
      "83/83 [==============================] - 0s 645us/step - loss: 0.0550 - accuracy: 0.9758 - val_loss: 4.1737 - val_accuracy: 0.5591\n",
      "Epoch 790/1000\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.1156 - accuracy: 0.9576 - val_loss: 4.2394 - val_accuracy: 0.6364\n",
      "Epoch 791/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.1805 - accuracy: 0.9348 - val_loss: 4.2674 - val_accuracy: 0.6182\n",
      "Epoch 792/1000\n",
      "83/83 [==============================] - 0s 670us/step - loss: 0.0656 - accuracy: 0.9758 - val_loss: 4.1135 - val_accuracy: 0.6273\n",
      "Epoch 793/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.0641 - accuracy: 0.9727 - val_loss: 4.4865 - val_accuracy: 0.6091\n",
      "Epoch 794/1000\n",
      "83/83 [==============================] - 0s 648us/step - loss: 0.0741 - accuracy: 0.9758 - val_loss: 4.2534 - val_accuracy: 0.6818\n",
      "Epoch 795/1000\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.1270 - accuracy: 0.9515 - val_loss: 4.3685 - val_accuracy: 0.6364\n",
      "Epoch 796/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.0920 - accuracy: 0.9667 - val_loss: 4.4187 - val_accuracy: 0.6273\n",
      "Epoch 797/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.0668 - accuracy: 0.9727 - val_loss: 4.3857 - val_accuracy: 0.6727\n",
      "Epoch 798/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.0542 - accuracy: 0.9803 - val_loss: 4.3051 - val_accuracy: 0.6636\n",
      "Epoch 799/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.1007 - accuracy: 0.9561 - val_loss: 4.3870 - val_accuracy: 0.6409\n",
      "Epoch 800/1000\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.0833 - accuracy: 0.9773 - val_loss: 4.2007 - val_accuracy: 0.6636\n",
      "Epoch 801/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.0465 - accuracy: 0.9818 - val_loss: 4.3249 - val_accuracy: 0.6818\n",
      "Epoch 802/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.1376 - accuracy: 0.9576 - val_loss: 4.3498 - val_accuracy: 0.6136\n",
      "Epoch 803/1000\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.1058 - accuracy: 0.9652 - val_loss: 4.0061 - val_accuracy: 0.6591\n",
      "Epoch 804/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.0739 - accuracy: 0.9712 - val_loss: 4.2690 - val_accuracy: 0.6636\n",
      "Epoch 805/1000\n",
      "83/83 [==============================] - 0s 646us/step - loss: 0.0624 - accuracy: 0.9712 - val_loss: 4.1948 - val_accuracy: 0.6545\n",
      "Epoch 806/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.0852 - accuracy: 0.9652 - val_loss: 4.1131 - val_accuracy: 0.6591\n",
      "Epoch 807/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.0660 - accuracy: 0.9848 - val_loss: 4.3154 - val_accuracy: 0.6591\n",
      "Epoch 808/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.2095 - accuracy: 0.9242 - val_loss: 3.9165 - val_accuracy: 0.6500\n",
      "Epoch 809/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.3027 - accuracy: 0.9076 - val_loss: 4.3235 - val_accuracy: 0.6682\n",
      "Epoch 810/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.1361 - accuracy: 0.9485 - val_loss: 4.2618 - val_accuracy: 0.6409\n",
      "Epoch 811/1000\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.0845 - accuracy: 0.9712 - val_loss: 4.0076 - val_accuracy: 0.6591\n",
      "Epoch 812/1000\n",
      "83/83 [==============================] - 0s 639us/step - loss: 0.0598 - accuracy: 0.9758 - val_loss: 4.0695 - val_accuracy: 0.6545\n",
      "Epoch 813/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.0664 - accuracy: 0.9788 - val_loss: 4.0502 - val_accuracy: 0.6591\n",
      "Epoch 814/1000\n",
      "83/83 [==============================] - 0s 654us/step - loss: 0.0469 - accuracy: 0.9833 - val_loss: 4.1504 - val_accuracy: 0.6500\n",
      "Epoch 815/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.1281 - accuracy: 0.9561 - val_loss: 4.0297 - val_accuracy: 0.6455\n",
      "Epoch 816/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.2480 - accuracy: 0.9182 - val_loss: 4.2847 - val_accuracy: 0.6273\n",
      "Epoch 817/1000\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.1639 - accuracy: 0.9348 - val_loss: 4.3679 - val_accuracy: 0.6545\n",
      "Epoch 818/1000\n",
      "83/83 [==============================] - 0s 657us/step - loss: 0.0691 - accuracy: 0.9788 - val_loss: 4.0857 - val_accuracy: 0.6545\n",
      "Epoch 819/1000\n",
      "83/83 [==============================] - 0s 686us/step - loss: 0.0558 - accuracy: 0.9803 - val_loss: 4.1098 - val_accuracy: 0.6864\n",
      "Epoch 820/1000\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.0504 - accuracy: 0.9864 - val_loss: 4.1035 - val_accuracy: 0.6955\n",
      "Epoch 821/1000\n",
      "83/83 [==============================] - 0s 670us/step - loss: 0.0449 - accuracy: 0.9864 - val_loss: 4.0393 - val_accuracy: 0.6818\n",
      "Epoch 822/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.0391 - accuracy: 0.9864 - val_loss: 4.1185 - val_accuracy: 0.6682\n",
      "Epoch 823/1000\n",
      "83/83 [==============================] - 0s 668us/step - loss: 0.0554 - accuracy: 0.9773 - val_loss: 4.1645 - val_accuracy: 0.6409\n",
      "Epoch 824/1000\n",
      "83/83 [==============================] - 0s 658us/step - loss: 0.0750 - accuracy: 0.9682 - val_loss: 4.1799 - val_accuracy: 0.6318\n",
      "Epoch 825/1000\n",
      "83/83 [==============================] - 0s 653us/step - loss: 0.0639 - accuracy: 0.9788 - val_loss: 4.3491 - val_accuracy: 0.6318\n",
      "Epoch 826/1000\n",
      "83/83 [==============================] - 0s 686us/step - loss: 0.2774 - accuracy: 0.9015 - val_loss: 4.6760 - val_accuracy: 0.6000\n",
      "Epoch 827/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.1320 - accuracy: 0.9455 - val_loss: 4.2401 - val_accuracy: 0.6273\n",
      "Epoch 828/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.0699 - accuracy: 0.9742 - val_loss: 4.1318 - val_accuracy: 0.6545\n",
      "Epoch 829/1000\n",
      "83/83 [==============================] - 0s 666us/step - loss: 0.0502 - accuracy: 0.9833 - val_loss: 3.9775 - val_accuracy: 0.6636\n",
      "Epoch 830/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.0555 - accuracy: 0.9803 - val_loss: 4.0676 - val_accuracy: 0.6864\n",
      "Epoch 831/1000\n",
      "83/83 [==============================] - 0s 664us/step - loss: 0.0690 - accuracy: 0.9697 - val_loss: 4.2599 - val_accuracy: 0.6500\n",
      "Epoch 832/1000\n",
      "83/83 [==============================] - 0s 661us/step - loss: 0.1232 - accuracy: 0.9500 - val_loss: 4.5150 - val_accuracy: 0.6455\n",
      "Epoch 833/1000\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.0874 - accuracy: 0.9667 - val_loss: 4.2291 - val_accuracy: 0.6227\n",
      "Epoch 834/1000\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.1038 - accuracy: 0.9636 - val_loss: 4.5463 - val_accuracy: 0.6045\n",
      "Epoch 835/1000\n",
      "83/83 [==============================] - 0s 694us/step - loss: 0.3821 - accuracy: 0.8788 - val_loss: 4.6117 - val_accuracy: 0.5818\n",
      "Epoch 836/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 676us/step - loss: 0.1970 - accuracy: 0.9303 - val_loss: 4.3555 - val_accuracy: 0.6682\n",
      "Epoch 837/1000\n",
      "83/83 [==============================] - 0s 658us/step - loss: 0.0700 - accuracy: 0.9788 - val_loss: 4.3985 - val_accuracy: 0.6364\n",
      "Epoch 838/1000\n",
      "83/83 [==============================] - 0s 645us/step - loss: 0.0760 - accuracy: 0.9667 - val_loss: 4.2215 - val_accuracy: 0.6591\n",
      "Epoch 839/1000\n",
      "83/83 [==============================] - 0s 662us/step - loss: 0.0487 - accuracy: 0.9879 - val_loss: 4.2785 - val_accuracy: 0.6545\n",
      "Epoch 840/1000\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.0373 - accuracy: 0.9894 - val_loss: 4.2851 - val_accuracy: 0.6864\n",
      "Epoch 841/1000\n",
      "83/83 [==============================] - 0s 677us/step - loss: 0.0549 - accuracy: 0.9773 - val_loss: 4.4076 - val_accuracy: 0.6455\n",
      "Epoch 842/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.0798 - accuracy: 0.9667 - val_loss: 4.4246 - val_accuracy: 0.6364\n",
      "Epoch 843/1000\n",
      "83/83 [==============================] - 0s 655us/step - loss: 0.1410 - accuracy: 0.9409 - val_loss: 4.4428 - val_accuracy: 0.6455\n",
      "Epoch 844/1000\n",
      "83/83 [==============================] - 0s 658us/step - loss: 0.2351 - accuracy: 0.9106 - val_loss: 4.1699 - val_accuracy: 0.6455\n",
      "Epoch 845/1000\n",
      "83/83 [==============================] - 0s 760us/step - loss: 0.0899 - accuracy: 0.9712 - val_loss: 4.3435 - val_accuracy: 0.6545\n",
      "Epoch 846/1000\n",
      "83/83 [==============================] - 0s 672us/step - loss: 0.0638 - accuracy: 0.9758 - val_loss: 4.3039 - val_accuracy: 0.6364\n",
      "Epoch 847/1000\n",
      "83/83 [==============================] - 0s 667us/step - loss: 0.0504 - accuracy: 0.9864 - val_loss: 4.2851 - val_accuracy: 0.6636\n",
      "Epoch 848/1000\n",
      "83/83 [==============================] - 0s 699us/step - loss: 0.0582 - accuracy: 0.9833 - val_loss: 4.3072 - val_accuracy: 0.6409\n",
      "Epoch 849/1000\n",
      "83/83 [==============================] - 0s 690us/step - loss: 0.0442 - accuracy: 0.9879 - val_loss: 4.3530 - val_accuracy: 0.6409\n",
      "Epoch 850/1000\n",
      "83/83 [==============================] - 0s 674us/step - loss: 0.1513 - accuracy: 0.9439 - val_loss: 4.3951 - val_accuracy: 0.6091\n",
      "Epoch 851/1000\n",
      "83/83 [==============================] - 0s 655us/step - loss: 0.0646 - accuracy: 0.9742 - val_loss: 4.3383 - val_accuracy: 0.6455\n",
      "Epoch 852/1000\n",
      "83/83 [==============================] - 0s 693us/step - loss: 0.1294 - accuracy: 0.9455 - val_loss: 4.6562 - val_accuracy: 0.6091\n",
      "Epoch 853/1000\n",
      "83/83 [==============================] - 0s 671us/step - loss: 0.1587 - accuracy: 0.9439 - val_loss: 4.2755 - val_accuracy: 0.6318\n",
      "Epoch 854/1000\n",
      "83/83 [==============================] - 0s 672us/step - loss: 0.1108 - accuracy: 0.9561 - val_loss: 4.4331 - val_accuracy: 0.6227\n",
      "Epoch 855/1000\n",
      "83/83 [==============================] - 0s 671us/step - loss: 0.1544 - accuracy: 0.9409 - val_loss: 4.5726 - val_accuracy: 0.6091\n",
      "Epoch 856/1000\n",
      "83/83 [==============================] - 0s 673us/step - loss: 0.0923 - accuracy: 0.9727 - val_loss: 4.3327 - val_accuracy: 0.6182\n",
      "Epoch 857/1000\n",
      "83/83 [==============================] - 0s 669us/step - loss: 0.3216 - accuracy: 0.8697 - val_loss: 4.4710 - val_accuracy: 0.6273\n",
      "Epoch 858/1000\n",
      "83/83 [==============================] - 0s 668us/step - loss: 0.1118 - accuracy: 0.9652 - val_loss: 4.2671 - val_accuracy: 0.6364\n",
      "Epoch 859/1000\n",
      "83/83 [==============================] - 0s 668us/step - loss: 0.0796 - accuracy: 0.9758 - val_loss: 4.3385 - val_accuracy: 0.6136\n",
      "Epoch 860/1000\n",
      "83/83 [==============================] - 0s 674us/step - loss: 0.0619 - accuracy: 0.9773 - val_loss: 4.2854 - val_accuracy: 0.6636\n",
      "Epoch 861/1000\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.0533 - accuracy: 0.9818 - val_loss: 4.2358 - val_accuracy: 0.6727\n",
      "Epoch 862/1000\n",
      "83/83 [==============================] - 0s 658us/step - loss: 0.0439 - accuracy: 0.9848 - val_loss: 4.2782 - val_accuracy: 0.6682\n",
      "Epoch 863/1000\n",
      "83/83 [==============================] - 0s 667us/step - loss: 0.0831 - accuracy: 0.9682 - val_loss: 4.3934 - val_accuracy: 0.6636\n",
      "Epoch 864/1000\n",
      "83/83 [==============================] - 0s 660us/step - loss: 0.0579 - accuracy: 0.9818 - val_loss: 4.3961 - val_accuracy: 0.6545\n",
      "Epoch 865/1000\n",
      "83/83 [==============================] - 0s 767us/step - loss: 0.0736 - accuracy: 0.9727 - val_loss: 4.3494 - val_accuracy: 0.6591\n",
      "Epoch 866/1000\n",
      "83/83 [==============================] - 0s 664us/step - loss: 0.0767 - accuracy: 0.9727 - val_loss: 4.4095 - val_accuracy: 0.6682\n",
      "Epoch 867/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.0890 - accuracy: 0.9606 - val_loss: 4.4159 - val_accuracy: 0.6409\n",
      "Epoch 868/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.1063 - accuracy: 0.9606 - val_loss: 4.5546 - val_accuracy: 0.6500\n",
      "Epoch 869/1000\n",
      "83/83 [==============================] - 0s 662us/step - loss: 0.0515 - accuracy: 0.9788 - val_loss: 4.4388 - val_accuracy: 0.6727\n",
      "Epoch 870/1000\n",
      "83/83 [==============================] - 0s 671us/step - loss: 0.0402 - accuracy: 0.9848 - val_loss: 4.4475 - val_accuracy: 0.6591\n",
      "Epoch 871/1000\n",
      "83/83 [==============================] - 0s 770us/step - loss: 0.0590 - accuracy: 0.9773 - val_loss: 4.4555 - val_accuracy: 0.6682\n",
      "Epoch 872/1000\n",
      "83/83 [==============================] - 0s 794us/step - loss: 0.0324 - accuracy: 0.9924 - val_loss: 4.3496 - val_accuracy: 0.6818\n",
      "Epoch 873/1000\n",
      "83/83 [==============================] - 0s 740us/step - loss: 0.0673 - accuracy: 0.9803 - val_loss: 4.7139 - val_accuracy: 0.6455\n",
      "Epoch 874/1000\n",
      "83/83 [==============================] - 0s 721us/step - loss: 0.1986 - accuracy: 0.9333 - val_loss: 4.8985 - val_accuracy: 0.6136\n",
      "Epoch 875/1000\n",
      "83/83 [==============================] - 0s 700us/step - loss: 0.2103 - accuracy: 0.9136 - val_loss: 4.6426 - val_accuracy: 0.6500\n",
      "Epoch 876/1000\n",
      "83/83 [==============================] - 0s 661us/step - loss: 0.1507 - accuracy: 0.9424 - val_loss: 4.4011 - val_accuracy: 0.6591\n",
      "Epoch 877/1000\n",
      "83/83 [==============================] - 0s 675us/step - loss: 0.0668 - accuracy: 0.9682 - val_loss: 4.5012 - val_accuracy: 0.6455\n",
      "Epoch 878/1000\n",
      "83/83 [==============================] - 0s 700us/step - loss: 0.0457 - accuracy: 0.9864 - val_loss: 4.4977 - val_accuracy: 0.6500\n",
      "Epoch 879/1000\n",
      "83/83 [==============================] - 0s 671us/step - loss: 0.0541 - accuracy: 0.9773 - val_loss: 4.2847 - val_accuracy: 0.7000\n",
      "Epoch 880/1000\n",
      "83/83 [==============================] - 0s 684us/step - loss: 0.0624 - accuracy: 0.9773 - val_loss: 4.4146 - val_accuracy: 0.6545\n",
      "Epoch 881/1000\n",
      "83/83 [==============================] - 0s 679us/step - loss: 0.0532 - accuracy: 0.9803 - val_loss: 4.6421 - val_accuracy: 0.6409\n",
      "Epoch 882/1000\n",
      "83/83 [==============================] - 0s 691us/step - loss: 0.3007 - accuracy: 0.8909 - val_loss: 4.2996 - val_accuracy: 0.6273\n",
      "Epoch 883/1000\n",
      "83/83 [==============================] - 0s 688us/step - loss: 0.2314 - accuracy: 0.9197 - val_loss: 3.9347 - val_accuracy: 0.6409\n",
      "Epoch 884/1000\n",
      "83/83 [==============================] - 0s 653us/step - loss: 0.1280 - accuracy: 0.9576 - val_loss: 4.0324 - val_accuracy: 0.6273\n",
      "Epoch 885/1000\n",
      "83/83 [==============================] - 0s 689us/step - loss: 0.0731 - accuracy: 0.9727 - val_loss: 4.0658 - val_accuracy: 0.6864\n",
      "Epoch 886/1000\n",
      "83/83 [==============================] - 0s 661us/step - loss: 0.0628 - accuracy: 0.9773 - val_loss: 4.1671 - val_accuracy: 0.6682\n",
      "Epoch 887/1000\n",
      "83/83 [==============================] - 0s 679us/step - loss: 0.0472 - accuracy: 0.9864 - val_loss: 4.0665 - val_accuracy: 0.6909\n",
      "Epoch 888/1000\n",
      "83/83 [==============================] - 0s 691us/step - loss: 0.0565 - accuracy: 0.9773 - val_loss: 4.1548 - val_accuracy: 0.6727\n",
      "Epoch 889/1000\n",
      "83/83 [==============================] - 0s 664us/step - loss: 0.0467 - accuracy: 0.9833 - val_loss: 4.2464 - val_accuracy: 0.6773\n",
      "Epoch 890/1000\n",
      "83/83 [==============================] - 0s 682us/step - loss: 0.1113 - accuracy: 0.9636 - val_loss: 4.5448 - val_accuracy: 0.5864\n",
      "Epoch 891/1000\n",
      "83/83 [==============================] - 0s 676us/step - loss: 0.1758 - accuracy: 0.9394 - val_loss: 4.1932 - val_accuracy: 0.6500\n",
      "Epoch 892/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 675us/step - loss: 0.0661 - accuracy: 0.9803 - val_loss: 4.1387 - val_accuracy: 0.6773\n",
      "Epoch 893/1000\n",
      "83/83 [==============================] - 0s 831us/step - loss: 0.0573 - accuracy: 0.9773 - val_loss: 4.0735 - val_accuracy: 0.6864\n",
      "Epoch 894/1000\n",
      "83/83 [==============================] - 0s 718us/step - loss: 0.0330 - accuracy: 0.9894 - val_loss: 4.1520 - val_accuracy: 0.6773\n",
      "Epoch 895/1000\n",
      "83/83 [==============================] - 0s 731us/step - loss: 0.0457 - accuracy: 0.9818 - val_loss: 4.2894 - val_accuracy: 0.6545\n",
      "Epoch 896/1000\n",
      "83/83 [==============================] - 0s 709us/step - loss: 0.0532 - accuracy: 0.9803 - val_loss: 4.2488 - val_accuracy: 0.6773\n",
      "Epoch 897/1000\n",
      "83/83 [==============================] - 0s 684us/step - loss: 0.0503 - accuracy: 0.9848 - val_loss: 4.2170 - val_accuracy: 0.6500\n",
      "Epoch 898/1000\n",
      "83/83 [==============================] - 0s 680us/step - loss: 0.0655 - accuracy: 0.9742 - val_loss: 4.4348 - val_accuracy: 0.6591\n",
      "Epoch 899/1000\n",
      "83/83 [==============================] - 0s 668us/step - loss: 0.2106 - accuracy: 0.9212 - val_loss: 4.3144 - val_accuracy: 0.6182\n",
      "Epoch 900/1000\n",
      "83/83 [==============================] - 0s 672us/step - loss: 0.1222 - accuracy: 0.9530 - val_loss: 4.2682 - val_accuracy: 0.6500\n",
      "Epoch 901/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.0929 - accuracy: 0.9576 - val_loss: 4.3247 - val_accuracy: 0.6364\n",
      "Epoch 902/1000\n",
      "83/83 [==============================] - 0s 645us/step - loss: 0.1096 - accuracy: 0.9576 - val_loss: 4.4779 - val_accuracy: 0.6545\n",
      "Epoch 903/1000\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 4.4569 - val_accuracy: 0.6364\n",
      "Epoch 904/1000\n",
      "83/83 [==============================] - 0s 662us/step - loss: 0.0950 - accuracy: 0.9621 - val_loss: 4.2447 - val_accuracy: 0.6636\n",
      "Epoch 905/1000\n",
      "83/83 [==============================] - 0s 665us/step - loss: 0.0566 - accuracy: 0.9758 - val_loss: 4.4845 - val_accuracy: 0.6455\n",
      "Epoch 906/1000\n",
      "83/83 [==============================] - 0s 663us/step - loss: 0.1204 - accuracy: 0.9485 - val_loss: 4.1533 - val_accuracy: 0.6500\n",
      "Epoch 907/1000\n",
      "83/83 [==============================] - 0s 682us/step - loss: 0.1109 - accuracy: 0.9621 - val_loss: 4.4197 - val_accuracy: 0.6409\n",
      "Epoch 908/1000\n",
      "83/83 [==============================] - 0s 660us/step - loss: 0.2265 - accuracy: 0.9061 - val_loss: 4.2960 - val_accuracy: 0.6182\n",
      "Epoch 909/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.1862 - accuracy: 0.9318 - val_loss: 4.2264 - val_accuracy: 0.6091\n",
      "Epoch 910/1000\n",
      "83/83 [==============================] - 0s 652us/step - loss: 0.0877 - accuracy: 0.9712 - val_loss: 4.2350 - val_accuracy: 0.6727\n",
      "Epoch 911/1000\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.0646 - accuracy: 0.9803 - val_loss: 4.3110 - val_accuracy: 0.6545\n",
      "Epoch 912/1000\n",
      "83/83 [==============================] - 0s 674us/step - loss: 0.0445 - accuracy: 0.9879 - val_loss: 4.4227 - val_accuracy: 0.6455\n",
      "Epoch 913/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.0308 - accuracy: 0.9939 - val_loss: 4.4367 - val_accuracy: 0.6500\n",
      "Epoch 914/1000\n",
      "83/83 [==============================] - 0s 655us/step - loss: 0.0572 - accuracy: 0.9818 - val_loss: 4.5092 - val_accuracy: 0.6409\n",
      "Epoch 915/1000\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.1274 - accuracy: 0.9455 - val_loss: 4.3658 - val_accuracy: 0.6273\n",
      "Epoch 916/1000\n",
      "83/83 [==============================] - 0s 646us/step - loss: 0.0581 - accuracy: 0.9803 - val_loss: 4.6756 - val_accuracy: 0.6364\n",
      "Epoch 917/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.0565 - accuracy: 0.9803 - val_loss: 4.4267 - val_accuracy: 0.6409\n",
      "Epoch 918/1000\n",
      "83/83 [==============================] - 0s 651us/step - loss: 0.0445 - accuracy: 0.9894 - val_loss: 4.6071 - val_accuracy: 0.6136\n",
      "Epoch 919/1000\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.1412 - accuracy: 0.9515 - val_loss: 4.4703 - val_accuracy: 0.6682\n",
      "Epoch 920/1000\n",
      "83/83 [==============================] - 0s 648us/step - loss: 0.1421 - accuracy: 0.9485 - val_loss: 4.4614 - val_accuracy: 0.6409\n",
      "Epoch 921/1000\n",
      "83/83 [==============================] - 0s 667us/step - loss: 0.1668 - accuracy: 0.9424 - val_loss: 4.8262 - val_accuracy: 0.6000\n",
      "Epoch 922/1000\n",
      "83/83 [==============================] - 0s 647us/step - loss: 0.1583 - accuracy: 0.9379 - val_loss: 4.4616 - val_accuracy: 0.6045\n",
      "Epoch 923/1000\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.1041 - accuracy: 0.9636 - val_loss: 4.2920 - val_accuracy: 0.6455\n",
      "Epoch 924/1000\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.1216 - accuracy: 0.9470 - val_loss: 4.5688 - val_accuracy: 0.6500\n",
      "Epoch 925/1000\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.0484 - accuracy: 0.9833 - val_loss: 4.4829 - val_accuracy: 0.6364\n",
      "Epoch 926/1000\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.0725 - accuracy: 0.9697 - val_loss: 4.5190 - val_accuracy: 0.6091\n",
      "Epoch 927/1000\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.0708 - accuracy: 0.9697 - val_loss: 4.6343 - val_accuracy: 0.6091\n",
      "Epoch 928/1000\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.0677 - accuracy: 0.9773 - val_loss: 4.4017 - val_accuracy: 0.6500\n",
      "Epoch 929/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.1690 - accuracy: 0.9333 - val_loss: 4.5810 - val_accuracy: 0.5773\n",
      "Epoch 930/1000\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.2097 - accuracy: 0.9242 - val_loss: 4.4379 - val_accuracy: 0.6364\n",
      "Epoch 931/1000\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.0607 - accuracy: 0.9758 - val_loss: 4.5417 - val_accuracy: 0.6545\n",
      "Epoch 932/1000\n",
      "83/83 [==============================] - 0s 644us/step - loss: 0.0530 - accuracy: 0.9848 - val_loss: 4.0250 - val_accuracy: 0.6818\n",
      "Epoch 933/1000\n",
      "83/83 [==============================] - 0s 648us/step - loss: 0.1104 - accuracy: 0.9697 - val_loss: 4.2325 - val_accuracy: 0.6273\n",
      "Epoch 934/1000\n",
      "83/83 [==============================] - 0s 644us/step - loss: 0.0592 - accuracy: 0.9758 - val_loss: 4.0715 - val_accuracy: 0.6727\n",
      "Epoch 935/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.0483 - accuracy: 0.9833 - val_loss: 4.1518 - val_accuracy: 0.6682\n",
      "Epoch 936/1000\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.0495 - accuracy: 0.9818 - val_loss: 4.1977 - val_accuracy: 0.6455\n",
      "Epoch 937/1000\n",
      "83/83 [==============================] - 0s 651us/step - loss: 0.0859 - accuracy: 0.9773 - val_loss: 4.3240 - val_accuracy: 0.6636\n",
      "Epoch 938/1000\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.0640 - accuracy: 0.9833 - val_loss: 4.0971 - val_accuracy: 0.6091\n",
      "Epoch 939/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.0940 - accuracy: 0.9652 - val_loss: 4.1108 - val_accuracy: 0.6591\n",
      "Epoch 940/1000\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.0423 - accuracy: 0.9848 - val_loss: 4.0500 - val_accuracy: 0.6682\n",
      "Epoch 941/1000\n",
      "83/83 [==============================] - 0s 664us/step - loss: 0.1598 - accuracy: 0.9439 - val_loss: 4.4143 - val_accuracy: 0.6364\n",
      "Epoch 942/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.3277 - accuracy: 0.8924 - val_loss: 4.5503 - val_accuracy: 0.6136\n",
      "Epoch 943/1000\n",
      "83/83 [==============================] - 0s 658us/step - loss: 0.0832 - accuracy: 0.9667 - val_loss: 4.4096 - val_accuracy: 0.6273\n",
      "Epoch 944/1000\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.0678 - accuracy: 0.9742 - val_loss: 4.2586 - val_accuracy: 0.6545\n",
      "Epoch 945/1000\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.0351 - accuracy: 0.9894 - val_loss: 4.2233 - val_accuracy: 0.6500\n",
      "Epoch 946/1000\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.0371 - accuracy: 0.9879 - val_loss: 4.3181 - val_accuracy: 0.6455\n",
      "Epoch 947/1000\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.0365 - accuracy: 0.9909 - val_loss: 4.3577 - val_accuracy: 0.6227\n",
      "Epoch 948/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 656us/step - loss: 0.0499 - accuracy: 0.9803 - val_loss: 4.5412 - val_accuracy: 0.6273\n",
      "Epoch 949/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.0894 - accuracy: 0.9576 - val_loss: 4.2670 - val_accuracy: 0.6364\n",
      "Epoch 950/1000\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.0631 - accuracy: 0.9773 - val_loss: 4.3202 - val_accuracy: 0.6545\n",
      "Epoch 951/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.0451 - accuracy: 0.9848 - val_loss: 4.2052 - val_accuracy: 0.6773\n",
      "Epoch 952/1000\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.0343 - accuracy: 0.9894 - val_loss: 4.1834 - val_accuracy: 0.6773\n",
      "Epoch 953/1000\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.0483 - accuracy: 0.9833 - val_loss: 4.1551 - val_accuracy: 0.6636\n",
      "Epoch 954/1000\n",
      "83/83 [==============================] - 0s 702us/step - loss: 0.0626 - accuracy: 0.9758 - val_loss: 4.2047 - val_accuracy: 0.6636\n",
      "Epoch 955/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.1497 - accuracy: 0.9470 - val_loss: 4.6003 - val_accuracy: 0.5273\n",
      "Epoch 956/1000\n",
      "83/83 [==============================] - 0s 675us/step - loss: 0.2507 - accuracy: 0.9106 - val_loss: 4.2143 - val_accuracy: 0.6227\n",
      "Epoch 957/1000\n",
      "83/83 [==============================] - 0s 661us/step - loss: 0.1148 - accuracy: 0.9485 - val_loss: 4.4532 - val_accuracy: 0.6091\n",
      "Epoch 958/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.1854 - accuracy: 0.9212 - val_loss: 3.9352 - val_accuracy: 0.6318\n",
      "Epoch 959/1000\n",
      "83/83 [==============================] - 0s 663us/step - loss: 0.1211 - accuracy: 0.9515 - val_loss: 4.4674 - val_accuracy: 0.6318\n",
      "Epoch 960/1000\n",
      "83/83 [==============================] - 0s 682us/step - loss: 0.1334 - accuracy: 0.9576 - val_loss: 4.0667 - val_accuracy: 0.6273\n",
      "Epoch 961/1000\n",
      "83/83 [==============================] - 0s 669us/step - loss: 0.0803 - accuracy: 0.9682 - val_loss: 4.2035 - val_accuracy: 0.6500\n",
      "Epoch 962/1000\n",
      "83/83 [==============================] - 0s 661us/step - loss: 0.0401 - accuracy: 0.9894 - val_loss: 4.1611 - val_accuracy: 0.6364\n",
      "Epoch 963/1000\n",
      "83/83 [==============================] - 0s 654us/step - loss: 0.0377 - accuracy: 0.9924 - val_loss: 4.2113 - val_accuracy: 0.6682\n",
      "Epoch 964/1000\n",
      "83/83 [==============================] - 0s 644us/step - loss: 0.0411 - accuracy: 0.9879 - val_loss: 4.1962 - val_accuracy: 0.6455\n",
      "Epoch 965/1000\n",
      "83/83 [==============================] - 0s 645us/step - loss: 0.0507 - accuracy: 0.9818 - val_loss: 4.1214 - val_accuracy: 0.6909\n",
      "Epoch 966/1000\n",
      "83/83 [==============================] - 0s 660us/step - loss: 0.0333 - accuracy: 0.9848 - val_loss: 4.2368 - val_accuracy: 0.6500\n",
      "Epoch 967/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.0831 - accuracy: 0.9652 - val_loss: 4.2603 - val_accuracy: 0.6591\n",
      "Epoch 968/1000\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.0409 - accuracy: 0.9864 - val_loss: 4.2382 - val_accuracy: 0.6273\n",
      "Epoch 969/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.0720 - accuracy: 0.9727 - val_loss: 4.2457 - val_accuracy: 0.6591\n",
      "Epoch 970/1000\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.0726 - accuracy: 0.9712 - val_loss: 4.2873 - val_accuracy: 0.6545\n",
      "Epoch 971/1000\n",
      "83/83 [==============================] - 0s 646us/step - loss: 0.1158 - accuracy: 0.9515 - val_loss: 4.0509 - val_accuracy: 0.6636\n",
      "Epoch 972/1000\n",
      "83/83 [==============================] - 0s 709us/step - loss: 0.1321 - accuracy: 0.9485 - val_loss: 4.4185 - val_accuracy: 0.6091\n",
      "Epoch 973/1000\n",
      "83/83 [==============================] - 0s 661us/step - loss: 0.1504 - accuracy: 0.9424 - val_loss: 4.4862 - val_accuracy: 0.6636\n",
      "Epoch 974/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.0553 - accuracy: 0.9818 - val_loss: 4.4137 - val_accuracy: 0.6500\n",
      "Epoch 975/1000\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.1711 - accuracy: 0.9409 - val_loss: 4.3211 - val_accuracy: 0.6000\n",
      "Epoch 976/1000\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.1355 - accuracy: 0.9561 - val_loss: 4.3386 - val_accuracy: 0.6318\n",
      "Epoch 977/1000\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.0514 - accuracy: 0.9833 - val_loss: 4.2549 - val_accuracy: 0.6545\n",
      "Epoch 978/1000\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.0423 - accuracy: 0.9894 - val_loss: 4.1991 - val_accuracy: 0.6591\n",
      "Epoch 979/1000\n",
      "83/83 [==============================] - 0s 646us/step - loss: 0.0919 - accuracy: 0.9652 - val_loss: 4.3287 - val_accuracy: 0.6273\n",
      "Epoch 980/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.2097 - accuracy: 0.9182 - val_loss: 4.4970 - val_accuracy: 0.6318\n",
      "Epoch 981/1000\n",
      "83/83 [==============================] - 0s 648us/step - loss: 0.0891 - accuracy: 0.9652 - val_loss: 4.5517 - val_accuracy: 0.6364\n",
      "Epoch 982/1000\n",
      "83/83 [==============================] - 0s 646us/step - loss: 0.0474 - accuracy: 0.9833 - val_loss: 4.4125 - val_accuracy: 0.6591\n",
      "Epoch 983/1000\n",
      "83/83 [==============================] - 0s 645us/step - loss: 0.0486 - accuracy: 0.9864 - val_loss: 4.3819 - val_accuracy: 0.6409\n",
      "Epoch 984/1000\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.0353 - accuracy: 0.9894 - val_loss: 4.3316 - val_accuracy: 0.6500\n",
      "Epoch 985/1000\n",
      "83/83 [==============================] - 0s 653us/step - loss: 0.0285 - accuracy: 0.9924 - val_loss: 4.3436 - val_accuracy: 0.6636\n",
      "Epoch 986/1000\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.0552 - accuracy: 0.9788 - val_loss: 4.4250 - val_accuracy: 0.5818\n",
      "Epoch 987/1000\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.0738 - accuracy: 0.9742 - val_loss: 4.3484 - val_accuracy: 0.6409\n",
      "Epoch 988/1000\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.0306 - accuracy: 0.9924 - val_loss: 4.2830 - val_accuracy: 0.6682\n",
      "Epoch 989/1000\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.0868 - accuracy: 0.9712 - val_loss: 5.0068 - val_accuracy: 0.5909\n",
      "Epoch 990/1000\n",
      "83/83 [==============================] - 0s 642us/step - loss: 0.1812 - accuracy: 0.9258 - val_loss: 4.3800 - val_accuracy: 0.5864\n",
      "Epoch 991/1000\n",
      "83/83 [==============================] - 0s 639us/step - loss: 0.1440 - accuracy: 0.9303 - val_loss: 4.5244 - val_accuracy: 0.6364\n",
      "Epoch 992/1000\n",
      "83/83 [==============================] - 0s 694us/step - loss: 0.0777 - accuracy: 0.9727 - val_loss: 4.3454 - val_accuracy: 0.6682\n",
      "Epoch 993/1000\n",
      "83/83 [==============================] - 0s 643us/step - loss: 0.0659 - accuracy: 0.9773 - val_loss: 4.4456 - val_accuracy: 0.6409\n",
      "Epoch 994/1000\n",
      "83/83 [==============================] - 0s 644us/step - loss: 0.1203 - accuracy: 0.9470 - val_loss: 4.3946 - val_accuracy: 0.6591\n",
      "Epoch 995/1000\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.0976 - accuracy: 0.9667 - val_loss: 4.1654 - val_accuracy: 0.6500\n",
      "Epoch 996/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.0427 - accuracy: 0.9848 - val_loss: 4.1901 - val_accuracy: 0.6682\n",
      "Epoch 997/1000\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.2555 - accuracy: 0.9197 - val_loss: 4.5159 - val_accuracy: 0.5727\n",
      "Epoch 998/1000\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.1896 - accuracy: 0.9212 - val_loss: 4.2371 - val_accuracy: 0.6545\n",
      "Epoch 999/1000\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.0375 - accuracy: 0.9879 - val_loss: 4.2995 - val_accuracy: 0.6455\n",
      "Epoch 1000/1000\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.0557 - accuracy: 0.9803 - val_loss: 4.2137 - val_accuracy: 0.6182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79        20\n",
      "           1       0.79      0.95      0.86        20\n",
      "           2       0.52      0.55      0.54        20\n",
      "           3       0.95      0.90      0.92        20\n",
      "           4       0.46      0.30      0.36        20\n",
      "           5       0.67      0.70      0.68        20\n",
      "           6       1.00      0.80      0.89        20\n",
      "           7       0.70      0.70      0.70        20\n",
      "           8       0.33      0.40      0.36        20\n",
      "           9       0.59      0.95      0.73        20\n",
      "          10       0.58      0.35      0.44        20\n",
      "\n",
      "    accuracy                           0.67       220\n",
      "   macro avg       0.68      0.67      0.66       220\n",
      "weighted avg       0.68      0.67      0.66       220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 500 if choosenIndex == 0 else 1000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "model = getNetwork()\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.25)\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "report = classification_report(y_test, pred)\n",
    "classification_report_csv(report, \"NN\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Models in C code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models in Pipeline\n",
    "modelsInPipeline = []\n",
    "modelsInPipeline.append('LR')\n",
    "modelsInPipeline.append('SVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmptzodjen8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmptzodjen8/assets\n"
     ]
    }
   ],
   "source": [
    "directories = ['s/', 's3/', 's6/']\n",
    "\n",
    "# Neural network with TinyMLGen\n",
    "with open('exportedModels/' + directories[choosenIndex] + 'NNmodel.h', 'w') as f:\n",
    "    f.write(tiny.port(model, optimize=False))\n",
    "\n",
    "# Classifiers with MicroMLGen\n",
    "for name, model in models:\n",
    "    prepath = 'exportedModels/' + directories[choosenIndex]\n",
    "    path = prepath + name + '.h'\n",
    "    if name in modelsInPipeline:\n",
    "        model = model[1]\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(port(model, optimize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
