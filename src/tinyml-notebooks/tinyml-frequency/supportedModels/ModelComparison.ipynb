{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Comparison for TinyML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "import pickle\n",
    "\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,  classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold,StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dense, Input, concatenate, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from micromlgen import port\n",
    "import tinymlgen as tiny\n",
    "\n",
    "import warnings\n",
    "import sys\n",
    "import seaborn as sbs\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change 'chosenIndex' to change the chosen Test (s/s3/s6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataXPath = ['../data/X.pkl', '../data/XS3.pkl', '../data/XS6.pkl']\n",
    "dataYPath = ['../data/y.pkl', '../data/yS3.pkl', '../data/yS6.pkl']\n",
    "choosenIndex = 2\n",
    "\n",
    "with open(dataXPath[choosenIndex], 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open(dataYPath[choosenIndex], 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000]\n",
      "['2000', '2100', '2200', '2300', '2400', '2500', '2600', '2700', '2800', '2900', '3000']\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y))\n",
    "labels = [str(el) for el in list(np.unique(y))]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "# Convert the labels in values like 0...n for the NN tests\n",
    "\n",
    "labels = []\n",
    "uniques = list(np.unique(y))\n",
    "\n",
    "[labels.append(uniques.index(el)) for el in y]\n",
    "\n",
    "y = np.array(labels)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4285.7  4294.7  4306.07 4293.05 4286.88 4312.93 4301.12 4287.5  4284.17\n",
      "  4290.04 4285.91 4269.82 4283.57 4264.58 4259.01 4268.87 4246.28 4241.\n",
      "  4245.03 4236.4  4226.57 4236.02 4231.11 4218.93 4228.91 4229.2  4215.02\n",
      "  4215.3  4214.23 4204.33 4204.88 4209.74]\n",
      " [4002.17 4015.39 4018.22 4015.45 4016.49 4028.62 4032.57 4026.54 4027.87\n",
      "  4029.12 4038.97 4038.42 4029.67 4036.72 4036.27 4039.25 4029.92 4038.12\n",
      "  4039.1  4041.64 4040.98 4042.86 4039.91 4048.86 4053.43 4053.   4054.18\n",
      "  4053.5  4055.97 4057.14 4057.02 4063.76]\n",
      " [5257.57 5255.8  5257.39 5218.43 5261.56 5252.22 5241.1  5202.98 5263.5\n",
      "  5186.01 5237.17 5252.8  5204.57 5229.22 5210.17 5241.79 5208.13 5210.38\n",
      "  5218.85 5226.97 5264.2  5170.76 5247.67 5202.49 5187.13 5157.64 5175.31\n",
      "  5161.65 5183.45 5260.1  5194.25 5213.14]\n",
      " [4651.56 4657.4  4666.16 4644.15 4662.31 4664.16 4654.97 4659.11 4669.47\n",
      "  4650.75 4661.73 4665.12 4661.75 4659.81 4674.65 4656.95 4659.17 4670.53\n",
      "  4666.99 4663.3  4676.72 4664.78 4666.05 4681.82 4683.45 4668.1  4689.23\n",
      "  4682.67 4673.25 4691.56 4700.54 4679.8 ]\n",
      " [4735.3  4729.42 4717.07 4722.76 4712.48 4727.77 4795.11 4794.24 4859.97\n",
      "  4719.84 4691.06 4739.58 4693.46 4722.77 4753.11 4727.86 4698.81 4711.94\n",
      "  4708.91 4686.69 4719.82 4711.56 4693.49 4731.29 4709.92 4740.18 4676.35\n",
      "  4697.   4786.77 4715.63 4692.2  4745.64]\n",
      " [4765.68 4742.69 4764.18 4766.98 4753.5  4765.03 4777.96 4776.65 4762.14\n",
      "  4775.39 4760.58 4773.51 4777.7  4762.86 4764.15 4779.97 4770.71 4758.22\n",
      "  4777.26 4766.52 4764.66 4772.45 4779.05 4756.2  4783.25 4777.67 4771.36\n",
      "  4777.55 4787.29 4769.71 4784.51 4785.54]\n",
      " [4002.47 4010.66 4005.25 4011.19 4013.33 4012.11 4009.63 4024.49 4018.02\n",
      "  4018.7  4028.64 4029.98 4026.63 4037.55 4036.71 4043.96 4041.27 4047.02\n",
      "  4047.47 4049.88 4049.34 4044.36 4051.23 4059.64 4059.76 4061.42 4066.79\n",
      "  4068.62 4065.62 4077.77 4078.07 4077.16]\n",
      " [4820.02 4941.13 4787.46 4905.74 4830.28 4836.14 4813.37 4893.81 4809.42\n",
      "  4810.19 4821.27 4803.58 4841.25 4835.8  4813.65 4798.87 4814.53 4820.08\n",
      "  4806.26 4815.5  4811.94 4810.81 4850.12 4827.5  4820.11 4827.92 4816.76\n",
      "  4794.   4818.17 4811.19 4820.62 4830.26]\n",
      " [4664.23 4602.29 4614.51 4721.94 4589.06 4597.87 4612.86 4604.48 4601.48\n",
      "  4583.23 4608.76 4616.14 4613.56 4636.3  4633.15 4587.39 4616.32 4575.76\n",
      "  4602.66 4624.45 4702.51 4693.34 4705.25 4583.44 4613.92 4593.66 4595.71\n",
      "  4617.23 4664.8  4647.6  4603.59 4605.26]\n",
      " [3420.3  3471.7  3453.37 3460.44 3421.77 3467.86 3444.45 3431.95 3425.22\n",
      "  3443.57 3419.61 3437.86 3415.95 3441.99 3447.86 3395.02 3439.68 3405.78\n",
      "  3402.81 3576.6  3448.41 3411.03 3439.68 3433.46 3418.4  3403.06 3443.6\n",
      "  3440.73 3393.42 3428.64 3462.34 3408.88]\n",
      " [3534.76 3407.07 3472.43 3439.88 3389.8  3459.44 3445.7  3435.68 3412.32\n",
      "  3377.38 3472.32 3451.13 3444.22 3385.5  3377.67 3427.09 3451.98 3391.1\n",
      "  3440.78 3394.05 3406.37 3380.76 3380.84 3356.54 3462.29 3395.96 3399.81\n",
      "  3423.79 3443.24 3384.91 3401.64 3360.22]\n",
      " [5191.13 5194.17 5179.63 5290.21 5148.35 5163.3  5142.59 5150.89 5093.9\n",
      "  5177.19 5110.95 5133.1  5095.89 5132.07 5142.18 5109.25 5125.3  5183.34\n",
      "  5066.84 5105.3  5088.96 5058.95 5106.06 5092.76 5114.53 5095.59 5105.46\n",
      "  5196.08 5046.21 5202.38 5031.64 5074.97]\n",
      " [4865.58 4943.24 4885.97 4896.96 4842.37 4886.88 4849.64 4834.77 4909.31\n",
      "  4855.25 4930.09 4881.89 4936.58 4826.33 4843.53 4858.44 4856.07 4836.49\n",
      "  4853.73 4831.6  4893.07 4897.62 4891.65 4869.94 4896.91 4861.16 4880.22\n",
      "  4894.24 4861.36 4843.61 4884.29 4933.94]\n",
      " [4254.18 4242.38 4223.12 4210.77 4326.77 4269.2  4267.28 4247.9  4253.01\n",
      "  4253.75 4245.53 4210.58 4223.53 4209.31 4242.13 4244.26 4218.62 4292.42\n",
      "  4189.36 4219.81 4322.94 4197.92 4238.33 4224.01 4244.71 4224.76 4215.24\n",
      "  4192.82 4234.32 4235.28 4211.8  4191.02]\n",
      " [5150.5  5164.47 5159.41 5140.03 5171.83 5139.82 5150.98 5182.42 5144.68\n",
      "  5158.65 5160.27 5144.38 5145.73 5162.4  5135.74 5145.82 5146.84 5151.85\n",
      "  5155.42 5159.39 5128.59 5163.74 5179.62 5153.2  5283.92 5149.83 5115.69\n",
      "  5125.16 5154.91 5127.91 5128.34 5136.82]\n",
      " [5115.28 5100.14 5140.86 5103.59 5111.23 5104.   5177.86 5095.19 5152.26\n",
      "  5146.83 5119.66 5115.37 5130.94 5114.77 5126.37 5116.97 5125.33 5103.63\n",
      "  5126.59 5112.95 5135.58 5118.36 5142.06 5124.39 5144.45 5094.82 5133.73\n",
      "  5137.12 5143.46 5106.89 5166.78 5102.06]\n",
      " [4752.5  4748.46 4781.07 4712.68 4733.74 4721.6  4775.09 4732.71 4829.22\n",
      "  4756.3  4726.25 4835.06 4716.55 4722.02 4772.16 4747.23 4821.45 4810.81\n",
      "  4788.8  4829.42 4733.39 4716.1  4790.02 4692.82 4710.27 4762.85 4791.99\n",
      "  4724.04 4721.56 4700.05 4709.76 4708.64]\n",
      " [3457.11 3440.68 3425.24 3389.65 3427.69 3391.66 3401.73 3348.58 3377.16\n",
      "  3384.58 3384.15 3458.96 3383.7  3369.54 3386.63 3379.35 3466.23 3390.46\n",
      "  3416.7  3400.61 3397.6  3350.04 3481.69 3366.18 3367.78 3404.51 3371.62\n",
      "  3420.17 3378.68 3435.37 3385.76 3357.21]\n",
      " [4679.7  4777.33 4755.85 4735.23 4692.14 4711.15 4707.25 4747.79 4804.29\n",
      "  4705.44 4679.45 4712.42 4700.79 4684.75 4742.83 4725.22 4685.69 4758.27\n",
      "  4739.43 4733.49 4713.97 4732.17 4747.39 4701.97 4739.73 4728.02 4697.88\n",
      "  4702.53 4703.69 4688.31 4713.13 4681.43]\n",
      " [4270.12 4278.75 4279.86 4270.87 4272.37 4268.41 4261.5  4268.99 4257.7\n",
      "  4253.92 4259.34 4255.53 4244.94 4251.93 4246.96 4243.18 4242.38 4249.07\n",
      "  4235.28 4232.97 4243.99 4224.87 4225.93 4238.12 4231.85 4215.54 4249.51\n",
      "  4228.06 4219.23 4223.69 4214.31 4218.31]\n",
      " [3340.74 3333.96 3352.55 3367.47 3356.27 3331.28 3372.61 3329.7  3354.12\n",
      "  3329.81 3359.57 3360.44 3373.48 3333.25 3331.84 3319.45 3325.1  3351.54\n",
      "  3364.07 3324.72 3359.44 3317.77 3327.83 3324.62 3325.11 3307.52 3313.27\n",
      "  3312.06 3319.08 3294.34 3309.55 3294.65]\n",
      " [4924.29 4967.02 4938.77 4957.79 4955.72 4974.13 4944.19 4965.16 4943.18\n",
      "  4959.14 4927.71 4953.   4898.14 4923.66 4906.06 4889.64 4858.89 4877.42\n",
      "  4819.19 4826.44 4812.5  4815.16 4784.86 4813.27 4783.44 4807.07 4817.37\n",
      "  4837.42 4812.71 4884.95 4843.17 4859.87]\n",
      " [5275.51 5312.64 5327.85 5281.37 5264.73 5277.93 5265.26 5246.33 5252.56\n",
      "  5264.74 5228.95 5246.81 5305.25 5239.67 5237.7  5271.23 5298.48 5248.39\n",
      "  5257.11 5240.54 5256.98 5254.62 5229.22 5273.58 5293.88 5249.31 5262.99\n",
      "  5271.79 5264.37 5275.21 5282.36 5292.8 ]\n",
      " [4750.57 4774.14 4777.3  4741.59 4745.97 4788.41 4749.17 4868.37 4767.9\n",
      "  4752.1  4752.32 4766.03 4731.67 4731.21 4770.52 4740.97 4725.05 4721.65\n",
      "  4778.73 4709.39 4758.1  4716.25 4723.83 4723.33 4774.37 4683.37 4788.55\n",
      "  4718.01 4695.3  4699.25 4729.94 4704.67]\n",
      " [3478.55 3493.23 3465.95 3486.2  3475.95 3487.38 3464.87 3484.39 3475.04\n",
      "  3488.39 3461.96 3479.34 3463.85 3499.5  3468.01 3501.8  3466.98 3494.05\n",
      "  3458.8  3481.4  3475.21 3480.84 3468.15 3492.25 3468.55 3492.18 3477.07\n",
      "  3496.67 3499.37 3492.85 3465.29 3483.11]\n",
      " [4112.87 4114.2  4129.02 4146.03 4202.02 4164.43 4182.07 4202.28 4206.4\n",
      "  4180.35 4239.43 4242.76 4284.8  4326.21 4382.43 4329.35 4372.68 4461.37\n",
      "  4437.74 4397.61 4459.84 4348.29 4364.28 4370.   4350.68 4312.58 4314.72\n",
      "  4348.65 4276.31 4264.81 4275.13 4254.56]\n",
      " [4153.73 4130.63 4126.78 4146.77 4109.95 4110.43 4189.23 4126.23 4081.9\n",
      "  4187.12 4115.   4196.57 4091.1  4166.24 4081.5  4131.9  4101.34 4150.82\n",
      "  4089.49 4125.47 4142.65 4146.74 4133.97 4148.9  4123.5  4163.61 4156.79\n",
      "  4135.38 4128.2  4199.4  4155.58 4160.09]\n",
      " [4359.69 4395.7  4352.16 4499.05 4525.27 4374.56 4379.99 4375.98 4342.5\n",
      "  4399.45 4377.29 4428.61 4347.26 4422.98 4339.84 4362.41 4356.04 4356.98\n",
      "  4332.62 4367.72 4344.28 4373.22 4315.38 4374.43 4347.51 4339.81 4364.91\n",
      "  4471.74 4451.46 4363.99 4306.87 4346.41]\n",
      " [4307.52 4559.18 4270.34 4316.13 4278.87 4439.45 4196.18 4363.03 4210.08\n",
      "  4257.27 4215.7  4246.51 4302.35 4414.07 4436.26 4416.25 4473.64 4306.71\n",
      "  4251.65 4300.12 4208.48 4553.79 4244.55 4368.72 4225.75 4247.62 4223.32\n",
      "  4272.41 4199.41 4280.62 4345.38 4529.19]\n",
      " [4345.76 4347.77 4358.19 4344.57 4362.89 4359.02 4354.61 4377.75 4386.73\n",
      "  4363.75 4389.35 4384.35 4387.63 4382.88 4412.42 4406.08 4394.62 4410.67\n",
      "  4403.07 4396.45 4427.96 4401.97 4395.77 4423.38 4400.75 4401.78 4439.74\n",
      "  4406.39 4417.78 4406.24 4413.69 4421.78]\n",
      " [4095.94 4103.27 4098.79 4082.23 4084.71 4082.13 4074.8  4080.52 4082.14\n",
      "  4076.12 4069.03 4077.21 4079.78 4076.3  4080.34 4079.21 4072.46 4081.62\n",
      "  4082.64 4075.47 4098.6  4081.35 4073.76 4085.81 4080.68 4075.61 4079.66\n",
      "  4075.09 4069.73 4081.01 4099.79 4069.97]\n",
      " [4376.98 4373.45 4352.69 4393.36 4357.2  4388.48 4386.78 4396.32 4382.52\n",
      "  4434.76 4410.97 4460.8  4460.44 4482.06 4454.65 4498.46 4487.73 4487.73\n",
      "  4472.04 4485.48 4450.98 4473.96 4451.46 4448.7  4425.34 4442.9  4406.7\n",
      "  4429.38 4403.38 4406.04 4380.32 4404.35]\n",
      " [4797.16 4807.69 4811.41 4795.26 4943.98 4817.04 4797.49 4817.23 4917.55\n",
      "  4786.97 4833.74 4827.47 4800.43 4893.22 4831.76 4793.54 4875.63 4820.86\n",
      "  4793.77 4811.42 4811.12 4797.51 4866.96 4828.44 4793.59 4793.34 4831.19\n",
      "  4834.23 4833.72 4822.07 4789.6  4781.12]\n",
      " [4326.5  4313.28 4334.06 4377.97 4348.72 4339.06 4377.83 4341.31 4396.3\n",
      "  4336.89 4357.39 4374.14 4346.4  4351.34 4374.67 4341.64 4503.5  4325.51\n",
      "  4391.36 4313.47 4349.49 4309.36 4317.95 4329.54 4363.59 4314.09 4313.33\n",
      "  4305.59 4316.62 4300.16 4371.72 4303.  ]\n",
      " [4350.69 4380.19 4347.42 4368.03 4344.8  4355.59 4349.48 4374.36 4336.41\n",
      "  4367.51 4355.29 4353.55 4347.08 4379.53 4333.05 4358.9  4349.86 4371.24\n",
      "  4339.95 4379.58 4334.74 4358.96 4349.   4364.19 4333.17 4364.09 4329.73\n",
      "  4351.47 4344.39 4357.79 4321.39 4358.84]\n",
      " [3139.56 3145.24 3156.18 3146.81 3165.   3168.77 3157.09 3153.37 3142.03\n",
      "  3118.46 3113.23 3096.71 3079.39 3072.83 3068.36 3052.63 3047.87 3048.32\n",
      "  3041.72 3026.71 3041.03 3024.61 3046.64 3028.91 3031.67 3025.35 3039.48\n",
      "  3047.61 3039.29 3022.45 3020.3  3007.02]\n",
      " [5119.93 5079.43 5140.83 5103.77 5109.05 5107.13 5113.04 5086.16 5104.14\n",
      "  5095.59 5098.54 5090.65 5122.6  5081.5  5102.06 5098.99 5094.2  5082.89\n",
      "  5111.5  5096.5  5086.74 5091.81 5112.38 5083.05 5107.41 5101.81 5106.95\n",
      "  5093.78 5102.72 5078.46 5114.43 5100.44]\n",
      " [3457.59 3473.45 3451.91 3475.61 3465.55 3471.38 3456.21 3480.47 3456.19\n",
      "  3478.24 3461.24 3497.56 3464.95 3474.71 3463.07 3482.16 3464.88 3480.55\n",
      "  3454.96 3481.15 3462.21 3489.01 3462.61 3481.6  3459.68 3487.87 3465.76\n",
      "  3478.38 3461.52 3485.99 3459.53 3487.02]\n",
      " [4555.19 4558.4  4568.49 4560.54 4572.85 4578.34 4569.73 4572.08 4585.08\n",
      "  4580.6  4591.27 4607.13 4616.53 4634.12 4664.33 4669.08 4686.11 4709.41\n",
      "  4732.39 4743.03 4780.16 4791.41 4817.91 4830.38 4856.13 4865.35 4895.63\n",
      "  4914.54 4910.56 4916.06 4925.28 4935.6 ]\n",
      " [3466.59 3447.47 3452.28 3457.73 3464.07 3452.46 3465.74 3455.13 3468.09\n",
      "  3452.34 3460.42 3454.73 3471.83 3455.93 3459.06 3455.48 3466.64 3455.16\n",
      "  3462.27 3453.01 3462.28 3452.76 3464.37 3456.86 3467.78 3459.48 3466.13\n",
      "  3460.67 3472.15 3451.17 3456.15 3453.21]\n",
      " [3490.86 3417.89 3437.13 3449.86 3500.27 3448.81 3455.06 3454.55 3401.51\n",
      "  3410.71 3411.61 3464.13 3472.06 3431.53 3413.51 3401.16 3440.57 3523.88\n",
      "  3421.51 3437.09 3569.49 3436.42 3423.62 3450.31 3431.76 3444.18 3463.39\n",
      "  3458.05 3479.85 3466.26 3436.82 3529.58]\n",
      " [4809.63 4827.15 4821.63 4822.46 4822.57 4834.29 4813.53 4825.55 4836.94\n",
      "  4830.8  4825.39 4829.9  4829.5  4848.92 4834.13 4827.88 4820.99 4832.51\n",
      "  4831.54 4844.2  4836.97 4832.64 4837.1  4837.93 4836.71 4835.11 4838.05\n",
      "  4826.76 4825.33 4840.72 4829.8  4820.88]\n",
      " [4739.72 4786.17 4784.74 4774.3  4832.83 4772.58 4800.19 4880.47 4806.61\n",
      "  4884.52 4818.68 4798.81 4765.96 4781.76 4740.41 4757.42 4771.06 4748.08\n",
      "  4746.15 4763.89 4787.83 4762.62 4759.93 4796.91 4779.49 4772.13 4714.18\n",
      "  4752.15 4790.36 4753.05 4718.41 4750.89]\n",
      " [5122.73 5127.31 5108.74 5117.47 5133.94 5120.53 5134.29 5139.88 5126.79\n",
      "  5128.34 5145.07 5134.28 5128.45 5154.02 5159.86 5129.63 5145.63 5133.63\n",
      "  5143.87 5141.34 5141.39 5131.01 5150.76 5155.29 5130.88 5134.03 5146.19\n",
      "  5125.66 5139.74 5143.43 5132.19 5124.32]\n",
      " [5141.3  5178.74 5193.21 5221.78 5153.85 5172.79 5199.5  5167.09 5176.07\n",
      "  5171.94 5246.73 5180.73 5181.85 5192.92 5189.98 5184.74 5181.82 5169.34\n",
      "  5149.46 5250.59 5173.06 5169.02 5155.33 5184.67 5144.28 5184.   5197.82\n",
      "  5232.59 5207.76 5164.19 5150.66 5202.27]\n",
      " [4104.29 4125.42 4090.69 4115.67 4126.93 4124.05 4128.15 4145.56 4117.43\n",
      "  4181.76 4131.53 4136.7  4100.95 4136.17 4118.77 4123.94 4144.36 4155.53\n",
      "  4095.   4159.59 4086.52 4120.76 4063.21 4123.13 4088.3  4125.28 4066.25\n",
      "  4079.52 4060.81 4128.38 4088.45 4100.62]\n",
      " [4436.73 4458.57 4453.48 4452.73 4449.1  4473.09 4445.98 4467.97 4455.01\n",
      "  4459.47 4432.29 4458.73 4434.12 4436.22 4437.74 4445.24 4419.05 4450.04\n",
      "  4438.14 4439.45 4443.15 4459.28 4431.36 4455.52 4450.23 4448.53 4447.89\n",
      "  4471.02 4454.86 4468.51 4468.86 4472.95]\n",
      " [4222.8  4243.97 4216.22 4260.41 4267.95 4258.96 4198.2  4241.33 4201.91\n",
      "  4211.57 4170.52 4216.69 4221.86 4218.76 4203.71 4235.46 4203.77 4251.42\n",
      "  4203.09 4189.91 4192.78 4188.11 4239.3  4228.64 4270.26 4396.84 4369.27\n",
      "  4384.77 4357.79 4403.54 4448.78 4560.48]\n",
      " [3503.56 3535.53 3513.47 3537.61 3529.33 3551.51 3530.83 3567.04 3551.93\n",
      "  3574.7  3563.49 3606.7  3596.18 3656.31 3630.2  3668.47 3665.95 3724.14\n",
      "  3677.69 3709.75 3700.52 3725.59 3695.82 3731.6  3695.82 3706.79 3686.97\n",
      "  3689.79 3660.7  3703.09 3690.12 3740.75]\n",
      " [4736.99 4586.85 4635.36 4572.12 4598.31 4602.42 4598.17 4655.31 4655.51\n",
      "  4583.37 4598.74 4657.48 4616.14 4636.51 4675.13 4610.08 4620.56 4587.78\n",
      "  4654.47 4603.82 4624.57 4590.98 4603.97 4604.36 4592.93 4570.48 4601.\n",
      "  4681.72 4601.25 4599.84 4600.46 4608.59]\n",
      " [5170.73 5182.02 5181.82 5188.6  5163.47 5178.6  5209.28 5175.79 5156.03\n",
      "  5178.18 5169.58 5262.89 5160.94 5262.22 5172.01 5175.31 5192.28 5169.78\n",
      "  5165.78 5213.58 5268.22 5168.63 5168.13 5188.98 5167.69 5174.11 5181.31\n",
      "  5205.57 5201.68 5202.16 5185.35 5216.08]\n",
      " [2943.23 2934.87 2939.47 2936.32 2934.86 2931.84 2945.77 2924.5  2935.37\n",
      "  2930.54 2943.31 2934.45 2946.32 2951.45 2945.31 2936.31 2933.86 2931.45\n",
      "  2945.94 2927.81 2938.77 2939.38 2951.77 2946.97 2944.75 2932.79 2944.36\n",
      "  2938.89 2948.67 2928.74 2942.19 2934.57]\n",
      " [4789.03 4793.18 4785.75 4797.48 4837.8  4788.01 4795.47 4759.14 4819.49\n",
      "  4819.13 4786.9  4755.72 4808.2  4835.46 4788.35 4773.55 4773.96 4764.91\n",
      "  4776.32 4762.8  4765.24 4806.88 4800.29 4809.03 4760.35 4840.61 4751.24\n",
      "  4771.56 4763.85 4760.79 4735.32 4775.93]\n",
      " [4287.35 4271.45 4256.54 4295.47 4254.78 4238.26 4249.32 4235.07 4219.68\n",
      "  4218.14 4213.93 4206.08 4192.37 4185.68 4169.37 4176.26 4178.99 4150.04\n",
      "  4152.72 4155.48 4148.3  4150.37 4149.74 4138.06 4148.39 4151.17 4145.09\n",
      "  4145.85 4149.71 4148.49 4148.89 4155.89]\n",
      " [4725.82 4851.83 4802.74 4704.9  4678.74 4720.5  4689.1  4725.43 4700.09\n",
      "  4703.97 4820.97 4709.54 4765.   4724.33 4752.74 4725.76 4678.24 4806.11\n",
      "  4710.9  4697.72 4685.41 4721.24 4674.26 4741.82 4725.9  4716.58 4704.64\n",
      "  4700.35 4677.03 4698.46 4705.54 4709.89]\n",
      " [4003.54 4015.01 4035.22 4042.33 3993.47 4037.93 4044.44 4005.24 4019.8\n",
      "  4055.4  4017.   4014.86 4035.51 4041.02 4076.74 4053.41 4042.22 4041.02\n",
      "  4052.93 4055.74 4042.59 4105.98 4093.65 4046.97 4056.64 3984.83 4031.18\n",
      "  4008.44 4003.73 3984.12 4021.04 3985.68]\n",
      " [2939.35 2947.   2956.6  2952.13 2937.14 2940.   2926.55 2952.98 2935.78\n",
      "  2951.92 2932.31 2954.64 2927.75 2954.73 2931.67 2937.91 2928.31 2940.03\n",
      "  2929.99 2942.56 2926.62 2933.7  2926.05 2944.78 2922.8  2938.83 2939.74\n",
      "  2944.95 2934.32 2942.4  2932.66 2964.01]\n",
      " [4368.83 4365.47 4358.91 4397.31 4341.68 4377.52 4341.13 4356.19 4350.26\n",
      "  4359.04 4346.94 4362.36 4330.71 4334.11 4331.06 4345.36 4321.26 4341.35\n",
      "  4321.79 4326.72 4329.84 4346.67 4316.01 4338.75 4324.44 4329.34 4310.85\n",
      "  4340.19 4312.26 4322.71 4323.66 4331.59]\n",
      " [4855.35 4787.33 4787.64 4784.21 4791.72 4803.55 4788.88 4791.84 4850.25\n",
      "  4833.18 4797.74 4899.43 4806.73 4798.83 4806.78 4855.92 4912.15 4876.73\n",
      "  4792.82 4807.14 4772.03 4788.93 4803.16 4792.55 4819.56 4805.27 4811.32\n",
      "  4802.47 4822.47 4857.83 4994.88 4845.  ]\n",
      " [3528.39 3456.13 3445.39 3444.02 3502.26 3493.14 3475.21 3515.52 3516.71\n",
      "  3501.44 3470.78 3454.31 3436.17 3426.77 3519.81 3440.3  3488.03 3494.34\n",
      "  3434.5  3451.61 3434.9  3455.25 3458.6  3413.61 3483.53 3420.43 3413.2\n",
      "  3437.83 3563.6  3437.64 3426.59 3537.3 ]\n",
      " [4343.48 4370.37 4359.61 4359.33 4375.31 4373.85 4346.06 4373.01 4359.28\n",
      "  4358.1  4353.59 4368.13 4345.73 4369.43 4362.62 4360.3  4363.92 4373.71\n",
      "  4357.04 4380.88 4369.29 4356.06 4350.11 4373.01 4361.82 4358.4  4360.76\n",
      "  4358.93 4352.1  4365.37 4353.84 4346.96]\n",
      " [4857.1  4894.18 4842.91 4865.68 4933.94 4853.46 4849.79 4971.5  4832.89\n",
      "  5003.18 4912.25 4868.81 4863.   4852.09 4841.14 4861.57 4868.69 4837.15\n",
      "  4867.31 4869.86 4848.   4865.58 4850.71 4880.11 4847.89 4876.92 4867.66\n",
      "  4875.06 4876.47 4862.16 4861.54 4854.55]\n",
      " [4272.84 4277.48 4308.22 4280.13 4278.37 4282.   4292.89 4278.4  4298.38\n",
      "  4274.4  4283.93 4289.19 4295.58 4278.02 4303.87 4288.67 4288.69 4295.25\n",
      "  4302.56 4278.25 4298.31 4297.46 4296.38 4311.08 4313.52 4292.67 4294.32\n",
      "  4308.38 4299.86 4288.11 4318.13 4295.93]\n",
      " [3022.82 3006.89 3016.5  2993.52 3012.8  2980.41 3006.97 2990.26 2998.76\n",
      "  2986.7  3012.19 2986.08 3003.85 3007.9  3007.04 2984.96 3001.81 3002.98\n",
      "  3031.56 2990.99 2994.72 3002.69 3013.18 3000.91 2997.84 2978.72 2995.78\n",
      "  2975.74 2992.99 3040.6  3013.51 2981.34]\n",
      " [4443.96 4325.08 4346.18 4318.01 4365.98 4390.04 4387.93 4437.65 4345.98\n",
      "  4322.65 4349.04 4323.94 4410.27 4345.1  4354.93 4416.28 4375.89 4337.52\n",
      "  4364.77 4323.53 4361.55 4340.79 4340.74 4330.32 4367.08 4352.68 4358.1\n",
      "  4335.62 4363.41 4331.92 4356.68 4334.71]\n",
      " [4002.16 4014.91 4009.26 4033.3  4008.21 4019.82 4033.35 4024.27 4054.26\n",
      "  4041.95 3992.9  4044.   4033.3  4031.66 4015.41 4049.69 3994.21 4049.56\n",
      "  4034.53 4062.37 4039.32 4025.84 4067.71 4023.92 4063.55 4023.17 4001.04\n",
      "  4051.97 4060.68 4089.95 4013.36 4091.71]\n",
      " [4566.27 4622.07 4552.01 4589.47 4548.31 4551.3  4558.06 4678.86 4666.49\n",
      "  4567.51 4542.53 4564.06 4555.42 4583.9  4549.82 4553.93 4550.91 4592.35\n",
      "  4540.98 4607.29 4560.06 4639.17 4562.61 4636.74 4555.72 4607.52 4571.36\n",
      "  4596.26 4560.65 4594.04 4554.18 4557.  ]\n",
      " [4749.46 4701.58 4704.79 4713.58 4718.67 4751.57 4701.11 4718.08 4715.09\n",
      "  4819.12 4714.49 4709.28 4746.   4817.17 4713.66 4783.05 4697.97 4688.71\n",
      "  4744.   4697.52 4686.31 4695.58 4809.73 4719.07 4716.77 4688.84 4688.62\n",
      "  4685.77 4687.51 4707.42 4705.4  4829.58]\n",
      " [4859.01 4886.11 4851.97 4847.39 4873.13 4816.38 4881.38 4875.08 4855.19\n",
      "  4852.39 4933.7  4894.39 4864.75 4856.96 4846.34 4854.87 4863.21 4814.6\n",
      "  4884.55 4842.97 4931.71 4893.59 4870.11 4854.56 4836.88 4846.57 4851.28\n",
      "  4858.86 4864.17 4820.83 4859.3  4864.36]\n",
      " [4841.8  4865.39 4915.53 4857.6  5077.96 4922.64 4882.25 4909.13 4908.7\n",
      "  4935.88 4979.43 5028.16 4951.6  4943.19 5091.8  4992.78 4952.23 4970.86\n",
      "  4961.39 5026.58 4950.85 4930.52 4921.36 5149.36 4909.95 4905.15 4932.35\n",
      "  4932.05 4908.65 4919.82 4906.27 4888.14]\n",
      " [3425.13 3445.79 3607.06 3470.42 3437.15 3430.98 3425.96 3428.81 3463.31\n",
      "  3426.59 3434.87 3442.57 3441.56 3443.   3429.98 3435.04 3442.1  3442.41\n",
      "  3405.11 3460.02 3473.26 3438.8  3477.31 3424.7  3418.71 3463.05 3409.26\n",
      "  3399.28 3438.9  3480.71 3448.55 3402.38]\n",
      " [3379.47 3335.28 3419.18 3354.84 3380.3  3372.75 3470.73 3370.31 3408.79\n",
      "  3351.29 3338.38 3336.42 3401.35 3324.06 3345.01 3353.88 3358.66 3334.46\n",
      "  3354.61 3353.11 3355.72 3414.9  3354.91 3416.67 3395.95 3372.64 3388.58\n",
      "  3361.99 3355.01 3346.83 3353.17 3330.64]\n",
      " [5235.52 5255.65 5224.64 5274.27 5243.22 5330.41 5272.23 5259.56 5249.34\n",
      "  5259.1  5228.87 5262.02 5295.56 5234.87 5241.74 5239.22 5205.93 5263.11\n",
      "  5193.84 5244.04 5206.39 5213.39 5199.71 5236.73 5207.49 5229.84 5177.51\n",
      "  5229.72 5235.47 5296.56 5197.1  5284.45]\n",
      " [3444.69 3422.68 3456.54 3495.59 3562.23 3446.74 3429.63 3497.88 3437.7\n",
      "  3424.53 3417.   3426.74 3421.21 3442.93 3427.54 3435.67 3433.81 3444.38\n",
      "  3420.87 3416.9  3530.52 3512.84 3433.67 3435.53 3500.76 3531.65 3419.78\n",
      "  3422.7  3484.88 3448.   3452.5  3414.29]\n",
      " [3389.34 3400.56 3357.29 3413.64 3384.86 3403.48 3380.52 3349.29 3402.92\n",
      "  3377.83 3373.57 3377.78 3491.69 3365.49 3384.3  3369.59 3371.58 3385.93\n",
      "  3433.33 3453.98 3434.61 3378.25 3420.08 3356.77 3351.04 3390.63 3391.9\n",
      "  3573.44 3435.13 3366.89 3390.44 3351.61]\n",
      " [3461.42 3481.88 3469.19 3489.3  3465.58 3483.37 3461.92 3481.82 3472.9\n",
      "  3502.52 3459.34 3480.99 3465.83 3483.29 3464.09 3484.77 3462.42 3479.52\n",
      "  3462.32 3486.06 3456.58 3489.45 3462.3  3481.86 3456.05 3479.41 3454.39\n",
      "  3476.09 3464.41 3474.91 3457.15 3479.11]\n",
      " [4795.28 4831.77 4816.32 4778.99 4798.05 4791.   4787.12 4802.84 4784.13\n",
      "  4783.49 4796.51 4798.48 4777.28 4799.14 4814.27 4815.63 4798.29 4802.29\n",
      "  4795.3  4792.18 4825.82 4796.79 4786.37 4810.21 4785.04 4793.49 4804.08\n",
      "  4799.93 4784.93 4812.89 4784.06 4796.15]\n",
      " [3283.57 3288.58 3260.38 3321.66 3310.78 3289.62 3281.01 3290.89 3443.25\n",
      "  3373.07 3432.72 3299.6  3295.27 3295.88 3432.02 3308.48 3338.76 3304.41\n",
      "  3391.35 3353.98 3303.2  3291.57 3303.03 3297.97 3289.39 3308.96 3323.81\n",
      "  3348.54 3313.65 3379.54 3312.9  3326.81]\n",
      " [4711.9  4698.75 4714.76 4686.84 4720.88 4689.14 4722.75 4709.29 4736.76\n",
      "  4704.53 4753.2  4710.68 4723.72 4731.27 4741.59 4714.45 4772.3  4755.41\n",
      "  4802.37 4782.15 4852.55 4799.73 4858.39 4854.72 4861.16 4859.39 4916.77\n",
      "  4852.39 4900.98 4880.09 4870.12 4850.03]\n",
      " [4858.83 4812.46 4850.38 4836.02 4842.4  4833.2  4864.05 4826.29 4850.39\n",
      "  4843.65 4850.37 4823.52 4864.36 4839.39 4848.98 4840.88 4859.79 4829.55\n",
      "  4857.51 4834.77 4844.14 4843.77 4862.45 4827.78 4860.98 4843.62 4850.18\n",
      "  4848.05 4865.94 4829.04 4855.49 4850.6 ]\n",
      " [5100.73 5211.89 5120.86 5158.67 5205.75 5166.41 5108.7  5183.48 5127.32\n",
      "  5194.13 5140.56 5155.81 5131.25 5163.05 5159.54 5172.98 5151.17 5256.62\n",
      "  5138.96 5179.39 5164.83 5172.66 5174.98 5161.11 5144.35 5183.24 5232.25\n",
      "  5165.02 5136.3  5161.04 5174.98 5255.35]\n",
      " [4704.39 4718.11 4814.63 4679.6  4676.51 4774.53 4711.21 4732.44 4710.38\n",
      "  4721.54 4711.89 4740.19 4732.77 4719.56 4712.63 4733.72 4748.02 4720.78\n",
      "  4738.   4719.85 4713.26 4738.58 4727.34 4773.4  4718.86 4775.12 4704.85\n",
      "  4714.37 4708.39 4739.22 4707.18 4706.54]\n",
      " [3483.89 3476.85 3498.37 3498.34 3486.32 3482.57 3502.62 3495.92 3506.31\n",
      "  3475.98 3497.66 3496.87 3508.68 3511.08 3524.88 3501.97 3504.8  3547.68\n",
      "  3526.91 3498.45 3499.14 3536.44 3495.36 3493.51 3498.17 3506.05 3494.92\n",
      "  3486.46 3496.84 3488.95 3497.64 3484.12]\n",
      " [4704.24 4677.11 4705.89 4680.08 4719.71 4686.58 4667.96 4677.94 4666.68\n",
      "  4665.41 4681.82 4722.63 4730.99 4769.49 4668.28 4657.34 4739.27 4719.54\n",
      "  4731.55 4673.63 4654.87 4705.36 4737.49 4836.72 4648.23 4737.91 4671.85\n",
      "  4682.56 4677.14 4668.04 4639.6  4706.23]\n",
      " [4247.59 4217.75 4145.46 4151.51 4161.01 4244.96 4278.13 4306.34 4248.42\n",
      "  4314.91 4298.88 4152.21 4144.73 4138.09 4064.28 4359.53 4382.2  4350.11\n",
      "  4355.62 4334.62 4265.43 4237.07 4211.2  4103.65 4035.98 4106.93 4075.69\n",
      "  4188.84 4254.58 4313.88 4372.53 4340.13]\n",
      " [3382.69 3362.19 3385.65 3362.53 3378.06 3360.7  3378.23 3352.19 3371.49\n",
      "  3348.61 3355.83 3333.07 3352.5  3327.74 3349.69 3332.8  3347.94 3333.53\n",
      "  3356.09 3321.72 3338.48 3330.79 3370.85 3334.29 3374.7  3350.36 3374.52\n",
      "  3381.72 3400.02 3391.66 3421.04 3403.86]\n",
      " [4835.32 4798.38 4823.89 4813.13 4819.42 4802.55 4836.68 4803.77 4829.1\n",
      "  4817.77 4834.17 4815.13 4845.83 4818.29 4826.7  4817.22 4847.15 4808.93\n",
      "  4844.56 4825.62 4831.01 4820.18 4845.55 4815.42 4846.46 4825.82 4841.27\n",
      "  4821.66 4858.08 4814.05 4836.   4826.47]\n",
      " [4656.94 4634.58 4641.58 4639.43 4645.11 4626.11 4654.64 4630.2  4638.02\n",
      "  4629.55 4646.33 4626.98 4651.37 4629.03 4631.82 4627.54 4644.54 4614.12\n",
      "  4640.91 4626.16 4627.6  4610.37 4638.97 4609.62 4626.12 4619.44 4628.79\n",
      "  4607.14 4641.2  4609.43 4622.93 4611.05]\n",
      " [4694.96 4713.65 4680.24 4701.96 4701.85 4699.04 4683.05 4712.84 4671.26\n",
      "  4701.1  4708.29 4700.13 4684.47 4713.74 4680.57 4698.58 4692.57 4708.92\n",
      "  4694.94 4711.34 4697.96 4699.63 4698.94 4717.54 4695.73 4721.58 4717.17\n",
      "  4726.88 4698.18 4727.05 4695.95 4731.55]\n",
      " [4649.04 4638.78 4583.5  4638.35 4639.19 4648.66 4623.03 4654.51 4587.15\n",
      "  4634.97 4635.22 4626.87 4656.63 4630.14 4702.34 4657.75 4625.57 4683.38\n",
      "  4813.83 4694.53 4664.46 4673.75 4703.61 4678.64 4751.11 4703.02 4683.09\n",
      "  4706.98 4716.31 4699.59 4705.45 4791.75]\n",
      " [4979.41 5073.   4964.23 4996.83 5083.45 4957.59 4946.41 4948.52 4955.53\n",
      "  4971.86 4963.04 4925.9  4953.53 4982.23 4948.92 4985.22 5013.57 5103.63\n",
      "  5035.21 5064.01 5063.06 5113.22 5146.66 5198.06 5190.12 5244.77 5252.04\n",
      "  5368.19 5477.78 5312.04 5405.34 5271.56]\n",
      " [4668.59 4650.92 4656.08 4667.55 4669.88 4642.58 4660.98 4652.95 4658.15\n",
      "  4646.87 4672.26 4642.23 4670.61 4658.79 4662.24 4646.19 4675.74 4657.26\n",
      "  4658.23 4662.17 4675.03 4652.63 4688.19 4668.11 4666.67 4669.03 4686.36\n",
      "  4649.95 4675.37 4674.38 4668.75 4671.64]\n",
      " [4085.56 4101.86 4088.27 4078.45 4084.69 4086.5  4078.1  4084.1  4081.64\n",
      "  4072.27 4064.96 4067.96 4059.69 4054.89 4058.25 4047.67 4049.13 4051.48\n",
      "  4044.77 4038.36 4048.69 4036.48 4042.33 4055.08 4062.19 4033.72 4057.47\n",
      "  4064.09 4036.38 4040.94 4049.85 4035.64]\n",
      " [4408.33 4419.75 4349.43 4369.33 4366.   4362.23 4349.48 4390.88 4378.32\n",
      "  4348.03 4317.97 4325.88 4343.18 4366.64 4357.59 4335.33 4371.75 4449.2\n",
      "  4484.23 4357.61 4308.57 4348.04 4340.34 4435.52 4332.99 4322.66 4398.13\n",
      "  4363.58 4325.06 4333.93 4332.34 4449.3 ]\n",
      " [4218.47 4230.68 4227.56 4220.4  4213.14 4219.94 4199.4  4203.06 4202.18\n",
      "  4188.3  4197.36 4207.21 4199.05 4202.33 4219.21 4218.82 4225.65 4279.5\n",
      "  4285.4  4321.56 4338.73 4369.06 4376.43 4418.43 4442.36 4437.55 4455.86\n",
      "  4481.92 4465.58 4510.98 4492.8  4492.97]\n",
      " [3477.09 3462.07 3473.6  3459.39 3475.61 3458.14 3480.05 3455.14 3471.68\n",
      "  3460.85 3475.45 3456.7  3475.87 3460.94 3476.57 3471.75 3478.74 3456.49\n",
      "  3477.79 3462.69 3474.31 3459.8  3480.29 3462.34 3478.59 3477.01 3477.04\n",
      "  3466.79 3484.35 3459.84 3488.84 3463.13]\n",
      " [4213.85 4213.75 4221.48 4217.9  4222.71 4229.88 4225.27 4217.77 4227.7\n",
      "  4223.63 4216.87 4235.79 4231.31 4224.17 4233.65 4252.11 4226.95 4235.33\n",
      "  4243.8  4236.93 4258.1  4245.56 4246.08 4245.18 4254.16 4260.4  4250.96\n",
      "  4258.47 4266.96 4266.72 4262.91 4265.16]\n",
      " [4634.13 4773.01 4662.66 4761.46 4708.18 4658.82 4659.81 4739.3  4655.47\n",
      "  4645.38 4643.58 4681.22 4609.73 4654.5  4608.94 4637.85 4643.58 4661.06\n",
      "  4622.11 4616.56 4633.43 4646.4  4600.9  4637.5  4598.9  4652.98 4607.91\n",
      "  4603.61 4612.46 4608.79 4604.01 4643.61]\n",
      " [4816.78 4794.12 4820.19 4806.26 4806.68 4795.44 4815.24 4792.31 4840.99\n",
      "  4853.97 4874.01 4799.4  4824.38 4797.49 4813.05 4810.4  4818.15 4830.17\n",
      "  4860.37 4804.13 4801.4  4799.   4814.39 4819.62 4831.27 4807.15 4793.93\n",
      "  4798.3  4810.08 4787.87 4815.37 4799.49]\n",
      " [4645.83 4687.47 4764.07 4666.86 4707.09 4674.98 4656.26 4735.2  4671.67\n",
      "  4666.63 4742.37 4663.68 4703.46 4692.99 4658.93 4667.57 4689.3  4703.75\n",
      "  4704.13 4689.61 4681.13 4695.1  4727.36 4753.31 4715.78 4753.54 4718.23\n",
      "  4722.99 4710.34 4757.73 4746.47 4738.15]\n",
      " [4315.5  4304.29 4310.06 4314.18 4333.96 4295.46 4361.34 4309.29 4320.92\n",
      "  4342.44 4329.01 4306.53 4325.76 4311.83 4312.93 4292.17 4335.96 4310.52\n",
      "  4303.39 4305.93 4314.32 4282.38 4304.78 4287.12 4300.17 4305.86 4322.5\n",
      "  4282.62 4308.37 4293.67 4318.07 4300.99]\n",
      " [3499.85 3489.02 3507.33 3490.   3497.99 3485.33 3513.14 3486.32 3495.87\n",
      "  3492.64 3502.24 3486.22 3510.23 3493.78 3507.85 3493.7  3504.43 3486.37\n",
      "  3504.5  3495.03 3505.86 3497.45 3502.38 3490.22 3505.05 3500.04 3501.36\n",
      "  3494.5  3507.69 3491.63 3503.21 3499.09]\n",
      " [4653.97 4632.72 4619.03 4620.4  4608.19 4601.52 4609.74 4600.52 4583.28\n",
      "  4585.28 4585.74 4569.36 4580.65 4575.27 4576.08 4580.86 4595.01 4572.34\n",
      "  4578.37 4585.97 4570.54 4577.41 4585.38 4581.55 4583.13 4592.04 4570.39\n",
      "  4571.05 4567.33 4548.03 4509.26 4499.65]\n",
      " [4683.58 4791.32 4713.44 4709.   4708.43 4734.52 4698.89 4716.08 4696.09\n",
      "  4706.01 4704.06 4707.7  4733.15 4784.96 4678.15 4739.09 4682.88 4694.83\n",
      "  4683.35 4687.45 4677.61 4694.9  4675.33 4667.4  4759.68 4677.44 4681.82\n",
      "  4700.85 4676.23 4697.05 4761.92 4692.28]\n",
      " [5283.41 5314.32 5265.98 5294.11 5310.21 5284.53 5285.84 5296.63 5260.66\n",
      "  5311.93 5311.25 5331.58 5267.8  5299.06 5275.37 5305.48 5320.36 5289.03\n",
      "  5270.61 5320.58 5263.86 5308.43 5374.94 5284.81 5291.21 5303.25 5311.74\n",
      "  5274.19 5277.51 5287.58 5268.41 5289.32]\n",
      " [4189.53 4227.73 4199.03 4209.76 4229.29 4216.19 4198.85 4242.62 4221.19\n",
      "  4263.3  4239.91 4307.56 4203.6  4216.57 4221.08 4204.88 4217.39 4218.29\n",
      "  4248.75 4258.53 4260.51 4237.46 4243.11 4239.48 4218.77 4236.94 4279.71\n",
      "  4254.08 4251.83 4304.75 4223.03 4252.56]\n",
      " [4846.98 4826.82 4789.96 4834.16 4834.95 4847.17 4817.88 4918.97 4787.35\n",
      "  4795.48 4813.92 4860.58 4816.9  4853.15 4798.28 4805.89 4832.06 4823.83\n",
      "  4840.88 4835.27 4810.62 4814.76 4816.11 4831.82 4829.58 4822.28 4826.38\n",
      "  4793.75 4823.52 4915.24 4890.7  4818.85]\n",
      " [4564.07 4517.82 4533.58 4483.67 4633.4  4494.59 4609.   4482.75 4547.11\n",
      "  4517.16 4529.32 4597.14 4542.36 4495.41 4568.72 4708.25 4595.74 4511.72\n",
      "  4511.57 4583.94 4573.44 4506.48 4517.15 4489.38 4610.77 4601.57 4554.72\n",
      "  4500.39 4508.32 4499.86 4500.99 4490.1 ]\n",
      " [3437.39 3413.56 3438.16 3434.38 3422.73 3432.95 3438.2  3419.85 3420.91\n",
      "  3480.28 3581.63 3416.36 3428.41 3591.27 3440.17 3401.12 3418.03 3486.55\n",
      "  3440.14 3427.2  3569.86 3460.59 3408.28 3430.28 3447.97 3417.9  3423.67\n",
      "  3408.56 3409.21 3487.39 3482.   3452.13]\n",
      " [4125.47 4177.23 4093.27 4154.9  4130.82 4139.66 4130.6  4159.63 4187.15\n",
      "  4100.83 4094.41 4118.65 4086.13 4136.52 4080.69 4162.52 4166.99 4150.36\n",
      "  4104.06 4135.31 4096.31 4150.07 4117.5  4145.48 4108.42 4119.2  4123.27\n",
      "  4132.4  4110.15 4147.49 4116.22 4155.76]\n",
      " [4251.91 4231.04 4211.25 4407.37 4241.13 4184.19 4249.27 4245.64 4224.17\n",
      "  4216.35 4234.6  4227.68 4238.92 4301.9  4225.94 4184.31 4278.94 4241.41\n",
      "  4242.68 4234.44 4222.92 4271.78 4242.83 4217.47 4232.77 4243.39 4304.8\n",
      "  4221.08 4259.76 4190.93 4308.49 4219.78]\n",
      " [3346.15 3351.03 3347.85 3348.77 3334.66 3366.99 3356.8  3348.53 3338.63\n",
      "  3352.49 3331.83 3356.48 3333.87 3335.02 3328.38 3340.72 3325.21 3335.11\n",
      "  3330.81 3331.33 3328.39 3346.46 3319.35 3327.08 3323.82 3323.17 3318.57\n",
      "  3344.35 3329.22 3325.86 3312.5  3318.87]\n",
      " [3508.57 3510.81 3528.09 3492.45 3522.03 3499.62 3517.81 3500.66 3513.55\n",
      "  3494.39 3522.55 3515.5  3532.48 3497.68 3515.25 3503.76 3528.23 3511.08\n",
      "  3517.99 3503.65 3519.74 3502.69 3512.33 3511.46 3524.04 3503.22 3545.26\n",
      "  3520.82 3545.56 3513.68 3526.76 3505.99]\n",
      " [4400.25 4420.85 4403.39 4430.74 4393.17 4407.14 4401.35 4417.32 4391.47\n",
      "  4426.22 4397.57 4412.08 4405.13 4430.58 4389.99 4422.89 4412.66 4415.1\n",
      "  4400.89 4420.76 4386.18 4419.38 4397.04 4409.02 4393.7  4423.54 4385.48\n",
      "  4407.63 4401.54 4415.98 4387.74 4415.7 ]\n",
      " [5046.86 5023.3  5046.02 5046.77 5021.21 5033.84 5047.41 5018.7  5022.66\n",
      "  5028.3  5018.98 5021.13 5032.08 5005.1  5018.88 5027.16 5024.23 5024.45\n",
      "  5035.82 5007.71 5017.2  5034.44 5028.33 5018.27 5044.12 5022.74 5021.98\n",
      "  5030.06 5035.97 5014.36 5057.21 5028.22]\n",
      " [4923.37 4922.29 4916.32 4922.6  4947.08 4909.91 4919.24 4895.82 4902.78\n",
      "  4887.02 4921.54 4876.23 4895.7  4912.18 4886.1  4885.08 4889.67 4877.6\n",
      "  4889.64 4881.45 4874.31 4853.01 4910.32 4885.55 4883.21 4892.46 4895.28\n",
      "  4866.08 4902.98 4877.91 4889.73 4876.94]\n",
      " [3408.15 3453.01 3465.69 3400.33 3461.34 3400.81 3394.17 3381.94 3583.43\n",
      "  3373.87 3415.38 3463.1  3413.18 3402.48 3453.72 3426.21 3398.59 3524.18\n",
      "  3407.08 3445.91 3400.43 3442.18 3396.58 3371.35 3480.82 3370.12 3438.25\n",
      "  3477.1  3412.48 3433.84 3453.75 3391.83]\n",
      " [3468.53 3464.92 3461.8  3463.59 3464.71 3460.98 3462.7  3459.22 3461.33\n",
      "  3457.6  3464.47 3452.67 3456.73 3459.09 3458.44 3459.06 3458.7  3456.68\n",
      "  3464.69 3457.36 3454.11 3448.54 3460.67 3454.25 3446.29 3455.71 3455.36\n",
      "  3455.23 3457.82 3459.04 3451.8  3459.77]\n",
      " [5122.52 5130.6  5168.34 5203.53 5141.61 5194.39 5113.81 5141.32 5129.31\n",
      "  5163.49 5151.04 5137.57 5132.03 5185.29 5123.21 5128.17 5140.09 5114.48\n",
      "  5112.96 5163.53 5114.59 5119.03 5156.12 5103.47 5091.66 5135.   5114.06\n",
      "  5122.24 5130.06 5099.37 5094.09 5112.96]\n",
      " [5173.27 5181.81 5178.9  5197.49 5161.4  5220.57 5160.09 5170.82 5170.34\n",
      "  5183.27 5193.44 5197.93 5188.66 5184.45 5195.93 5226.43 5203.72 5166.52\n",
      "  5181.67 5174.76 5181.58 5178.99 5199.4  5209.58 5191.43 5240.6  5233.7\n",
      "  5259.9  5245.28 5179.26 5200.94 5171.12]\n",
      " [4555.87 4560.8  4576.56 4565.73 4567.32 4585.57 4574.2  4577.78 4582.93\n",
      "  4569.53 4578.12 4578.6  4577.69 4569.57 4577.09 4582.99 4575.12 4577.22\n",
      "  4580.71 4583.99 4578.5  4583.75 4571.31 4567.83 4576.52 4577.36 4571.55\n",
      "  4576.4  4568.85 4567.23 4583.13 4578.76]\n",
      " [3466.9  3464.63 3494.25 3464.79 3486.17 3462.78 3473.27 3468.63 3486.31\n",
      "  3462.83 3475.49 3477.11 3476.89 3463.3  3485.61 3467.14 3477.5  3469.47\n",
      "  3476.22 3462.13 3483.27 3472.93 3476.84 3466.99 3482.41 3460.39 3478.69\n",
      "  3464.63 3479.14 3463.41 3483.64 3464.15]\n",
      " [3294.89 3270.02 3311.02 3266.8  3280.21 3258.06 3288.19 3259.08 3262.34\n",
      "  3241.47 3266.79 3229.19 3267.26 3226.96 3238.68 3225.58 3264.61 3208.4\n",
      "  3225.93 3204.63 3227.44 3211.42 3219.15 3192.93 3197.86 3191.55 3251.63\n",
      "  3185.29 3208.22 3209.24 3214.28 3211.9 ]\n",
      " [3351.29 3366.15 3340.66 3363.33 3360.8  3356.23 3344.96 3354.76 3336.81\n",
      "  3347.52 3345.27 3344.28 3339.98 3346.   3333.24 3337.65 3333.55 3333.6\n",
      "  3326.66 3336.05 3328.36 3328.43 3327.83 3326.13 3316.01 3327.84 3323.31\n",
      "  3317.61 3323.47 3328.72 3323.36 3328.23]\n",
      " [4632.12 4703.21 4632.14 4684.41 4628.6  4676.17 4638.65 4645.41 4665.11\n",
      "  4667.35 4622.55 4711.59 4670.46 4689.11 4640.44 4648.46 4622.61 4670.37\n",
      "  4714.15 4651.16 4616.98 4653.2  4639.63 4676.98 4629.35 4657.68 4599.53\n",
      "  4644.6  4694.39 4643.91 4605.96 4676.58]\n",
      " [4786.31 4767.72 4763.16 4762.86 4761.15 4748.43 4750.36 4768.34 4770.88\n",
      "  4765.94 4792.72 4781.71 4737.92 4875.34 4814.33 4905.74 4763.9  4809.36\n",
      "  4820.08 4787.86 4794.38 4748.09 4823.67 4815.54 4776.42 4767.16 4818.87\n",
      "  4766.54 4810.89 4795.43 4828.59 4866.5 ]\n",
      " [5050.4  5005.18 5033.18 5022.68 5026.96 5006.35 5040.21 5009.31 5025.03\n",
      "  5011.06 5021.64 4991.56 5038.17 5007.59 5017.93 5018.49 5027.26 4991.55\n",
      "  5035.6  5020.5  5021.97 5017.56 5040.46 5009.15 5040.56 5026.69 5029.81\n",
      "  5015.32 5060.9  5017.79 5045.27 5035.81]\n",
      " [4732.01 4717.68 4735.41 4707.36 4750.56 4719.63 4733.43 4711.23 4738.19\n",
      "  4698.61 4735.55 4715.14 4717.75 4702.31 4732.13 4710.05 4718.04 4709.3\n",
      "  4714.22 4692.4  4730.15 4698.49 4723.53 4708.07 4725.98 4694.88 4735.59\n",
      "  4708.26 4709.55 4706.   4720.55 4692.09]\n",
      " [3270.23 3289.52 3281.25 3283.87 3296.79 3301.49 3361.59 3392.76 3263.5\n",
      "  3349.75 3312.26 3264.16 3264.96 3306.11 3277.27 3315.94 3323.1  3361.19\n",
      "  3324.02 3417.27 3307.64 3318.31 3429.88 3282.12 3440.71 3305.88 3294.56\n",
      "  3350.77 3314.59 3343.39 3337.31 3304.21]\n",
      " [3484.86 3406.23 3428.8  3472.55 3495.38 3489.48 3449.38 3453.44 3399.1\n",
      "  3417.2  3481.53 3459.91 3409.75 3433.55 3447.27 3416.79 3430.44 3417.89\n",
      "  3424.64 3501.89 3417.59 3435.17 3439.53 3488.59 3433.33 3440.48 3440.06\n",
      "  3397.92 3414.4  3592.73 3524.42 3494.1 ]\n",
      " [3008.16 2998.17 3004.64 3002.11 3005.54 2996.59 3015.58 3003.86 3008.49\n",
      "  3010.46 3012.52 3001.04 3008.97 3009.43 3007.81 3006.16 3011.75 3006.74\n",
      "  3017.66 3010.65 3013.58 3006.34 3018.23 2998.33 3009.89 3003.96 3004.15\n",
      "  3001.06 3014.5  3004.55 3011.89 3009.05]\n",
      " [4088.46 4095.93 4109.79 4102.89 4107.8  4103.05 4107.21 4114.94 4112.58\n",
      "  4123.64 4120.66 4132.62 4117.51 4128.42 4121.35 4135.88 4132.05 4132.42\n",
      "  4118.62 4147.96 4133.78 4127.52 4132.43 4143.06 4133.93 4144.35 4148.87\n",
      "  4147.6  4155.53 4167.83 4166.23 4185.89]\n",
      " [4104.95 4080.01 4144.83 4152.52 4127.33 4166.84 4166.83 4126.64 4137.49\n",
      "  4116.31 4145.48 4138.21 4199.55 4176.68 4124.54 4163.61 4154.92 4188.49\n",
      "  4120.8  4143.37 4148.37 4097.86 4177.25 4232.12 4188.13 4156.27 4169.35\n",
      "  4183.5  4223.1  4275.63 4281.2  4292.11]\n",
      " [3588.25 3588.35 3631.72 3574.38 3579.48 3581.45 3657.65 3607.31 3614.17\n",
      "  3674.89 3678.23 3611.64 3548.51 3569.21 3553.86 3560.31 3628.71 3552.27\n",
      "  3617.16 3565.6  3594.53 3553.38 3553.84 3580.48 3606.77 3586.61 3559.64\n",
      "  3606.7  3732.4  3570.96 3539.58 3576.13]\n",
      " [4312.1  4299.56 4318.85 4288.35 4300.14 4300.54 4312.96 4280.5  4312.95\n",
      "  4291.43 4300.37 4291.53 4313.58 4284.53 4320.24 4317.24 4310.96 4291.9\n",
      "  4311.74 4284.05 4315.07 4312.03 4311.13 4293.3  4318.35 4294.34 4306.1\n",
      "  4311.88 4313.99 4296.17 4333.91 4309.29]\n",
      " [3463.19 3486.98 3465.69 3484.44 3465.56 3491.57 3463.02 3487.18 3462.18\n",
      "  3483.06 3461.75 3488.   3464.21 3478.92 3462.16 3485.16 3473.69 3482.51\n",
      "  3458.99 3484.25 3459.27 3482.45 3457.54 3480.46 3451.7  3480.46 3474.79\n",
      "  3474.95 3451.98 3483.14 3461.34 3482.96]\n",
      " [4391.18 4355.98 4360.23 4409.22 4502.06 4403.92 4364.36 4365.67 4388.26\n",
      "  4384.68 4353.19 4374.29 4383.88 4428.54 4394.35 4387.23 4394.08 4338.54\n",
      "  4367.56 4363.44 4358.25 4353.49 4376.58 4347.08 4346.22 4360.15 4415.4\n",
      "  4344.68 4393.95 4359.56 4370.88 4364.71]\n",
      " [4378.38 4388.55 4265.17 4169.21 4207.29 4175.49 4271.46 4252.13 4299.34\n",
      "  4308.71 4302.82 4304.25 4416.65 4289.75 4370.67 4384.37 4462.96 4362.92\n",
      "  4377.75 4355.43 4428.33 4335.81 4348.99 4394.37 4324.61 4300.07 4310.41\n",
      "  4312.8  4424.54 4308.11 4340.11 4313.4 ]\n",
      " [4399.56 4383.35 4344.62 4344.94 4366.86 4395.58 4402.57 4393.96 4377.64\n",
      "  4340.96 4374.3  4352.16 4499.03 4314.68 4366.29 4334.04 4368.77 4353.26\n",
      "  4348.49 4344.75 4385.26 4501.24 4354.29 4341.97 4345.5  4334.7  4408.75\n",
      "  4421.93 4352.5  4348.87 4382.73 4344.42]\n",
      " [4728.38 4724.57 4775.85 4716.22 4740.22 4750.39 4742.38 4758.11 4816.93\n",
      "  4710.82 4824.32 4726.15 4762.74 4789.43 4783.93 4734.47 4780.79 4748.25\n",
      "  4744.3  4737.69 4759.64 4742.68 4769.55 4738.42 4906.72 4723.54 4821.33\n",
      "  4763.64 4768.74 4735.06 4781.73 4717.38]\n",
      " [3607.7  3585.7  3598.79 3581.7  3603.94 3581.92 3596.32 3597.02 3599.39\n",
      "  3580.11 3602.65 3585.04 3593.27 3605.26 3594.55 3590.01 3623.31 3582.58\n",
      "  3597.61 3574.29 3616.1  3578.85 3590.34 3582.8  3617.51 3584.3  3598.35\n",
      "  3584.99 3592.88 3589.29 3590.54 3577.09]\n",
      " [4703.28 4684.56 4717.95 4664.46 4663.72 4714.44 4685.54 4680.33 4631.7\n",
      "  4703.15 4664.96 4805.79 4675.7  4699.91 4655.44 4639.2  4680.25 4651.61\n",
      "  4641.15 4708.95 4645.61 4665.94 4654.32 4714.42 4684.59 4683.05 4707.86\n",
      "  4793.09 4709.64 4695.54 4644.92 4663.26]\n",
      " [4147.47 4186.18 4290.81 4221.17 4196.5  4242.94 4175.   4153.82 4181.91\n",
      "  4202.47 4153.63 4170.47 4160.83 4194.75 4179.08 4174.49 4166.4  4170.33\n",
      "  4217.47 4168.94 4160.62 4183.53 4195.62 4175.25 4242.43 4179.14 4243.18\n",
      "  4203.64 4223.27 4235.25 4176.65 4179.78]\n",
      " [3846.49 3855.18 3886.02 3886.62 3947.02 3917.18 3911.85 3906.3  3912.79\n",
      "  3970.94 3956.48 3934.23 3938.61 3938.89 3957.56 4021.3  4058.68 3970.92\n",
      "  4001.3  4021.12 4039.99 4034.51 4035.18 4016.23 4038.15 4102.56 4059.05\n",
      "  4058.13 4219.7  4159.88 4273.65 4186.51]\n",
      " [4832.23 4800.8  4827.27 4829.17 4822.04 4812.19 4826.68 4803.48 4819.11\n",
      "  4822.13 4810.71 4802.57 4830.77 4807.53 4813.66 4814.26 4811.47 4803.52\n",
      "  4824.42 4807.27 4808.57 4809.87 4816.73 4797.75 4823.32 4810.33 4804.95\n",
      "  4807.06 4819.71 4793.7  4814.04 4813.13]\n",
      " [2980.89 2970.08 2977.76 2968.04 2986.94 2972.68 2979.57 2967.29 2996.01\n",
      "  2975.72 2987.46 2966.46 2986.49 2966.03 2997.33 2967.54 2979.12 2975.03\n",
      "  3040.17 2966.31 2984.2  2971.84 2982.48 2965.5  2985.12 2966.63 2980.36\n",
      "  2971.84 2976.25 2963.32 2984.45 2960.58]\n",
      " [4450.96 4434.52 4438.71 4447.3  4429.56 4430.22 4437.   4430.71 4423.52\n",
      "  4432.13 4421.89 4418.77 4425.6  4417.05 4404.58 4416.53 4419.63 4401.55\n",
      "  4406.88 4399.82 4403.89 4399.44 4395.98 4387.11 4390.26 4392.79 4384.13\n",
      "  4380.91 4381.13 4370.82 4386.89 4380.98]\n",
      " [4187.63 4247.11 4176.51 4176.12 4197.25 4229.85 4212.74 4151.81 4219.08\n",
      "  4176.35 4169.55 4214.41 4161.77 4159.55 4175.68 4199.34 4207.73 4226.1\n",
      "  4213.46 4276.85 4406.07 4177.09 4295.34 4294.36 4204.34 4205.41 4258.12\n",
      "  4193.98 4210.47 4254.17 4221.89 4221.63]\n",
      " [3476.94 3475.6  3466.62 3488.05 3466.63 3485.08 3465.27 3479.54 3463.03\n",
      "  3487.81 3465.91 3485.38 3465.13 3488.97 3461.39 3486.08 3475.61 3485.68\n",
      "  3468.01 3485.91 3465.82 3487.21 3465.59 3485.13 3475.41 3481.63 3468.08\n",
      "  3498.46 3499.24 3489.44 3486.29 3491.39]\n",
      " [4382.17 4388.4  4370.05 4405.84 4383.26 4387.74 4379.57 4402.95 4378.21\n",
      "  4418.42 4380.66 4395.06 4406.02 4398.81 4370.59 4390.85 4379.49 4382.98\n",
      "  4366.97 4393.04 4368.84 4383.35 4374.12 4385.52 4365.97 4429.83 4375.73\n",
      "  4378.87 4377.05 4397.68 4380.76 4412.16]\n",
      " [3682.72 3689.29 3769.13 3716.27 3777.45 3776.38 3714.93 3663.46 3674.32\n",
      "  3692.75 3701.82 3668.19 3676.94 3730.51 3747.54 3704.53 3697.89 3832.09\n",
      "  3710.3  3654.18 3684.91 3655.09 3710.37 3692.57 3684.86 3678.48 3713.91\n",
      "  3712.93 3680.84 3699.98 3683.45 3688.84]\n",
      " [3555.99 3644.19 3565.53 3570.74 3602.29 3622.49 3667.03 3662.66 3697.84\n",
      "  3597.78 3571.46 3614.29 3632.12 3565.61 3633.44 3606.19 3597.83 3637.\n",
      "  3564.09 3598.16 3563.3  3566.63 3553.67 3596.57 3593.19 3608.72 3645.85\n",
      "  3595.19 3581.89 3577.85 3565.49 3579.95]\n",
      " [4392.45 4384.06 4376.02 4382.81 4387.26 4368.22 4389.12 4385.25 4378.78\n",
      "  4377.7  4391.3  4376.89 4387.1  4390.45 4376.25 4371.71 4391.   4374.43\n",
      "  4382.75 4387.97 4382.91 4371.47 4394.82 4378.16 4380.64 4381.87 4391.56\n",
      "  4366.62 4395.56 4381.02 4373.55 4376.65]\n",
      " [5280.09 5356.54 5252.66 5319.28 5267.39 5250.06 5396.2  5362.71 5281.91\n",
      "  5257.94 5259.99 5302.93 5307.86 5287.28 5273.16 5239.71 5283.94 5324.06\n",
      "  5324.06 5268.69 5255.6  5245.31 5258.03 5275.96 5247.02 5270.74 5256.46\n",
      "  5273.19 5268.2  5328.26 5320.12 5264.48]\n",
      " [5098.96 5099.14 5119.71 5103.36 5097.7  5121.16 5109.52 5103.93 5109.07\n",
      "  5107.36 5105.76 5108.09 5103.81 5089.84 5092.31 5111.25 5083.23 5098.52\n",
      "  5098.74 5090.47 5086.72 5101.11 5078.41 5089.69 5101.17 5091.93 5078.83\n",
      "  5108.19 5089.92 5093.69 5100.24 5104.94]\n",
      " [5346.6  5326.18 5389.32 5350.32 5405.92 5331.67 5353.16 5323.08 5358.99\n",
      "  5331.76 5338.   5344.13 5348.7  5313.8  5390.44 5320.23 5373.24 5392.13\n",
      "  5433.01 5364.5  5364.89 5361.17 5385.62 5348.91 5364.01 5354.8  5351.44\n",
      "  5349.55 5336.57 5325.41 5366.17 5342.61]\n",
      " [4438.91 4433.82 4455.96 4404.09 4392.06 4482.16 4387.37 4419.64 4400.68\n",
      "  4419.47 4412.65 4419.27 4435.81 4437.23 4410.17 4473.81 4403.99 4440.18\n",
      "  4424.73 4427.85 4393.81 4421.57 4417.79 4435.6  4442.29 4656.14 4426.84\n",
      "  4440.3  4461.05 4427.45 4390.26 4447.85]\n",
      " [3440.58 3431.76 3458.62 3431.59 3438.85 3425.08 3446.99 3432.09 3448.64\n",
      "  3423.19 3444.76 3425.02 3439.73 3422.66 3433.91 3419.07 3422.11 3416.07\n",
      "  3431.84 3404.4  3428.6  3414.08 3422.62 3407.65 3427.67 3404.64 3427.02\n",
      "  3429.79 3420.17 3398.99 3417.79 3389.15]\n",
      " [2994.53 3006.32 2999.89 3007.1  2990.01 3011.39 3007.6  2987.03 2980.05\n",
      "  2998.94 2978.01 2986.48 2970.43 2972.66 2959.82 2968.94 2966.67 2954.73\n",
      "  2952.11 2947.72 2947.08 2944.41 2935.4  2934.03 2930.75 2941.85 2926.95\n",
      "  2933.3  2929.55 2926.21 2924.4  2928.85]\n",
      " [4632.58 4641.37 4626.28 4607.96 4597.06 4631.39 4618.39 4624.63 4620.\n",
      "  4617.74 4583.8  4611.49 4594.74 4592.02 4672.97 4631.01 4574.02 4612.71\n",
      "  4700.97 4595.74 4580.79 4606.04 4614.41 4630.97 4618.02 4627.48 4620.1\n",
      "  4634.92 4784.23 4581.31 4616.58 4610.25]\n",
      " [3539.81 3450.45 3455.73 3582.68 3498.28 3478.31 3445.69 3431.37 3483.01\n",
      "  3499.93 3543.02 3406.46 3445.3  3436.89 3450.51 3408.07 3470.83 3445.46\n",
      "  3457.74 3408.2  3485.4  3429.83 3471.48 3474.25 3575.54 3438.1  3453.1\n",
      "  3518.81 3444.31 3513.46 3479.95 3434.05]\n",
      " [4635.88 4646.13 4642.41 4645.27 4645.7  4623.82 4637.68 4638.41 4668.17\n",
      "  4639.12 4654.17 4726.17 4673.72 4633.29 4712.68 4609.35 4658.95 4645.98\n",
      "  4627.73 4618.22 4638.2  4676.99 4655.03 4638.7  4622.08 4614.58 4667.87\n",
      "  4613.55 4618.43 4638.07 4705.35 4612.99]\n",
      " [4526.98 4572.83 4556.87 4568.31 4517.47 4514.65 4636.86 4516.05 4532.33\n",
      "  4492.38 4545.92 4535.68 4593.15 4561.63 4528.79 4524.3  4568.72 4548.4\n",
      "  4654.5  4524.4  4592.45 4518.67 4626.22 4506.7  4514.5  4483.41 4515.22\n",
      "  4542.93 4510.75 4474.47 4516.51 4482.53]\n",
      " [2992.58 3013.42 2994.9  3009.64 3000.23 3013.64 2995.14 3007.82 2999.76\n",
      "  3004.58 2995.6  3011.93 2990.71 3010.88 2998.51 3004.49 2996.66 3010.\n",
      "  2995.44 3009.07 2997.45 3008.07 2999.29 3012.   2999.85 3007.73 3006.37\n",
      "  3013.1  3005.11 3015.1  3004.55 3009.75]\n",
      " [3005.47 2996.94 3007.24 3003.06 3014.87 2998.61 3008.03 3003.42 3017.9\n",
      "  2996.03 3018.42 3004.15 3009.31 3005.53 3015.37 3016.06 3014.5  2999.92\n",
      "  3008.64 3003.48 3018.34 3013.39 3024.29 3006.75 3010.28 3008.41 3018.22\n",
      "  3007.93 3029.51 3004.63 3020.21 3004.53]\n",
      " [4068.41 3989.   3997.43 4023.25 4007.49 4043.99 4114.35 4011.61 4065.11\n",
      "  4019.05 4100.27 4037.75 4082.67 4034.76 4134.32 4087.05 4015.17 4138.01\n",
      "  4050.38 4042.37 4024.81 4069.48 4049.26 4092.   4118.85 4121.62 4024.68\n",
      "  4023.44 4030.06 4014.05 4046.66 4012.48]\n",
      " [4103.37 4113.94 4111.85 4108.31 4108.06 4120.85 4124.18 4118.31 4126.3\n",
      "  4127.11 4121.12 4132.72 4139.43 4131.55 4141.13 4143.63 4138.39 4142.77\n",
      "  4149.53 4138.66 4143.16 4148.66 4148.38 4146.02 4145.85 4144.15 4138.46\n",
      "  4149.42 4158.46 4152.65 4153.96 4150.99]\n",
      " [2993.25 2987.62 2994.53 2979.29 3003.74 2985.24 2999.87 2994.79 3008.88\n",
      "  2987.97 3008.12 2998.18 3010.95 3005.99 3029.54 3001.94 3022.13 3011.19\n",
      "  3024.95 3012.19 3033.98 3019.88 3033.83 3024.03 3029.56 3017.82 3044.7\n",
      "  3022.61 3039.07 3028.08 3038.43 3037.53]\n",
      " [4057.48 4033.41 4086.84 4022.15 4068.18 4047.75 4053.2  4064.78 4101.1\n",
      "  4027.59 4087.65 4149.32 4044.79 4010.2  4064.9  4022.67 4154.01 4074.05\n",
      "  4039.46 4052.25 4108.22 4025.34 4047.41 4024.15 4072.27 4018.08 4046.76\n",
      "  4052.28 4051.41 4027.77 4058.04 4022.95]\n",
      " [4457.32 4462.45 4468.01 4444.07 4502.33 4393.24 4397.65 4429.56 4404.88\n",
      "  4485.54 4426.2  4415.07 4434.01 4436.19 4428.73 4481.06 4470.32 4422.59\n",
      "  4463.4  4628.81 4429.05 4430.02 4426.35 4435.44 4439.75 4415.63 4464.82\n",
      "  4416.61 4434.09 4425.08 4438.46 4431.01]\n",
      " [4423.56 4414.35 4418.72 4417.01 4424.79 4405.56 4439.62 4418.76 4426.56\n",
      "  4424.17 4462.67 4420.4  4442.08 4441.13 4459.41 4449.16 4496.62 4498.52\n",
      "  4499.61 4527.18 4536.36 4521.58 4546.   4530.28 4534.93 4522.19 4532.45\n",
      "  4496.85 4512.41 4497.03 4489.7  4478.5 ]\n",
      " [3469.52 3453.01 3469.64 3446.51 3474.42 3447.09 3466.46 3453.07 3470.15\n",
      "  3446.4  3469.11 3450.85 3460.22 3451.55 3465.19 3447.55 3462.68 3441.86\n",
      "  3470.75 3444.42 3463.8  3439.65 3459.15 3452.01 3463.64 3440.01 3461.98\n",
      "  3443.86 3466.2  3449.84 3457.76 3438.23]\n",
      " [4645.49 4669.84 4665.98 4662.03 4656.98 4677.14 4655.01 4669.99 4662.48\n",
      "  4669.64 4657.28 4681.38 4658.83 4665.7  4668.37 4672.04 4653.11 4680.33\n",
      "  4658.58 4665.7  4667.08 4674.39 4649.61 4665.29 4653.26 4657.92 4655.12\n",
      "  4666.06 4645.68 4673.79 4655.24 4651.42]\n",
      " [3378.92 3386.75 3363.34 3392.07 3366.31 3403.18 3335.18 3323.   3439.18\n",
      "  3365.85 3377.97 3351.04 3444.73 3346.96 3409.03 3334.36 3389.42 3332.43\n",
      "  3381.35 3386.47 3340.22 3347.69 3433.91 3475.85 3443.88 3534.21 3354.55\n",
      "  3395.2  3316.84 3339.93 3336.2  3319.27]\n",
      " [3443.79 3451.37 3447.77 3450.09 3444.86 3448.38 3445.81 3447.63 3452.22\n",
      "  3451.7  3444.05 3450.51 3452.24 3446.73 3452.92 3451.87 3445.87 3456.67\n",
      "  3453.81 3445.92 3458.57 3450.51 3450.28 3451.23 3454.2  3453.73 3465.44\n",
      "  3455.08 3454.22 3452.03 3454.55 3448.05]\n",
      " [4067.84 4122.82 4041.88 4094.44 4155.1  4019.63 3997.34 4059.51 3999.89\n",
      "  4012.57 4029.37 4037.02 4012.82 4088.09 4011.07 4061.39 4030.63 4151.91\n",
      "  3996.92 4263.81 4057.98 4067.14 4088.93 4008.94 4017.25 4039.   4009.49\n",
      "  4069.26 4030.3  4013.87 4078.83 4040.15]\n",
      " [4239.81 4297.5  4287.48 4310.69 4263.36 4306.22 4252.59 4262.9  4243.17\n",
      "  4376.7  4245.06 4331.46 4255.82 4290.95 4269.71 4263.83 4276.07 4322.13\n",
      "  4254.23 4299.95 4247.45 4265.65 4282.49 4324.95 4320.28 4331.42 4279.12\n",
      "  4313.18 4307.86 4355.88 4331.43 4304.13]\n",
      " [2937.02 2927.73 2942.15 2934.02 2924.33 2924.38 2933.17 2923.76 2915.83\n",
      "  2928.03 2914.59 2934.3  2911.05 2920.82 2933.48 2941.38 2900.91 2891.63\n",
      "  2909.49 2901.91 2895.31 2880.15 2876.5  2862.36 2853.79 2864.39 2858.31\n",
      "  2858.26 2861.56 2853.31 2847.33 2864.46]\n",
      " [3037.84 3043.26 3037.   3047.65 3026.86 3041.31 3026.82 3032.32 3025.76\n",
      "  3028.14 3008.52 3021.4  3007.53 3011.22 2999.49 3020.11 3011.59 3009.5\n",
      "  2998.59 3015.21 3005.11 3018.12 2994.09 3006.03 2997.87 3007.32 2999.77\n",
      "  3021.99 2991.18 3003.06 2994.11 3020.87]\n",
      " [4135.54 4137.29 4117.18 4126.89 4113.39 4109.   4113.89 4103.52 4106.25\n",
      "  4108.74 4096.59 4096.58 4096.86 4088.52 4083.95 4086.03 4081.03 4079.15\n",
      "  4082.14 4080.82 4073.79 4075.7  4079.47 4077.1  4078.72 4089.26 4079.83\n",
      "  4084.29 4104.23 4085.77 4084.41 4108.82]\n",
      " [4355.59 4405.65 4350.69 4328.93 4404.66 4330.76 4333.92 4346.01 4340.08\n",
      "  4351.64 4336.27 4381.85 4347.64 4334.12 4338.68 4346.49 4402.36 4332.08\n",
      "  4334.59 4430.6  4351.   4361.93 4373.78 4359.14 4367.1  4354.05 4355.45\n",
      "  4360.29 4351.65 4406.23 4411.36 4486.39]\n",
      " [5246.8  5245.06 5242.41 5214.57 5229.33 5238.57 5265.44 5210.96 5248.49\n",
      "  5257.44 5237.75 5240.11 5261.55 5226.73 5245.65 5222.92 5342.47 5233.09\n",
      "  5266.08 5230.67 5271.57 5247.52 5348.5  5232.46 5275.82 5252.42 5254.67\n",
      "  5264.61 5270.34 5238.94 5276.09 5226.21]\n",
      " [3474.27 3453.32 3458.36 3442.5  3473.44 3471.79 3464.66 3443.36 3462.02\n",
      "  3441.18 3466.   3449.25 3475.42 3444.09 3463.13 3446.53 3460.46 3448.77\n",
      "  3456.56 3442.83 3470.17 3447.85 3461.62 3436.76 3462.18 3445.15 3471.75\n",
      "  3447.77 3456.95 3447.18 3470.32 3450.49]\n",
      " [4255.64 4233.25 4266.11 4238.01 4248.65 4237.89 4261.2  4230.35 4264.88\n",
      "  4239.96 4254.29 4243.4  4264.98 4237.15 4260.19 4248.42 4254.61 4241.52\n",
      "  4268.97 4245.62 4259.54 4262.21 4254.98 4237.58 4268.26 4241.9  4264.88\n",
      "  4252.57 4286.48 4237.31 4265.54 4244.24]\n",
      " [3935.69 3946.84 3942.98 3942.12 3937.69 3947.89 3943.42 3940.45 3963.1\n",
      "  3947.4  3941.44 3943.05 3945.3  3938.83 3943.12 3943.01 3941.28 3945.34\n",
      "  3945.68 3938.97 3943.46 3940.85 3929.98 3931.79 3926.65 3915.12 3914.42\n",
      "  3922.26 3904.49 3901.79 3905.27 3898.51]\n",
      " [4350.87 4316.95 4340.91 4323.97 4334.1  4316.67 4343.24 4310.2  4333.7\n",
      "  4318.03 4341.52 4317.14 4352.6  4334.94 4340.9  4336.7  4351.07 4326.4\n",
      "  4351.91 4340.63 4345.21 4324.89 4351.02 4319.   4342.56 4327.99 4342.08\n",
      "  4319.52 4351.04 4328.26 4342.8  4333.27]\n",
      " [4715.47 4710.39 4783.47 4678.53 4740.05 4682.99 4694.93 4684.33 4703.54\n",
      "  4697.6  4757.81 4725.24 4727.71 4732.68 4777.74 4702.28 4734.14 4743.07\n",
      "  4701.58 4906.65 4752.98 4696.26 4702.76 4858.44 4719.75 4806.54 4731.8\n",
      "  4709.89 4702.83 4762.86 4695.25 4685.61]\n",
      " [4702.81 4715.39 4728.16 4722.94 4700.99 4738.59 4714.53 4727.38 4705.\n",
      "  4713.51 4688.58 4735.91 4721.15 4745.27 4702.2  4705.97 4690.1  4704.29\n",
      "  4735.59 4721.15 4724.13 4709.3  4764.61 4749.16 4827.97 4731.04 4743.11\n",
      "  4700.48 4717.63 4677.75 4695.26 4698.18]\n",
      " [4024.57 4066.29 4033.06 4057.85 4057.57 4058.2  4138.84 4063.62 4150.75\n",
      "  4151.92 4030.35 4168.43 4085.13 4110.49 4035.19 4081.86 4132.89 4130.23\n",
      "  4205.75 4093.39 4109.92 4077.36 4091.62 4106.69 4062.97 4092.09 4050.93\n",
      "  4130.62 4090.34 4069.53 4034.66 4072.18]\n",
      " [3497.27 3468.1  3501.81 3455.54 3481.08 3458.26 3481.16 3467.57 3480.\n",
      "  3462.16 3487.84 3463.41 3489.8  3462.82 3481.98 3468.31 3488.04 3459.56\n",
      "  3483.83 3463.35 3480.14 3460.05 3488.75 3459.58 3480.82 3466.81 3485.11\n",
      "  3466.29 3485.09 3462.28 3485.37 3466.38]\n",
      " [4739.76 4727.22 4837.41 4707.91 4730.66 4706.51 4737.74 4706.7  4732.\n",
      "  4709.39 4731.95 4746.04 4738.33 4712.16 4745.28 4714.59 4731.48 4690.34\n",
      "  4855.96 4719.67 4738.75 4708.8  4735.19 4749.11 4743.64 4730.19 4740.08\n",
      "  4722.21 4776.37 4798.88 4768.3  4835.65]\n",
      " [4288.57 4361.2  4339.95 4275.23 4394.47 4317.1  4288.42 4219.66 4295.34\n",
      "  4244.81 4243.51 4241.58 4231.9  4226.42 4289.1  4218.48 4282.96 4133.71\n",
      "  4091.49 4064.93 4045.78 4108.48 4044.   4013.12 4065.23 4000.13 4020.9\n",
      "  4028.99 4057.83 4040.24 4095.32 4110.1 ]\n",
      " [3253.41 3259.43 3272.4  3252.21 3250.37 3242.24 3276.14 3262.91 3353.52\n",
      "  3269.21 3291.28 3400.12 3265.64 3263.08 3251.35 3306.54 3291.03 3285.47\n",
      "  3277.99 3260.8  3263.13 3260.55 3315.25 3267.96 3337.84 3288.93 3292.22\n",
      "  3297.82 3263.64 3272.44 3302.47 3331.04]\n",
      " [4969.91 4974.92 4983.22 4980.04 4969.9  4991.62 4979.32 4978.66 4993.55\n",
      "  4987.49 4983.51 4990.34 4994.62 4991.44 4994.85 5009.29 4999.44 4994.99\n",
      "  4994.83 5001.56 5040.26 5002.79 4986.55 5007.08 4989.39 4987.5  4989.21\n",
      "  4992.82 4977.   4982.71 4986.31 4974.44]\n",
      " [4417.18 4307.35 4313.12 4293.02 4313.93 4339.4  4310.77 4284.03 4315.88\n",
      "  4299.66 4306.6  4340.25 4274.22 4280.71 4349.03 4349.64 4380.3  4319.41\n",
      "  4367.9  4293.   4318.02 4336.79 4328.1  4338.21 4420.78 4356.57 4359.07\n",
      "  4324.07 4348.81 4331.15 4368.21 4308.2 ]\n",
      " [4221.82 4251.1  4233.09 4235.09 4209.84 4242.1  4227.26 4242.49 4232.84\n",
      "  4247.3  4211.38 4242.88 4226.81 4244.5  4223.1  4243.77 4217.36 4233.92\n",
      "  4227.83 4236.01 4227.43 4248.31 4217.28 4236.51 4234.27 4243.57 4223.12\n",
      "  4250.59 4228.65 4236.57 4234.49 4246.83]\n",
      " [4790.78 4785.53 4771.79 4774.76 4787.54 4764.67 4783.8  4784.02 4772.72\n",
      "  4769.85 4776.11 4767.58 4768.77 4787.39 4764.01 4768.55 4782.71 4769.49\n",
      "  4772.53 4785.58 4770.31 4766.43 4791.67 4788.24 4762.33 4781.98 4775.8\n",
      "  4772.57 4795.49 4799.51 4783.25 4808.99]\n",
      " [5090.67 5046.46 5075.17 5132.94 5066.22 5053.84 5090.81 5049.91 5108.74\n",
      "  5058.35 5083.54 5063.51 5080.88 5048.02 5085.66 5099.18 5134.7  5073.21\n",
      "  5122.77 5094.58 5073.52 5083.03 5089.68 5058.05 5133.44 5079.41 5074.95\n",
      "  5088.63 5142.58 5068.14 5124.44 5082.93]\n",
      " [4133.54 3986.82 4026.59 4010.07 4257.04 4384.9  4581.4  4690.82 4772.2\n",
      "  4759.65 4717.81 4669.97 4525.21 4446.72 4136.76 3977.76 3832.07 3723.07\n",
      "  3713.48 3651.57 3636.2  3664.3  3777.5  3873.74 3963.23 4025.58 4136.13\n",
      "  4352.33 4418.78 4437.36 4406.52 4477.44]\n",
      " [3433.79 3437.31 3454.54 3423.67 3430.5  3404.23 3365.94 3486.69 3382.31\n",
      "  3388.74 3401.94 3352.76 3432.52 3399.54 3521.04 3526.16 3455.66 3445.37\n",
      "  3513.06 3460.9  3576.42 3472.71 3407.11 3381.83 3402.35 3385.42 3616.72\n",
      "  3391.12 3458.63 3372.74 3354.4  3390.55]\n",
      " [4176.6  4189.83 4185.88 4174.35 4191.41 4190.17 4175.48 4186.32 4186.54\n",
      "  4186.22 4178.76 4188.85 4185.39 4186.12 4186.73 4177.2  4187.29 4183.56\n",
      "  4176.38 4178.22 4181.28 4170.27 4175.63 4181.6  4165.82 4160.34 4170.19\n",
      "  4163.15 4153.94 4161.76 4160.1  4145.65]\n",
      " [5154.84 5166.92 5141.48 5175.19 5127.18 5237.52 5216.88 5173.35 5139.47\n",
      "  5172.06 5140.6  5190.31 5155.53 5169.91 5141.52 5236.87 5161.87 5221.95\n",
      "  5264.53 5183.73 5174.89 5209.52 5178.   5214.85 5201.13 5382.2  5174.46\n",
      "  5248.78 5159.7  5173.32 5144.82 5192.24]\n",
      " [4846.8  4864.78 4847.22 4813.84 4825.29 4817.   4814.18 4862.05 4819.14\n",
      "  4843.95 4864.08 4894.71 4821.35 4872.69 4853.55 4922.29 4841.72 4841.02\n",
      "  4845.98 4847.84 4913.32 4819.89 4885.38 4893.2  4830.81 4849.52 4839.29\n",
      "  4824.83 4871.13 4870.35 4835.33 4850.17]\n",
      " [3265.78 3249.77 3258.71 3255.   3264.37 3249.46 3270.94 3244.48 3269.59\n",
      "  3250.89 3272.13 3239.19 3259.24 3257.23 3256.58 3233.11 3251.5  3229.11\n",
      "  3246.44 3245.94 3246.85 3253.61 3256.17 3234.05 3249.71 3239.56 3253.96\n",
      "  3240.83 3279.1  3238.38 3260.22 3253.8 ]\n",
      " [4279.75 4255.68 4232.08 4213.22 4223.32 4189.14 4160.57 4127.32 4079.52\n",
      "  4104.9  4096.69 4053.13 4068.87 4276.42 4211.27 4362.14 4380.   4534.13\n",
      "  4662.51 4803.75 4825.32 4636.13 4391.01 4151.99 4170.88 4392.7  4434.67\n",
      "  4503.75 4439.22 4322.94 4277.26 4257.52]\n",
      " [4600.74 4581.25 4604.2  4563.88 4569.73 4548.23 4544.54 4575.22 4551.34\n",
      "  4587.01 4535.38 4530.29 4583.93 4567.42 4544.3  4703.52 4551.54 4566.89\n",
      "  4576.5  4500.94 4665.52 4514.84 4577.27 4633.8  4565.97 4585.92 4692.24\n",
      "  4689.58 4623.49 4582.07 4641.39 4662.51]\n",
      " [5285.55 5282.18 5275.48 5296.83 5272.31 5290.69 5341.41 5268.65 5394.69\n",
      "  5278.17 5273.84 5239.54 5296.05 5252.53 5268.62 5244.68 5262.12 5223.75\n",
      "  5279.61 5245.22 5239.48 5233.62 5291.22 5219.05 5335.8  5254.07 5272.85\n",
      "  5246.19 5287.28 5248.86 5285.16 5238.74]\n",
      " [5307.88 5213.49 5198.98 5235.97 5199.83 5220.19 5203.77 5215.71 5191.85\n",
      "  5250.54 5199.78 5251.68 5200.12 5267.64 5210.59 5273.81 5207.4  5223.54\n",
      "  5231.7  5225.01 5191.65 5246.71 5199.84 5233.88 5242.22 5272.77 5244.94\n",
      "  5239.84 5206.26 5223.63 5216.14 5309.34]\n",
      " [4706.44 4702.91 4713.08 4703.84 4687.84 4752.25 4750.44 4743.33 4805.18\n",
      "  4760.33 4702.08 4688.86 4912.85 4705.71 4692.07 4708.87 4705.16 4704.29\n",
      "  4722.33 4715.33 4720.   4793.73 4855.14 4789.83 4764.61 4808.4  4728.26\n",
      "  4787.18 4752.76 4749.38 4732.47 4793.22]\n",
      " [4209.91 4305.31 4220.88 4238.75 4253.13 4291.76 4279.74 4330.96 4264.89\n",
      "  4309.9  4325.15 4269.98 4240.02 4214.99 4283.82 4172.44 4203.37 4179.06\n",
      "  4157.79 4154.53 4181.97 4172.92 4198.88 4199.04 4296.73 4181.56 4229.56\n",
      "  4182.35 4224.66 4205.9  4211.27 4260.  ]\n",
      " [3480.26 3471.51 3469.72 3473.74 3470.91 3471.99 3484.27 3475.62 3471.87\n",
      "  3466.02 3485.   3469.37 3475.41 3473.04 3486.28 3468.39 3477.27 3465.42\n",
      "  3476.83 3472.87 3477.3  3471.32 3483.31 3491.89 3480.21 3476.28 3478.91\n",
      "  3470.11 3482.29 3478.49 3496.49 3473.93]\n",
      " [4652.46 4650.89 4695.93 4649.42 4698.41 4774.86 4667.48 4713.71 4679.14\n",
      "  4675.88 4698.3  4649.93 4667.52 4627.43 4702.54 4647.82 4638.7  4635.31\n",
      "  4710.21 4711.43 4706.27 4703.81 4759.02 4646.14 4689.31 4633.65 4657.69\n",
      "  4658.16 4670.   4663.07 4691.47 4680.79]\n",
      " [4335.51 4344.18 4352.99 4356.16 4324.72 4348.37 4340.75 4354.15 4323.59\n",
      "  4352.92 4325.17 4357.36 4339.74 4349.29 4320.02 4348.08 4335.31 4335.53\n",
      "  4336.23 4351.59 4316.66 4343.31 4324.46 4333.77 4325.57 4352.03 4313.\n",
      "  4341.52 4328.9  4343.33 4322.53 4349.01]\n",
      " [4609.34 4513.51 4539.77 4503.62 4552.52 4601.3  4545.8  4502.33 4566.77\n",
      "  4563.45 4590.96 4539.5  4526.36 4544.66 4587.81 4542.76 4593.12 4592.53\n",
      "  4583.63 4529.8  4551.58 4573.63 4528.17 4544.26 4552.24 4529.28 4625.72\n",
      "  4607.04 4737.63 4557.49 4554.22 4529.91]\n",
      " [4618.25 4564.32 4559.44 4604.17 4566.06 4585.01 4592.3  4606.11 4644.23\n",
      "  4733.4  4647.77 4626.66 4609.16 4581.48 4597.86 4664.3  4597.26 4615.43\n",
      "  4606.53 4604.16 4586.19 4667.96 4592.64 4640.65 4618.68 4634.86 4602.43\n",
      "  4676.04 4624.89 4689.11 4655.36 4669.67]\n",
      " [5067.87 5071.12 5050.21 5063.58 5066.49 5063.59 5052.65 5067.23 5046.64\n",
      "  5052.82 5068.6  5058.76 5049.82 5074.46 5051.93 5049.27 5064.59 5066.11\n",
      "  5048.9  5069.64 5068.82 5053.63 5066.82 5073.41 5056.15 5069.15 5069.85\n",
      "  5064.69 5071.56 5090.14 5057.89 5072.95]\n",
      " [3476.57 3493.95 3470.84 3490.21 3464.87 3484.58 3473.92 3484.53 3468.17\n",
      "  3491.33 3466.   3485.1  3479.13 3490.03 3467.85 3494.22 3477.94 3480.79\n",
      "  3466.93 3480.23 3465.11 3489.49 3466.66 3483.81 3467.93 3489.01 3459.92\n",
      "  3478.99 3474.33 3483.63 3468.86 3490.39]\n",
      " [5115.59 5120.5  5125.92 5117.15 5120.08 5129.12 5107.5  5116.68 5123.58\n",
      "  5119.62 5108.93 5135.29 5106.84 5108.73 5127.59 5120.34 5107.1  5119.49\n",
      "  5105.11 5100.74 5111.04 5114.73 5100.49 5108.44 5105.99 5099.66 5105.32\n",
      "  5109.21 5090.25 5101.62 5106.55 5094.3 ]\n",
      " [3456.85 3468.39 3457.19 3490.95 3456.14 3477.19 3475.04 3487.94 3454.96\n",
      "  3470.77 3447.29 3468.72 3453.58 3455.7  3449.22 3460.93 3443.33 3470.91\n",
      "  3456.1  3476.51 3464.19 3459.63 3450.51 3455.96 3452.38 3456.66 3442.76\n",
      "  3468.85 3448.47 3458.52 3455.42 3459.25]\n",
      " [4335.91 4351.99 4364.61 4370.35 4305.96 4333.53 4313.11 4380.19 4364.22\n",
      "  4326.17 4334.86 4343.79 4310.05 4370.67 4349.76 4364.26 4327.44 4356.63\n",
      "  4307.21 4360.73 4325.47 4362.73 4560.88 4346.86 4429.17 4352.34 4329.06\n",
      "  4348.85 4341.57 4364.92 4352.6  4339.96]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Spotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 42\n",
    "scoring = 'f1_macro'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "\n",
    "#models.append(('XGB', XGBClassifier(random_state=seed)))\n",
    "models.append(('GNB', GaussianNB(var_smoothing=2e-9)))\n",
    "models.append(('LR', LogisticRegression(random_state=seed)))\n",
    "models.append(('CART' , DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('SVC' , SVC(gamma=0.05, random_state=seed)))\n",
    "models.append(('RF', RandomForestClassifier(random_state=seed, n_estimators = 50)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB - 0,52 0,04\n",
      "LR - 0,10 0,04\n",
      "CART - 0,55 0,06\n",
      "SVC - 0,11 0,01\n",
      "RF - 0,65 0,05\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    # Dividere dati in n = num_folds\n",
    "    kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = np.array([])\n",
    "    for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "        X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "        X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "        model.fit(X_cross_train, y_cross_train)  \n",
    "        y_pred = model.predict(X_cross_test)\n",
    "        f1s = f1_score(y_cross_test, y_pred, average=\"weighted\")\n",
    "        cv_results = np.append(cv_results, [f1s])\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #msg = \"%s - %f - %f\" % (name, cv_results.mean(), cv_results.std())\n",
    "    msg = \"{} - {:.2f} {:.2f}\".format(name, cv_results.mean(), cv_results.std()).replace('.', ',')\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "fig.suptitle('Algorithms Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione dei modelli sul Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model GNB: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85        20\n",
      "           1       1.00      0.90      0.95        20\n",
      "           2       0.18      0.10      0.13        20\n",
      "           3       0.88      0.70      0.78        20\n",
      "           4       0.37      0.50      0.43        20\n",
      "           5       0.00      0.00      0.00        20\n",
      "           6       0.79      0.95      0.86        20\n",
      "           7       0.47      0.70      0.56        20\n",
      "           8       0.54      0.70      0.61        20\n",
      "           9       0.40      0.20      0.27        20\n",
      "          10       0.50      0.75      0.60        20\n",
      "\n",
      "    accuracy                           0.59       220\n",
      "   macro avg       0.53      0.59      0.55       220\n",
      "weighted avg       0.53      0.59      0.55       220\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model LR: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.10      0.11        20\n",
      "           1       0.00      0.00      0.00        20\n",
      "           2       0.33      0.15      0.21        20\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.22      0.20      0.21        20\n",
      "           5       0.00      0.00      0.00        20\n",
      "           6       0.13      0.45      0.20        20\n",
      "           7       0.17      0.05      0.08        20\n",
      "           8       0.10      0.10      0.10        20\n",
      "           9       0.16      0.35      0.22        20\n",
      "          10       0.22      0.30      0.26        20\n",
      "\n",
      "    accuracy                           0.15       220\n",
      "   macro avg       0.13      0.15      0.13       220\n",
      "weighted avg       0.13      0.15      0.13       220\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model CART: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.55      0.65        20\n",
      "           1       0.86      0.95      0.90        20\n",
      "           2       0.22      0.20      0.21        20\n",
      "           3       0.68      0.85      0.76        20\n",
      "           4       0.25      0.15      0.19        20\n",
      "           5       0.24      0.25      0.24        20\n",
      "           6       0.80      0.80      0.80        20\n",
      "           7       0.46      0.60      0.52        20\n",
      "           8       0.41      0.35      0.38        20\n",
      "           9       0.43      0.50      0.47        20\n",
      "          10       0.50      0.55      0.52        20\n",
      "\n",
      "    accuracy                           0.52       220\n",
      "   macro avg       0.51      0.52      0.51       220\n",
      "weighted avg       0.51      0.52      0.51       220\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model SVC: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        20\n",
      "           1       0.00      0.00      0.00        20\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.93      0.70      0.80        20\n",
      "           4       0.00      0.00      0.00        20\n",
      "           5       0.00      0.00      0.00        20\n",
      "           6       0.00      0.00      0.00        20\n",
      "           7       0.00      0.00      0.00        20\n",
      "           8       0.00      0.00      0.00        20\n",
      "           9       0.00      0.00      0.00        20\n",
      "          10       0.10      1.00      0.18        20\n",
      "\n",
      "    accuracy                           0.15       220\n",
      "   macro avg       0.09      0.15      0.09       220\n",
      "weighted avg       0.09      0.15      0.09       220\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Model RF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91        20\n",
      "           1       1.00      0.95      0.97        20\n",
      "           2       0.50      0.35      0.41        20\n",
      "           3       1.00      0.85      0.92        20\n",
      "           4       0.55      0.30      0.39        20\n",
      "           5       0.43      0.45      0.44        20\n",
      "           6       1.00      0.90      0.95        20\n",
      "           7       0.56      0.70      0.62        20\n",
      "           8       0.52      0.55      0.54        20\n",
      "           9       0.50      0.60      0.55        20\n",
      "          10       0.42      0.55      0.48        20\n",
      "\n",
      "    accuracy                           0.65       220\n",
      "   macro avg       0.66      0.65      0.65       220\n",
      "weighted avg       0.66      0.65      0.65       220\n",
      "\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tasks = ['S', 'S3', 'S6']\n",
    "def classification_report_csv(report, model_name):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    index = 0\n",
    "    row = lines[-4].split('    ')\n",
    "    accuracy = row[-2]\n",
    "    for line in lines[2:-5]:\n",
    "        row = {}\n",
    "        row_data = line.split('      ')\n",
    "        row['class'] = uniques[index]\n",
    "        row['precision'] = float(row_data[2]) \n",
    "        row['recall'] = float(row_data[3]) \n",
    "        row['f1_score'] = float(row_data[4])\n",
    "        row['accuracy'] = accuracy\n",
    "        report_data.append(row)\n",
    "        index += 1\n",
    "    dataframe = pd.DataFrame.from_dict(report_data)\n",
    "    dataframe.to_csv(tasks[choosenIndex]+ '/classificationReports/'+'classification_report' + model_name +  '.csv', index = False)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED)\n",
    "for name, model in models:\n",
    "    model.fit(X_train,  y_train)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    print(f\"Model {name}: \")\n",
    "    report = classification_report(y_test, pred_test)\n",
    "    print(report)\n",
    "    classification_report_csv(report, name)\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione Inferance Rate medio (|X_test| = 50/50/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAASzUlEQVR4nO3df7DldV3H8ecrjJSIawVjyQ/XWMLWQNIbaphQUQOTK6b9YGP6YeSOFdnvorKkX+OPcpzRMNqCUIdA0jS2ttAyBB1SFuXXQuQKGcvksERzy/yB4Ls/zne/e7je3Xt2vZ/7vefe52PmzN7z+Z7z3ff57j37Ot/v53M+n1QVkiQBfNnQBUiSVg5DQZLUMxQkST1DQZLUMxQkSb3HDV3Al+LII4+sdevWDV2GJE2Vm2+++cGqOmqhbVMdCuvWrWP79u1DlyFJUyXJJ/a1zctHkqSeoSBJ6hkKkqSeoSBJ6hkKkqTeVIZCko1JtszNzQ1diiStKlMZClW1tao2z8zMDF2KJK0qUxkKkqQ2pvrLa5IO3vuff/rQJSy5069//9AlTD3PFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJvRUTCkm+KcklSd6R5KeGrkeS1qKmoZDksiQPJLljXvtZSe5OsjPJhQBVdVdVvRz4QeC0lnVJkhbW+kzhcuCs8YYkhwAXA2cDG4BNSTZ0214I/B2wrXFdkqQFNA2FqroeeGhe86nAzqq6p6oeBq4Czukef01VnQ2c17IuSdLChlhP4WjgvrH7u4BnJzkDeDHwFeznTCHJZmAzwHHHHdesSElai1bMIjtVdR1w3QSP2wJsAZidna22VUnS2jLE6KP7gWPH7h/TtU0sycYkW+bm5pa0MEla64YIhZuAE5I8NcmhwLnANQeyg6raWlWbZ2ZmmhQoSWtV6yGpVwI3Aicm2ZXk/Kp6BLgAuBa4C7i6qna0rEOSNJmmfQpVtWkf7dtw2KkkrTgr5hvNB8I+BUlqYypDwT4FSWpjKkNBktTGVIaCl48kqY2pDAUvH0lSG1MZCpKkNgwFSVJvKkPBPgVJamMqQ8E+BUlqYypDQZLUhqEgSeoZCpKk3lSGgh3NktTGVIaCHc2S1MZUhoIkqQ1DQZLUMxQkST1DQZLUm8pQcPSRJLUxlaHg6CNJamMqQ0GS1IahIEnqGQqSpJ6hIEnqGQqSpN5UhoJDUiWpjakMBYekSlIbUxkKkqQ2DAVJUs9QkCT1DAVJUs9QkCT1DAVJUs9QkCT1DAVJUs9QkCT1pjIUnOZCktqYylBwmgtJamMqQ0GS1IahIEnqGQqSpJ6hIEnqGQqSpJ6hIEnqGQqSpJ6hIEnqGQqSpJ6hIEnqGQqSpJ6hIEnqGQqSpN7jhi5gXJIXAd8LHAFcWlXvGbYiSVpbmp8pJLksyQNJ7pjXflaSu5PsTHIhQFW9u6peBrwc+KHWtUmSHms5Lh9dDpw13pDkEOBi4GxgA7ApyYaxh7yy2y5JWkbNQ6Gqrgcemtd8KrCzqu6pqoeBq4BzMvJa4O+r6iML7S/J5iTbk2zfvXt32+IlaY0Zqk/haOC+sfu7gGcDPwucCcwkWV9Vl8x/YlVtAbYAzM7O1jLUKmmV++Nf2jp0CUvugtdvPKjnraiO5qp6I/DGoeuQpLVqqCGp9wPHjt0/pmubSJKNSbbMzc0teWGStJYNFQo3ASckeWqSQ4FzgWsmfXJVba2qzTMzM80KlKS1aDmGpF4J3AicmGRXkvOr6hHgAuBa4C7g6qra0boWSdL+Ne9TqKpN+2jfBmw7mH0m2QhsXL9+/ZdSmiRpnqmc5sLLR5LUxkShkOSwJL+V5M+6+yckeUHb0iRJy23SM4W/AD4HPLe7fz/w+00qkiQNZtJQOL6qXgd8HqCqPg2kWVWLcEiqJLUxaSg8nOQJQAEkOZ7RmcMg7FOQpDYmHX30KuAfgGOTXAGcBvx4q6IkScOYKBSq6r1JPgI8h9Flo5+rqgebViZJWnYHMiT1aOAQ4FDg+Ule3KakxdmnIEltTHSmkOQy4GRgB/CFrrmAv25U135V1VZg6+zs7MuG+PslabWatE/hOVW1YfGHSZKm2aSXj26ctzKaJGkVmvRM4a2MguGTjIaiBqiqOrlZZZKkZTdpKFwK/AhwO3v7FAbjhHiS1Makl492V9U1VXVvVX1iz61pZfvhl9ckqY1JzxQ+muQvga2MfZO5qgYZfSRJamPSUHgCozD4nrG2wYakSpLamPQbzS9tXYgkaXj7DYUkv1pVr0vyJrrJ8MZV1SuaVSZJWnaLnSnc2f25vXUhB8LRR5LUxmKh8Argb6vqLctRzKSc5kKS2lhsSOqRy1KFJGlFWOxM4Yn7mw3VIamStLosFgozwAtYeOlNh6RK0iqzWCj8R1X9xLJUIkka3GJ9Co9fliokSSvCYqGwHiDJ25ahlom58poktbHY5aO7k/ww8G0LdTgP1dHskFRJamOxUHg5cB7wRGDjvG12NEvSKrPfUKiqDwAfSLK9qi5dppokSQOZdEK8S5N8G7Bu/DlV9dZGdUmSBjBRKHQdzccDtwCPds3FaJlOSdIqMel6CrPAhqr6oplSJUmrx6TLcd4BfF3LQiRJw5v0TOFI4M4kH+axy3G+sElVkqRBTBoKF7UsQpK0Mkw6+uj9rQuRJA1vseU4/5cFluFkNGtqVdURTapahCuvSVIb++1orqqvqqojFrh91VCB0NW1tao2z8zMDFWCJK1Kk44+kiStAYaCJKlnKEiSeoaCJKlnKEiSeoaCJKlnKEiSeoaCJKlnKEiSeoaCJKlnKEiSeoaCJKlnKEiSeismFJJ8Q5JLk7xj6Fokaa1qGgpJLkvyQJI75rWfleTuJDuTXAhQVfdU1fkt65Ek7V/rM4XLgbPGG5IcAlwMnA1sADYl2dC4DknSBJqGQlVdDzw0r/lUYGd3ZvAwcBVwzqT7TLI5yfYk23fv3r2E1UqShuhTOBq4b+z+LuDoJF+b5BLgW5L8+r6eXFVbqmq2qmaPOuqo1rVK0pqy3zWal1NV/Rfw8qHrkKS1bIgzhfuBY8fuH9O1TSzJxiRb5ubmlrQwSVrrhgiFm4ATkjw1yaHAucA1B7KDqtpaVZtnZmaaFChJa1XrIalXAjcCJybZleT8qnoEuAC4FrgLuLqqdrSsQ5I0maZ9ClW1aR/t24BtB7vfJBuBjevXrz/YXUiSFrBivtF8ILx8JEltTGUoSJLaMBQkSb2pDAWHpEpSG1MZCvYpSFIbUxkKkqQ2DAVJUm8qQ8E+BUlqYypDwT4FSWpjKkNBktSGoSBJ6k1lKNinIEltTGUo2KcgSW1MZShIktowFCRJPUNBktQzFCRJvaYrr7Uyycprz/qVty5fQcvk5j/80aFLmHqnvem0oUtYch/82Q8OXYJWkak8U3D0kSS1MZWhIElqw1CQJPUMBUlSz1CQJPUMBUlSz1CQJPWmMhScJVWS2pjKUPB7CpLUxlSGgiSpDUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJPUNBktQzFCRJvakMBae5kKQ2pjIUnOZCktqYylCQJLVhKEiSeoaCJKlnKEiSeoaCJKlnKEiSeoaCJKlnKEiSeoaCJKlnKEiSeoaCJKlnKEiSeoaCJKn3uKEL2CPJVwJvBh4GrquqKwYuSZLWnKZnCkkuS/JAkjvmtZ+V5O4kO5Nc2DW/GHhHVb0MeGHLuiRJC2t9+ehy4KzxhiSHABcDZwMbgE1JNgDHAPd1D3u0cV2SpAU0vXxUVdcnWTev+VRgZ1XdA5DkKuAcYBejYLiF/YRVks3AZoDjjjtu6Ytehf7jd08auoQld9xv3z50CdKqNERH89HsPSOAURgcDfw18JIkfwJs3deTq2pLVc1W1exRRx3VtlJJWmNWTEdzVf0f8NKh65CktWyIM4X7gWPH7h/TtU0sycYkW+bm5pa0MEla64YIhZuAE5I8NcmhwLnANQeyg6raWlWbZ2ZmmhQoSWtV6yGpVwI3Aicm2ZXk/Kp6BLgAuBa4C7i6qna0rEOSNJnWo4827aN9G7DtYPebZCOwcf369Qe7C0nSAqZymgsvH0lSG1MZCpKkNqYyFBx9JEltpKqGruGgJdkNfGLoOoAjgQeHLmIF8Djs5bHYy2Ox10o5Fk+pqgW//TvVobBSJNleVbND1zE0j8NeHou9PBZ7TcOxmMrLR5KkNgwFSVLPUFgaW4YuYIXwOOzlsdjLY7HXij8W9ilIknqeKUiSeoaCJKlnKByAJI8muSXJHUm2Jnli174uyWe6bXtuhw5c7pJI8nVJrkry8SQ3J9mW5Bu7bT+f5LNJZsYef0aSue4Y/GuSP0py0thxeSjJvd3P/zjcK1s6ST61QNtFSe7vXuedSRacB2w1SPKbSXYkua17va9K8up5jzklyV3dz4cn+dOx36nrkjx7mOqXTpInJfnLJPd0r+vGJN/XvSeqm7Ntz2P/NskZ3c/XdWvW35Lkrm51ycEYCgfmM1V1SlV9M/AQ8DNj2z7ebdtze3igGpdMkgDvAq6rquOr6lnArwNP6h6yidFU6C+e99QbquoU4FuAFwBH7DkujKZJ/5Xu/pnL8DKG9IbuNZ8D/GmSLx+4niWX5LmM/o2fWVUnA2cC/wz80LyHngtc2f3854zePyd0v1MvZfSlrqnVvVfeDVxfVd/Qva5zGa0XA6MVJn9zP7s4r/tdOQ147ZAfKg2Fg3cjo2VEV7PvAD5fVZfsaaiqW6vqhiTHA4cDr2QUDl+kqj7DaM3t1X6c9quqPgZ8GvjqoWtp4OuBB6vqcwBV9WBVXQ/897xP/z8IXNn93jwbeGVVfaF7zr1V9XfLXfgS+07g4XnvlU9U1Zu6u7cCc0m+e5H9HA78H/BomzIXZygchCSHAN/FYxcHOn7sEsnFA5W21L4ZuHkf284FrgJuYLRexpPmPyDJVwMnANc3q3AKJHkm8LGqemDoWhp4D3Bskn9L8uYkp3ftVzL6HSHJc4CHunB8OnBLVQ32n14jTwc+sshj/oDRh6iFXJHkNuBu4PeGPD6GwoF5QpJbgE8yuoTy3rFt45ePfmbBZ68um4Cruk977wR+YGzbtye5ldEyq9dW1SeHKHAF+IUkO4APMfoPYdWpqk8BzwI2A7uBtyf5ceDtwPcn+TIee+loTUhycZJbk9y0p607gyLJ8xZ4ynnd5bfjgF9O8pRlKvWLGAoH5jPddb+nAOGxfQqr0Q5Gb/jHSHISozOA9yb5d0Zv+vFLSDdU1TMYfXo6P8kp7Utdkd5QVU8HXgJcmuTxQxfUQlU9WlXXVdWrGK2q+JKqug+4Fzid0et/e/fwHcAzurPt1WQH8Mw9d7oPht8FzJ90bn9nC1TVbkZnHIN1vBsKB6GqPg28AvilJE1XrxvY+4CvGB8NkeRk4I3ARVW1rrs9GXjy/E83VXUv8Brg15az6JWmqq4BtgM/NnQtSy3JiUlOGGs6hb0zF18JvAG4p6p2AVTVxxkdi9/pOmf3jN773uWruon3AY9P8lNjbYfNf1BVvYdR39LJC+0kyWGMBmh8vEWRkzAUDlJVfRS4jX10sq4GNfq6+/cBZ3bDB3cArwbOYDQqady76K4hz3MJ8Pwk6xqWOrTDMlqDfM/tFxd4zO8Cv9hdTllNDgfe0g27vQ3YAFzUbfsrRmeL8y8d/SSjy687k9wBXA5MdX9L9155EXB6N+T6w8BbWPgD0R8Ax85ru6K7NH0zcHlV7asvrzmnuZAk9VbbpxZJ0pfAUJAk9QwFSVLPUJAk9QwFSVLPUNCaluRF3QyWT+vur+uGSS7V/v88yYbu599Yqv1KrRgKWus2AR+gwfdNkhxSVT9ZVXd2TYaCVjxDQWtWksOB5wHns8AX75IcluTq7otZ70ryoSSz3bZNSW7PaG2N144951NJXt/N/fTcbq782SSvoZs7K8kV3RnJvya5vJtM7ookZyb5YJKPJTm129/XJHl3RmsV/Ev3jXKpGUNBa9k5wD9U1b8B/5Vk/jxPPw38d1VtAH6Lbh6oJE8GXstouuRTgG9N8qLuOV8JfKiqnlFVH9izo6q6kL3rcZzXNa8HXg88rbv9MKOQ+mX2nlX8DvDRbrK03wDeukSvXVqQoaC1bBOj6b/p/px/Cel5e7ZX1R2MpjUB+FZGCw/trqpHgCuA53fbHmU0a+wk7q2q27uZZncA/9RNl3A7sG6shrd1NbwP+NokR0z8CqUDtJonc5P2KcnXMPqkf1KSAg4BCvhS18L47AHMhf+5sZ+/MHb/C/je1EA8U9Ba9f3A26rqKd1Mr8cymup5fKKyDzJaMYxuBNFJXfuHGU18dmQ3BfQm4P0T/J2fP4glOW8AzutqOIPRKmf/c4D7kCZmKGit2sQXz/T6TkZrUO/xZuCoJHcCv8/oEs9cVf0ncCGjtYhvBW6uqr+Z4O/cAtyW5IoDqPMi4FndDKSvYRVOv62VxVlSpX3ozgK+vKo+260t/I/AiVX18MClSc143VLat8OAf+4u+QT4aQNBq51nCpKknn0KkqSeoSBJ6hkKkqSeoSBJ6hkKkqTe/wOTBq+bnqK3RwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAATqElEQVR4nO3df5BlZX3n8fcnGKJIHE2gTOTXIMPijgFRO6jBFRJJCiqOEM0mTKhs4hKmTEJM1vwiiUaSTSqarGuViouzgSAWGWRNNIyZBDUGQYtVBuXXQNARYhgqFoNYnTX+4Nd3/zhnzlya7unbQ58+fbvfr6ou7n3uved+76F7Pvec5znPk6pCkiSA7xi6AEnS8mEoSJI6hoIkqWMoSJI6hoIkqfOUoQt4Mg455JBau3bt0GVI0kS56aabHqiqQ2d7bKJDYe3atWzfvn3oMiRpoiT58lyPefpIktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktSZyFBIsiHJ5unp6aFLkaQVZSIvXquqrcDWqamp84auRZpUn3zFKUOXsOhOue6TQ5cw8SbySEGS1A9DQZLUMRQkSR1DQZLUMRQkSR1DQZLUMRQkSR1DQZLUMRQkSR1DQZLUMRQkSR1DQZLUMRQkSZ1lEwpJ/mOSi5N8MMkvDl2PJK1GvYZCkkuT3J/k9hntpye5K8nOJBcAVNWdVfV64KeAk/usS5I0u76PFC4DTh9tSHIAcBFwBrAe2JhkffvYq4G/Bbb1XJckaRa9hkJVXQc8OKP5JGBnVd1dVQ8BVwJnts+/uqrOAM6Za5tJNiXZnmT77t27+ypdklalIVZeOwy4d+T+LuAlSU4FXgN8F/s4UqiqzcBmgKmpqeqtSklahZbNcpxVdS1w7cBlSNKqNsToo/uAI0buH962jS3JhiSbp6enF7UwSVrthgiFG4Fjkxyd5EDgbODqhWygqrZW1aY1a9b0UqAkrVZ9D0ndAtwAHJdkV5Jzq+oR4HzgGuBO4Kqq2tFnHZKk8fTap1BVG+do38aTGHaaZAOwYd26dfu7CUnSLJbNFc0L4ekjSerHRIaCJKkfhoIkqTORoeCQVEnqx0SGgn0KktSPiQwFSVI/DAVJUmciQ8E+BUnqx0SGgn0KktSPiQwFSVI/DAVJUmciQ8E+BUnqx0SGgn0KktSPiQwFSVI/DAVJUsdQkCR1DAVJUmciQ8HRR5LUj4kMBUcfSVI/JjIUJEn9MBQkSR1DQZLUMRQkSR1DQZLUMRQkSZ2JDAWvU5CkfkxkKHidgiT1YyJDQZLUD0NBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktSZyFDwimZJ6sdEhoJXNEtSPyYyFCRJ/TAUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1HnK0AWMSnIW8OPAM4BLquqjw1YkSatL70cKSS5Ncn+S22e0n57kriQ7k1wAUFUfrqrzgNcDP913bZKkx1uK00eXAaePNiQ5ALgIOANYD2xMsn7kKW9qH5ckLaHeQ6GqrgMenNF8ErCzqu6uqoeAK4Ez03gb8HdV9bnZtpdkU5LtSbbv3r273+IlaZXZZ59CkucB7wAeA94AvBk4C/gC8HNVded+vu9hwL0j93cBLwF+BTgNWJNkXVVdPPOFVbUZ2AwwNTVV+/n+ktR5969vHbqERXf+2zfs1+vm62jeDPwZcDDwCeC3gdcBrwLeDbxyv951DlX1TuCdi7lNSdL45jt99N3teshbgIer6spqbAWe9STe9z7giJH7h7dtY0myIcnm6enpJ1GCJGmm+ULhgJHb/3PGYwc+ife9ETg2ydFJDgTOBq4e98VtUG1as2bNkyhBkjTTfKFwUZKDAarqPXsak6wDPj7OGyTZAtwAHJdkV5Jzq+oR4HzgGuBO4Kqq2rE/H0CStHj22adQVe+do30n8GvjvEFVbZyjfRuwbZxtzJRkA7Bh3bp1+/NySdIc9nmkkOS8JMe2t5PkL5L8W5Jbk7xwaUp8Ik8fSVI/5jt99KvAP7e3NwInAEcDb8RRQpK04swXCo9U1cPt7VcBl1fVV6vq48DT+y1NkrTU5guFx5J8f5Kn0lyTMNq5/LT+yto3h6RKUj/mC4XfB7bTnEK6es8IoSSnAHf3W9rc7FOQpH7MN/roI0mOormI7WsjD23HWUwlacWZd0K89pqCbyd5c5L/3TY/Bzi1z8IkSUtv3FlS/wL4NvCy9v59wB/1UtEY7FOQpH6MGwrHVNWfAg8DVNU3gPRW1TzsU5CkfowbCg8leRpQAEmOoTlykCStIOOu0fwW4O+BI5JcAZwM/HxfRUmShjFWKFTVx5J8DngpzWmjX62qB3qtTJK05BayHOdhNFNpHwi8Islr+ilpfnY0S1I/xjpSSHIpzbxHO2iW5oSmf+Gve6prn9pFfrZOTU2dN8T7S9JKNW6fwkuran2vlUiSBjfu6aMbkhgKkrTCjXukcDlNMHyFZihqgKqqE3qrTJK05MYNhUuAnwVuY2+fwmBceU2S+jHu6aPdVXV1Vd1TVV/e89NrZfvgFc2S1I9xjxQ+n+Qvga2MXMlcVYOMPpIk9WPcUHgaTRj82EjbYENSJUn9GPeK5tf1XYgkaXj7DIUkv1VVf5rkXbST4Y2qqjf0VpkkacnNd6RwR/vf7X0XIkka3nyh8AbgI1X1vqUoRpI0rPmGpB6yJFUskBPiSVI/5jtSeOa+ZkMdakiqE+JJUj/mC4U1wKuYfelNh6RK0gozXyj8S1X91yWpRJI0uPn6FJ66JFVIkpaF+UJhHUCS9y9BLZKkgc13+uiuJD8D/NBsHc7OfSRJK8t8ofB64BzgmcCGGY/Z0SxJK8w+Q6GqPgV8Ksn2qrpkiWqSJA1k3AnxLknyQ8Da0ddU1eU91SVJGsBYodB2NB8D3Aw82jYXzTKdkqQVYtz1FKaA9VX1hJlSh+BynJLUj3GX47wd+L4+C1kIl+OUpH6Me6RwCHBHks/y+OU4X91LVZKkQYwbChf2WYQkaXkYd/TRJ/suRJI0vPmW4/x/zLIMJ82sqVVVz+ilKknSIOa7eO27l6oQSdLwxh19JElaBQwFSVLHUJAkdQwFSVLHUJAkdQwFSVLHUJAkdZZNKCR5bpJLknxw6FokabXqNRSSXJrk/iS3z2g/PcldSXYmuQCgqu6uqnP7rEeStG99HylcBpw+2pDkAOAi4AxgPbAxyfqe65AkjaHXUKiq64AHZzSfBOxsjwweAq4EzuyzDknSeIboUzgMuHfk/i7gsCTfm+Ri4IVJfmeuFyfZlGR7ku27d+/uu1ZJWlXGXU+hd1X1VeD1YzxvM7AZYGpqalksDypJK8UQRwr3AUeM3D+8bRtbkg1JNk9PTy9qYZK02g0RCjcCxyY5OsmBwNnA1QvZgGs0S1I/+h6SugW4ATguya4k51bVI8D5wDXAncBVVbWjzzokSePptU+hqjbO0b4N2Nbne0uSFm7ZXNG8EPYpSFI/JjIU7FOQpH5MZChIkvoxkaHg6SNJ6sdEhoKnjySpHxMZCpKkfhgKkqTORIaCfQqS1I+JDAX7FCSpHxMZCpKkfhgKkqSOoSBJ6kxkKNjRLEn9mMhQsKNZkvoxkaEgSeqHoSBJ6hgKkqSOoSBJ6vS6HGdfkmwANqxbt27oUjRhTn7XyUOXsOg+/SufHroErSATeaTg6CNJ6sdEhoIkqR+GgiSpYyhIkjqGgiSpYyhIkjordkjqi3/z8qUraInc9Gf/ZegSJK1wE3mk4JBUSerHRIaCJKkfhoIkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6ExkKSTYk2Tw9PT10KZK0okxkKDjNhST1YyJDQZLUD0NBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJnacMXcAeSZ4OvAd4CLi2qq4YuCRJWnV6PVJIcmmS+5PcPqP99CR3JdmZ5IK2+TXAB6vqPODVfdYlSZpd36ePLgNOH21IcgBwEXAGsB7YmGQ9cDhwb/u0R3uuS5I0i15PH1XVdUnWzmg+CdhZVXcDJLkSOBPYRRMMN7OPsEqyCdgEcOSRRy5+0SvQv/zh8UOXsOiO/P3bhi5BWpGG6Gg+jL1HBNCEwWHAXwOvTfK/gK1zvbiqNlfVVFVNHXroof1WKkmrzLLpaK6qfwdeN3QdkrSaDXGkcB9wxMj9w9u2sSXZkGTz9PT0ohYmSavdEKFwI3BskqOTHAicDVy9kA1U1daq2rRmzZpeCpSk1arvIalbgBuA45LsSnJuVT0CnA9cA9wJXFVVO/qsQ5I0nr5HH22co30bsG1/t5tkA7Bh3bp1+7sJSdIsJnKaC08fSVI/JjIUJEn9mMhQcPSRJPUjVTV0DfstyW7gy0PXARwCPDB0EcuA+2Ev98Ve7ou9lsu+OKqqZr36d6JDYblIsr2qpoauY2juh73cF3u5L/aahH0xkaePJEn9MBQkSR1DYXFsHrqAZcL9sJf7Yi/3xV7Lfl/YpyBJ6nikIEnqGAqSpI6hsABJHk1yc5Lbk2xN8sy2fW2Sb7aP7fk5cOByF0WS70tyZZIvJbkpybYk/6F97NeSfCvJmpHnn5pkut0H/5TkfyQ5fmS/PJjknvb2x4f7ZIsnyddnabswyX3t57wjyazzgK0ESX4vyY4kt7af9y1J/mTGc05Mcmd7++Ak7x35nbo2yUuGqX7xJHl2kr9Mcnf7uW5I8hPt30S1c7btee5Hkpza3r62XbP+5iR3tqtLDsZQWJhvVtWJVfUDwIPAL4889qX2sT0/Dw1U46JJEuBDwLVVdUxVvRj4HeDZ7VM20kyF/poZL72+qk4EXgi8CnjGnv1CM036b7b3T1uCjzGkd7Sf+UzgvUm+c+B6Fl2Sl9H8P35RVZ0AnAb8I/DTM556NrClvf3nNH8/x7a/U6+juahrYrV/Kx8Grquq57af62ya9WKgWWHy9/axiXPa35WTgbcN+aXSUNh/N9AsI7qS/TDwcFVdvKehqm6pquuTHAMcDLyJJhyeoKq+SbPm9krfT/tUVV8EvgE8a+haevD9wANV9W2Aqnqgqq4Dvjbj2/9PAVva35uXAG+qqsfa19xTVX+71IUvsh8BHprxt/LlqnpXe/cWYDrJj86znYOBfwce7afM+RkK+yHJAcArefziQMeMnCK5aKDSFtsPADfN8djZwJXA9TTrZTx75hOSPAs4FriutwonQJIXAV+sqvuHrqUHHwWOSPKFJO9JckrbvoXmd4QkLwUebMPx+cDNVTXYP3o9eT7wuXme88c0X6Jmc0WSW4G7gP8+5P4xFBbmaUluBr5CcwrlYyOPjZ4++uVZX72ybASubL/t/RXwn0ce+09JbqFZZvWaqvrKEAUuA/8tyQ7gMzT/IKw4VfV14MXAJmA38IEkPw98APjJJN/B408drQpJLkpyS5Ib97S1R1AkefksLzmnPf12JPAbSY5aolKfwFBYmG+25/2OAsLj+xRWoh00f/CPk+R4miOAjyX5Z5o/+tFTSNdX1Qtovj2dm+TE/ktdlt5RVc8HXgtckuSpQxfUh6p6tKquraq30Kyq+Nqquhe4BziF5vN/oH36DuAF7dH2SrIDeNGeO+0Xw1cCMyed29fRAlW1m+aIY7COd0NhP1TVN4A3AL+epNfV6wb2CeC7RkdDJDkBeCdwYVWtbX+eAzxn5rebqroHeCvw20tZ9HJTVVcD24GfG7qWxZbkuCTHjjSdyN6Zi7cA7wDurqpdAFX1JZp98Qdt5+ye0Xs/vnRV9+ITwFOT/OJI20Ezn1RVH6XpWzphto0kOYhmgMaX+ihyHIbCfqqqzwO3Mkcn60pQzeXuPwGc1g4f3AH8CXAqzaikUR+iPYc8w8XAK5Ks7bHUoR2UZg3yPT9vnOU5fwi8sT2dspIcDLyvHXZ7K7AeuLB97P/QHC3OPHX0CzSnX3cmuR24DJjo/pb2b+Us4JR2yPVngfcx+xeiPwaOmNF2RXtq+ibgsqqaqy+vd05zIUnqrLRvLZKkJ8FQkCR1DAVJUsdQkCR1DAVJUsdQ0KqW5Kx2BsvntffXtsMkF2v7f55kfXv7dxdru1JfDAWtdhuBT9HD9SZJDqiqX6iqO9omQ0HLnqGgVSvJwcDLgXOZ5cK7JAcluaq9MOtDST6TZKp9bGOS29KsrfG2kdd8Pcnb27mfXtbOlT+V5K20c2cluaI9IvmnJJe1k8ldkeS0JJ9O8sUkJ7Xb+54kH06zVsH/ba8ol3pjKGg1OxP4+6r6AvDVJDPnefol4GtVtR54M+08UEmeA7yNZrrkE4EfTHJW+5qnA5+pqhdU1af2bKiqLmDvehzntM3rgLcDz2t/foYmpH6DvUcVfwB8vp0s7XeByxfps0uzMhS0mm2kmf6b9r8zTyG9fM/jVXU7zbQmAD9Is/DQ7qp6BLgCeEX72KM0s8aO456quq2daXYH8A/tdAm3AWtHanh/W8MngO9N8oyxP6G0QCt5MjdpTkm+h+ab/vFJCjgAKODJroXxrQXMhf/tkduPjdx/DP82NRCPFLRa/STw/qo6qp3p9QiaqZ5HJyr7NM2KYbQjiI5v2z9LM/HZIe0U0BuBT47xng/vx5Kc1wPntDWcSrPK2b8tcBvS2AwFrVYbeeJMr39Fswb1Hu8BDk1yB/BHNKd4pqvqX4ELaNYivgW4qar+Zoz33AzcmuSKBdR5IfDidgbSt7ICp9/W8uIsqdIc2qOA76yqb7VrC38cOK6qHhq4NKk3nreU5nYQ8I/tKZ8Av2QgaKXzSEGS1LFPQZLUMRQkSR1DQZLUMRQkSR1DQZLU+f/DRfIxqPK4JAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = ['InfTime', 'InfTimeS3', 'InfTimeS6']\n",
    "for c in columns:\n",
    "    csv = read_csv(\"InfTimeReport.csv\")\n",
    "    g = sbs.barplot(x=csv['Algoritmo'], y=csv[c])\n",
    "    g.set_yscale(\"log\")\n",
    "    plt.ylabel(c)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoria occupata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = ['MemOccupata', 'MemOccupataS3', 'MemOccupataS6']\n",
    "for c in columns:   \n",
    "    csv = read_csv(\"MemOccupationReport.csv\")\n",
    "    g = sbs.barplot(x=csv['Algoritmo'], y=csv[c])\n",
    "    plt.ylabel(c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNetwork(): \n",
    "    n = 100\n",
    "    model = Sequential(name=\"Sequential-NN\")\n",
    "    model.add(layers.Dense(X.shape[1], activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(layers.Dense(np.unique(y).size * n, activation='relu'))\n",
    "    model.add(layers.Dense(np.unique(y).size, activation='softmax'))\n",
    "    learn_rate = 0.0001 if choosenIndex == 2 else 0.001\n",
    "    opt = Adam(learning_rate=learn_rate)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "BATCH_SIZE = 8\n",
    "num_folds = 10\n",
    "\n",
    "\n",
    "kf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "cv_results = np.array([])\n",
    "for train_idx, test_idx, in kf.split(X_train, y_train):\n",
    "    X_cross_train, y_cross_train = X_train[train_idx], y_train[train_idx]\n",
    "    X_cross_train = scaler.fit_transform(X_cross_train)\n",
    "    X_cross_test, y_cross_test = X_train[test_idx], y_train[test_idx]\n",
    "    X_cross_test = scaler.transform(X_cross_test)\n",
    "    model = getNetwork()\n",
    "    model.fit(X_cross_train, y_cross_train, epochs=EPOCHS, batch_size=BATCH_SIZE)  \n",
    "    y_pred = model.predict(X_cross_test)\n",
    "    predictions_categorical = np.argmax(y_pred, axis=1)\n",
    "    f1s = f1_score(y_cross_test, predictions_categorical, average=\"weighted\")\n",
    "    cv_results = np.append(cv_results, [f1s])\n",
    "\n",
    "print(f'Average score of Cross Validation: {cv_results.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential-NN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1100)              36300     \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 11)                12111     \n",
      "=================================================================\n",
      "Total params: 49,467\n",
      "Trainable params: 49,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1500\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 248.6070 - accuracy: 0.0833 - val_loss: 39.7449 - val_accuracy: 0.0818\n",
      "Epoch 2/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 33.9008 - accuracy: 0.0864 - val_loss: 39.5402 - val_accuracy: 0.0818\n",
      "Epoch 3/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 35.9962 - accuracy: 0.1015 - val_loss: 45.2857 - val_accuracy: 0.1045\n",
      "Epoch 4/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 32.8056 - accuracy: 0.0879 - val_loss: 31.6098 - val_accuracy: 0.0636\n",
      "Epoch 5/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 36.7736 - accuracy: 0.1061 - val_loss: 49.4397 - val_accuracy: 0.0545\n",
      "Epoch 6/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 36.6149 - accuracy: 0.1182 - val_loss: 42.7743 - val_accuracy: 0.0727\n",
      "Epoch 7/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 34.5808 - accuracy: 0.1030 - val_loss: 50.2097 - val_accuracy: 0.0773\n",
      "Epoch 8/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 38.1428 - accuracy: 0.0939 - val_loss: 31.2767 - val_accuracy: 0.0727\n",
      "Epoch 9/1500\n",
      "83/83 [==============================] - 0s 595us/step - loss: 47.3246 - accuracy: 0.0833 - val_loss: 34.0797 - val_accuracy: 0.1273\n",
      "Epoch 10/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 32.8682 - accuracy: 0.1030 - val_loss: 27.0223 - val_accuracy: 0.1318\n",
      "Epoch 11/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 33.3511 - accuracy: 0.0985 - val_loss: 35.6054 - val_accuracy: 0.0909\n",
      "Epoch 12/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 33.9006 - accuracy: 0.0894 - val_loss: 31.1090 - val_accuracy: 0.1000\n",
      "Epoch 13/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 36.5122 - accuracy: 0.1091 - val_loss: 60.5301 - val_accuracy: 0.0818\n",
      "Epoch 14/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 47.2996 - accuracy: 0.0848 - val_loss: 52.0297 - val_accuracy: 0.0909\n",
      "Epoch 15/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 40.2495 - accuracy: 0.0833 - val_loss: 33.1969 - val_accuracy: 0.1227\n",
      "Epoch 16/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 32.0060 - accuracy: 0.0818 - val_loss: 42.1710 - val_accuracy: 0.1000\n",
      "Epoch 17/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 30.4973 - accuracy: 0.0970 - val_loss: 30.2734 - val_accuracy: 0.1000\n",
      "Epoch 18/1500\n",
      "83/83 [==============================] - 0s 656us/step - loss: 28.9392 - accuracy: 0.1015 - val_loss: 33.8496 - val_accuracy: 0.0727\n",
      "Epoch 19/1500\n",
      "83/83 [==============================] - 0s 639us/step - loss: 37.4239 - accuracy: 0.0818 - val_loss: 61.1697 - val_accuracy: 0.0682\n",
      "Epoch 20/1500\n",
      "83/83 [==============================] - 0s 627us/step - loss: 35.6905 - accuracy: 0.0909 - val_loss: 48.4516 - val_accuracy: 0.1227\n",
      "Epoch 21/1500\n",
      "83/83 [==============================] - 0s 585us/step - loss: 34.1486 - accuracy: 0.0818 - val_loss: 37.5075 - val_accuracy: 0.0773\n",
      "Epoch 22/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 37.6622 - accuracy: 0.0803 - val_loss: 46.1825 - val_accuracy: 0.0773\n",
      "Epoch 23/1500\n",
      "83/83 [==============================] - 0s 642us/step - loss: 37.4767 - accuracy: 0.0773 - val_loss: 43.4509 - val_accuracy: 0.0864\n",
      "Epoch 24/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 28.2775 - accuracy: 0.1045 - val_loss: 24.0084 - val_accuracy: 0.1000\n",
      "Epoch 25/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 24.2422 - accuracy: 0.1258 - val_loss: 21.0439 - val_accuracy: 0.0500\n",
      "Epoch 26/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 35.0230 - accuracy: 0.0985 - val_loss: 23.7875 - val_accuracy: 0.0955\n",
      "Epoch 27/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 26.9854 - accuracy: 0.1030 - val_loss: 40.7790 - val_accuracy: 0.1136\n",
      "Epoch 28/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 26.1789 - accuracy: 0.0879 - val_loss: 21.4275 - val_accuracy: 0.0727\n",
      "Epoch 29/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 23.3919 - accuracy: 0.0879 - val_loss: 20.5351 - val_accuracy: 0.0682\n",
      "Epoch 30/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 30.2446 - accuracy: 0.0773 - val_loss: 26.0055 - val_accuracy: 0.1091\n",
      "Epoch 31/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 27.6146 - accuracy: 0.1106 - val_loss: 31.0711 - val_accuracy: 0.0773\n",
      "Epoch 32/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 30.2184 - accuracy: 0.1227 - val_loss: 31.3995 - val_accuracy: 0.1182\n",
      "Epoch 33/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 22.9384 - accuracy: 0.1242 - val_loss: 31.8691 - val_accuracy: 0.0591\n",
      "Epoch 34/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 32.4786 - accuracy: 0.1000 - val_loss: 36.5340 - val_accuracy: 0.1000\n",
      "Epoch 35/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 26.6918 - accuracy: 0.1121 - val_loss: 47.2623 - val_accuracy: 0.0773\n",
      "Epoch 36/1500\n",
      "83/83 [==============================] - 0s 627us/step - loss: 34.1606 - accuracy: 0.1000 - val_loss: 30.7877 - val_accuracy: 0.1409\n",
      "Epoch 37/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 30.1112 - accuracy: 0.0909 - val_loss: 31.3286 - val_accuracy: 0.1000\n",
      "Epoch 38/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 21.7897 - accuracy: 0.1197 - val_loss: 28.5551 - val_accuracy: 0.0773\n",
      "Epoch 39/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 22.8649 - accuracy: 0.1030 - val_loss: 32.5014 - val_accuracy: 0.1500\n",
      "Epoch 40/1500\n",
      "83/83 [==============================] - 0s 596us/step - loss: 26.6115 - accuracy: 0.1030 - val_loss: 27.0752 - val_accuracy: 0.0818\n",
      "Epoch 41/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 31.1094 - accuracy: 0.0985 - val_loss: 23.8980 - val_accuracy: 0.0864\n",
      "Epoch 42/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 20.0511 - accuracy: 0.1106 - val_loss: 28.2375 - val_accuracy: 0.0727\n",
      "Epoch 43/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 26.1239 - accuracy: 0.1197 - val_loss: 20.6710 - val_accuracy: 0.1273\n",
      "Epoch 44/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 20.7526 - accuracy: 0.1045 - val_loss: 15.5735 - val_accuracy: 0.0773\n",
      "Epoch 45/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 18.7056 - accuracy: 0.1076 - val_loss: 38.1456 - val_accuracy: 0.0682\n",
      "Epoch 46/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 21.0801 - accuracy: 0.0955 - val_loss: 17.6726 - val_accuracy: 0.0909\n",
      "Epoch 47/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 27.4689 - accuracy: 0.1045 - val_loss: 32.6443 - val_accuracy: 0.0727\n",
      "Epoch 48/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 23.3206 - accuracy: 0.1197 - val_loss: 33.8463 - val_accuracy: 0.0864\n",
      "Epoch 49/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 28.1906 - accuracy: 0.1136 - val_loss: 29.8284 - val_accuracy: 0.1227\n",
      "Epoch 50/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 24.7113 - accuracy: 0.1076 - val_loss: 29.4645 - val_accuracy: 0.0818\n",
      "Epoch 51/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 23.9971 - accuracy: 0.1091 - val_loss: 41.5083 - val_accuracy: 0.0727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 20.0364 - accuracy: 0.1197 - val_loss: 20.4250 - val_accuracy: 0.0727\n",
      "Epoch 53/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 17.8072 - accuracy: 0.1136 - val_loss: 18.1075 - val_accuracy: 0.0591\n",
      "Epoch 54/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 19.9017 - accuracy: 0.1288 - val_loss: 22.0233 - val_accuracy: 0.0636\n",
      "Epoch 55/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 25.2230 - accuracy: 0.1167 - val_loss: 18.4040 - val_accuracy: 0.0773\n",
      "Epoch 56/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 25.7067 - accuracy: 0.1106 - val_loss: 34.5734 - val_accuracy: 0.1182\n",
      "Epoch 57/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 21.8691 - accuracy: 0.1333 - val_loss: 24.3020 - val_accuracy: 0.0636\n",
      "Epoch 58/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 18.3957 - accuracy: 0.1061 - val_loss: 37.7053 - val_accuracy: 0.0773\n",
      "Epoch 59/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 23.4549 - accuracy: 0.1015 - val_loss: 25.1496 - val_accuracy: 0.0636\n",
      "Epoch 60/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 19.4957 - accuracy: 0.1273 - val_loss: 28.4066 - val_accuracy: 0.0727\n",
      "Epoch 61/1500\n",
      "83/83 [==============================] - 0s 684us/step - loss: 25.6649 - accuracy: 0.1045 - val_loss: 30.8313 - val_accuracy: 0.0909\n",
      "Epoch 62/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 22.0571 - accuracy: 0.1076 - val_loss: 24.2258 - val_accuracy: 0.0818\n",
      "Epoch 63/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 23.5551 - accuracy: 0.1227 - val_loss: 22.6478 - val_accuracy: 0.0818\n",
      "Epoch 64/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 20.8200 - accuracy: 0.1227 - val_loss: 19.9775 - val_accuracy: 0.1000\n",
      "Epoch 65/1500\n",
      "83/83 [==============================] - 0s 594us/step - loss: 24.2787 - accuracy: 0.1182 - val_loss: 16.4759 - val_accuracy: 0.0545\n",
      "Epoch 66/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 22.0547 - accuracy: 0.1394 - val_loss: 17.9637 - val_accuracy: 0.1045\n",
      "Epoch 67/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 15.1816 - accuracy: 0.0939 - val_loss: 10.7194 - val_accuracy: 0.0864\n",
      "Epoch 68/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 18.5949 - accuracy: 0.1106 - val_loss: 15.2112 - val_accuracy: 0.0591\n",
      "Epoch 69/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 18.8820 - accuracy: 0.1152 - val_loss: 17.5807 - val_accuracy: 0.0818\n",
      "Epoch 70/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 20.5475 - accuracy: 0.1273 - val_loss: 16.8915 - val_accuracy: 0.0591\n",
      "Epoch 71/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 16.5569 - accuracy: 0.0939 - val_loss: 18.7250 - val_accuracy: 0.1136\n",
      "Epoch 72/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 17.8199 - accuracy: 0.1500 - val_loss: 16.5882 - val_accuracy: 0.0818\n",
      "Epoch 73/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 17.2292 - accuracy: 0.1091 - val_loss: 17.2320 - val_accuracy: 0.1227\n",
      "Epoch 74/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 17.0163 - accuracy: 0.1167 - val_loss: 14.4446 - val_accuracy: 0.1045\n",
      "Epoch 75/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 18.1419 - accuracy: 0.1076 - val_loss: 22.0188 - val_accuracy: 0.0864\n",
      "Epoch 76/1500\n",
      "83/83 [==============================] - 0s 593us/step - loss: 16.1327 - accuracy: 0.1000 - val_loss: 11.4547 - val_accuracy: 0.1455\n",
      "Epoch 77/1500\n",
      "83/83 [==============================] - 0s 595us/step - loss: 17.9877 - accuracy: 0.1242 - val_loss: 20.9205 - val_accuracy: 0.0909\n",
      "Epoch 78/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 17.0252 - accuracy: 0.1197 - val_loss: 24.6514 - val_accuracy: 0.0864\n",
      "Epoch 79/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 18.4895 - accuracy: 0.1030 - val_loss: 23.0782 - val_accuracy: 0.1318\n",
      "Epoch 80/1500\n",
      "83/83 [==============================] - 0s 595us/step - loss: 22.2363 - accuracy: 0.1136 - val_loss: 34.3288 - val_accuracy: 0.0909\n",
      "Epoch 81/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 22.3776 - accuracy: 0.1091 - val_loss: 21.0592 - val_accuracy: 0.0818\n",
      "Epoch 82/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 20.2017 - accuracy: 0.0818 - val_loss: 18.9368 - val_accuracy: 0.0682\n",
      "Epoch 83/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 16.0042 - accuracy: 0.1273 - val_loss: 14.1795 - val_accuracy: 0.0682\n",
      "Epoch 84/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 13.4312 - accuracy: 0.1197 - val_loss: 12.8439 - val_accuracy: 0.1091\n",
      "Epoch 85/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 15.8380 - accuracy: 0.1212 - val_loss: 19.0269 - val_accuracy: 0.1091\n",
      "Epoch 86/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 11.9593 - accuracy: 0.1379 - val_loss: 20.1278 - val_accuracy: 0.1318\n",
      "Epoch 87/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 15.7423 - accuracy: 0.1167 - val_loss: 24.2229 - val_accuracy: 0.1545\n",
      "Epoch 88/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 15.6517 - accuracy: 0.1030 - val_loss: 15.0062 - val_accuracy: 0.0955\n",
      "Epoch 89/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 14.5546 - accuracy: 0.1242 - val_loss: 15.0106 - val_accuracy: 0.0773\n",
      "Epoch 90/1500\n",
      "83/83 [==============================] - 0s 596us/step - loss: 12.2894 - accuracy: 0.1379 - val_loss: 15.6749 - val_accuracy: 0.0818\n",
      "Epoch 91/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 16.9070 - accuracy: 0.1258 - val_loss: 15.8522 - val_accuracy: 0.0773\n",
      "Epoch 92/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 12.3663 - accuracy: 0.1348 - val_loss: 26.7041 - val_accuracy: 0.0545\n",
      "Epoch 93/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 15.1346 - accuracy: 0.1242 - val_loss: 14.2019 - val_accuracy: 0.1455\n",
      "Epoch 94/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 14.9667 - accuracy: 0.1439 - val_loss: 17.5693 - val_accuracy: 0.0955\n",
      "Epoch 95/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 14.9373 - accuracy: 0.1212 - val_loss: 19.8168 - val_accuracy: 0.0864\n",
      "Epoch 96/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 15.2700 - accuracy: 0.1242 - val_loss: 11.9857 - val_accuracy: 0.1545\n",
      "Epoch 97/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 14.4512 - accuracy: 0.1061 - val_loss: 17.8964 - val_accuracy: 0.1000\n",
      "Epoch 98/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 14.3580 - accuracy: 0.1273 - val_loss: 13.7978 - val_accuracy: 0.1045\n",
      "Epoch 99/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 10.1092 - accuracy: 0.1258 - val_loss: 12.8069 - val_accuracy: 0.1364\n",
      "Epoch 100/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 12.7965 - accuracy: 0.1303 - val_loss: 10.1432 - val_accuracy: 0.1136\n",
      "Epoch 101/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 9.8508 - accuracy: 0.1530 - val_loss: 10.8122 - val_accuracy: 0.1000\n",
      "Epoch 102/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 14.2572 - accuracy: 0.1348 - val_loss: 13.1762 - val_accuracy: 0.1318\n",
      "Epoch 103/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 14.3705 - accuracy: 0.1258 - val_loss: 22.6907 - val_accuracy: 0.1318\n",
      "Epoch 104/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 14.0858 - accuracy: 0.1152 - val_loss: 13.0681 - val_accuracy: 0.0773\n",
      "Epoch 105/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 13.7201 - accuracy: 0.0985 - val_loss: 17.3808 - val_accuracy: 0.1545\n",
      "Epoch 106/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 13.6896 - accuracy: 0.1152 - val_loss: 9.5992 - val_accuracy: 0.0818\n",
      "Epoch 107/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 10.9117 - accuracy: 0.1515 - val_loss: 14.4890 - val_accuracy: 0.0818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 12.4786 - accuracy: 0.1167 - val_loss: 13.0305 - val_accuracy: 0.0955\n",
      "Epoch 109/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 12.1079 - accuracy: 0.1167 - val_loss: 12.6598 - val_accuracy: 0.0818\n",
      "Epoch 110/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 15.4191 - accuracy: 0.1318 - val_loss: 10.5289 - val_accuracy: 0.0818\n",
      "Epoch 111/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 11.0428 - accuracy: 0.1333 - val_loss: 13.6166 - val_accuracy: 0.1000\n",
      "Epoch 112/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 10.7523 - accuracy: 0.1394 - val_loss: 11.5651 - val_accuracy: 0.1227\n",
      "Epoch 113/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 9.1617 - accuracy: 0.1470 - val_loss: 8.3493 - val_accuracy: 0.1182\n",
      "Epoch 114/1500\n",
      "83/83 [==============================] - 0s 644us/step - loss: 9.2343 - accuracy: 0.1485 - val_loss: 11.1899 - val_accuracy: 0.0864\n",
      "Epoch 115/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 10.3694 - accuracy: 0.1273 - val_loss: 11.7138 - val_accuracy: 0.0955\n",
      "Epoch 116/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 8.6885 - accuracy: 0.1500 - val_loss: 12.2074 - val_accuracy: 0.0955\n",
      "Epoch 117/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 10.1723 - accuracy: 0.1167 - val_loss: 8.5716 - val_accuracy: 0.0818\n",
      "Epoch 118/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 10.8625 - accuracy: 0.1424 - val_loss: 11.5032 - val_accuracy: 0.0682\n",
      "Epoch 119/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 8.9459 - accuracy: 0.1364 - val_loss: 8.8332 - val_accuracy: 0.0955\n",
      "Epoch 120/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 11.3278 - accuracy: 0.1379 - val_loss: 14.5660 - val_accuracy: 0.0909\n",
      "Epoch 121/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 9.4594 - accuracy: 0.1485 - val_loss: 11.6828 - val_accuracy: 0.0909\n",
      "Epoch 122/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 10.2352 - accuracy: 0.1258 - val_loss: 14.2041 - val_accuracy: 0.1227\n",
      "Epoch 123/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 10.6161 - accuracy: 0.1591 - val_loss: 15.6232 - val_accuracy: 0.1045\n",
      "Epoch 124/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 10.3348 - accuracy: 0.1500 - val_loss: 16.4364 - val_accuracy: 0.1000\n",
      "Epoch 125/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 11.8243 - accuracy: 0.1470 - val_loss: 9.3424 - val_accuracy: 0.1409\n",
      "Epoch 126/1500\n",
      "83/83 [==============================] - 0s 596us/step - loss: 10.1786 - accuracy: 0.1409 - val_loss: 9.8976 - val_accuracy: 0.1318\n",
      "Epoch 127/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 9.2374 - accuracy: 0.1576 - val_loss: 11.2394 - val_accuracy: 0.1727\n",
      "Epoch 128/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 8.9035 - accuracy: 0.1515 - val_loss: 7.7235 - val_accuracy: 0.1818\n",
      "Epoch 129/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 9.2796 - accuracy: 0.1470 - val_loss: 10.4890 - val_accuracy: 0.1000\n",
      "Epoch 130/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 8.3005 - accuracy: 0.1606 - val_loss: 8.6271 - val_accuracy: 0.1045\n",
      "Epoch 131/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 7.9458 - accuracy: 0.1409 - val_loss: 7.2470 - val_accuracy: 0.1000\n",
      "Epoch 132/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 6.0553 - accuracy: 0.1727 - val_loss: 8.3058 - val_accuracy: 0.1591\n",
      "Epoch 133/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 6.9720 - accuracy: 0.1409 - val_loss: 6.0963 - val_accuracy: 0.1409\n",
      "Epoch 134/1500\n",
      "83/83 [==============================] - 0s 594us/step - loss: 7.5658 - accuracy: 0.1606 - val_loss: 7.1705 - val_accuracy: 0.0955\n",
      "Epoch 135/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 7.0971 - accuracy: 0.1530 - val_loss: 10.7704 - val_accuracy: 0.1409\n",
      "Epoch 136/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 8.6992 - accuracy: 0.1561 - val_loss: 6.3790 - val_accuracy: 0.1727\n",
      "Epoch 137/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 7.1020 - accuracy: 0.1561 - val_loss: 8.2732 - val_accuracy: 0.1455\n",
      "Epoch 138/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 7.0142 - accuracy: 0.1606 - val_loss: 8.5434 - val_accuracy: 0.0773\n",
      "Epoch 139/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 6.7785 - accuracy: 0.1379 - val_loss: 6.4500 - val_accuracy: 0.1045\n",
      "Epoch 140/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 8.3910 - accuracy: 0.1606 - val_loss: 7.5280 - val_accuracy: 0.1273\n",
      "Epoch 141/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 7.0401 - accuracy: 0.1394 - val_loss: 6.9533 - val_accuracy: 0.1000\n",
      "Epoch 142/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 6.3652 - accuracy: 0.1727 - val_loss: 9.7011 - val_accuracy: 0.0955\n",
      "Epoch 143/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 6.8876 - accuracy: 0.1712 - val_loss: 7.2541 - val_accuracy: 0.1545\n",
      "Epoch 144/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 6.7419 - accuracy: 0.1470 - val_loss: 6.0828 - val_accuracy: 0.1227\n",
      "Epoch 145/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 6.0990 - accuracy: 0.1712 - val_loss: 6.6028 - val_accuracy: 0.0773\n",
      "Epoch 146/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 6.6494 - accuracy: 0.1773 - val_loss: 6.1700 - val_accuracy: 0.0955\n",
      "Epoch 147/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 6.7167 - accuracy: 0.1621 - val_loss: 8.2973 - val_accuracy: 0.0909\n",
      "Epoch 148/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 5.8971 - accuracy: 0.1727 - val_loss: 8.0758 - val_accuracy: 0.1045\n",
      "Epoch 149/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 6.2911 - accuracy: 0.1652 - val_loss: 6.7978 - val_accuracy: 0.1864\n",
      "Epoch 150/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 5.8767 - accuracy: 0.1455 - val_loss: 7.7061 - val_accuracy: 0.0955\n",
      "Epoch 151/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 5.2562 - accuracy: 0.1742 - val_loss: 5.2636 - val_accuracy: 0.0909\n",
      "Epoch 152/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 5.3762 - accuracy: 0.1742 - val_loss: 6.9618 - val_accuracy: 0.1318\n",
      "Epoch 153/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 5.3758 - accuracy: 0.1818 - val_loss: 6.1853 - val_accuracy: 0.1182\n",
      "Epoch 154/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 6.1516 - accuracy: 0.1561 - val_loss: 6.3313 - val_accuracy: 0.0818\n",
      "Epoch 155/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 5.8092 - accuracy: 0.1621 - val_loss: 8.0828 - val_accuracy: 0.1409\n",
      "Epoch 156/1500\n",
      "83/83 [==============================] - 0s 627us/step - loss: 4.7836 - accuracy: 0.1758 - val_loss: 5.7785 - val_accuracy: 0.2045\n",
      "Epoch 157/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 6.3289 - accuracy: 0.1894 - val_loss: 4.9391 - val_accuracy: 0.1136\n",
      "Epoch 158/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 4.2350 - accuracy: 0.1985 - val_loss: 6.2297 - val_accuracy: 0.1182\n",
      "Epoch 159/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 6.0715 - accuracy: 0.1697 - val_loss: 6.4897 - val_accuracy: 0.0818\n",
      "Epoch 160/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 5.2219 - accuracy: 0.1591 - val_loss: 6.1653 - val_accuracy: 0.1227\n",
      "Epoch 161/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 5.0632 - accuracy: 0.1788 - val_loss: 5.4675 - val_accuracy: 0.1091\n",
      "Epoch 162/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 5.1765 - accuracy: 0.1591 - val_loss: 5.7207 - val_accuracy: 0.1091\n",
      "Epoch 163/1500\n",
      "83/83 [==============================] - 0s 595us/step - loss: 4.7429 - accuracy: 0.1848 - val_loss: 4.2645 - val_accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 4.3317 - accuracy: 0.1803 - val_loss: 4.5244 - val_accuracy: 0.1364\n",
      "Epoch 165/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 3.8616 - accuracy: 0.2091 - val_loss: 3.4163 - val_accuracy: 0.1227\n",
      "Epoch 166/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 3.8530 - accuracy: 0.2197 - val_loss: 5.4812 - val_accuracy: 0.1909\n",
      "Epoch 167/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 3.3340 - accuracy: 0.1924 - val_loss: 4.1327 - val_accuracy: 0.0955\n",
      "Epoch 168/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 4.3189 - accuracy: 0.2000 - val_loss: 4.3478 - val_accuracy: 0.1364\n",
      "Epoch 169/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 3.7987 - accuracy: 0.2076 - val_loss: 3.9652 - val_accuracy: 0.1318\n",
      "Epoch 170/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 3.3012 - accuracy: 0.2197 - val_loss: 3.8577 - val_accuracy: 0.1727\n",
      "Epoch 171/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 3.3316 - accuracy: 0.1909 - val_loss: 4.5995 - val_accuracy: 0.1682\n",
      "Epoch 172/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 3.6799 - accuracy: 0.2167 - val_loss: 4.5333 - val_accuracy: 0.1955\n",
      "Epoch 173/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 3.2922 - accuracy: 0.2258 - val_loss: 3.5304 - val_accuracy: 0.1727\n",
      "Epoch 174/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 3.5180 - accuracy: 0.1909 - val_loss: 4.1728 - val_accuracy: 0.1409\n",
      "Epoch 175/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 3.3164 - accuracy: 0.2273 - val_loss: 3.3247 - val_accuracy: 0.1409\n",
      "Epoch 176/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 2.6834 - accuracy: 0.2364 - val_loss: 3.6230 - val_accuracy: 0.1045\n",
      "Epoch 177/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 2.9819 - accuracy: 0.2515 - val_loss: 4.3473 - val_accuracy: 0.1091\n",
      "Epoch 178/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 3.1570 - accuracy: 0.2152 - val_loss: 4.0879 - val_accuracy: 0.1500\n",
      "Epoch 179/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 2.9019 - accuracy: 0.2045 - val_loss: 3.1240 - val_accuracy: 0.2364\n",
      "Epoch 180/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 3.2065 - accuracy: 0.2318 - val_loss: 3.4435 - val_accuracy: 0.1636\n",
      "Epoch 181/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 2.8946 - accuracy: 0.2303 - val_loss: 3.5064 - val_accuracy: 0.1955\n",
      "Epoch 182/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 2.6604 - accuracy: 0.2348 - val_loss: 3.4378 - val_accuracy: 0.1955\n",
      "Epoch 183/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 2.6408 - accuracy: 0.2242 - val_loss: 3.5860 - val_accuracy: 0.1318\n",
      "Epoch 184/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 2.4357 - accuracy: 0.2591 - val_loss: 2.9033 - val_accuracy: 0.1682\n",
      "Epoch 185/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 2.5543 - accuracy: 0.2636 - val_loss: 3.3827 - val_accuracy: 0.1182\n",
      "Epoch 186/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 2.4985 - accuracy: 0.2606 - val_loss: 3.1515 - val_accuracy: 0.2091\n",
      "Epoch 187/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 2.3550 - accuracy: 0.2576 - val_loss: 3.3329 - val_accuracy: 0.1818\n",
      "Epoch 188/1500\n",
      "83/83 [==============================] - 0s 671us/step - loss: 2.2906 - accuracy: 0.2455 - val_loss: 3.3976 - val_accuracy: 0.1500\n",
      "Epoch 189/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 2.5128 - accuracy: 0.2848 - val_loss: 3.7913 - val_accuracy: 0.0955\n",
      "Epoch 190/1500\n",
      "83/83 [==============================] - 0s 595us/step - loss: 2.3041 - accuracy: 0.2894 - val_loss: 2.9359 - val_accuracy: 0.1591\n",
      "Epoch 191/1500\n",
      "83/83 [==============================] - 0s 627us/step - loss: 2.1707 - accuracy: 0.2970 - val_loss: 2.9123 - val_accuracy: 0.1636\n",
      "Epoch 192/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 2.1403 - accuracy: 0.2879 - val_loss: 2.9635 - val_accuracy: 0.2000\n",
      "Epoch 193/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 2.0632 - accuracy: 0.2788 - val_loss: 3.2035 - val_accuracy: 0.1773\n",
      "Epoch 194/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 2.1247 - accuracy: 0.2788 - val_loss: 2.6119 - val_accuracy: 0.2864\n",
      "Epoch 195/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 2.1033 - accuracy: 0.2621 - val_loss: 2.9411 - val_accuracy: 0.1591\n",
      "Epoch 196/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 2.0773 - accuracy: 0.2909 - val_loss: 2.6794 - val_accuracy: 0.2136\n",
      "Epoch 197/1500\n",
      "83/83 [==============================] - 0s 625us/step - loss: 2.0279 - accuracy: 0.2848 - val_loss: 2.8286 - val_accuracy: 0.1682\n",
      "Epoch 198/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 2.0762 - accuracy: 0.2606 - val_loss: 2.8323 - val_accuracy: 0.1727\n",
      "Epoch 199/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 1.9935 - accuracy: 0.3030 - val_loss: 2.6932 - val_accuracy: 0.2591\n",
      "Epoch 200/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 2.0031 - accuracy: 0.3136 - val_loss: 2.6309 - val_accuracy: 0.2591\n",
      "Epoch 201/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 1.9313 - accuracy: 0.3121 - val_loss: 2.7851 - val_accuracy: 0.1500\n",
      "Epoch 202/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 1.9339 - accuracy: 0.3030 - val_loss: 2.6515 - val_accuracy: 0.2227\n",
      "Epoch 203/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 1.8904 - accuracy: 0.3136 - val_loss: 2.5793 - val_accuracy: 0.2545\n",
      "Epoch 204/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 1.8329 - accuracy: 0.3273 - val_loss: 2.6838 - val_accuracy: 0.1955\n",
      "Epoch 205/1500\n",
      "83/83 [==============================] - 0s 631us/step - loss: 1.8986 - accuracy: 0.3167 - val_loss: 2.6533 - val_accuracy: 0.2045\n",
      "Epoch 206/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 1.8563 - accuracy: 0.3258 - val_loss: 2.7299 - val_accuracy: 0.1682\n",
      "Epoch 207/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 1.8058 - accuracy: 0.3333 - val_loss: 2.7261 - val_accuracy: 0.1500\n",
      "Epoch 208/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 1.7872 - accuracy: 0.3364 - val_loss: 2.8201 - val_accuracy: 0.1500\n",
      "Epoch 209/1500\n",
      "83/83 [==============================] - 0s 661us/step - loss: 1.7688 - accuracy: 0.3697 - val_loss: 2.5797 - val_accuracy: 0.2455\n",
      "Epoch 210/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 1.7590 - accuracy: 0.3848 - val_loss: 2.6334 - val_accuracy: 0.2500\n",
      "Epoch 211/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 1.7408 - accuracy: 0.3742 - val_loss: 2.7238 - val_accuracy: 0.1773\n",
      "Epoch 212/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 1.7987 - accuracy: 0.3545 - val_loss: 2.6458 - val_accuracy: 0.1909\n",
      "Epoch 213/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 1.7961 - accuracy: 0.3303 - val_loss: 2.5966 - val_accuracy: 0.2773\n",
      "Epoch 214/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 1.6989 - accuracy: 0.3591 - val_loss: 2.5937 - val_accuracy: 0.2591\n",
      "Epoch 215/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 1.7602 - accuracy: 0.3697 - val_loss: 2.9509 - val_accuracy: 0.1273\n",
      "Epoch 216/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 1.7659 - accuracy: 0.3606 - val_loss: 2.9982 - val_accuracy: 0.1545\n",
      "Epoch 217/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 1.7692 - accuracy: 0.3818 - val_loss: 2.6517 - val_accuracy: 0.2591\n",
      "Epoch 218/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 1.7369 - accuracy: 0.3682 - val_loss: 2.9119 - val_accuracy: 0.1727\n",
      "Epoch 219/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 1.6633 - accuracy: 0.3955 - val_loss: 2.7011 - val_accuracy: 0.2136\n",
      "Epoch 220/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 1.8084 - accuracy: 0.3545 - val_loss: 2.7702 - val_accuracy: 0.1864\n",
      "Epoch 221/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 1.7012 - accuracy: 0.3848 - val_loss: 2.9188 - val_accuracy: 0.2318\n",
      "Epoch 222/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 1.7180 - accuracy: 0.3561 - val_loss: 2.7895 - val_accuracy: 0.1864\n",
      "Epoch 223/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 1.6917 - accuracy: 0.3758 - val_loss: 3.0231 - val_accuracy: 0.1773\n",
      "Epoch 224/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 1.8130 - accuracy: 0.3197 - val_loss: 2.7069 - val_accuracy: 0.2636\n",
      "Epoch 225/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 1.6845 - accuracy: 0.3803 - val_loss: 2.9273 - val_accuracy: 0.2136\n",
      "Epoch 226/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 1.6732 - accuracy: 0.3864 - val_loss: 2.9718 - val_accuracy: 0.2136\n",
      "Epoch 227/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 1.6597 - accuracy: 0.3924 - val_loss: 2.6631 - val_accuracy: 0.2636\n",
      "Epoch 228/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 1.5912 - accuracy: 0.4091 - val_loss: 2.8143 - val_accuracy: 0.1909\n",
      "Epoch 229/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 1.6334 - accuracy: 0.3833 - val_loss: 2.8380 - val_accuracy: 0.1591\n",
      "Epoch 230/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 1.6507 - accuracy: 0.3909 - val_loss: 2.7434 - val_accuracy: 0.2773\n",
      "Epoch 231/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 1.7797 - accuracy: 0.3667 - val_loss: 2.9004 - val_accuracy: 0.1773\n",
      "Epoch 232/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 1.7944 - accuracy: 0.3303 - val_loss: 2.8042 - val_accuracy: 0.2364\n",
      "Epoch 233/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 1.6726 - accuracy: 0.3879 - val_loss: 2.8529 - val_accuracy: 0.2182\n",
      "Epoch 234/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 1.6661 - accuracy: 0.3970 - val_loss: 2.7752 - val_accuracy: 0.2182\n",
      "Epoch 235/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 1.6557 - accuracy: 0.3924 - val_loss: 2.5701 - val_accuracy: 0.2682\n",
      "Epoch 236/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 1.6805 - accuracy: 0.4045 - val_loss: 2.7866 - val_accuracy: 0.2091\n",
      "Epoch 237/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 1.6313 - accuracy: 0.4000 - val_loss: 2.6643 - val_accuracy: 0.2455\n",
      "Epoch 238/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 1.5796 - accuracy: 0.4242 - val_loss: 2.6009 - val_accuracy: 0.2636\n",
      "Epoch 239/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 1.5970 - accuracy: 0.4076 - val_loss: 2.7119 - val_accuracy: 0.2636\n",
      "Epoch 240/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 1.4897 - accuracy: 0.4530 - val_loss: 2.9929 - val_accuracy: 0.1727\n",
      "Epoch 241/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 1.5646 - accuracy: 0.4273 - val_loss: 3.1555 - val_accuracy: 0.2136\n",
      "Epoch 242/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 1.7566 - accuracy: 0.3561 - val_loss: 2.9706 - val_accuracy: 0.2136\n",
      "Epoch 243/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 1.6083 - accuracy: 0.4106 - val_loss: 2.9054 - val_accuracy: 0.2545\n",
      "Epoch 244/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 1.5851 - accuracy: 0.4045 - val_loss: 2.8281 - val_accuracy: 0.2636\n",
      "Epoch 245/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 1.6512 - accuracy: 0.4439 - val_loss: 2.9052 - val_accuracy: 0.2318\n",
      "Epoch 246/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 1.6582 - accuracy: 0.4152 - val_loss: 3.1321 - val_accuracy: 0.1864\n",
      "Epoch 247/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 1.6015 - accuracy: 0.4136 - val_loss: 2.7297 - val_accuracy: 0.2909\n",
      "Epoch 248/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 1.4993 - accuracy: 0.4667 - val_loss: 2.6960 - val_accuracy: 0.2545\n",
      "Epoch 249/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 1.4250 - accuracy: 0.5061 - val_loss: 2.7278 - val_accuracy: 0.2227\n",
      "Epoch 250/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 1.3544 - accuracy: 0.4955 - val_loss: 2.9224 - val_accuracy: 0.2318\n",
      "Epoch 251/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 1.5329 - accuracy: 0.4439 - val_loss: 2.9119 - val_accuracy: 0.1773\n",
      "Epoch 252/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 1.5479 - accuracy: 0.4379 - val_loss: 2.7200 - val_accuracy: 0.2591\n",
      "Epoch 253/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 1.4534 - accuracy: 0.4788 - val_loss: 2.9638 - val_accuracy: 0.2500\n",
      "Epoch 254/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 1.5012 - accuracy: 0.4545 - val_loss: 2.9589 - val_accuracy: 0.2091\n",
      "Epoch 255/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 1.5222 - accuracy: 0.4530 - val_loss: 2.9236 - val_accuracy: 0.2364\n",
      "Epoch 256/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 1.5607 - accuracy: 0.4121 - val_loss: 2.9508 - val_accuracy: 0.2409\n",
      "Epoch 257/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 1.5008 - accuracy: 0.4545 - val_loss: 2.7606 - val_accuracy: 0.2682\n",
      "Epoch 258/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 1.4227 - accuracy: 0.4864 - val_loss: 2.7326 - val_accuracy: 0.2955\n",
      "Epoch 259/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 1.5053 - accuracy: 0.4561 - val_loss: 2.7640 - val_accuracy: 0.2091\n",
      "Epoch 260/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 1.3554 - accuracy: 0.5091 - val_loss: 2.7240 - val_accuracy: 0.2909\n",
      "Epoch 261/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 1.3352 - accuracy: 0.4939 - val_loss: 2.8807 - val_accuracy: 0.2045\n",
      "Epoch 262/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 1.4130 - accuracy: 0.4803 - val_loss: 2.8920 - val_accuracy: 0.2682\n",
      "Epoch 263/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 1.5403 - accuracy: 0.4364 - val_loss: 2.8291 - val_accuracy: 0.2455\n",
      "Epoch 264/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 1.4525 - accuracy: 0.4515 - val_loss: 2.7267 - val_accuracy: 0.3318\n",
      "Epoch 265/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 1.3850 - accuracy: 0.5030 - val_loss: 2.7858 - val_accuracy: 0.2273\n",
      "Epoch 266/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 1.3852 - accuracy: 0.4818 - val_loss: 3.0499 - val_accuracy: 0.2273\n",
      "Epoch 267/1500\n",
      "83/83 [==============================] - 0s 596us/step - loss: 1.4889 - accuracy: 0.4682 - val_loss: 2.9706 - val_accuracy: 0.2500\n",
      "Epoch 268/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 1.3456 - accuracy: 0.5121 - val_loss: 2.9314 - val_accuracy: 0.2636\n",
      "Epoch 269/1500\n",
      "83/83 [==============================] - 0s 595us/step - loss: 1.3116 - accuracy: 0.5167 - val_loss: 2.9686 - val_accuracy: 0.2409\n",
      "Epoch 270/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 1.3001 - accuracy: 0.5303 - val_loss: 2.9184 - val_accuracy: 0.2636\n",
      "Epoch 271/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 1.4307 - accuracy: 0.4727 - val_loss: 2.9502 - val_accuracy: 0.2500\n",
      "Epoch 272/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 1.3551 - accuracy: 0.5318 - val_loss: 2.9675 - val_accuracy: 0.1727\n",
      "Epoch 273/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 1.2740 - accuracy: 0.5500 - val_loss: 2.9251 - val_accuracy: 0.2909\n",
      "Epoch 274/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 1.3121 - accuracy: 0.5470 - val_loss: 3.1009 - val_accuracy: 0.2591\n",
      "Epoch 275/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 1.3169 - accuracy: 0.5197 - val_loss: 2.8146 - val_accuracy: 0.2409\n",
      "Epoch 276/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 612us/step - loss: 1.3617 - accuracy: 0.5106 - val_loss: 2.9353 - val_accuracy: 0.2500\n",
      "Epoch 277/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 1.3294 - accuracy: 0.5121 - val_loss: 2.9130 - val_accuracy: 0.3227\n",
      "Epoch 278/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 1.5129 - accuracy: 0.4439 - val_loss: 3.1068 - val_accuracy: 0.2273\n",
      "Epoch 279/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 1.3246 - accuracy: 0.5212 - val_loss: 3.0091 - val_accuracy: 0.2455\n",
      "Epoch 280/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 1.3308 - accuracy: 0.5227 - val_loss: 2.8490 - val_accuracy: 0.2545\n",
      "Epoch 281/1500\n",
      "83/83 [==============================] - 0s 593us/step - loss: 1.3160 - accuracy: 0.5121 - val_loss: 2.8585 - val_accuracy: 0.2955\n",
      "Epoch 282/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 1.2110 - accuracy: 0.5515 - val_loss: 3.0492 - val_accuracy: 0.3045\n",
      "Epoch 283/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 1.3511 - accuracy: 0.5076 - val_loss: 3.0543 - val_accuracy: 0.3045\n",
      "Epoch 284/1500\n",
      "83/83 [==============================] - 0s 592us/step - loss: 1.3692 - accuracy: 0.5197 - val_loss: 2.8937 - val_accuracy: 0.3000\n",
      "Epoch 285/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 1.2948 - accuracy: 0.5318 - val_loss: 3.1640 - val_accuracy: 0.2227\n",
      "Epoch 286/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 1.5062 - accuracy: 0.4636 - val_loss: 2.8516 - val_accuracy: 0.2409\n",
      "Epoch 287/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 1.4760 - accuracy: 0.4697 - val_loss: 2.9940 - val_accuracy: 0.2227\n",
      "Epoch 288/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 1.4512 - accuracy: 0.4833 - val_loss: 2.8699 - val_accuracy: 0.2955\n",
      "Epoch 289/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 1.1944 - accuracy: 0.5727 - val_loss: 3.1539 - val_accuracy: 0.2000\n",
      "Epoch 290/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 1.1971 - accuracy: 0.5955 - val_loss: 2.9834 - val_accuracy: 0.2955\n",
      "Epoch 291/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 1.2258 - accuracy: 0.5576 - val_loss: 2.9261 - val_accuracy: 0.2318\n",
      "Epoch 292/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 1.2487 - accuracy: 0.5364 - val_loss: 2.8595 - val_accuracy: 0.3136\n",
      "Epoch 293/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 1.2218 - accuracy: 0.5515 - val_loss: 3.0455 - val_accuracy: 0.2318\n",
      "Epoch 294/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 1.2541 - accuracy: 0.5500 - val_loss: 2.9146 - val_accuracy: 0.2136\n",
      "Epoch 295/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 1.1655 - accuracy: 0.5879 - val_loss: 3.0880 - val_accuracy: 0.2409\n",
      "Epoch 296/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 1.1696 - accuracy: 0.5667 - val_loss: 3.0694 - val_accuracy: 0.1955\n",
      "Epoch 297/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 1.2687 - accuracy: 0.5500 - val_loss: 3.1240 - val_accuracy: 0.2636\n",
      "Epoch 298/1500\n",
      "83/83 [==============================] - 0s 637us/step - loss: 1.1572 - accuracy: 0.5924 - val_loss: 2.9867 - val_accuracy: 0.1727\n",
      "Epoch 299/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 1.1625 - accuracy: 0.5742 - val_loss: 3.0900 - val_accuracy: 0.1818\n",
      "Epoch 300/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 1.3133 - accuracy: 0.5318 - val_loss: 3.1828 - val_accuracy: 0.2409\n",
      "Epoch 301/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 1.2282 - accuracy: 0.5545 - val_loss: 2.8449 - val_accuracy: 0.3091\n",
      "Epoch 302/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 1.3275 - accuracy: 0.5212 - val_loss: 2.9787 - val_accuracy: 0.1909\n",
      "Epoch 303/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 1.2652 - accuracy: 0.5333 - val_loss: 3.0648 - val_accuracy: 0.2318\n",
      "Epoch 304/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 1.1964 - accuracy: 0.5636 - val_loss: 2.9695 - val_accuracy: 0.3000\n",
      "Epoch 305/1500\n",
      "83/83 [==============================] - 0s 640us/step - loss: 1.0808 - accuracy: 0.6182 - val_loss: 3.4548 - val_accuracy: 0.1500\n",
      "Epoch 306/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 1.1749 - accuracy: 0.5818 - val_loss: 3.0799 - val_accuracy: 0.2500\n",
      "Epoch 307/1500\n",
      "83/83 [==============================] - 0s 594us/step - loss: 1.1754 - accuracy: 0.5667 - val_loss: 3.1696 - val_accuracy: 0.2136\n",
      "Epoch 308/1500\n",
      "83/83 [==============================] - 0s 676us/step - loss: 1.1486 - accuracy: 0.5848 - val_loss: 2.9690 - val_accuracy: 0.2955\n",
      "Epoch 309/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 1.0414 - accuracy: 0.6258 - val_loss: 2.8441 - val_accuracy: 0.3227\n",
      "Epoch 310/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 1.0959 - accuracy: 0.6136 - val_loss: 3.0973 - val_accuracy: 0.2909\n",
      "Epoch 311/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 1.0264 - accuracy: 0.6182 - val_loss: 2.7209 - val_accuracy: 0.3455\n",
      "Epoch 312/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 1.1141 - accuracy: 0.5970 - val_loss: 3.1026 - val_accuracy: 0.2364\n",
      "Epoch 313/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 1.1282 - accuracy: 0.5818 - val_loss: 2.9228 - val_accuracy: 0.2682\n",
      "Epoch 314/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 1.0334 - accuracy: 0.6348 - val_loss: 2.8075 - val_accuracy: 0.3091\n",
      "Epoch 315/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 1.2148 - accuracy: 0.5682 - val_loss: 3.1137 - val_accuracy: 0.2000\n",
      "Epoch 316/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 1.4262 - accuracy: 0.4894 - val_loss: 3.0530 - val_accuracy: 0.3000\n",
      "Epoch 317/1500\n",
      "83/83 [==============================] - 0s 592us/step - loss: 1.0268 - accuracy: 0.6227 - val_loss: 2.9760 - val_accuracy: 0.2727\n",
      "Epoch 318/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 1.1209 - accuracy: 0.5894 - val_loss: 3.0651 - val_accuracy: 0.2500\n",
      "Epoch 319/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 1.0522 - accuracy: 0.6212 - val_loss: 2.9120 - val_accuracy: 0.2409\n",
      "Epoch 320/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 1.0141 - accuracy: 0.6379 - val_loss: 2.9098 - val_accuracy: 0.2727\n",
      "Epoch 321/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 1.0248 - accuracy: 0.6379 - val_loss: 2.9667 - val_accuracy: 0.2545\n",
      "Epoch 322/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 1.3227 - accuracy: 0.5288 - val_loss: 3.0714 - val_accuracy: 0.2273\n",
      "Epoch 323/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 1.0343 - accuracy: 0.6258 - val_loss: 3.0583 - val_accuracy: 0.2682\n",
      "Epoch 324/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 1.1380 - accuracy: 0.6121 - val_loss: 3.0250 - val_accuracy: 0.2591\n",
      "Epoch 325/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.9558 - accuracy: 0.6727 - val_loss: 3.0986 - val_accuracy: 0.3091\n",
      "Epoch 326/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.9734 - accuracy: 0.6485 - val_loss: 2.9667 - val_accuracy: 0.2682\n",
      "Epoch 327/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.9766 - accuracy: 0.6576 - val_loss: 2.8565 - val_accuracy: 0.2773\n",
      "Epoch 328/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.8819 - accuracy: 0.7091 - val_loss: 2.9265 - val_accuracy: 0.3045\n",
      "Epoch 329/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.9042 - accuracy: 0.7000 - val_loss: 3.1597 - val_accuracy: 0.2591\n",
      "Epoch 330/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.9563 - accuracy: 0.6364 - val_loss: 2.9200 - val_accuracy: 0.3000\n",
      "Epoch 331/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.9431 - accuracy: 0.6667 - val_loss: 3.0112 - val_accuracy: 0.3182\n",
      "Epoch 332/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.9960 - accuracy: 0.6455 - val_loss: 3.1333 - val_accuracy: 0.1955\n",
      "Epoch 333/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.9519 - accuracy: 0.6667 - val_loss: 3.0576 - val_accuracy: 0.2864\n",
      "Epoch 334/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.9527 - accuracy: 0.6803 - val_loss: 3.0943 - val_accuracy: 0.3136\n",
      "Epoch 335/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.9064 - accuracy: 0.6818 - val_loss: 2.9095 - val_accuracy: 0.2955\n",
      "Epoch 336/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 1.1170 - accuracy: 0.6258 - val_loss: 2.9835 - val_accuracy: 0.3045\n",
      "Epoch 337/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.9947 - accuracy: 0.6409 - val_loss: 3.1309 - val_accuracy: 0.2136\n",
      "Epoch 338/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.8481 - accuracy: 0.7000 - val_loss: 3.2432 - val_accuracy: 0.2909\n",
      "Epoch 339/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.9691 - accuracy: 0.6803 - val_loss: 3.1914 - val_accuracy: 0.2091\n",
      "Epoch 340/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.9301 - accuracy: 0.6606 - val_loss: 3.1052 - val_accuracy: 0.2727\n",
      "Epoch 341/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 1.0628 - accuracy: 0.6333 - val_loss: 3.0089 - val_accuracy: 0.3091\n",
      "Epoch 342/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 1.0659 - accuracy: 0.6333 - val_loss: 3.1840 - val_accuracy: 0.2773\n",
      "Epoch 343/1500\n",
      "83/83 [==============================] - 0s 595us/step - loss: 1.1526 - accuracy: 0.5788 - val_loss: 3.3276 - val_accuracy: 0.2727\n",
      "Epoch 344/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 1.0081 - accuracy: 0.6242 - val_loss: 3.0573 - val_accuracy: 0.3318\n",
      "Epoch 345/1500\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.9072 - accuracy: 0.6833 - val_loss: 3.1897 - val_accuracy: 0.2864\n",
      "Epoch 346/1500\n",
      "83/83 [==============================] - 0s 593us/step - loss: 0.8527 - accuracy: 0.7106 - val_loss: 2.9149 - val_accuracy: 0.3227\n",
      "Epoch 347/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.8134 - accuracy: 0.7182 - val_loss: 3.1395 - val_accuracy: 0.2409\n",
      "Epoch 348/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.8406 - accuracy: 0.7030 - val_loss: 3.0674 - val_accuracy: 0.2682\n",
      "Epoch 349/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 1.0840 - accuracy: 0.6076 - val_loss: 3.0422 - val_accuracy: 0.3182\n",
      "Epoch 350/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.9021 - accuracy: 0.6803 - val_loss: 3.0719 - val_accuracy: 0.3182\n",
      "Epoch 351/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.8812 - accuracy: 0.6864 - val_loss: 3.1535 - val_accuracy: 0.2955\n",
      "Epoch 352/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.8388 - accuracy: 0.6894 - val_loss: 3.1274 - val_accuracy: 0.2909\n",
      "Epoch 353/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.8578 - accuracy: 0.6939 - val_loss: 3.3659 - val_accuracy: 0.2636\n",
      "Epoch 354/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.8906 - accuracy: 0.7030 - val_loss: 3.1185 - val_accuracy: 0.2909\n",
      "Epoch 355/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.8478 - accuracy: 0.6939 - val_loss: 3.4159 - val_accuracy: 0.2318\n",
      "Epoch 356/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.8059 - accuracy: 0.7409 - val_loss: 3.1509 - val_accuracy: 0.2864\n",
      "Epoch 357/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.8370 - accuracy: 0.7061 - val_loss: 3.0853 - val_accuracy: 0.3227\n",
      "Epoch 358/1500\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.8148 - accuracy: 0.7242 - val_loss: 3.5783 - val_accuracy: 0.1773\n",
      "Epoch 359/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 1.0586 - accuracy: 0.6303 - val_loss: 3.3450 - val_accuracy: 0.2364\n",
      "Epoch 360/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.8802 - accuracy: 0.6758 - val_loss: 3.3107 - val_accuracy: 0.3045\n",
      "Epoch 361/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.8414 - accuracy: 0.7076 - val_loss: 3.1850 - val_accuracy: 0.2955\n",
      "Epoch 362/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.8804 - accuracy: 0.7000 - val_loss: 3.2785 - val_accuracy: 0.3136\n",
      "Epoch 363/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.7956 - accuracy: 0.7136 - val_loss: 3.0490 - val_accuracy: 0.2955\n",
      "Epoch 364/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.7833 - accuracy: 0.7318 - val_loss: 3.1843 - val_accuracy: 0.2818\n",
      "Epoch 365/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.7861 - accuracy: 0.7197 - val_loss: 3.1691 - val_accuracy: 0.2955\n",
      "Epoch 366/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.8227 - accuracy: 0.7030 - val_loss: 3.3338 - val_accuracy: 0.2591\n",
      "Epoch 367/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.7639 - accuracy: 0.7288 - val_loss: 3.3747 - val_accuracy: 0.2818\n",
      "Epoch 368/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.8509 - accuracy: 0.7000 - val_loss: 3.0719 - val_accuracy: 0.3318\n",
      "Epoch 369/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.8534 - accuracy: 0.6924 - val_loss: 3.3246 - val_accuracy: 0.3182\n",
      "Epoch 370/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.9049 - accuracy: 0.6803 - val_loss: 3.2606 - val_accuracy: 0.3182\n",
      "Epoch 371/1500\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.8912 - accuracy: 0.6924 - val_loss: 3.5280 - val_accuracy: 0.2773\n",
      "Epoch 372/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.7841 - accuracy: 0.7318 - val_loss: 3.1557 - val_accuracy: 0.3000\n",
      "Epoch 373/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.8219 - accuracy: 0.6848 - val_loss: 3.5564 - val_accuracy: 0.2727\n",
      "Epoch 374/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.7906 - accuracy: 0.7242 - val_loss: 3.2770 - val_accuracy: 0.3045\n",
      "Epoch 375/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.7138 - accuracy: 0.7591 - val_loss: 3.1628 - val_accuracy: 0.3273\n",
      "Epoch 376/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.6938 - accuracy: 0.7727 - val_loss: 3.1590 - val_accuracy: 0.2591\n",
      "Epoch 377/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.8035 - accuracy: 0.6939 - val_loss: 3.3898 - val_accuracy: 0.2727\n",
      "Epoch 378/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 1.0241 - accuracy: 0.6167 - val_loss: 3.3349 - val_accuracy: 0.2864\n",
      "Epoch 379/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.9034 - accuracy: 0.6803 - val_loss: 3.5141 - val_accuracy: 0.2182\n",
      "Epoch 380/1500\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.8028 - accuracy: 0.7136 - val_loss: 3.2859 - val_accuracy: 0.2773\n",
      "Epoch 381/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.8538 - accuracy: 0.6939 - val_loss: 3.2863 - val_accuracy: 0.3227\n",
      "Epoch 382/1500\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.7674 - accuracy: 0.7424 - val_loss: 3.6381 - val_accuracy: 0.2045\n",
      "Epoch 383/1500\n",
      "83/83 [==============================] - 0s 596us/step - loss: 1.2164 - accuracy: 0.6015 - val_loss: 3.4790 - val_accuracy: 0.3182\n",
      "Epoch 384/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.8311 - accuracy: 0.6939 - val_loss: 3.2405 - val_accuracy: 0.3500\n",
      "Epoch 385/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.7907 - accuracy: 0.7136 - val_loss: 3.1973 - val_accuracy: 0.3000\n",
      "Epoch 386/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.8188 - accuracy: 0.6939 - val_loss: 3.4389 - val_accuracy: 0.2773\n",
      "Epoch 387/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.8712 - accuracy: 0.6636 - val_loss: 3.2479 - val_accuracy: 0.3409\n",
      "Epoch 388/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 612us/step - loss: 0.7769 - accuracy: 0.7121 - val_loss: 3.2028 - val_accuracy: 0.3000\n",
      "Epoch 389/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.7068 - accuracy: 0.7576 - val_loss: 3.4022 - val_accuracy: 0.2773\n",
      "Epoch 390/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.6935 - accuracy: 0.7636 - val_loss: 3.1810 - val_accuracy: 0.3318\n",
      "Epoch 391/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.8492 - accuracy: 0.6803 - val_loss: 3.4427 - val_accuracy: 0.2545\n",
      "Epoch 392/1500\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.9032 - accuracy: 0.6758 - val_loss: 3.1773 - val_accuracy: 0.3045\n",
      "Epoch 393/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.7083 - accuracy: 0.7379 - val_loss: 3.2694 - val_accuracy: 0.3364\n",
      "Epoch 394/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.7152 - accuracy: 0.7530 - val_loss: 3.1435 - val_accuracy: 0.3136\n",
      "Epoch 395/1500\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.6825 - accuracy: 0.7742 - val_loss: 3.7452 - val_accuracy: 0.2000\n",
      "Epoch 396/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.8150 - accuracy: 0.6939 - val_loss: 3.3396 - val_accuracy: 0.3364\n",
      "Epoch 397/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.7986 - accuracy: 0.7273 - val_loss: 3.4691 - val_accuracy: 0.2864\n",
      "Epoch 398/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.9270 - accuracy: 0.6939 - val_loss: 3.4605 - val_accuracy: 0.2500\n",
      "Epoch 399/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.7794 - accuracy: 0.7227 - val_loss: 3.4591 - val_accuracy: 0.3136\n",
      "Epoch 400/1500\n",
      "83/83 [==============================] - 0s 645us/step - loss: 0.6771 - accuracy: 0.7697 - val_loss: 3.2349 - val_accuracy: 0.3455\n",
      "Epoch 401/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.7688 - accuracy: 0.7379 - val_loss: 3.3779 - val_accuracy: 0.3364\n",
      "Epoch 402/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.7103 - accuracy: 0.7379 - val_loss: 3.3896 - val_accuracy: 0.2682\n",
      "Epoch 403/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.7881 - accuracy: 0.7242 - val_loss: 3.3500 - val_accuracy: 0.2773\n",
      "Epoch 404/1500\n",
      "83/83 [==============================] - 0s 595us/step - loss: 0.6777 - accuracy: 0.7803 - val_loss: 3.3128 - val_accuracy: 0.2636\n",
      "Epoch 405/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.7396 - accuracy: 0.7439 - val_loss: 3.3643 - val_accuracy: 0.2818\n",
      "Epoch 406/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.7010 - accuracy: 0.7545 - val_loss: 3.4194 - val_accuracy: 0.2682\n",
      "Epoch 407/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.7631 - accuracy: 0.7121 - val_loss: 3.3384 - val_accuracy: 0.3182\n",
      "Epoch 408/1500\n",
      "83/83 [==============================] - 0s 638us/step - loss: 0.7662 - accuracy: 0.7258 - val_loss: 3.6446 - val_accuracy: 0.2273\n",
      "Epoch 409/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.7048 - accuracy: 0.7606 - val_loss: 3.4199 - val_accuracy: 0.3091\n",
      "Epoch 410/1500\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.6118 - accuracy: 0.7924 - val_loss: 3.6157 - val_accuracy: 0.2818\n",
      "Epoch 411/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.6997 - accuracy: 0.7621 - val_loss: 3.3301 - val_accuracy: 0.3273\n",
      "Epoch 412/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.6068 - accuracy: 0.7939 - val_loss: 3.6609 - val_accuracy: 0.2409\n",
      "Epoch 413/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.7891 - accuracy: 0.7045 - val_loss: 3.3307 - val_accuracy: 0.3227\n",
      "Epoch 414/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.7995 - accuracy: 0.6909 - val_loss: 3.3627 - val_accuracy: 0.3182\n",
      "Epoch 415/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.6702 - accuracy: 0.7652 - val_loss: 3.3229 - val_accuracy: 0.3136\n",
      "Epoch 416/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 1.1308 - accuracy: 0.5833 - val_loss: 3.3784 - val_accuracy: 0.3273\n",
      "Epoch 417/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.6321 - accuracy: 0.7894 - val_loss: 3.4928 - val_accuracy: 0.3136\n",
      "Epoch 418/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.6932 - accuracy: 0.7455 - val_loss: 3.6449 - val_accuracy: 0.2818\n",
      "Epoch 419/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.7864 - accuracy: 0.7030 - val_loss: 3.5423 - val_accuracy: 0.2773\n",
      "Epoch 420/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.7009 - accuracy: 0.7409 - val_loss: 3.6643 - val_accuracy: 0.2545\n",
      "Epoch 421/1500\n",
      "83/83 [==============================] - 0s 657us/step - loss: 0.8331 - accuracy: 0.6773 - val_loss: 3.7641 - val_accuracy: 0.2864\n",
      "Epoch 422/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.6995 - accuracy: 0.7227 - val_loss: 3.4350 - val_accuracy: 0.3045\n",
      "Epoch 423/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.5605 - accuracy: 0.8091 - val_loss: 3.5040 - val_accuracy: 0.2500\n",
      "Epoch 424/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.5893 - accuracy: 0.8045 - val_loss: 3.2969 - val_accuracy: 0.3727\n",
      "Epoch 425/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.7403 - accuracy: 0.7197 - val_loss: 3.3742 - val_accuracy: 0.3182\n",
      "Epoch 426/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.6249 - accuracy: 0.7682 - val_loss: 3.5318 - val_accuracy: 0.3273\n",
      "Epoch 427/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.7027 - accuracy: 0.7606 - val_loss: 3.2956 - val_accuracy: 0.2727\n",
      "Epoch 428/1500\n",
      "83/83 [==============================] - 0s 593us/step - loss: 0.6383 - accuracy: 0.7636 - val_loss: 3.5269 - val_accuracy: 0.3545\n",
      "Epoch 429/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.6313 - accuracy: 0.7788 - val_loss: 3.3533 - val_accuracy: 0.3500\n",
      "Epoch 430/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.6341 - accuracy: 0.7727 - val_loss: 3.5304 - val_accuracy: 0.3409\n",
      "Epoch 431/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.5736 - accuracy: 0.8061 - val_loss: 3.4033 - val_accuracy: 0.3500\n",
      "Epoch 432/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.6377 - accuracy: 0.7788 - val_loss: 3.4154 - val_accuracy: 0.3455\n",
      "Epoch 433/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.6299 - accuracy: 0.7758 - val_loss: 3.4685 - val_accuracy: 0.3227\n",
      "Epoch 434/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.7172 - accuracy: 0.7545 - val_loss: 3.3829 - val_accuracy: 0.3364\n",
      "Epoch 435/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.8144 - accuracy: 0.7030 - val_loss: 3.8175 - val_accuracy: 0.2773\n",
      "Epoch 436/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.6968 - accuracy: 0.7576 - val_loss: 3.5686 - val_accuracy: 0.2227\n",
      "Epoch 437/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.5745 - accuracy: 0.8091 - val_loss: 3.3344 - val_accuracy: 0.3545\n",
      "Epoch 438/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.7568 - accuracy: 0.7212 - val_loss: 3.4870 - val_accuracy: 0.3636\n",
      "Epoch 439/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.6713 - accuracy: 0.7758 - val_loss: 3.4340 - val_accuracy: 0.3455\n",
      "Epoch 440/1500\n",
      "83/83 [==============================] - 0s 595us/step - loss: 0.5908 - accuracy: 0.8045 - val_loss: 3.5686 - val_accuracy: 0.2727\n",
      "Epoch 441/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.7094 - accuracy: 0.7318 - val_loss: 3.5808 - val_accuracy: 0.2682\n",
      "Epoch 442/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.8710 - accuracy: 0.7030 - val_loss: 3.4133 - val_accuracy: 0.3091\n",
      "Epoch 443/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.8221 - accuracy: 0.7182 - val_loss: 3.6825 - val_accuracy: 0.3091\n",
      "Epoch 444/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.7875 - accuracy: 0.7182 - val_loss: 3.4945 - val_accuracy: 0.3227\n",
      "Epoch 445/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.6496 - accuracy: 0.7727 - val_loss: 3.4106 - val_accuracy: 0.3773\n",
      "Epoch 446/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.5886 - accuracy: 0.7833 - val_loss: 3.6121 - val_accuracy: 0.3091\n",
      "Epoch 447/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.6081 - accuracy: 0.7939 - val_loss: 3.4172 - val_accuracy: 0.3500\n",
      "Epoch 448/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.5751 - accuracy: 0.8076 - val_loss: 3.7576 - val_accuracy: 0.2955\n",
      "Epoch 449/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.9855 - accuracy: 0.6652 - val_loss: 4.5131 - val_accuracy: 0.2409\n",
      "Epoch 450/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 1.0246 - accuracy: 0.6530 - val_loss: 3.6448 - val_accuracy: 0.3273\n",
      "Epoch 451/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.6904 - accuracy: 0.7667 - val_loss: 3.7616 - val_accuracy: 0.2864\n",
      "Epoch 452/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.6233 - accuracy: 0.7818 - val_loss: 3.5712 - val_accuracy: 0.3318\n",
      "Epoch 453/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.5511 - accuracy: 0.8227 - val_loss: 3.5136 - val_accuracy: 0.3000\n",
      "Epoch 454/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.7365 - accuracy: 0.7379 - val_loss: 3.4352 - val_accuracy: 0.3227\n",
      "Epoch 455/1500\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.6785 - accuracy: 0.7636 - val_loss: 3.6685 - val_accuracy: 0.3091\n",
      "Epoch 456/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.6896 - accuracy: 0.7591 - val_loss: 3.5958 - val_accuracy: 0.3182\n",
      "Epoch 457/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.5426 - accuracy: 0.8030 - val_loss: 3.4025 - val_accuracy: 0.3682\n",
      "Epoch 458/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.5749 - accuracy: 0.7909 - val_loss: 3.5313 - val_accuracy: 0.3500\n",
      "Epoch 459/1500\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.4713 - accuracy: 0.8485 - val_loss: 3.6314 - val_accuracy: 0.2818\n",
      "Epoch 460/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.6161 - accuracy: 0.7848 - val_loss: 3.5896 - val_accuracy: 0.3091\n",
      "Epoch 461/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.6082 - accuracy: 0.7939 - val_loss: 3.6778 - val_accuracy: 0.3136\n",
      "Epoch 462/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.5744 - accuracy: 0.7864 - val_loss: 3.7012 - val_accuracy: 0.3227\n",
      "Epoch 463/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.5835 - accuracy: 0.7894 - val_loss: 3.7238 - val_accuracy: 0.2864\n",
      "Epoch 464/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.5427 - accuracy: 0.7909 - val_loss: 3.8475 - val_accuracy: 0.2636\n",
      "Epoch 465/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.6148 - accuracy: 0.7909 - val_loss: 3.6979 - val_accuracy: 0.3409\n",
      "Epoch 466/1500\n",
      "83/83 [==============================] - 0s 593us/step - loss: 0.5440 - accuracy: 0.8015 - val_loss: 3.6080 - val_accuracy: 0.3409\n",
      "Epoch 467/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.7537 - accuracy: 0.7182 - val_loss: 3.8516 - val_accuracy: 0.3409\n",
      "Epoch 468/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.7204 - accuracy: 0.7273 - val_loss: 3.9055 - val_accuracy: 0.2136\n",
      "Epoch 469/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.7284 - accuracy: 0.7273 - val_loss: 3.7119 - val_accuracy: 0.3500\n",
      "Epoch 470/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.5707 - accuracy: 0.7985 - val_loss: 3.4736 - val_accuracy: 0.3227\n",
      "Epoch 471/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.6907 - accuracy: 0.7500 - val_loss: 3.8796 - val_accuracy: 0.2682\n",
      "Epoch 472/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.5782 - accuracy: 0.7985 - val_loss: 3.5906 - val_accuracy: 0.3409\n",
      "Epoch 473/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.6476 - accuracy: 0.7773 - val_loss: 3.7349 - val_accuracy: 0.3318\n",
      "Epoch 474/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.6048 - accuracy: 0.8015 - val_loss: 3.6113 - val_accuracy: 0.3591\n",
      "Epoch 475/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.5701 - accuracy: 0.8045 - val_loss: 3.6379 - val_accuracy: 0.2955\n",
      "Epoch 476/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.4981 - accuracy: 0.8364 - val_loss: 3.7190 - val_accuracy: 0.2909\n",
      "Epoch 477/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.7799 - accuracy: 0.7182 - val_loss: 3.5272 - val_accuracy: 0.2955\n",
      "Epoch 478/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.6495 - accuracy: 0.7848 - val_loss: 3.6234 - val_accuracy: 0.2955\n",
      "Epoch 479/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.4980 - accuracy: 0.8379 - val_loss: 3.5563 - val_accuracy: 0.3273\n",
      "Epoch 480/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.6628 - accuracy: 0.7636 - val_loss: 4.2092 - val_accuracy: 0.2864\n",
      "Epoch 481/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.6412 - accuracy: 0.7803 - val_loss: 3.7520 - val_accuracy: 0.2636\n",
      "Epoch 482/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.7248 - accuracy: 0.7364 - val_loss: 3.7367 - val_accuracy: 0.2909\n",
      "Epoch 483/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.5742 - accuracy: 0.8076 - val_loss: 3.4867 - val_accuracy: 0.3227\n",
      "Epoch 484/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.5142 - accuracy: 0.8303 - val_loss: 3.9747 - val_accuracy: 0.3136\n",
      "Epoch 485/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.5241 - accuracy: 0.8121 - val_loss: 3.7491 - val_accuracy: 0.2955\n",
      "Epoch 486/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.5229 - accuracy: 0.8212 - val_loss: 3.6591 - val_accuracy: 0.3273\n",
      "Epoch 487/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.6628 - accuracy: 0.7485 - val_loss: 3.9601 - val_accuracy: 0.2864\n",
      "Epoch 488/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.6849 - accuracy: 0.7636 - val_loss: 3.9418 - val_accuracy: 0.2818\n",
      "Epoch 489/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.6812 - accuracy: 0.7394 - val_loss: 3.7320 - val_accuracy: 0.3091\n",
      "Epoch 490/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.4725 - accuracy: 0.8455 - val_loss: 3.6770 - val_accuracy: 0.3455\n",
      "Epoch 491/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.5069 - accuracy: 0.8212 - val_loss: 4.0745 - val_accuracy: 0.2091\n",
      "Epoch 492/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.6392 - accuracy: 0.7621 - val_loss: 3.9017 - val_accuracy: 0.3227\n",
      "Epoch 493/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.6268 - accuracy: 0.7606 - val_loss: 3.6255 - val_accuracy: 0.3091\n",
      "Epoch 494/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.6036 - accuracy: 0.7955 - val_loss: 3.6613 - val_accuracy: 0.3682\n",
      "Epoch 495/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.5048 - accuracy: 0.8455 - val_loss: 4.0164 - val_accuracy: 0.2636\n",
      "Epoch 496/1500\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.5430 - accuracy: 0.8076 - val_loss: 3.5712 - val_accuracy: 0.3864\n",
      "Epoch 497/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.6284 - accuracy: 0.7667 - val_loss: 3.7634 - val_accuracy: 0.3227\n",
      "Epoch 498/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.6572 - accuracy: 0.7561 - val_loss: 4.0800 - val_accuracy: 0.2636\n",
      "Epoch 499/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.6039 - accuracy: 0.7742 - val_loss: 3.6359 - val_accuracy: 0.3409\n",
      "Epoch 500/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 633us/step - loss: 0.6301 - accuracy: 0.7606 - val_loss: 3.8084 - val_accuracy: 0.3182\n",
      "Epoch 501/1500\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.5466 - accuracy: 0.8045 - val_loss: 3.8636 - val_accuracy: 0.3227\n",
      "Epoch 502/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.5339 - accuracy: 0.7970 - val_loss: 3.7199 - val_accuracy: 0.2818\n",
      "Epoch 503/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.5454 - accuracy: 0.7909 - val_loss: 3.7963 - val_accuracy: 0.2955\n",
      "Epoch 504/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.4727 - accuracy: 0.8227 - val_loss: 3.6364 - val_accuracy: 0.3318\n",
      "Epoch 505/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.4513 - accuracy: 0.8576 - val_loss: 3.7284 - val_accuracy: 0.4045\n",
      "Epoch 506/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.5007 - accuracy: 0.8121 - val_loss: 4.0043 - val_accuracy: 0.3273\n",
      "Epoch 507/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.7351 - accuracy: 0.7197 - val_loss: 3.7190 - val_accuracy: 0.3409\n",
      "Epoch 508/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.7200 - accuracy: 0.7394 - val_loss: 3.5821 - val_accuracy: 0.3500\n",
      "Epoch 509/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.5956 - accuracy: 0.7803 - val_loss: 3.7225 - val_accuracy: 0.2864\n",
      "Epoch 510/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.5194 - accuracy: 0.8167 - val_loss: 4.0204 - val_accuracy: 0.3182\n",
      "Epoch 511/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.8164 - accuracy: 0.6894 - val_loss: 4.0378 - val_accuracy: 0.2909\n",
      "Epoch 512/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.6786 - accuracy: 0.7470 - val_loss: 4.2370 - val_accuracy: 0.2591\n",
      "Epoch 513/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.8037 - accuracy: 0.7212 - val_loss: 3.7051 - val_accuracy: 0.3318\n",
      "Epoch 514/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.8758 - accuracy: 0.6833 - val_loss: 4.0935 - val_accuracy: 0.2636\n",
      "Epoch 515/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.7234 - accuracy: 0.7409 - val_loss: 4.4554 - val_accuracy: 0.2682\n",
      "Epoch 516/1500\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.7970 - accuracy: 0.7121 - val_loss: 3.8268 - val_accuracy: 0.3455\n",
      "Epoch 517/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.5049 - accuracy: 0.8242 - val_loss: 3.6801 - val_accuracy: 0.3409\n",
      "Epoch 518/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.3894 - accuracy: 0.8788 - val_loss: 3.7093 - val_accuracy: 0.3545\n",
      "Epoch 519/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.4191 - accuracy: 0.8758 - val_loss: 3.7979 - val_accuracy: 0.3000\n",
      "Epoch 520/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.4813 - accuracy: 0.8288 - val_loss: 3.6958 - val_accuracy: 0.3727\n",
      "Epoch 521/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.3853 - accuracy: 0.8879 - val_loss: 3.6422 - val_accuracy: 0.4045\n",
      "Epoch 522/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.4294 - accuracy: 0.8576 - val_loss: 3.7374 - val_accuracy: 0.3682\n",
      "Epoch 523/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.4779 - accuracy: 0.8364 - val_loss: 3.7587 - val_accuracy: 0.3545\n",
      "Epoch 524/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.4752 - accuracy: 0.8333 - val_loss: 3.7730 - val_accuracy: 0.3273\n",
      "Epoch 525/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.4621 - accuracy: 0.8318 - val_loss: 3.7930 - val_accuracy: 0.3045\n",
      "Epoch 526/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.4492 - accuracy: 0.8258 - val_loss: 3.7619 - val_accuracy: 0.3091\n",
      "Epoch 527/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.4723 - accuracy: 0.8439 - val_loss: 3.8045 - val_accuracy: 0.3500\n",
      "Epoch 528/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.7337 - accuracy: 0.7500 - val_loss: 4.4520 - val_accuracy: 0.2409\n",
      "Epoch 529/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.7990 - accuracy: 0.7106 - val_loss: 4.3070 - val_accuracy: 0.3273\n",
      "Epoch 530/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.4891 - accuracy: 0.8227 - val_loss: 3.7849 - val_accuracy: 0.3545\n",
      "Epoch 531/1500\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.5035 - accuracy: 0.8348 - val_loss: 4.0405 - val_accuracy: 0.3136\n",
      "Epoch 532/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.4185 - accuracy: 0.8591 - val_loss: 3.9362 - val_accuracy: 0.3318\n",
      "Epoch 533/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.6838 - accuracy: 0.7652 - val_loss: 4.1057 - val_accuracy: 0.3409\n",
      "Epoch 534/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.4789 - accuracy: 0.8364 - val_loss: 4.1389 - val_accuracy: 0.2273\n",
      "Epoch 535/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.5456 - accuracy: 0.8136 - val_loss: 3.9096 - val_accuracy: 0.3727\n",
      "Epoch 536/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.5545 - accuracy: 0.7939 - val_loss: 4.2109 - val_accuracy: 0.2727\n",
      "Epoch 537/1500\n",
      "83/83 [==============================] - 0s 675us/step - loss: 0.5767 - accuracy: 0.7924 - val_loss: 4.0735 - val_accuracy: 0.3045\n",
      "Epoch 538/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.7006 - accuracy: 0.7409 - val_loss: 4.5281 - val_accuracy: 0.2636\n",
      "Epoch 539/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.5814 - accuracy: 0.7894 - val_loss: 3.8331 - val_accuracy: 0.3045\n",
      "Epoch 540/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 1.0663 - accuracy: 0.6500 - val_loss: 4.1506 - val_accuracy: 0.2955\n",
      "Epoch 541/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.7460 - accuracy: 0.7364 - val_loss: 3.8642 - val_accuracy: 0.3409\n",
      "Epoch 542/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.4065 - accuracy: 0.8470 - val_loss: 4.0799 - val_accuracy: 0.3136\n",
      "Epoch 543/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.3968 - accuracy: 0.8561 - val_loss: 3.9506 - val_accuracy: 0.3545\n",
      "Epoch 544/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.4492 - accuracy: 0.8318 - val_loss: 3.7411 - val_accuracy: 0.3364\n",
      "Epoch 545/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.3720 - accuracy: 0.8818 - val_loss: 3.7245 - val_accuracy: 0.3636\n",
      "Epoch 546/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.3931 - accuracy: 0.8818 - val_loss: 3.8391 - val_accuracy: 0.3227\n",
      "Epoch 547/1500\n",
      "83/83 [==============================] - 0s 596us/step - loss: 0.3697 - accuracy: 0.8803 - val_loss: 3.7689 - val_accuracy: 0.4000\n",
      "Epoch 548/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.3757 - accuracy: 0.8652 - val_loss: 3.8932 - val_accuracy: 0.3545\n",
      "Epoch 549/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.5388 - accuracy: 0.8076 - val_loss: 4.3700 - val_accuracy: 0.2591\n",
      "Epoch 550/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.5791 - accuracy: 0.7788 - val_loss: 4.1071 - val_accuracy: 0.3000\n",
      "Epoch 551/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.4741 - accuracy: 0.8364 - val_loss: 3.8942 - val_accuracy: 0.3136\n",
      "Epoch 552/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.4299 - accuracy: 0.8621 - val_loss: 4.0025 - val_accuracy: 0.3591\n",
      "Epoch 553/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.6535 - accuracy: 0.7606 - val_loss: 4.0150 - val_accuracy: 0.3182\n",
      "Epoch 554/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.5028 - accuracy: 0.8106 - val_loss: 3.7541 - val_accuracy: 0.3500\n",
      "Epoch 555/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.4122 - accuracy: 0.8636 - val_loss: 4.0566 - val_accuracy: 0.3091\n",
      "Epoch 556/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.4416 - accuracy: 0.8394 - val_loss: 4.0971 - val_accuracy: 0.2909\n",
      "Epoch 557/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.8584 - accuracy: 0.6939 - val_loss: 4.0477 - val_accuracy: 0.3045\n",
      "Epoch 558/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.4837 - accuracy: 0.8273 - val_loss: 3.7837 - val_accuracy: 0.3364\n",
      "Epoch 559/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.4553 - accuracy: 0.8303 - val_loss: 3.8675 - val_accuracy: 0.2864\n",
      "Epoch 560/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.6167 - accuracy: 0.7697 - val_loss: 4.2388 - val_accuracy: 0.2500\n",
      "Epoch 561/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.7581 - accuracy: 0.7303 - val_loss: 4.7877 - val_accuracy: 0.2773\n",
      "Epoch 562/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.6436 - accuracy: 0.7727 - val_loss: 4.0121 - val_accuracy: 0.3636\n",
      "Epoch 563/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.4916 - accuracy: 0.8348 - val_loss: 4.2463 - val_accuracy: 0.2455\n",
      "Epoch 564/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.5005 - accuracy: 0.8212 - val_loss: 4.3062 - val_accuracy: 0.2636\n",
      "Epoch 565/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.6448 - accuracy: 0.7833 - val_loss: 4.0582 - val_accuracy: 0.3591\n",
      "Epoch 566/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.3966 - accuracy: 0.8682 - val_loss: 4.0742 - val_accuracy: 0.3727\n",
      "Epoch 567/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.4779 - accuracy: 0.8318 - val_loss: 3.9421 - val_accuracy: 0.3227\n",
      "Epoch 568/1500\n",
      "83/83 [==============================] - 0s 595us/step - loss: 0.4126 - accuracy: 0.8621 - val_loss: 3.8958 - val_accuracy: 0.3545\n",
      "Epoch 569/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.4281 - accuracy: 0.8485 - val_loss: 4.1344 - val_accuracy: 0.2818\n",
      "Epoch 570/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.4534 - accuracy: 0.8379 - val_loss: 4.1888 - val_accuracy: 0.2864\n",
      "Epoch 571/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.5533 - accuracy: 0.7727 - val_loss: 4.1217 - val_accuracy: 0.3045\n",
      "Epoch 572/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.8045 - accuracy: 0.7212 - val_loss: 4.5037 - val_accuracy: 0.2591\n",
      "Epoch 573/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 1.0446 - accuracy: 0.6379 - val_loss: 5.4848 - val_accuracy: 0.2045\n",
      "Epoch 574/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 1.0672 - accuracy: 0.6333 - val_loss: 4.1744 - val_accuracy: 0.3591\n",
      "Epoch 575/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.6224 - accuracy: 0.7621 - val_loss: 4.2227 - val_accuracy: 0.3409\n",
      "Epoch 576/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.4238 - accuracy: 0.8500 - val_loss: 4.0107 - val_accuracy: 0.3409\n",
      "Epoch 577/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.4198 - accuracy: 0.8424 - val_loss: 4.0659 - val_accuracy: 0.3727\n",
      "Epoch 578/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.3597 - accuracy: 0.8833 - val_loss: 4.1605 - val_accuracy: 0.3091\n",
      "Epoch 579/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.4057 - accuracy: 0.8636 - val_loss: 4.0634 - val_accuracy: 0.3455\n",
      "Epoch 580/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.5383 - accuracy: 0.8136 - val_loss: 4.1329 - val_accuracy: 0.3545\n",
      "Epoch 581/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.5043 - accuracy: 0.8242 - val_loss: 4.0386 - val_accuracy: 0.3773\n",
      "Epoch 582/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.3800 - accuracy: 0.8697 - val_loss: 4.4529 - val_accuracy: 0.2864\n",
      "Epoch 583/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.3958 - accuracy: 0.8682 - val_loss: 3.9318 - val_accuracy: 0.3273\n",
      "Epoch 584/1500\n",
      "83/83 [==============================] - 0s 596us/step - loss: 0.5472 - accuracy: 0.8227 - val_loss: 4.4571 - val_accuracy: 0.3182\n",
      "Epoch 585/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.5208 - accuracy: 0.8045 - val_loss: 4.2234 - val_accuracy: 0.3136\n",
      "Epoch 586/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.5736 - accuracy: 0.7848 - val_loss: 4.0625 - val_accuracy: 0.3773\n",
      "Epoch 587/1500\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.6892 - accuracy: 0.7530 - val_loss: 4.3945 - val_accuracy: 0.2727\n",
      "Epoch 588/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.6709 - accuracy: 0.7515 - val_loss: 4.3861 - val_accuracy: 0.3136\n",
      "Epoch 589/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.5249 - accuracy: 0.8000 - val_loss: 4.2084 - val_accuracy: 0.3091\n",
      "Epoch 590/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.4493 - accuracy: 0.8439 - val_loss: 4.2083 - val_accuracy: 0.3364\n",
      "Epoch 591/1500\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.5455 - accuracy: 0.8015 - val_loss: 4.0344 - val_accuracy: 0.3136\n",
      "Epoch 592/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.4673 - accuracy: 0.8364 - val_loss: 4.0136 - val_accuracy: 0.3545\n",
      "Epoch 593/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.4626 - accuracy: 0.8455 - val_loss: 4.4210 - val_accuracy: 0.2682\n",
      "Epoch 594/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.4296 - accuracy: 0.8561 - val_loss: 4.1988 - val_accuracy: 0.3091\n",
      "Epoch 595/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.4920 - accuracy: 0.8045 - val_loss: 4.2591 - val_accuracy: 0.3318\n",
      "Epoch 596/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.5296 - accuracy: 0.8045 - val_loss: 4.3756 - val_accuracy: 0.3364\n",
      "Epoch 597/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.4258 - accuracy: 0.8561 - val_loss: 4.0091 - val_accuracy: 0.4000\n",
      "Epoch 598/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.5068 - accuracy: 0.8258 - val_loss: 4.5452 - val_accuracy: 0.2682\n",
      "Epoch 599/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.8618 - accuracy: 0.7091 - val_loss: 4.3728 - val_accuracy: 0.3455\n",
      "Epoch 600/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.4584 - accuracy: 0.8470 - val_loss: 3.9335 - val_accuracy: 0.4045\n",
      "Epoch 601/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.3990 - accuracy: 0.8758 - val_loss: 4.4000 - val_accuracy: 0.2864\n",
      "Epoch 602/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.5517 - accuracy: 0.8061 - val_loss: 4.1633 - val_accuracy: 0.3591\n",
      "Epoch 603/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.5000 - accuracy: 0.8212 - val_loss: 4.2317 - val_accuracy: 0.3545\n",
      "Epoch 604/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.5183 - accuracy: 0.8242 - val_loss: 3.9634 - val_accuracy: 0.3727\n",
      "Epoch 605/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.4665 - accuracy: 0.8470 - val_loss: 4.1635 - val_accuracy: 0.2636\n",
      "Epoch 606/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.4722 - accuracy: 0.8273 - val_loss: 4.6870 - val_accuracy: 0.2636\n",
      "Epoch 607/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.4932 - accuracy: 0.8121 - val_loss: 4.3831 - val_accuracy: 0.3364\n",
      "Epoch 608/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.4002 - accuracy: 0.8636 - val_loss: 4.3091 - val_accuracy: 0.3273\n",
      "Epoch 609/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.8413 - accuracy: 0.7121 - val_loss: 4.7513 - val_accuracy: 0.2500\n",
      "Epoch 610/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.8971 - accuracy: 0.6924 - val_loss: 4.1177 - val_accuracy: 0.3682\n",
      "Epoch 611/1500\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.5176 - accuracy: 0.8212 - val_loss: 4.1775 - val_accuracy: 0.3682\n",
      "Epoch 612/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 604us/step - loss: 0.3117 - accuracy: 0.8985 - val_loss: 4.1241 - val_accuracy: 0.3727\n",
      "Epoch 613/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.3557 - accuracy: 0.8833 - val_loss: 4.0970 - val_accuracy: 0.3273\n",
      "Epoch 614/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.3253 - accuracy: 0.8909 - val_loss: 4.0688 - val_accuracy: 0.3591\n",
      "Epoch 615/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.3773 - accuracy: 0.8788 - val_loss: 4.0336 - val_accuracy: 0.3864\n",
      "Epoch 616/1500\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.4010 - accuracy: 0.8667 - val_loss: 4.2522 - val_accuracy: 0.3364\n",
      "Epoch 617/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.4120 - accuracy: 0.8667 - val_loss: 4.2453 - val_accuracy: 0.3545\n",
      "Epoch 618/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.4987 - accuracy: 0.8348 - val_loss: 3.7744 - val_accuracy: 0.3909\n",
      "Epoch 619/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.5367 - accuracy: 0.7970 - val_loss: 4.3250 - val_accuracy: 0.3091\n",
      "Epoch 620/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.7792 - accuracy: 0.7258 - val_loss: 4.6906 - val_accuracy: 0.2864\n",
      "Epoch 621/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.6187 - accuracy: 0.7818 - val_loss: 4.5621 - val_accuracy: 0.2818\n",
      "Epoch 622/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.4929 - accuracy: 0.8212 - val_loss: 3.8642 - val_accuracy: 0.4045\n",
      "Epoch 623/1500\n",
      "83/83 [==============================] - 0s 592us/step - loss: 0.2909 - accuracy: 0.9227 - val_loss: 3.9003 - val_accuracy: 0.3864\n",
      "Epoch 624/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.3092 - accuracy: 0.9045 - val_loss: 4.1473 - val_accuracy: 0.3318\n",
      "Epoch 625/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.5182 - accuracy: 0.8242 - val_loss: 4.2769 - val_accuracy: 0.2773\n",
      "Epoch 626/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.4089 - accuracy: 0.8576 - val_loss: 4.4581 - val_accuracy: 0.3045\n",
      "Epoch 627/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.5337 - accuracy: 0.8106 - val_loss: 4.7364 - val_accuracy: 0.2682\n",
      "Epoch 628/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.5496 - accuracy: 0.8106 - val_loss: 4.5169 - val_accuracy: 0.3045\n",
      "Epoch 629/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.4412 - accuracy: 0.8485 - val_loss: 4.1597 - val_accuracy: 0.3182\n",
      "Epoch 630/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.5108 - accuracy: 0.8227 - val_loss: 4.4995 - val_accuracy: 0.2909\n",
      "Epoch 631/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.4571 - accuracy: 0.8288 - val_loss: 4.2107 - val_accuracy: 0.3591\n",
      "Epoch 632/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.5260 - accuracy: 0.8121 - val_loss: 4.5880 - val_accuracy: 0.3227\n",
      "Epoch 633/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.5209 - accuracy: 0.8045 - val_loss: 5.3304 - val_accuracy: 0.2000\n",
      "Epoch 634/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.9057 - accuracy: 0.6955 - val_loss: 4.6992 - val_accuracy: 0.2636\n",
      "Epoch 635/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.4974 - accuracy: 0.8121 - val_loss: 4.4162 - val_accuracy: 0.3136\n",
      "Epoch 636/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.4248 - accuracy: 0.8758 - val_loss: 4.2873 - val_accuracy: 0.3591\n",
      "Epoch 637/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.4307 - accuracy: 0.8561 - val_loss: 4.0913 - val_accuracy: 0.3545\n",
      "Epoch 638/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.5218 - accuracy: 0.8182 - val_loss: 4.4236 - val_accuracy: 0.3500\n",
      "Epoch 639/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.3848 - accuracy: 0.8667 - val_loss: 4.1683 - val_accuracy: 0.3455\n",
      "Epoch 640/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.4272 - accuracy: 0.8621 - val_loss: 4.2773 - val_accuracy: 0.3818\n",
      "Epoch 641/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.3636 - accuracy: 0.8636 - val_loss: 4.0971 - val_accuracy: 0.3818\n",
      "Epoch 642/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.5141 - accuracy: 0.8076 - val_loss: 4.3643 - val_accuracy: 0.3636\n",
      "Epoch 643/1500\n",
      "83/83 [==============================] - 0s 594us/step - loss: 0.4679 - accuracy: 0.8379 - val_loss: 4.0779 - val_accuracy: 0.3909\n",
      "Epoch 644/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.4021 - accuracy: 0.8742 - val_loss: 4.0384 - val_accuracy: 0.3409\n",
      "Epoch 645/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.3165 - accuracy: 0.8985 - val_loss: 4.4979 - val_accuracy: 0.3273\n",
      "Epoch 646/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.3953 - accuracy: 0.8697 - val_loss: 4.2431 - val_accuracy: 0.3682\n",
      "Epoch 647/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.4457 - accuracy: 0.8288 - val_loss: 4.6575 - val_accuracy: 0.3182\n",
      "Epoch 648/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.4692 - accuracy: 0.8394 - val_loss: 4.1618 - val_accuracy: 0.3682\n",
      "Epoch 649/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.3270 - accuracy: 0.8894 - val_loss: 4.0994 - val_accuracy: 0.3455\n",
      "Epoch 650/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.3173 - accuracy: 0.9030 - val_loss: 4.2486 - val_accuracy: 0.3864\n",
      "Epoch 651/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.4096 - accuracy: 0.8470 - val_loss: 4.1781 - val_accuracy: 0.3682\n",
      "Epoch 652/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 1.1243 - accuracy: 0.6379 - val_loss: 4.9014 - val_accuracy: 0.2773\n",
      "Epoch 653/1500\n",
      "83/83 [==============================] - 0s 672us/step - loss: 0.9348 - accuracy: 0.7076 - val_loss: 4.7156 - val_accuracy: 0.3045\n",
      "Epoch 654/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.4459 - accuracy: 0.8409 - val_loss: 4.1931 - val_accuracy: 0.3636\n",
      "Epoch 655/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.3626 - accuracy: 0.8727 - val_loss: 4.5468 - val_accuracy: 0.2818\n",
      "Epoch 656/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.5202 - accuracy: 0.8242 - val_loss: 4.3751 - val_accuracy: 0.3864\n",
      "Epoch 657/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.5040 - accuracy: 0.8167 - val_loss: 4.2340 - val_accuracy: 0.3591\n",
      "Epoch 658/1500\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.3730 - accuracy: 0.8697 - val_loss: 4.0356 - val_accuracy: 0.4000\n",
      "Epoch 659/1500\n",
      "83/83 [==============================] - 0s 595us/step - loss: 0.4099 - accuracy: 0.8576 - val_loss: 4.1829 - val_accuracy: 0.3545\n",
      "Epoch 660/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.4253 - accuracy: 0.8455 - val_loss: 4.3282 - val_accuracy: 0.3318\n",
      "Epoch 661/1500\n",
      "83/83 [==============================] - 0s 593us/step - loss: 0.4497 - accuracy: 0.8364 - val_loss: 4.3837 - val_accuracy: 0.3136\n",
      "Epoch 662/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.4476 - accuracy: 0.8333 - val_loss: 4.7675 - val_accuracy: 0.3045\n",
      "Epoch 663/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.4093 - accuracy: 0.8591 - val_loss: 4.7379 - val_accuracy: 0.3545\n",
      "Epoch 664/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.3585 - accuracy: 0.8773 - val_loss: 4.2437 - val_accuracy: 0.3864\n",
      "Epoch 665/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.3819 - accuracy: 0.8606 - val_loss: 4.0907 - val_accuracy: 0.3864\n",
      "Epoch 666/1500\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.3465 - accuracy: 0.8864 - val_loss: 4.2980 - val_accuracy: 0.3636\n",
      "Epoch 667/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.3980 - accuracy: 0.8591 - val_loss: 4.3053 - val_accuracy: 0.3773\n",
      "Epoch 668/1500\n",
      "83/83 [==============================] - 0s 641us/step - loss: 0.4608 - accuracy: 0.8333 - val_loss: 4.4647 - val_accuracy: 0.3318\n",
      "Epoch 669/1500\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.3496 - accuracy: 0.8712 - val_loss: 4.2145 - val_accuracy: 0.3864\n",
      "Epoch 670/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.3695 - accuracy: 0.8803 - val_loss: 4.3171 - val_accuracy: 0.3545\n",
      "Epoch 671/1500\n",
      "83/83 [==============================] - 0s 645us/step - loss: 0.4511 - accuracy: 0.8424 - val_loss: 4.5384 - val_accuracy: 0.3045\n",
      "Epoch 672/1500\n",
      "83/83 [==============================] - 0s 661us/step - loss: 0.3794 - accuracy: 0.8621 - val_loss: 4.3474 - val_accuracy: 0.3773\n",
      "Epoch 673/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.4320 - accuracy: 0.8394 - val_loss: 4.4763 - val_accuracy: 0.4000\n",
      "Epoch 674/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.3153 - accuracy: 0.8879 - val_loss: 4.2024 - val_accuracy: 0.3818\n",
      "Epoch 675/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.4580 - accuracy: 0.8409 - val_loss: 4.2762 - val_accuracy: 0.3682\n",
      "Epoch 676/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.5448 - accuracy: 0.8182 - val_loss: 4.8378 - val_accuracy: 0.2818\n",
      "Epoch 677/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 1.1713 - accuracy: 0.6455 - val_loss: 4.6312 - val_accuracy: 0.2955\n",
      "Epoch 678/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.8466 - accuracy: 0.6939 - val_loss: 4.1765 - val_accuracy: 0.3227\n",
      "Epoch 679/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.6323 - accuracy: 0.7697 - val_loss: 4.4087 - val_accuracy: 0.3545\n",
      "Epoch 680/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.4252 - accuracy: 0.8455 - val_loss: 4.1420 - val_accuracy: 0.4227\n",
      "Epoch 681/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.3016 - accuracy: 0.8924 - val_loss: 4.5562 - val_accuracy: 0.3091\n",
      "Epoch 682/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.4523 - accuracy: 0.8439 - val_loss: 4.4721 - val_accuracy: 0.3682\n",
      "Epoch 683/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.5899 - accuracy: 0.7833 - val_loss: 4.3635 - val_accuracy: 0.3591\n",
      "Epoch 684/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.3711 - accuracy: 0.8606 - val_loss: 4.3548 - val_accuracy: 0.3364\n",
      "Epoch 685/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.2924 - accuracy: 0.9015 - val_loss: 4.6867 - val_accuracy: 0.3318\n",
      "Epoch 686/1500\n",
      "83/83 [==============================] - 0s 650us/step - loss: 0.3468 - accuracy: 0.8848 - val_loss: 4.7141 - val_accuracy: 0.3091\n",
      "Epoch 687/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.4934 - accuracy: 0.8197 - val_loss: 4.9280 - val_accuracy: 0.3045\n",
      "Epoch 688/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.6202 - accuracy: 0.7909 - val_loss: 4.7763 - val_accuracy: 0.2682\n",
      "Epoch 689/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.7315 - accuracy: 0.7348 - val_loss: 4.5866 - val_accuracy: 0.3000\n",
      "Epoch 690/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.3902 - accuracy: 0.8621 - val_loss: 4.3532 - val_accuracy: 0.3636\n",
      "Epoch 691/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.4578 - accuracy: 0.8364 - val_loss: 4.7054 - val_accuracy: 0.3045\n",
      "Epoch 692/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.3659 - accuracy: 0.8697 - val_loss: 4.3955 - val_accuracy: 0.3500\n",
      "Epoch 693/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.3537 - accuracy: 0.8652 - val_loss: 4.3649 - val_accuracy: 0.3364\n",
      "Epoch 694/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.4886 - accuracy: 0.8212 - val_loss: 4.7144 - val_accuracy: 0.3182\n",
      "Epoch 695/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.5357 - accuracy: 0.8000 - val_loss: 4.5623 - val_accuracy: 0.3773\n",
      "Epoch 696/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.6251 - accuracy: 0.7697 - val_loss: 4.3325 - val_accuracy: 0.3636\n",
      "Epoch 697/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.3563 - accuracy: 0.8833 - val_loss: 4.3311 - val_accuracy: 0.3773\n",
      "Epoch 698/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.3761 - accuracy: 0.8576 - val_loss: 4.5881 - val_accuracy: 0.3636\n",
      "Epoch 699/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.5969 - accuracy: 0.7773 - val_loss: 4.4335 - val_accuracy: 0.3182\n",
      "Epoch 700/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.3677 - accuracy: 0.8758 - val_loss: 4.3326 - val_accuracy: 0.3455\n",
      "Epoch 701/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.3019 - accuracy: 0.9015 - val_loss: 4.3927 - val_accuracy: 0.3591\n",
      "Epoch 702/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.3454 - accuracy: 0.8742 - val_loss: 4.4500 - val_accuracy: 0.3545\n",
      "Epoch 703/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.4512 - accuracy: 0.8409 - val_loss: 4.4002 - val_accuracy: 0.3682\n",
      "Epoch 704/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.2821 - accuracy: 0.9045 - val_loss: 4.3951 - val_accuracy: 0.3591\n",
      "Epoch 705/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.4073 - accuracy: 0.8576 - val_loss: 4.7474 - val_accuracy: 0.3273\n",
      "Epoch 706/1500\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.5333 - accuracy: 0.8182 - val_loss: 4.4897 - val_accuracy: 0.3045\n",
      "Epoch 707/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.5897 - accuracy: 0.7803 - val_loss: 4.7999 - val_accuracy: 0.3500\n",
      "Epoch 708/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.5041 - accuracy: 0.8136 - val_loss: 4.7940 - val_accuracy: 0.3500\n",
      "Epoch 709/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.5530 - accuracy: 0.8076 - val_loss: 4.8445 - val_accuracy: 0.3045\n",
      "Epoch 710/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.4249 - accuracy: 0.8561 - val_loss: 5.0172 - val_accuracy: 0.3000\n",
      "Epoch 711/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.5659 - accuracy: 0.7864 - val_loss: 4.6049 - val_accuracy: 0.3545\n",
      "Epoch 712/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.4798 - accuracy: 0.8303 - val_loss: 4.7946 - val_accuracy: 0.3136\n",
      "Epoch 713/1500\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.3305 - accuracy: 0.9015 - val_loss: 4.6070 - val_accuracy: 0.3682\n",
      "Epoch 714/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.2944 - accuracy: 0.9076 - val_loss: 4.8848 - val_accuracy: 0.3182\n",
      "Epoch 715/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.4443 - accuracy: 0.8485 - val_loss: 4.6095 - val_accuracy: 0.3864\n",
      "Epoch 716/1500\n",
      "83/83 [==============================] - 0s 594us/step - loss: 0.2774 - accuracy: 0.8985 - val_loss: 4.6801 - val_accuracy: 0.3955\n",
      "Epoch 717/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.3216 - accuracy: 0.8848 - val_loss: 4.5701 - val_accuracy: 0.3727\n",
      "Epoch 718/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.3086 - accuracy: 0.9030 - val_loss: 4.4429 - val_accuracy: 0.3864\n",
      "Epoch 719/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.3958 - accuracy: 0.8500 - val_loss: 4.5783 - val_accuracy: 0.3409\n",
      "Epoch 720/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.5024 - accuracy: 0.7955 - val_loss: 4.5224 - val_accuracy: 0.3545\n",
      "Epoch 721/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.7395 - accuracy: 0.7455 - val_loss: 5.2146 - val_accuracy: 0.2955\n",
      "Epoch 722/1500\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.4722 - accuracy: 0.8333 - val_loss: 4.4979 - val_accuracy: 0.3727\n",
      "Epoch 723/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.7292 - accuracy: 0.7455 - val_loss: 4.5307 - val_accuracy: 0.3727\n",
      "Epoch 724/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 627us/step - loss: 0.3740 - accuracy: 0.8788 - val_loss: 4.5131 - val_accuracy: 0.3864\n",
      "Epoch 725/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.2829 - accuracy: 0.9045 - val_loss: 4.5503 - val_accuracy: 0.3591\n",
      "Epoch 726/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.2886 - accuracy: 0.9000 - val_loss: 4.5322 - val_accuracy: 0.3727\n",
      "Epoch 727/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.6390 - accuracy: 0.7758 - val_loss: 4.9593 - val_accuracy: 0.3136\n",
      "Epoch 728/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.6156 - accuracy: 0.7909 - val_loss: 4.5030 - val_accuracy: 0.3818\n",
      "Epoch 729/1500\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.4114 - accuracy: 0.8515 - val_loss: 4.4255 - val_accuracy: 0.3773\n",
      "Epoch 730/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.3861 - accuracy: 0.8758 - val_loss: 4.5602 - val_accuracy: 0.3091\n",
      "Epoch 731/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.7603 - accuracy: 0.7273 - val_loss: 4.9667 - val_accuracy: 0.2545\n",
      "Epoch 732/1500\n",
      "83/83 [==============================] - 0s 632us/step - loss: 1.3125 - accuracy: 0.6273 - val_loss: 4.7956 - val_accuracy: 0.3182\n",
      "Epoch 733/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.5195 - accuracy: 0.8061 - val_loss: 4.6630 - val_accuracy: 0.3318\n",
      "Epoch 734/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.3135 - accuracy: 0.9015 - val_loss: 4.2398 - val_accuracy: 0.3818\n",
      "Epoch 735/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.3702 - accuracy: 0.8652 - val_loss: 4.3383 - val_accuracy: 0.3364\n",
      "Epoch 736/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.2519 - accuracy: 0.9182 - val_loss: 4.8062 - val_accuracy: 0.3182\n",
      "Epoch 737/1500\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.2734 - accuracy: 0.9152 - val_loss: 4.4964 - val_accuracy: 0.3364\n",
      "Epoch 738/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.3261 - accuracy: 0.8894 - val_loss: 4.6939 - val_accuracy: 0.3455\n",
      "Epoch 739/1500\n",
      "83/83 [==============================] - 0s 596us/step - loss: 0.3463 - accuracy: 0.8909 - val_loss: 4.2194 - val_accuracy: 0.3864\n",
      "Epoch 740/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.2929 - accuracy: 0.8985 - val_loss: 4.5965 - val_accuracy: 0.3182\n",
      "Epoch 741/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.2999 - accuracy: 0.9030 - val_loss: 4.3865 - val_accuracy: 0.3909\n",
      "Epoch 742/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2775 - accuracy: 0.9167 - val_loss: 4.3902 - val_accuracy: 0.3636\n",
      "Epoch 743/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.3158 - accuracy: 0.8924 - val_loss: 4.3512 - val_accuracy: 0.3955\n",
      "Epoch 744/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.3200 - accuracy: 0.8955 - val_loss: 4.6397 - val_accuracy: 0.3545\n",
      "Epoch 745/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.4557 - accuracy: 0.8333 - val_loss: 4.8102 - val_accuracy: 0.3091\n",
      "Epoch 746/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.6974 - accuracy: 0.7652 - val_loss: 4.6716 - val_accuracy: 0.3273\n",
      "Epoch 747/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.8575 - accuracy: 0.7167 - val_loss: 4.6952 - val_accuracy: 0.2682\n",
      "Epoch 748/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 1.0862 - accuracy: 0.6773 - val_loss: 4.5748 - val_accuracy: 0.3455\n",
      "Epoch 749/1500\n",
      "83/83 [==============================] - 0s 670us/step - loss: 0.3410 - accuracy: 0.8848 - val_loss: 4.4046 - val_accuracy: 0.3682\n",
      "Epoch 750/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.3546 - accuracy: 0.8803 - val_loss: 4.8353 - val_accuracy: 0.3091\n",
      "Epoch 751/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.4205 - accuracy: 0.8485 - val_loss: 4.4830 - val_accuracy: 0.3545\n",
      "Epoch 752/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.3200 - accuracy: 0.8970 - val_loss: 4.8669 - val_accuracy: 0.2500\n",
      "Epoch 753/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.6913 - accuracy: 0.7606 - val_loss: 5.4553 - val_accuracy: 0.2455\n",
      "Epoch 754/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.4652 - accuracy: 0.8424 - val_loss: 4.5138 - val_accuracy: 0.3909\n",
      "Epoch 755/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.3428 - accuracy: 0.8970 - val_loss: 4.7698 - val_accuracy: 0.3182\n",
      "Epoch 756/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 1.5551 - accuracy: 0.5970 - val_loss: 5.1622 - val_accuracy: 0.2636\n",
      "Epoch 757/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 1.4765 - accuracy: 0.5833 - val_loss: 4.5362 - val_accuracy: 0.3682\n",
      "Epoch 758/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.4304 - accuracy: 0.8470 - val_loss: 4.4313 - val_accuracy: 0.4136\n",
      "Epoch 759/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.3630 - accuracy: 0.8652 - val_loss: 4.5009 - val_accuracy: 0.3818\n",
      "Epoch 760/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.3010 - accuracy: 0.9076 - val_loss: 4.5020 - val_accuracy: 0.3955\n",
      "Epoch 761/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.2875 - accuracy: 0.9167 - val_loss: 4.5194 - val_accuracy: 0.3818\n",
      "Epoch 762/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.2495 - accuracy: 0.9227 - val_loss: 4.3840 - val_accuracy: 0.3955\n",
      "Epoch 763/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.2715 - accuracy: 0.9045 - val_loss: 4.3675 - val_accuracy: 0.3818\n",
      "Epoch 764/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.4504 - accuracy: 0.8439 - val_loss: 4.4547 - val_accuracy: 0.4136\n",
      "Epoch 765/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.2247 - accuracy: 0.9379 - val_loss: 4.5341 - val_accuracy: 0.3955\n",
      "Epoch 766/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.2418 - accuracy: 0.9167 - val_loss: 4.4096 - val_accuracy: 0.3636\n",
      "Epoch 767/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.2340 - accuracy: 0.9364 - val_loss: 4.4388 - val_accuracy: 0.3773\n",
      "Epoch 768/1500\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.3418 - accuracy: 0.8758 - val_loss: 4.8260 - val_accuracy: 0.3364\n",
      "Epoch 769/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.3697 - accuracy: 0.8606 - val_loss: 4.5329 - val_accuracy: 0.3364\n",
      "Epoch 770/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.3221 - accuracy: 0.8894 - val_loss: 4.5062 - val_accuracy: 0.3364\n",
      "Epoch 771/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.3498 - accuracy: 0.8788 - val_loss: 4.4720 - val_accuracy: 0.3409\n",
      "Epoch 772/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.2635 - accuracy: 0.9197 - val_loss: 4.4408 - val_accuracy: 0.3818\n",
      "Epoch 773/1500\n",
      "83/83 [==============================] - 0s 595us/step - loss: 0.3505 - accuracy: 0.8727 - val_loss: 4.5451 - val_accuracy: 0.3182\n",
      "Epoch 774/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.2887 - accuracy: 0.9106 - val_loss: 4.5841 - val_accuracy: 0.3409\n",
      "Epoch 775/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.4431 - accuracy: 0.8394 - val_loss: 4.6631 - val_accuracy: 0.3455\n",
      "Epoch 776/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.7090 - accuracy: 0.7485 - val_loss: 4.7790 - val_accuracy: 0.3591\n",
      "Epoch 777/1500\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.6868 - accuracy: 0.7606 - val_loss: 4.6739 - val_accuracy: 0.3500\n",
      "Epoch 778/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.5454 - accuracy: 0.8152 - val_loss: 4.3543 - val_accuracy: 0.3864\n",
      "Epoch 779/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.3984 - accuracy: 0.8667 - val_loss: 4.5848 - val_accuracy: 0.4000\n",
      "Epoch 780/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.3129 - accuracy: 0.8909 - val_loss: 4.1440 - val_accuracy: 0.4000\n",
      "Epoch 781/1500\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.2898 - accuracy: 0.9061 - val_loss: 4.9059 - val_accuracy: 0.3682\n",
      "Epoch 782/1500\n",
      "83/83 [==============================] - 0s 640us/step - loss: 0.3166 - accuracy: 0.9000 - val_loss: 4.5303 - val_accuracy: 0.3682\n",
      "Epoch 783/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.2812 - accuracy: 0.9061 - val_loss: 4.5368 - val_accuracy: 0.4000\n",
      "Epoch 784/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.3280 - accuracy: 0.8879 - val_loss: 4.4052 - val_accuracy: 0.3773\n",
      "Epoch 785/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.4323 - accuracy: 0.8424 - val_loss: 5.4276 - val_accuracy: 0.2318\n",
      "Epoch 786/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.6307 - accuracy: 0.7682 - val_loss: 4.8718 - val_accuracy: 0.3273\n",
      "Epoch 787/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.5534 - accuracy: 0.7985 - val_loss: 5.0786 - val_accuracy: 0.3318\n",
      "Epoch 788/1500\n",
      "83/83 [==============================] - 0s 596us/step - loss: 0.3908 - accuracy: 0.8636 - val_loss: 4.5208 - val_accuracy: 0.3227\n",
      "Epoch 789/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.4344 - accuracy: 0.8561 - val_loss: 4.4574 - val_accuracy: 0.3909\n",
      "Epoch 790/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.3754 - accuracy: 0.8621 - val_loss: 4.5367 - val_accuracy: 0.3545\n",
      "Epoch 791/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.4229 - accuracy: 0.8561 - val_loss: 4.8270 - val_accuracy: 0.3318\n",
      "Epoch 792/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.6750 - accuracy: 0.7636 - val_loss: 5.0190 - val_accuracy: 0.3545\n",
      "Epoch 793/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.6541 - accuracy: 0.7773 - val_loss: 4.3910 - val_accuracy: 0.4045\n",
      "Epoch 794/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.3045 - accuracy: 0.8939 - val_loss: 4.4808 - val_accuracy: 0.3727\n",
      "Epoch 795/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.2179 - accuracy: 0.9394 - val_loss: 4.6397 - val_accuracy: 0.3182\n",
      "Epoch 796/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.2576 - accuracy: 0.9045 - val_loss: 4.5696 - val_accuracy: 0.3318\n",
      "Epoch 797/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.3584 - accuracy: 0.8667 - val_loss: 4.9644 - val_accuracy: 0.3227\n",
      "Epoch 798/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.3198 - accuracy: 0.8970 - val_loss: 4.6124 - val_accuracy: 0.3955\n",
      "Epoch 799/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.2510 - accuracy: 0.9091 - val_loss: 4.7560 - val_accuracy: 0.3182\n",
      "Epoch 800/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.2602 - accuracy: 0.9121 - val_loss: 4.5512 - val_accuracy: 0.4227\n",
      "Epoch 801/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.3482 - accuracy: 0.8773 - val_loss: 4.8470 - val_accuracy: 0.3545\n",
      "Epoch 802/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.5506 - accuracy: 0.8242 - val_loss: 6.1314 - val_accuracy: 0.2409\n",
      "Epoch 803/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.9974 - accuracy: 0.7076 - val_loss: 4.7253 - val_accuracy: 0.3773\n",
      "Epoch 804/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.3841 - accuracy: 0.8515 - val_loss: 4.6368 - val_accuracy: 0.4318\n",
      "Epoch 805/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.7120 - accuracy: 0.7424 - val_loss: 4.9602 - val_accuracy: 0.3182\n",
      "Epoch 806/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.6417 - accuracy: 0.7682 - val_loss: 5.0936 - val_accuracy: 0.3682\n",
      "Epoch 807/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.8519 - accuracy: 0.7333 - val_loss: 5.3737 - val_accuracy: 0.2500\n",
      "Epoch 808/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 1.3056 - accuracy: 0.6242 - val_loss: 5.0174 - val_accuracy: 0.2682\n",
      "Epoch 809/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.7254 - accuracy: 0.7409 - val_loss: 4.5850 - val_accuracy: 0.3727\n",
      "Epoch 810/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.4177 - accuracy: 0.8591 - val_loss: 4.4347 - val_accuracy: 0.4227\n",
      "Epoch 811/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.4056 - accuracy: 0.8712 - val_loss: 4.7530 - val_accuracy: 0.3455\n",
      "Epoch 812/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.4172 - accuracy: 0.8364 - val_loss: 4.2476 - val_accuracy: 0.3727\n",
      "Epoch 813/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.4110 - accuracy: 0.8424 - val_loss: 4.5489 - val_accuracy: 0.4045\n",
      "Epoch 814/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.2874 - accuracy: 0.9061 - val_loss: 4.6579 - val_accuracy: 0.3182\n",
      "Epoch 815/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.3131 - accuracy: 0.8955 - val_loss: 4.5821 - val_accuracy: 0.4000\n",
      "Epoch 816/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.2557 - accuracy: 0.9258 - val_loss: 4.3162 - val_accuracy: 0.3955\n",
      "Epoch 817/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.2352 - accuracy: 0.9394 - val_loss: 4.5374 - val_accuracy: 0.3955\n",
      "Epoch 818/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.3519 - accuracy: 0.8803 - val_loss: 5.1927 - val_accuracy: 0.3409\n",
      "Epoch 819/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.4299 - accuracy: 0.8515 - val_loss: 5.0255 - val_accuracy: 0.3273\n",
      "Epoch 820/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.5721 - accuracy: 0.7924 - val_loss: 4.6708 - val_accuracy: 0.3182\n",
      "Epoch 821/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.5866 - accuracy: 0.7667 - val_loss: 4.9444 - val_accuracy: 0.3227\n",
      "Epoch 822/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.4103 - accuracy: 0.8636 - val_loss: 4.5476 - val_accuracy: 0.3773\n",
      "Epoch 823/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.3429 - accuracy: 0.8833 - val_loss: 4.8976 - val_accuracy: 0.3545\n",
      "Epoch 824/1500\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.4718 - accuracy: 0.8273 - val_loss: 4.8407 - val_accuracy: 0.3409\n",
      "Epoch 825/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.4648 - accuracy: 0.8288 - val_loss: 4.4979 - val_accuracy: 0.3636\n",
      "Epoch 826/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.3504 - accuracy: 0.8894 - val_loss: 4.7043 - val_accuracy: 0.4409\n",
      "Epoch 827/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.3001 - accuracy: 0.9030 - val_loss: 4.6017 - val_accuracy: 0.3682\n",
      "Epoch 828/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.5700 - accuracy: 0.8030 - val_loss: 5.2185 - val_accuracy: 0.2818\n",
      "Epoch 829/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.5385 - accuracy: 0.8197 - val_loss: 4.4690 - val_accuracy: 0.3727\n",
      "Epoch 830/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.2441 - accuracy: 0.9182 - val_loss: 4.4509 - val_accuracy: 0.3636\n",
      "Epoch 831/1500\n",
      "83/83 [==============================] - 0s 596us/step - loss: 0.3614 - accuracy: 0.8606 - val_loss: 4.7366 - val_accuracy: 0.3091\n",
      "Epoch 832/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.3698 - accuracy: 0.8636 - val_loss: 4.7257 - val_accuracy: 0.3864\n",
      "Epoch 833/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.4493 - accuracy: 0.8333 - val_loss: 4.9447 - val_accuracy: 0.3727\n",
      "Epoch 834/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.4806 - accuracy: 0.8303 - val_loss: 5.4689 - val_accuracy: 0.3091\n",
      "Epoch 835/1500\n",
      "83/83 [==============================] - 0s 591us/step - loss: 0.7528 - accuracy: 0.7197 - val_loss: 4.9459 - val_accuracy: 0.3409\n",
      "Epoch 836/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 596us/step - loss: 0.3575 - accuracy: 0.8652 - val_loss: 5.0147 - val_accuracy: 0.3364\n",
      "Epoch 837/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.3846 - accuracy: 0.8576 - val_loss: 4.6301 - val_accuracy: 0.3545\n",
      "Epoch 838/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2268 - accuracy: 0.9288 - val_loss: 4.7458 - val_accuracy: 0.3818\n",
      "Epoch 839/1500\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.3757 - accuracy: 0.8636 - val_loss: 4.6426 - val_accuracy: 0.3500\n",
      "Epoch 840/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.3624 - accuracy: 0.8712 - val_loss: 4.6224 - val_accuracy: 0.3636\n",
      "Epoch 841/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.2817 - accuracy: 0.9091 - val_loss: 4.7711 - val_accuracy: 0.4045\n",
      "Epoch 842/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.2823 - accuracy: 0.9045 - val_loss: 4.7131 - val_accuracy: 0.3864\n",
      "Epoch 843/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.7038 - accuracy: 0.7727 - val_loss: 4.9790 - val_accuracy: 0.3455\n",
      "Epoch 844/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 1.2709 - accuracy: 0.6303 - val_loss: 5.4516 - val_accuracy: 0.3045\n",
      "Epoch 845/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.5722 - accuracy: 0.7970 - val_loss: 4.6502 - val_accuracy: 0.3773\n",
      "Epoch 846/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.2553 - accuracy: 0.9227 - val_loss: 4.5077 - val_accuracy: 0.3864\n",
      "Epoch 847/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.2741 - accuracy: 0.9076 - val_loss: 4.8029 - val_accuracy: 0.3682\n",
      "Epoch 848/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.2503 - accuracy: 0.9333 - val_loss: 4.8297 - val_accuracy: 0.3364\n",
      "Epoch 849/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.2694 - accuracy: 0.9227 - val_loss: 4.4551 - val_accuracy: 0.3909\n",
      "Epoch 850/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.3098 - accuracy: 0.8864 - val_loss: 4.7874 - val_accuracy: 0.4318\n",
      "Epoch 851/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.3553 - accuracy: 0.8712 - val_loss: 4.7751 - val_accuracy: 0.3909\n",
      "Epoch 852/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.2665 - accuracy: 0.9076 - val_loss: 4.6400 - val_accuracy: 0.3955\n",
      "Epoch 853/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.3140 - accuracy: 0.9106 - val_loss: 4.5345 - val_accuracy: 0.3636\n",
      "Epoch 854/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.2651 - accuracy: 0.9091 - val_loss: 4.6541 - val_accuracy: 0.3727\n",
      "Epoch 855/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.3971 - accuracy: 0.8682 - val_loss: 4.6566 - val_accuracy: 0.3909\n",
      "Epoch 856/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.9198 - accuracy: 0.6909 - val_loss: 6.1542 - val_accuracy: 0.2864\n",
      "Epoch 857/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.6742 - accuracy: 0.7667 - val_loss: 4.7531 - val_accuracy: 0.3636\n",
      "Epoch 858/1500\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.3732 - accuracy: 0.8697 - val_loss: 4.7109 - val_accuracy: 0.3500\n",
      "Epoch 859/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.2647 - accuracy: 0.9182 - val_loss: 4.5784 - val_accuracy: 0.3682\n",
      "Epoch 860/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.3852 - accuracy: 0.8621 - val_loss: 4.6910 - val_accuracy: 0.2773\n",
      "Epoch 861/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.6502 - accuracy: 0.7697 - val_loss: 5.0503 - val_accuracy: 0.4045\n",
      "Epoch 862/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.4703 - accuracy: 0.8364 - val_loss: 4.4097 - val_accuracy: 0.3818\n",
      "Epoch 863/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.3600 - accuracy: 0.8697 - val_loss: 4.6203 - val_accuracy: 0.4000\n",
      "Epoch 864/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.2913 - accuracy: 0.9015 - val_loss: 5.0630 - val_accuracy: 0.3682\n",
      "Epoch 865/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.2509 - accuracy: 0.9197 - val_loss: 5.1207 - val_accuracy: 0.2909\n",
      "Epoch 866/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.3284 - accuracy: 0.8894 - val_loss: 4.7365 - val_accuracy: 0.3636\n",
      "Epoch 867/1500\n",
      "83/83 [==============================] - 0s 666us/step - loss: 0.2527 - accuracy: 0.9091 - val_loss: 4.6749 - val_accuracy: 0.3545\n",
      "Epoch 868/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.2309 - accuracy: 0.9318 - val_loss: 4.6269 - val_accuracy: 0.3864\n",
      "Epoch 869/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.4395 - accuracy: 0.8470 - val_loss: 4.7297 - val_accuracy: 0.3955\n",
      "Epoch 870/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.6122 - accuracy: 0.7742 - val_loss: 4.6587 - val_accuracy: 0.3818\n",
      "Epoch 871/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.4941 - accuracy: 0.8000 - val_loss: 4.6762 - val_accuracy: 0.4273\n",
      "Epoch 872/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.3688 - accuracy: 0.8848 - val_loss: 4.9566 - val_accuracy: 0.3636\n",
      "Epoch 873/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.3437 - accuracy: 0.8833 - val_loss: 4.7109 - val_accuracy: 0.3636\n",
      "Epoch 874/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.8115 - accuracy: 0.7227 - val_loss: 4.9245 - val_accuracy: 0.3909\n",
      "Epoch 875/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.4562 - accuracy: 0.8364 - val_loss: 4.8986 - val_accuracy: 0.3500\n",
      "Epoch 876/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.4840 - accuracy: 0.8136 - val_loss: 4.6562 - val_accuracy: 0.3409\n",
      "Epoch 877/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.2713 - accuracy: 0.9121 - val_loss: 4.8223 - val_accuracy: 0.3318\n",
      "Epoch 878/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2700 - accuracy: 0.9076 - val_loss: 4.7492 - val_accuracy: 0.4045\n",
      "Epoch 879/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.2445 - accuracy: 0.9242 - val_loss: 4.7402 - val_accuracy: 0.3545\n",
      "Epoch 880/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.5896 - accuracy: 0.8076 - val_loss: 4.9836 - val_accuracy: 0.3227\n",
      "Epoch 881/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.4053 - accuracy: 0.8652 - val_loss: 5.1751 - val_accuracy: 0.3091\n",
      "Epoch 882/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.6654 - accuracy: 0.7515 - val_loss: 5.1383 - val_accuracy: 0.3591\n",
      "Epoch 883/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.3495 - accuracy: 0.8682 - val_loss: 4.6172 - val_accuracy: 0.3409\n",
      "Epoch 884/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.3079 - accuracy: 0.9045 - val_loss: 4.5948 - val_accuracy: 0.4000\n",
      "Epoch 885/1500\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.6386 - accuracy: 0.7803 - val_loss: 5.0103 - val_accuracy: 0.2909\n",
      "Epoch 886/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.8911 - accuracy: 0.7000 - val_loss: 4.6416 - val_accuracy: 0.3364\n",
      "Epoch 887/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.8050 - accuracy: 0.7288 - val_loss: 4.7324 - val_accuracy: 0.3455\n",
      "Epoch 888/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.2550 - accuracy: 0.9136 - val_loss: 4.5070 - val_accuracy: 0.3773\n",
      "Epoch 889/1500\n",
      "83/83 [==============================] - 0s 632us/step - loss: 0.2445 - accuracy: 0.9273 - val_loss: 4.6583 - val_accuracy: 0.3591\n",
      "Epoch 890/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.1726 - accuracy: 0.9652 - val_loss: 4.7128 - val_accuracy: 0.3364\n",
      "Epoch 891/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.3233 - accuracy: 0.9015 - val_loss: 4.6716 - val_accuracy: 0.3409\n",
      "Epoch 892/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.2887 - accuracy: 0.9061 - val_loss: 4.5653 - val_accuracy: 0.3636\n",
      "Epoch 893/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.2461 - accuracy: 0.9182 - val_loss: 4.3820 - val_accuracy: 0.3591\n",
      "Epoch 894/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.4161 - accuracy: 0.8394 - val_loss: 5.0555 - val_accuracy: 0.3227\n",
      "Epoch 895/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.5081 - accuracy: 0.8182 - val_loss: 4.7653 - val_accuracy: 0.3091\n",
      "Epoch 896/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.4672 - accuracy: 0.8455 - val_loss: 4.7088 - val_accuracy: 0.4182\n",
      "Epoch 897/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.3360 - accuracy: 0.8788 - val_loss: 4.9034 - val_accuracy: 0.3455\n",
      "Epoch 898/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.2960 - accuracy: 0.9061 - val_loss: 4.6399 - val_accuracy: 0.3136\n",
      "Epoch 899/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.3774 - accuracy: 0.8561 - val_loss: 4.8409 - val_accuracy: 0.3455\n",
      "Epoch 900/1500\n",
      "83/83 [==============================] - 0s 594us/step - loss: 0.5441 - accuracy: 0.8045 - val_loss: 4.8072 - val_accuracy: 0.3273\n",
      "Epoch 901/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2596 - accuracy: 0.9182 - val_loss: 4.5793 - val_accuracy: 0.3818\n",
      "Epoch 902/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.3578 - accuracy: 0.8712 - val_loss: 5.1672 - val_accuracy: 0.2955\n",
      "Epoch 903/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.9232 - accuracy: 0.6985 - val_loss: 5.6192 - val_accuracy: 0.2727\n",
      "Epoch 904/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 1.0917 - accuracy: 0.6561 - val_loss: 5.4167 - val_accuracy: 0.3227\n",
      "Epoch 905/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.3785 - accuracy: 0.8773 - val_loss: 4.7316 - val_accuracy: 0.4091\n",
      "Epoch 906/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.2297 - accuracy: 0.9409 - val_loss: 4.6950 - val_accuracy: 0.3818\n",
      "Epoch 907/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.2644 - accuracy: 0.9121 - val_loss: 4.7852 - val_accuracy: 0.3818\n",
      "Epoch 908/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.2601 - accuracy: 0.9152 - val_loss: 4.7828 - val_accuracy: 0.3909\n",
      "Epoch 909/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.2505 - accuracy: 0.9197 - val_loss: 4.9327 - val_accuracy: 0.3864\n",
      "Epoch 910/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2342 - accuracy: 0.9348 - val_loss: 4.6840 - val_accuracy: 0.3727\n",
      "Epoch 911/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.2947 - accuracy: 0.9000 - val_loss: 4.6972 - val_accuracy: 0.3500\n",
      "Epoch 912/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.2256 - accuracy: 0.9288 - val_loss: 4.6623 - val_accuracy: 0.3864\n",
      "Epoch 913/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.3095 - accuracy: 0.8939 - val_loss: 4.8823 - val_accuracy: 0.3682\n",
      "Epoch 914/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.3020 - accuracy: 0.9015 - val_loss: 4.6737 - val_accuracy: 0.4045\n",
      "Epoch 915/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.3049 - accuracy: 0.8955 - val_loss: 4.8682 - val_accuracy: 0.3773\n",
      "Epoch 916/1500\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.2619 - accuracy: 0.9091 - val_loss: 5.0154 - val_accuracy: 0.3455\n",
      "Epoch 917/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.2722 - accuracy: 0.9167 - val_loss: 4.6067 - val_accuracy: 0.3773\n",
      "Epoch 918/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.2134 - accuracy: 0.9318 - val_loss: 4.8630 - val_accuracy: 0.3591\n",
      "Epoch 919/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.5774 - accuracy: 0.7970 - val_loss: 5.4181 - val_accuracy: 0.3000\n",
      "Epoch 920/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 1.3578 - accuracy: 0.6076 - val_loss: 5.6733 - val_accuracy: 0.2864\n",
      "Epoch 921/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.8567 - accuracy: 0.7530 - val_loss: 5.2069 - val_accuracy: 0.3182\n",
      "Epoch 922/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.2864 - accuracy: 0.9167 - val_loss: 4.7417 - val_accuracy: 0.3955\n",
      "Epoch 923/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2181 - accuracy: 0.9424 - val_loss: 4.7096 - val_accuracy: 0.3955\n",
      "Epoch 924/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.3977 - accuracy: 0.8758 - val_loss: 4.6827 - val_accuracy: 0.4000\n",
      "Epoch 925/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.2143 - accuracy: 0.9424 - val_loss: 4.9336 - val_accuracy: 0.3909\n",
      "Epoch 926/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.3064 - accuracy: 0.8970 - val_loss: 4.7597 - val_accuracy: 0.3273\n",
      "Epoch 927/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.5256 - accuracy: 0.8227 - val_loss: 5.0634 - val_accuracy: 0.3182\n",
      "Epoch 928/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.4037 - accuracy: 0.8545 - val_loss: 4.8280 - val_accuracy: 0.3909\n",
      "Epoch 929/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.3192 - accuracy: 0.8848 - val_loss: 5.5211 - val_accuracy: 0.3409\n",
      "Epoch 930/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.4154 - accuracy: 0.8500 - val_loss: 5.0119 - val_accuracy: 0.3227\n",
      "Epoch 931/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.3256 - accuracy: 0.8894 - val_loss: 5.2461 - val_accuracy: 0.3000\n",
      "Epoch 932/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 1.2033 - accuracy: 0.6394 - val_loss: 5.2988 - val_accuracy: 0.3955\n",
      "Epoch 933/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.4115 - accuracy: 0.8530 - val_loss: 5.0097 - val_accuracy: 0.4091\n",
      "Epoch 934/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.2769 - accuracy: 0.9121 - val_loss: 5.3693 - val_accuracy: 0.3136\n",
      "Epoch 935/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.3113 - accuracy: 0.9015 - val_loss: 4.8898 - val_accuracy: 0.3818\n",
      "Epoch 936/1500\n",
      "83/83 [==============================] - 0s 591us/step - loss: 0.2941 - accuracy: 0.9030 - val_loss: 4.9464 - val_accuracy: 0.3955\n",
      "Epoch 937/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.1726 - accuracy: 0.9621 - val_loss: 5.1244 - val_accuracy: 0.4000\n",
      "Epoch 938/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.2259 - accuracy: 0.9394 - val_loss: 4.7313 - val_accuracy: 0.4364\n",
      "Epoch 939/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.4221 - accuracy: 0.8591 - val_loss: 5.2334 - val_accuracy: 0.3091\n",
      "Epoch 940/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.3900 - accuracy: 0.8606 - val_loss: 5.1186 - val_accuracy: 0.3182\n",
      "Epoch 941/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.3004 - accuracy: 0.8924 - val_loss: 4.9345 - val_accuracy: 0.3818\n",
      "Epoch 942/1500\n",
      "83/83 [==============================] - 0s 630us/step - loss: 0.5944 - accuracy: 0.7924 - val_loss: 5.0491 - val_accuracy: 0.4182\n",
      "Epoch 943/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.3453 - accuracy: 0.8712 - val_loss: 5.4880 - val_accuracy: 0.3045\n",
      "Epoch 944/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.3985 - accuracy: 0.8667 - val_loss: 5.3299 - val_accuracy: 0.3045\n",
      "Epoch 945/1500\n",
      "83/83 [==============================] - 0s 639us/step - loss: 0.4889 - accuracy: 0.8167 - val_loss: 5.1016 - val_accuracy: 0.3273\n",
      "Epoch 946/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.1916 - accuracy: 0.9394 - val_loss: 4.8738 - val_accuracy: 0.3727\n",
      "Epoch 947/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.1969 - accuracy: 0.9515 - val_loss: 4.9612 - val_accuracy: 0.3864\n",
      "Epoch 948/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 595us/step - loss: 0.2477 - accuracy: 0.9197 - val_loss: 5.1284 - val_accuracy: 0.3818\n",
      "Epoch 949/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.1940 - accuracy: 0.9455 - val_loss: 5.0033 - val_accuracy: 0.3636\n",
      "Epoch 950/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.5690 - accuracy: 0.7985 - val_loss: 4.7653 - val_accuracy: 0.3500\n",
      "Epoch 951/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.3629 - accuracy: 0.8803 - val_loss: 5.4405 - val_accuracy: 0.3091\n",
      "Epoch 952/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.5164 - accuracy: 0.8167 - val_loss: 5.0616 - val_accuracy: 0.3455\n",
      "Epoch 953/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.3203 - accuracy: 0.8758 - val_loss: 4.7564 - val_accuracy: 0.3909\n",
      "Epoch 954/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.2153 - accuracy: 0.9273 - val_loss: 5.0993 - val_accuracy: 0.3682\n",
      "Epoch 955/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.2072 - accuracy: 0.9333 - val_loss: 5.4386 - val_accuracy: 0.3318\n",
      "Epoch 956/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.3029 - accuracy: 0.8939 - val_loss: 5.2824 - val_accuracy: 0.3318\n",
      "Epoch 957/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.4208 - accuracy: 0.8530 - val_loss: 5.4924 - val_accuracy: 0.3727\n",
      "Epoch 958/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.7690 - accuracy: 0.7348 - val_loss: 4.7285 - val_accuracy: 0.3500\n",
      "Epoch 959/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.3083 - accuracy: 0.9061 - val_loss: 4.9339 - val_accuracy: 0.4045\n",
      "Epoch 960/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.3024 - accuracy: 0.8848 - val_loss: 5.0525 - val_accuracy: 0.3636\n",
      "Epoch 961/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.2250 - accuracy: 0.9364 - val_loss: 4.8431 - val_accuracy: 0.4182\n",
      "Epoch 962/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.2975 - accuracy: 0.9030 - val_loss: 4.9444 - val_accuracy: 0.4000\n",
      "Epoch 963/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.2025 - accuracy: 0.9470 - val_loss: 4.9032 - val_accuracy: 0.4045\n",
      "Epoch 964/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.6874 - accuracy: 0.7742 - val_loss: 6.0704 - val_accuracy: 0.3000\n",
      "Epoch 965/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.5415 - accuracy: 0.7955 - val_loss: 5.2429 - val_accuracy: 0.3409\n",
      "Epoch 966/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.3109 - accuracy: 0.8909 - val_loss: 5.0762 - val_accuracy: 0.3364\n",
      "Epoch 967/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.3104 - accuracy: 0.8894 - val_loss: 4.7823 - val_accuracy: 0.4045\n",
      "Epoch 968/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2958 - accuracy: 0.9015 - val_loss: 4.7833 - val_accuracy: 0.3955\n",
      "Epoch 969/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.4166 - accuracy: 0.8455 - val_loss: 5.0639 - val_accuracy: 0.3864\n",
      "Epoch 970/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.3662 - accuracy: 0.8742 - val_loss: 5.1180 - val_accuracy: 0.3364\n",
      "Epoch 971/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 1.0640 - accuracy: 0.6985 - val_loss: 6.6394 - val_accuracy: 0.2818\n",
      "Epoch 972/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 1.3023 - accuracy: 0.6258 - val_loss: 4.9634 - val_accuracy: 0.3000\n",
      "Epoch 973/1500\n",
      "83/83 [==============================] - 0s 652us/step - loss: 0.2999 - accuracy: 0.9000 - val_loss: 4.6388 - val_accuracy: 0.3591\n",
      "Epoch 974/1500\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.3692 - accuracy: 0.8591 - val_loss: 4.7119 - val_accuracy: 0.4045\n",
      "Epoch 975/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.2389 - accuracy: 0.9227 - val_loss: 4.8430 - val_accuracy: 0.3318\n",
      "Epoch 976/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.2281 - accuracy: 0.9258 - val_loss: 4.8951 - val_accuracy: 0.3455\n",
      "Epoch 977/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.1799 - accuracy: 0.9530 - val_loss: 4.5720 - val_accuracy: 0.4000\n",
      "Epoch 978/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.2178 - accuracy: 0.9333 - val_loss: 4.7044 - val_accuracy: 0.3727\n",
      "Epoch 979/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.2691 - accuracy: 0.9061 - val_loss: 5.0190 - val_accuracy: 0.3818\n",
      "Epoch 980/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.2500 - accuracy: 0.9182 - val_loss: 4.8001 - val_accuracy: 0.3409\n",
      "Epoch 981/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.7765 - accuracy: 0.7258 - val_loss: 5.3079 - val_accuracy: 0.3909\n",
      "Epoch 982/1500\n",
      "83/83 [==============================] - 0s 659us/step - loss: 0.9506 - accuracy: 0.6848 - val_loss: 5.2103 - val_accuracy: 0.3545\n",
      "Epoch 983/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.3666 - accuracy: 0.8742 - val_loss: 4.6385 - val_accuracy: 0.4227\n",
      "Epoch 984/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.2236 - accuracy: 0.9303 - val_loss: 4.9483 - val_accuracy: 0.3864\n",
      "Epoch 985/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.2486 - accuracy: 0.9106 - val_loss: 4.6442 - val_accuracy: 0.4364\n",
      "Epoch 986/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.2244 - accuracy: 0.9258 - val_loss: 4.7505 - val_accuracy: 0.4091\n",
      "Epoch 987/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.1886 - accuracy: 0.9394 - val_loss: 4.7481 - val_accuracy: 0.3909\n",
      "Epoch 988/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.2054 - accuracy: 0.9470 - val_loss: 5.1494 - val_accuracy: 0.3545\n",
      "Epoch 989/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.2165 - accuracy: 0.9273 - val_loss: 4.8705 - val_accuracy: 0.3955\n",
      "Epoch 990/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.2121 - accuracy: 0.9394 - val_loss: 5.2278 - val_accuracy: 0.3636\n",
      "Epoch 991/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2965 - accuracy: 0.8970 - val_loss: 4.8559 - val_accuracy: 0.4091\n",
      "Epoch 992/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.5416 - accuracy: 0.8076 - val_loss: 5.6618 - val_accuracy: 0.2409\n",
      "Epoch 993/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 1.4711 - accuracy: 0.6273 - val_loss: 4.5778 - val_accuracy: 0.3955\n",
      "Epoch 994/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.2905 - accuracy: 0.9106 - val_loss: 4.8620 - val_accuracy: 0.3500\n",
      "Epoch 995/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.4695 - accuracy: 0.8242 - val_loss: 4.9366 - val_accuracy: 0.3818\n",
      "Epoch 996/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.2580 - accuracy: 0.9106 - val_loss: 4.9228 - val_accuracy: 0.3318\n",
      "Epoch 997/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.1866 - accuracy: 0.9561 - val_loss: 4.9450 - val_accuracy: 0.3773\n",
      "Epoch 998/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.2497 - accuracy: 0.9212 - val_loss: 4.8158 - val_accuracy: 0.3682\n",
      "Epoch 999/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.1834 - accuracy: 0.9379 - val_loss: 4.7892 - val_accuracy: 0.4136\n",
      "Epoch 1000/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.2050 - accuracy: 0.9424 - val_loss: 4.8438 - val_accuracy: 0.3455\n",
      "Epoch 1001/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.2082 - accuracy: 0.9318 - val_loss: 4.7206 - val_accuracy: 0.4000\n",
      "Epoch 1002/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.3389 - accuracy: 0.8848 - val_loss: 4.8616 - val_accuracy: 0.3682\n",
      "Epoch 1003/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.2863 - accuracy: 0.9000 - val_loss: 4.9411 - val_accuracy: 0.3818\n",
      "Epoch 1004/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.3563 - accuracy: 0.8742 - val_loss: 5.0604 - val_accuracy: 0.3773\n",
      "Epoch 1005/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.9625 - accuracy: 0.7167 - val_loss: 5.7219 - val_accuracy: 0.2818\n",
      "Epoch 1006/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 1.1614 - accuracy: 0.6545 - val_loss: 5.1867 - val_accuracy: 0.3045\n",
      "Epoch 1007/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.4499 - accuracy: 0.8409 - val_loss: 4.7523 - val_accuracy: 0.4091\n",
      "Epoch 1008/1500\n",
      "83/83 [==============================] - 0s 637us/step - loss: 0.3223 - accuracy: 0.8894 - val_loss: 4.6435 - val_accuracy: 0.3909\n",
      "Epoch 1009/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.2639 - accuracy: 0.9106 - val_loss: 4.9459 - val_accuracy: 0.3227\n",
      "Epoch 1010/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.2110 - accuracy: 0.9212 - val_loss: 4.7206 - val_accuracy: 0.4182\n",
      "Epoch 1011/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.2398 - accuracy: 0.9303 - val_loss: 4.7324 - val_accuracy: 0.3545\n",
      "Epoch 1012/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.3178 - accuracy: 0.8848 - val_loss: 4.9046 - val_accuracy: 0.3909\n",
      "Epoch 1013/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.2909 - accuracy: 0.8924 - val_loss: 5.3014 - val_accuracy: 0.3455\n",
      "Epoch 1014/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.3413 - accuracy: 0.8970 - val_loss: 4.8492 - val_accuracy: 0.3864\n",
      "Epoch 1015/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.3044 - accuracy: 0.9076 - val_loss: 4.8939 - val_accuracy: 0.4000\n",
      "Epoch 1016/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.5403 - accuracy: 0.8182 - val_loss: 5.0815 - val_accuracy: 0.3909\n",
      "Epoch 1017/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.4073 - accuracy: 0.8470 - val_loss: 5.0383 - val_accuracy: 0.3227\n",
      "Epoch 1018/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.4395 - accuracy: 0.8470 - val_loss: 4.8420 - val_accuracy: 0.4000\n",
      "Epoch 1019/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.2375 - accuracy: 0.9258 - val_loss: 5.6233 - val_accuracy: 0.2864\n",
      "Epoch 1020/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 1.2009 - accuracy: 0.6303 - val_loss: 5.8736 - val_accuracy: 0.2636\n",
      "Epoch 1021/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.4426 - accuracy: 0.8470 - val_loss: 4.7312 - val_accuracy: 0.4000\n",
      "Epoch 1022/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.2202 - accuracy: 0.9303 - val_loss: 4.7187 - val_accuracy: 0.4227\n",
      "Epoch 1023/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.1608 - accuracy: 0.9682 - val_loss: 4.7585 - val_accuracy: 0.4318\n",
      "Epoch 1024/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.2086 - accuracy: 0.9288 - val_loss: 4.8517 - val_accuracy: 0.3955\n",
      "Epoch 1025/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.2542 - accuracy: 0.9030 - val_loss: 4.8170 - val_accuracy: 0.3864\n",
      "Epoch 1026/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.4679 - accuracy: 0.8136 - val_loss: 4.9390 - val_accuracy: 0.3545\n",
      "Epoch 1027/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.2964 - accuracy: 0.8818 - val_loss: 4.9061 - val_accuracy: 0.3909\n",
      "Epoch 1028/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.2473 - accuracy: 0.9152 - val_loss: 5.1699 - val_accuracy: 0.3409\n",
      "Epoch 1029/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.3945 - accuracy: 0.8515 - val_loss: 4.8640 - val_accuracy: 0.3864\n",
      "Epoch 1030/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.2643 - accuracy: 0.9015 - val_loss: 4.9839 - val_accuracy: 0.3727\n",
      "Epoch 1031/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.4215 - accuracy: 0.8394 - val_loss: 5.2107 - val_accuracy: 0.3182\n",
      "Epoch 1032/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.5341 - accuracy: 0.8197 - val_loss: 5.4306 - val_accuracy: 0.2909\n",
      "Epoch 1033/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.5159 - accuracy: 0.8136 - val_loss: 4.8869 - val_accuracy: 0.4000\n",
      "Epoch 1034/1500\n",
      "83/83 [==============================] - 0s 646us/step - loss: 0.4426 - accuracy: 0.8348 - val_loss: 5.1379 - val_accuracy: 0.3227\n",
      "Epoch 1035/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.3831 - accuracy: 0.8545 - val_loss: 5.1272 - val_accuracy: 0.3864\n",
      "Epoch 1036/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.3432 - accuracy: 0.8697 - val_loss: 5.2302 - val_accuracy: 0.4045\n",
      "Epoch 1037/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2555 - accuracy: 0.9106 - val_loss: 5.0360 - val_accuracy: 0.3727\n",
      "Epoch 1038/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.3639 - accuracy: 0.8773 - val_loss: 4.9500 - val_accuracy: 0.4000\n",
      "Epoch 1039/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.1842 - accuracy: 0.9470 - val_loss: 4.9915 - val_accuracy: 0.3727\n",
      "Epoch 1040/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.2457 - accuracy: 0.9182 - val_loss: 5.3260 - val_accuracy: 0.3773\n",
      "Epoch 1041/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.6172 - accuracy: 0.7864 - val_loss: 5.6386 - val_accuracy: 0.3682\n",
      "Epoch 1042/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.7190 - accuracy: 0.7212 - val_loss: 5.2094 - val_accuracy: 0.3455\n",
      "Epoch 1043/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.2691 - accuracy: 0.9000 - val_loss: 4.8614 - val_accuracy: 0.4045\n",
      "Epoch 1044/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.1855 - accuracy: 0.9530 - val_loss: 4.7836 - val_accuracy: 0.4545\n",
      "Epoch 1045/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.3075 - accuracy: 0.8970 - val_loss: 4.9686 - val_accuracy: 0.3727\n",
      "Epoch 1046/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.1854 - accuracy: 0.9379 - val_loss: 5.0804 - val_accuracy: 0.3727\n",
      "Epoch 1047/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2605 - accuracy: 0.9000 - val_loss: 5.0524 - val_accuracy: 0.3682\n",
      "Epoch 1048/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.6487 - accuracy: 0.7924 - val_loss: 5.7047 - val_accuracy: 0.3409\n",
      "Epoch 1049/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.3831 - accuracy: 0.8576 - val_loss: 4.8298 - val_accuracy: 0.3273\n",
      "Epoch 1050/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.7518 - accuracy: 0.7273 - val_loss: 5.7103 - val_accuracy: 0.2545\n",
      "Epoch 1051/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.5672 - accuracy: 0.7879 - val_loss: 5.2652 - val_accuracy: 0.3773\n",
      "Epoch 1052/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.2132 - accuracy: 0.9303 - val_loss: 5.1447 - val_accuracy: 0.3682\n",
      "Epoch 1053/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.2052 - accuracy: 0.9258 - val_loss: 4.9094 - val_accuracy: 0.3909\n",
      "Epoch 1054/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.1735 - accuracy: 0.9485 - val_loss: 4.9480 - val_accuracy: 0.3955\n",
      "Epoch 1055/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.2224 - accuracy: 0.9212 - val_loss: 5.1882 - val_accuracy: 0.3636\n",
      "Epoch 1056/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.3962 - accuracy: 0.8364 - val_loss: 5.6432 - val_accuracy: 0.2682\n",
      "Epoch 1057/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.5538 - accuracy: 0.8045 - val_loss: 5.5844 - val_accuracy: 0.3227\n",
      "Epoch 1058/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.6876 - accuracy: 0.7576 - val_loss: 6.2953 - val_accuracy: 0.2864\n",
      "Epoch 1059/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.3826 - accuracy: 0.8833 - val_loss: 5.4945 - val_accuracy: 0.3773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1060/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.2364 - accuracy: 0.9242 - val_loss: 5.1939 - val_accuracy: 0.3591\n",
      "Epoch 1061/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.3218 - accuracy: 0.8818 - val_loss: 5.2031 - val_accuracy: 0.3727\n",
      "Epoch 1062/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.2357 - accuracy: 0.9288 - val_loss: 5.0761 - val_accuracy: 0.3909\n",
      "Epoch 1063/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.2355 - accuracy: 0.9212 - val_loss: 4.9746 - val_accuracy: 0.4318\n",
      "Epoch 1064/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.2041 - accuracy: 0.9348 - val_loss: 4.9998 - val_accuracy: 0.4136\n",
      "Epoch 1065/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.1796 - accuracy: 0.9485 - val_loss: 4.9764 - val_accuracy: 0.4227\n",
      "Epoch 1066/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.2373 - accuracy: 0.9167 - val_loss: 5.3040 - val_accuracy: 0.3727\n",
      "Epoch 1067/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.2169 - accuracy: 0.9227 - val_loss: 5.2219 - val_accuracy: 0.3773\n",
      "Epoch 1068/1500\n",
      "83/83 [==============================] - 0s 697us/step - loss: 0.2453 - accuracy: 0.9167 - val_loss: 5.6056 - val_accuracy: 0.3273\n",
      "Epoch 1069/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.2581 - accuracy: 0.9015 - val_loss: 4.9459 - val_accuracy: 0.4455\n",
      "Epoch 1070/1500\n",
      "83/83 [==============================] - 0s 653us/step - loss: 0.2373 - accuracy: 0.9197 - val_loss: 4.9152 - val_accuracy: 0.3545\n",
      "Epoch 1071/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.1570 - accuracy: 0.9652 - val_loss: 5.1174 - val_accuracy: 0.4136\n",
      "Epoch 1072/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.2178 - accuracy: 0.9303 - val_loss: 5.2226 - val_accuracy: 0.3909\n",
      "Epoch 1073/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.5476 - accuracy: 0.7955 - val_loss: 5.1539 - val_accuracy: 0.3955\n",
      "Epoch 1074/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.4471 - accuracy: 0.8500 - val_loss: 5.1861 - val_accuracy: 0.3864\n",
      "Epoch 1075/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.4430 - accuracy: 0.8561 - val_loss: 5.8121 - val_accuracy: 0.2727\n",
      "Epoch 1076/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.5920 - accuracy: 0.7894 - val_loss: 5.2351 - val_accuracy: 0.3636\n",
      "Epoch 1077/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.5570 - accuracy: 0.8182 - val_loss: 5.5226 - val_accuracy: 0.3182\n",
      "Epoch 1078/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.2611 - accuracy: 0.9121 - val_loss: 5.1208 - val_accuracy: 0.4318\n",
      "Epoch 1079/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.1959 - accuracy: 0.9364 - val_loss: 5.2048 - val_accuracy: 0.3636\n",
      "Epoch 1080/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2727 - accuracy: 0.9106 - val_loss: 5.1675 - val_accuracy: 0.4182\n",
      "Epoch 1081/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.1849 - accuracy: 0.9409 - val_loss: 5.0518 - val_accuracy: 0.4000\n",
      "Epoch 1082/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.2079 - accuracy: 0.9258 - val_loss: 5.5508 - val_accuracy: 0.3773\n",
      "Epoch 1083/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.5363 - accuracy: 0.8030 - val_loss: 6.1928 - val_accuracy: 0.2864\n",
      "Epoch 1084/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 1.3837 - accuracy: 0.6091 - val_loss: 6.1216 - val_accuracy: 0.3136\n",
      "Epoch 1085/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.4663 - accuracy: 0.8515 - val_loss: 5.6546 - val_accuracy: 0.3318\n",
      "Epoch 1086/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.5009 - accuracy: 0.8015 - val_loss: 5.5501 - val_accuracy: 0.2955\n",
      "Epoch 1087/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.5422 - accuracy: 0.7955 - val_loss: 5.5614 - val_accuracy: 0.3091\n",
      "Epoch 1088/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.9421 - accuracy: 0.7091 - val_loss: 5.1295 - val_accuracy: 0.3636\n",
      "Epoch 1089/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.3104 - accuracy: 0.8833 - val_loss: 5.0793 - val_accuracy: 0.3136\n",
      "Epoch 1090/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.2537 - accuracy: 0.9182 - val_loss: 4.9514 - val_accuracy: 0.3727\n",
      "Epoch 1091/1500\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.1837 - accuracy: 0.9515 - val_loss: 4.9729 - val_accuracy: 0.4409\n",
      "Epoch 1092/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.2423 - accuracy: 0.9364 - val_loss: 4.9038 - val_accuracy: 0.4182\n",
      "Epoch 1093/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.1887 - accuracy: 0.9364 - val_loss: 5.2803 - val_accuracy: 0.3545\n",
      "Epoch 1094/1500\n",
      "83/83 [==============================] - 0s 692us/step - loss: 0.4455 - accuracy: 0.8364 - val_loss: 5.3345 - val_accuracy: 0.3227\n",
      "Epoch 1095/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.4360 - accuracy: 0.8273 - val_loss: 5.2866 - val_accuracy: 0.3818\n",
      "Epoch 1096/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.2342 - accuracy: 0.9136 - val_loss: 5.2494 - val_accuracy: 0.3909\n",
      "Epoch 1097/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.2284 - accuracy: 0.9333 - val_loss: 4.9943 - val_accuracy: 0.4091\n",
      "Epoch 1098/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.1659 - accuracy: 0.9470 - val_loss: 4.9291 - val_accuracy: 0.3773\n",
      "Epoch 1099/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.1618 - accuracy: 0.9515 - val_loss: 5.0001 - val_accuracy: 0.4091\n",
      "Epoch 1100/1500\n",
      "83/83 [==============================] - 0s 596us/step - loss: 0.4298 - accuracy: 0.8424 - val_loss: 5.4691 - val_accuracy: 0.3364\n",
      "Epoch 1101/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.6702 - accuracy: 0.7788 - val_loss: 5.3305 - val_accuracy: 0.2773\n",
      "Epoch 1102/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.3169 - accuracy: 0.9000 - val_loss: 5.0688 - val_accuracy: 0.3955\n",
      "Epoch 1103/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.2585 - accuracy: 0.8985 - val_loss: 5.0365 - val_accuracy: 0.3727\n",
      "Epoch 1104/1500\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.2757 - accuracy: 0.9000 - val_loss: 5.0410 - val_accuracy: 0.3955\n",
      "Epoch 1105/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.2781 - accuracy: 0.9030 - val_loss: 5.1274 - val_accuracy: 0.4318\n",
      "Epoch 1106/1500\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.2087 - accuracy: 0.9227 - val_loss: 5.0930 - val_accuracy: 0.3818\n",
      "Epoch 1107/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.1773 - accuracy: 0.9500 - val_loss: 5.3732 - val_accuracy: 0.3545\n",
      "Epoch 1108/1500\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.2120 - accuracy: 0.9333 - val_loss: 4.8174 - val_accuracy: 0.4273\n",
      "Epoch 1109/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.4075 - accuracy: 0.8621 - val_loss: 5.7796 - val_accuracy: 0.3318\n",
      "Epoch 1110/1500\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.9564 - accuracy: 0.7333 - val_loss: 5.8843 - val_accuracy: 0.3500\n",
      "Epoch 1111/1500\n",
      "83/83 [==============================] - 0s 594us/step - loss: 0.6778 - accuracy: 0.7773 - val_loss: 5.5157 - val_accuracy: 0.3409\n",
      "Epoch 1112/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.2360 - accuracy: 0.9273 - val_loss: 5.1255 - val_accuracy: 0.3682\n",
      "Epoch 1113/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.2131 - accuracy: 0.9303 - val_loss: 5.1365 - val_accuracy: 0.3864\n",
      "Epoch 1114/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.2093 - accuracy: 0.9333 - val_loss: 5.1622 - val_accuracy: 0.3818\n",
      "Epoch 1115/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.2328 - accuracy: 0.9167 - val_loss: 5.2736 - val_accuracy: 0.3409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1116/1500\n",
      "83/83 [==============================] - 0s 595us/step - loss: 1.2476 - accuracy: 0.6818 - val_loss: 5.3068 - val_accuracy: 0.3636\n",
      "Epoch 1117/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.5677 - accuracy: 0.7848 - val_loss: 5.0024 - val_accuracy: 0.3864\n",
      "Epoch 1118/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.1999 - accuracy: 0.9470 - val_loss: 4.9645 - val_accuracy: 0.3818\n",
      "Epoch 1119/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.1656 - accuracy: 0.9591 - val_loss: 5.1233 - val_accuracy: 0.4182\n",
      "Epoch 1120/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.1475 - accuracy: 0.9576 - val_loss: 4.7652 - val_accuracy: 0.4318\n",
      "Epoch 1121/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.2261 - accuracy: 0.9242 - val_loss: 4.9199 - val_accuracy: 0.4045\n",
      "Epoch 1122/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.2352 - accuracy: 0.9212 - val_loss: 5.2047 - val_accuracy: 0.3909\n",
      "Epoch 1123/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.3009 - accuracy: 0.8879 - val_loss: 5.0780 - val_accuracy: 0.4136\n",
      "Epoch 1124/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.2606 - accuracy: 0.9167 - val_loss: 5.3473 - val_accuracy: 0.3318\n",
      "Epoch 1125/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.5358 - accuracy: 0.8091 - val_loss: 5.7231 - val_accuracy: 0.3409\n",
      "Epoch 1126/1500\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.4882 - accuracy: 0.8288 - val_loss: 5.4822 - val_accuracy: 0.3636\n",
      "Epoch 1127/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.1640 - accuracy: 0.9561 - val_loss: 5.0150 - val_accuracy: 0.3727\n",
      "Epoch 1128/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.5873 - accuracy: 0.8091 - val_loss: 5.4682 - val_accuracy: 0.3409\n",
      "Epoch 1129/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.9219 - accuracy: 0.6909 - val_loss: 5.2352 - val_accuracy: 0.3364\n",
      "Epoch 1130/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.4623 - accuracy: 0.8227 - val_loss: 5.3595 - val_accuracy: 0.3545\n",
      "Epoch 1131/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.1931 - accuracy: 0.9424 - val_loss: 4.7694 - val_accuracy: 0.4136\n",
      "Epoch 1132/1500\n",
      "83/83 [==============================] - 0s 595us/step - loss: 0.2061 - accuracy: 0.9242 - val_loss: 5.2934 - val_accuracy: 0.3364\n",
      "Epoch 1133/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.2814 - accuracy: 0.8924 - val_loss: 5.3770 - val_accuracy: 0.3818\n",
      "Epoch 1134/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.4550 - accuracy: 0.8258 - val_loss: 5.2171 - val_accuracy: 0.3909\n",
      "Epoch 1135/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.3113 - accuracy: 0.8864 - val_loss: 5.1762 - val_accuracy: 0.3864\n",
      "Epoch 1136/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.1861 - accuracy: 0.9379 - val_loss: 5.1219 - val_accuracy: 0.3864\n",
      "Epoch 1137/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.2000 - accuracy: 0.9379 - val_loss: 5.2335 - val_accuracy: 0.4091\n",
      "Epoch 1138/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.2224 - accuracy: 0.9303 - val_loss: 5.1969 - val_accuracy: 0.3591\n",
      "Epoch 1139/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.3458 - accuracy: 0.8788 - val_loss: 4.9851 - val_accuracy: 0.3727\n",
      "Epoch 1140/1500\n",
      "83/83 [==============================] - 0s 595us/step - loss: 0.3327 - accuracy: 0.8682 - val_loss: 5.3465 - val_accuracy: 0.3682\n",
      "Epoch 1141/1500\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.2644 - accuracy: 0.8985 - val_loss: 5.1777 - val_accuracy: 0.3545\n",
      "Epoch 1142/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.1628 - accuracy: 0.9515 - val_loss: 5.0217 - val_accuracy: 0.4273\n",
      "Epoch 1143/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.3856 - accuracy: 0.8530 - val_loss: 5.4832 - val_accuracy: 0.4091\n",
      "Epoch 1144/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.3927 - accuracy: 0.8485 - val_loss: 5.1391 - val_accuracy: 0.3864\n",
      "Epoch 1145/1500\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.6647 - accuracy: 0.7652 - val_loss: 6.5153 - val_accuracy: 0.2500\n",
      "Epoch 1146/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.6635 - accuracy: 0.7712 - val_loss: 5.9745 - val_accuracy: 0.2955\n",
      "Epoch 1147/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.3400 - accuracy: 0.8879 - val_loss: 5.2167 - val_accuracy: 0.4000\n",
      "Epoch 1148/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.2470 - accuracy: 0.8955 - val_loss: 5.0633 - val_accuracy: 0.3727\n",
      "Epoch 1149/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.4867 - accuracy: 0.8167 - val_loss: 5.2604 - val_accuracy: 0.3909\n",
      "Epoch 1150/1500\n",
      "83/83 [==============================] - 0s 596us/step - loss: 0.3026 - accuracy: 0.8955 - val_loss: 4.8740 - val_accuracy: 0.4045\n",
      "Epoch 1151/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.1752 - accuracy: 0.9500 - val_loss: 5.1180 - val_accuracy: 0.4045\n",
      "Epoch 1152/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.1688 - accuracy: 0.9409 - val_loss: 5.1619 - val_accuracy: 0.3818\n",
      "Epoch 1153/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.2099 - accuracy: 0.9318 - val_loss: 5.1992 - val_accuracy: 0.3682\n",
      "Epoch 1154/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.3053 - accuracy: 0.9015 - val_loss: 5.1734 - val_accuracy: 0.3909\n",
      "Epoch 1155/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.8380 - accuracy: 0.7379 - val_loss: 6.4313 - val_accuracy: 0.2727\n",
      "Epoch 1156/1500\n",
      "83/83 [==============================] - 0s 594us/step - loss: 1.6262 - accuracy: 0.5773 - val_loss: 5.3072 - val_accuracy: 0.3636\n",
      "Epoch 1157/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.3461 - accuracy: 0.8561 - val_loss: 5.4956 - val_accuracy: 0.2955\n",
      "Epoch 1158/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 1.0688 - accuracy: 0.6667 - val_loss: 5.1082 - val_accuracy: 0.3682\n",
      "Epoch 1159/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.3392 - accuracy: 0.8758 - val_loss: 4.8065 - val_accuracy: 0.4000\n",
      "Epoch 1160/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.2187 - accuracy: 0.9439 - val_loss: 4.9752 - val_accuracy: 0.4273\n",
      "Epoch 1161/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.1319 - accuracy: 0.9697 - val_loss: 4.9966 - val_accuracy: 0.3864\n",
      "Epoch 1162/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.1567 - accuracy: 0.9576 - val_loss: 4.9577 - val_accuracy: 0.4000\n",
      "Epoch 1163/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.1784 - accuracy: 0.9424 - val_loss: 5.0151 - val_accuracy: 0.3773\n",
      "Epoch 1164/1500\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.2253 - accuracy: 0.9227 - val_loss: 4.8380 - val_accuracy: 0.3955\n",
      "Epoch 1165/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.1724 - accuracy: 0.9470 - val_loss: 5.1260 - val_accuracy: 0.4182\n",
      "Epoch 1166/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.1954 - accuracy: 0.9318 - val_loss: 5.1990 - val_accuracy: 0.3955\n",
      "Epoch 1167/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.2962 - accuracy: 0.8894 - val_loss: 4.8983 - val_accuracy: 0.3636\n",
      "Epoch 1168/1500\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.2475 - accuracy: 0.9106 - val_loss: 5.2854 - val_accuracy: 0.3318\n",
      "Epoch 1169/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.5449 - accuracy: 0.7985 - val_loss: 5.0636 - val_accuracy: 0.3773\n",
      "Epoch 1170/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.1941 - accuracy: 0.9394 - val_loss: 5.0777 - val_accuracy: 0.3909\n",
      "Epoch 1171/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.1842 - accuracy: 0.9485 - val_loss: 4.9726 - val_accuracy: 0.3909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1172/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.2202 - accuracy: 0.9167 - val_loss: 5.0687 - val_accuracy: 0.4227\n",
      "Epoch 1173/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.1786 - accuracy: 0.9470 - val_loss: 5.1586 - val_accuracy: 0.3500\n",
      "Epoch 1174/1500\n",
      "83/83 [==============================] - 0s 635us/step - loss: 0.2386 - accuracy: 0.9242 - val_loss: 5.5099 - val_accuracy: 0.3545\n",
      "Epoch 1175/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.3252 - accuracy: 0.8879 - val_loss: 5.2148 - val_accuracy: 0.3727\n",
      "Epoch 1176/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.4198 - accuracy: 0.8515 - val_loss: 5.2051 - val_accuracy: 0.4273\n",
      "Epoch 1177/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.4623 - accuracy: 0.8424 - val_loss: 5.2434 - val_accuracy: 0.3591\n",
      "Epoch 1178/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.2016 - accuracy: 0.9394 - val_loss: 5.1762 - val_accuracy: 0.3955\n",
      "Epoch 1179/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.1656 - accuracy: 0.9424 - val_loss: 4.9807 - val_accuracy: 0.3727\n",
      "Epoch 1180/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.3459 - accuracy: 0.8576 - val_loss: 4.9953 - val_accuracy: 0.4000\n",
      "Epoch 1181/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.5958 - accuracy: 0.7939 - val_loss: 5.4309 - val_accuracy: 0.3818\n",
      "Epoch 1182/1500\n",
      "83/83 [==============================] - 0s 592us/step - loss: 1.8559 - accuracy: 0.5348 - val_loss: 5.4848 - val_accuracy: 0.3000\n",
      "Epoch 1183/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 1.8853 - accuracy: 0.5424 - val_loss: 5.3274 - val_accuracy: 0.2727\n",
      "Epoch 1184/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 1.3027 - accuracy: 0.6273 - val_loss: 5.0333 - val_accuracy: 0.3500\n",
      "Epoch 1185/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.6615 - accuracy: 0.7818 - val_loss: 5.4534 - val_accuracy: 0.3364\n",
      "Epoch 1186/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.6432 - accuracy: 0.7712 - val_loss: 5.2720 - val_accuracy: 0.3136\n",
      "Epoch 1187/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.4643 - accuracy: 0.8394 - val_loss: 4.8781 - val_accuracy: 0.3909\n",
      "Epoch 1188/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.3066 - accuracy: 0.9030 - val_loss: 5.0207 - val_accuracy: 0.3727\n",
      "Epoch 1189/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.3827 - accuracy: 0.8758 - val_loss: 5.0113 - val_accuracy: 0.3364\n",
      "Epoch 1190/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.2695 - accuracy: 0.8970 - val_loss: 4.9380 - val_accuracy: 0.3818\n",
      "Epoch 1191/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.1874 - accuracy: 0.9394 - val_loss: 4.9567 - val_accuracy: 0.3727\n",
      "Epoch 1192/1500\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.3448 - accuracy: 0.8636 - val_loss: 5.2626 - val_accuracy: 0.3182\n",
      "Epoch 1193/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.7006 - accuracy: 0.7697 - val_loss: 4.8602 - val_accuracy: 0.3818\n",
      "Epoch 1194/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.2617 - accuracy: 0.9106 - val_loss: 4.9352 - val_accuracy: 0.3818\n",
      "Epoch 1195/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.1903 - accuracy: 0.9348 - val_loss: 4.8858 - val_accuracy: 0.3955\n",
      "Epoch 1196/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.1807 - accuracy: 0.9379 - val_loss: 5.2610 - val_accuracy: 0.3318\n",
      "Epoch 1197/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.2062 - accuracy: 0.9364 - val_loss: 4.9778 - val_accuracy: 0.3773\n",
      "Epoch 1198/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.2290 - accuracy: 0.9212 - val_loss: 4.8957 - val_accuracy: 0.3864\n",
      "Epoch 1199/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.2371 - accuracy: 0.9212 - val_loss: 5.0557 - val_accuracy: 0.3818\n",
      "Epoch 1200/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.1772 - accuracy: 0.9455 - val_loss: 4.9914 - val_accuracy: 0.3409\n",
      "Epoch 1201/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.1634 - accuracy: 0.9576 - val_loss: 5.1682 - val_accuracy: 0.3955\n",
      "Epoch 1202/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.2627 - accuracy: 0.9106 - val_loss: 5.5242 - val_accuracy: 0.3045\n",
      "Epoch 1203/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.7426 - accuracy: 0.7455 - val_loss: 5.1349 - val_accuracy: 0.3818\n",
      "Epoch 1204/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.3342 - accuracy: 0.8924 - val_loss: 5.2091 - val_accuracy: 0.3455\n",
      "Epoch 1205/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.3025 - accuracy: 0.8864 - val_loss: 5.2422 - val_accuracy: 0.3682\n",
      "Epoch 1206/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.6036 - accuracy: 0.7636 - val_loss: 5.0782 - val_accuracy: 0.3636\n",
      "Epoch 1207/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.3258 - accuracy: 0.8864 - val_loss: 5.0364 - val_accuracy: 0.3955\n",
      "Epoch 1208/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.3900 - accuracy: 0.8424 - val_loss: 5.4327 - val_accuracy: 0.3318\n",
      "Epoch 1209/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.4021 - accuracy: 0.8576 - val_loss: 5.3799 - val_accuracy: 0.3091\n",
      "Epoch 1210/1500\n",
      "83/83 [==============================] - 0s 653us/step - loss: 1.7024 - accuracy: 0.5803 - val_loss: 6.6076 - val_accuracy: 0.2864\n",
      "Epoch 1211/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.7888 - accuracy: 0.7545 - val_loss: 4.8908 - val_accuracy: 0.4000\n",
      "Epoch 1212/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2253 - accuracy: 0.9303 - val_loss: 4.9213 - val_accuracy: 0.3500\n",
      "Epoch 1213/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.3043 - accuracy: 0.8788 - val_loss: 4.9817 - val_accuracy: 0.4273\n",
      "Epoch 1214/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.1616 - accuracy: 0.9576 - val_loss: 5.1058 - val_accuracy: 0.3773\n",
      "Epoch 1215/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.2081 - accuracy: 0.9303 - val_loss: 4.8920 - val_accuracy: 0.3955\n",
      "Epoch 1216/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.2173 - accuracy: 0.9348 - val_loss: 5.0080 - val_accuracy: 0.3955\n",
      "Epoch 1217/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.1420 - accuracy: 0.9697 - val_loss: 4.9977 - val_accuracy: 0.3591\n",
      "Epoch 1218/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.1553 - accuracy: 0.9621 - val_loss: 4.9684 - val_accuracy: 0.3773\n",
      "Epoch 1219/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.1392 - accuracy: 0.9667 - val_loss: 4.9011 - val_accuracy: 0.3909\n",
      "Epoch 1220/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.1865 - accuracy: 0.9333 - val_loss: 4.9479 - val_accuracy: 0.3773\n",
      "Epoch 1221/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.1725 - accuracy: 0.9470 - val_loss: 5.1700 - val_accuracy: 0.3364\n",
      "Epoch 1222/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.1888 - accuracy: 0.9379 - val_loss: 4.9372 - val_accuracy: 0.4045\n",
      "Epoch 1223/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.1706 - accuracy: 0.9515 - val_loss: 5.1108 - val_accuracy: 0.3818\n",
      "Epoch 1224/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.2258 - accuracy: 0.9152 - val_loss: 5.2553 - val_accuracy: 0.2955\n",
      "Epoch 1225/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2485 - accuracy: 0.9197 - val_loss: 4.9129 - val_accuracy: 0.3955\n",
      "Epoch 1226/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.1962 - accuracy: 0.9424 - val_loss: 5.1732 - val_accuracy: 0.3273\n",
      "Epoch 1227/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.5326 - accuracy: 0.8136 - val_loss: 5.3598 - val_accuracy: 0.3182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1228/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.4714 - accuracy: 0.8303 - val_loss: 5.4788 - val_accuracy: 0.3409\n",
      "Epoch 1229/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.3778 - accuracy: 0.8591 - val_loss: 5.1523 - val_accuracy: 0.3364\n",
      "Epoch 1230/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.3281 - accuracy: 0.8773 - val_loss: 5.5723 - val_accuracy: 0.3227\n",
      "Epoch 1231/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.3573 - accuracy: 0.8712 - val_loss: 5.3297 - val_accuracy: 0.3955\n",
      "Epoch 1232/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.3115 - accuracy: 0.9106 - val_loss: 5.5472 - val_accuracy: 0.2773\n",
      "Epoch 1233/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.5076 - accuracy: 0.8182 - val_loss: 5.5480 - val_accuracy: 0.3000\n",
      "Epoch 1234/1500\n",
      "83/83 [==============================] - 0s 625us/step - loss: 0.9640 - accuracy: 0.6909 - val_loss: 5.9028 - val_accuracy: 0.3000\n",
      "Epoch 1235/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.8456 - accuracy: 0.7212 - val_loss: 5.1194 - val_accuracy: 0.3455\n",
      "Epoch 1236/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.3050 - accuracy: 0.9000 - val_loss: 5.1654 - val_accuracy: 0.3682\n",
      "Epoch 1237/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.2141 - accuracy: 0.9273 - val_loss: 5.4563 - val_accuracy: 0.3773\n",
      "Epoch 1238/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.1983 - accuracy: 0.9409 - val_loss: 5.5903 - val_accuracy: 0.3091\n",
      "Epoch 1239/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.1816 - accuracy: 0.9470 - val_loss: 5.3521 - val_accuracy: 0.3182\n",
      "Epoch 1240/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.2716 - accuracy: 0.9000 - val_loss: 5.0327 - val_accuracy: 0.4000\n",
      "Epoch 1241/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.1943 - accuracy: 0.9500 - val_loss: 5.4104 - val_accuracy: 0.3364\n",
      "Epoch 1242/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.2149 - accuracy: 0.9424 - val_loss: 5.5685 - val_accuracy: 0.3318\n",
      "Epoch 1243/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.2203 - accuracy: 0.9288 - val_loss: 5.3555 - val_accuracy: 0.3773\n",
      "Epoch 1244/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2020 - accuracy: 0.9242 - val_loss: 5.1159 - val_accuracy: 0.3955\n",
      "Epoch 1245/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.2479 - accuracy: 0.9167 - val_loss: 5.2438 - val_accuracy: 0.3864\n",
      "Epoch 1246/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.2691 - accuracy: 0.9152 - val_loss: 5.4022 - val_accuracy: 0.3773\n",
      "Epoch 1247/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.3070 - accuracy: 0.8833 - val_loss: 5.3593 - val_accuracy: 0.3636\n",
      "Epoch 1248/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.3358 - accuracy: 0.8818 - val_loss: 5.4066 - val_accuracy: 0.3364\n",
      "Epoch 1249/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.1885 - accuracy: 0.9379 - val_loss: 5.8466 - val_accuracy: 0.3000\n",
      "Epoch 1250/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.2401 - accuracy: 0.9091 - val_loss: 5.2061 - val_accuracy: 0.3545\n",
      "Epoch 1251/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.2856 - accuracy: 0.8879 - val_loss: 5.4890 - val_accuracy: 0.4182\n",
      "Epoch 1252/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.3035 - accuracy: 0.8864 - val_loss: 5.6863 - val_accuracy: 0.3227\n",
      "Epoch 1253/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.9370 - accuracy: 0.7182 - val_loss: 6.2415 - val_accuracy: 0.2909\n",
      "Epoch 1254/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.5765 - accuracy: 0.8030 - val_loss: 5.3429 - val_accuracy: 0.3773\n",
      "Epoch 1255/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.4257 - accuracy: 0.8333 - val_loss: 5.4320 - val_accuracy: 0.3773\n",
      "Epoch 1256/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.6069 - accuracy: 0.7712 - val_loss: 5.6039 - val_accuracy: 0.3727\n",
      "Epoch 1257/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.5786 - accuracy: 0.8212 - val_loss: 5.3688 - val_accuracy: 0.3636\n",
      "Epoch 1258/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.1478 - accuracy: 0.9606 - val_loss: 5.2131 - val_accuracy: 0.3909\n",
      "Epoch 1259/1500\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.1564 - accuracy: 0.9545 - val_loss: 5.1774 - val_accuracy: 0.3682\n",
      "Epoch 1260/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.2170 - accuracy: 0.9227 - val_loss: 5.3042 - val_accuracy: 0.3545\n",
      "Epoch 1261/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.1886 - accuracy: 0.9424 - val_loss: 5.3613 - val_accuracy: 0.3727\n",
      "Epoch 1262/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.1673 - accuracy: 0.9500 - val_loss: 5.3071 - val_accuracy: 0.3682\n",
      "Epoch 1263/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.2206 - accuracy: 0.9076 - val_loss: 5.6547 - val_accuracy: 0.3182\n",
      "Epoch 1264/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.2413 - accuracy: 0.9136 - val_loss: 5.2532 - val_accuracy: 0.3318\n",
      "Epoch 1265/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.1872 - accuracy: 0.9394 - val_loss: 5.1611 - val_accuracy: 0.3955\n",
      "Epoch 1266/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.4060 - accuracy: 0.8667 - val_loss: 5.7501 - val_accuracy: 0.3364\n",
      "Epoch 1267/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.4743 - accuracy: 0.8212 - val_loss: 5.7261 - val_accuracy: 0.3409\n",
      "Epoch 1268/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.3315 - accuracy: 0.8742 - val_loss: 5.4864 - val_accuracy: 0.3455\n",
      "Epoch 1269/1500\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.2490 - accuracy: 0.9061 - val_loss: 5.7463 - val_accuracy: 0.3273\n",
      "Epoch 1270/1500\n",
      "83/83 [==============================] - 0s 594us/step - loss: 0.3544 - accuracy: 0.8727 - val_loss: 5.4948 - val_accuracy: 0.3818\n",
      "Epoch 1271/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.6224 - accuracy: 0.7924 - val_loss: 4.9979 - val_accuracy: 0.3909\n",
      "Epoch 1272/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.4597 - accuracy: 0.8303 - val_loss: 4.9870 - val_accuracy: 0.3864\n",
      "Epoch 1273/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.1973 - accuracy: 0.9379 - val_loss: 5.0806 - val_accuracy: 0.4455\n",
      "Epoch 1274/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.5773 - accuracy: 0.7970 - val_loss: 5.4045 - val_accuracy: 0.3909\n",
      "Epoch 1275/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.5607 - accuracy: 0.7939 - val_loss: 5.7411 - val_accuracy: 0.3409\n",
      "Epoch 1276/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.1762 - accuracy: 0.9424 - val_loss: 5.4297 - val_accuracy: 0.3773\n",
      "Epoch 1277/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.1681 - accuracy: 0.9424 - val_loss: 5.4360 - val_accuracy: 0.3409\n",
      "Epoch 1278/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.1977 - accuracy: 0.9394 - val_loss: 5.3338 - val_accuracy: 0.3955\n",
      "Epoch 1279/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.1623 - accuracy: 0.9515 - val_loss: 5.6041 - val_accuracy: 0.3909\n",
      "Epoch 1280/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.1978 - accuracy: 0.9394 - val_loss: 5.2556 - val_accuracy: 0.4000\n",
      "Epoch 1281/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.1836 - accuracy: 0.9500 - val_loss: 6.0725 - val_accuracy: 0.2864\n",
      "Epoch 1282/1500\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.8786 - accuracy: 0.7303 - val_loss: 5.6824 - val_accuracy: 0.3227\n",
      "Epoch 1283/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.5503 - accuracy: 0.8136 - val_loss: 5.5356 - val_accuracy: 0.3818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1284/1500\n",
      "83/83 [==============================] - 0s 631us/step - loss: 0.1795 - accuracy: 0.9485 - val_loss: 5.5656 - val_accuracy: 0.3409\n",
      "Epoch 1285/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.1492 - accuracy: 0.9576 - val_loss: 5.7768 - val_accuracy: 0.3636\n",
      "Epoch 1286/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.1626 - accuracy: 0.9591 - val_loss: 5.6752 - val_accuracy: 0.3682\n",
      "Epoch 1287/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.1305 - accuracy: 0.9697 - val_loss: 5.4387 - val_accuracy: 0.4045\n",
      "Epoch 1288/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.5708 - accuracy: 0.8318 - val_loss: 6.8614 - val_accuracy: 0.2636\n",
      "Epoch 1289/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 3.9870 - accuracy: 0.3500 - val_loss: 5.4092 - val_accuracy: 0.2818\n",
      "Epoch 1290/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 2.9255 - accuracy: 0.3682 - val_loss: 3.9700 - val_accuracy: 0.2909\n",
      "Epoch 1291/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 2.7978 - accuracy: 0.3394 - val_loss: 5.0560 - val_accuracy: 0.2682\n",
      "Epoch 1292/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 1.1250 - accuracy: 0.6333 - val_loss: 5.5140 - val_accuracy: 0.3500\n",
      "Epoch 1293/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.4054 - accuracy: 0.8530 - val_loss: 5.7631 - val_accuracy: 0.3864\n",
      "Epoch 1294/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.2354 - accuracy: 0.9152 - val_loss: 5.3422 - val_accuracy: 0.4091\n",
      "Epoch 1295/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.2640 - accuracy: 0.9091 - val_loss: 5.2712 - val_accuracy: 0.3455\n",
      "Epoch 1296/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.1616 - accuracy: 0.9621 - val_loss: 5.2762 - val_accuracy: 0.4091\n",
      "Epoch 1297/1500\n",
      "83/83 [==============================] - 0s 589us/step - loss: 0.1377 - accuracy: 0.9788 - val_loss: 5.5098 - val_accuracy: 0.3545\n",
      "Epoch 1298/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.1940 - accuracy: 0.9348 - val_loss: 5.2292 - val_accuracy: 0.4136\n",
      "Epoch 1299/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.2503 - accuracy: 0.9152 - val_loss: 5.4388 - val_accuracy: 0.3636\n",
      "Epoch 1300/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.1941 - accuracy: 0.9515 - val_loss: 5.3733 - val_accuracy: 0.4091\n",
      "Epoch 1301/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.1332 - accuracy: 0.9606 - val_loss: 5.4081 - val_accuracy: 0.3727\n",
      "Epoch 1302/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.1400 - accuracy: 0.9697 - val_loss: 5.4323 - val_accuracy: 0.4000\n",
      "Epoch 1303/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.1354 - accuracy: 0.9652 - val_loss: 5.3014 - val_accuracy: 0.3545\n",
      "Epoch 1304/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.3620 - accuracy: 0.8803 - val_loss: 6.2690 - val_accuracy: 0.2318\n",
      "Epoch 1305/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.4001 - accuracy: 0.8576 - val_loss: 4.9729 - val_accuracy: 0.3818\n",
      "Epoch 1306/1500\n",
      "83/83 [==============================] - 0s 627us/step - loss: 0.3033 - accuracy: 0.8864 - val_loss: 5.6896 - val_accuracy: 0.3727\n",
      "Epoch 1307/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.2612 - accuracy: 0.9106 - val_loss: 5.5824 - val_accuracy: 0.4000\n",
      "Epoch 1308/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.1851 - accuracy: 0.9439 - val_loss: 5.5976 - val_accuracy: 0.3909\n",
      "Epoch 1309/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.2701 - accuracy: 0.8894 - val_loss: 5.3720 - val_accuracy: 0.3591\n",
      "Epoch 1310/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.1912 - accuracy: 0.9409 - val_loss: 5.5081 - val_accuracy: 0.3591\n",
      "Epoch 1311/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.5556 - accuracy: 0.7985 - val_loss: 5.3737 - val_accuracy: 0.3773\n",
      "Epoch 1312/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.4006 - accuracy: 0.8530 - val_loss: 5.8536 - val_accuracy: 0.2955\n",
      "Epoch 1313/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.4226 - accuracy: 0.8485 - val_loss: 5.3889 - val_accuracy: 0.3409\n",
      "Epoch 1314/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.4106 - accuracy: 0.8591 - val_loss: 5.4867 - val_accuracy: 0.3455\n",
      "Epoch 1315/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.3373 - accuracy: 0.8652 - val_loss: 5.4789 - val_accuracy: 0.4091\n",
      "Epoch 1316/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.2146 - accuracy: 0.9303 - val_loss: 5.3345 - val_accuracy: 0.4227\n",
      "Epoch 1317/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.1772 - accuracy: 0.9394 - val_loss: 5.2748 - val_accuracy: 0.3682\n",
      "Epoch 1318/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.2350 - accuracy: 0.9136 - val_loss: 5.5695 - val_accuracy: 0.3818\n",
      "Epoch 1319/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.1860 - accuracy: 0.9439 - val_loss: 5.4232 - val_accuracy: 0.3500\n",
      "Epoch 1320/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.3516 - accuracy: 0.8682 - val_loss: 5.5652 - val_accuracy: 0.3455\n",
      "Epoch 1321/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.1999 - accuracy: 0.9364 - val_loss: 5.5682 - val_accuracy: 0.3773\n",
      "Epoch 1322/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.2740 - accuracy: 0.9136 - val_loss: 5.4035 - val_accuracy: 0.3773\n",
      "Epoch 1323/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.2208 - accuracy: 0.9182 - val_loss: 5.5076 - val_accuracy: 0.3364\n",
      "Epoch 1324/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.2471 - accuracy: 0.9061 - val_loss: 5.5499 - val_accuracy: 0.2727\n",
      "Epoch 1325/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.5243 - accuracy: 0.8121 - val_loss: 5.5355 - val_accuracy: 0.3682\n",
      "Epoch 1326/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.3770 - accuracy: 0.8621 - val_loss: 5.8471 - val_accuracy: 0.3455\n",
      "Epoch 1327/1500\n",
      "83/83 [==============================] - 0s 684us/step - loss: 0.2854 - accuracy: 0.8955 - val_loss: 5.7110 - val_accuracy: 0.3545\n",
      "Epoch 1328/1500\n",
      "83/83 [==============================] - 0s 628us/step - loss: 0.1917 - accuracy: 0.9424 - val_loss: 6.0157 - val_accuracy: 0.3227\n",
      "Epoch 1329/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.3388 - accuracy: 0.8803 - val_loss: 6.0678 - val_accuracy: 0.3500\n",
      "Epoch 1330/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.3364 - accuracy: 0.8742 - val_loss: 5.5740 - val_accuracy: 0.3636\n",
      "Epoch 1331/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.3954 - accuracy: 0.8561 - val_loss: 6.0877 - val_accuracy: 0.3364\n",
      "Epoch 1332/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.7753 - accuracy: 0.7212 - val_loss: 6.0971 - val_accuracy: 0.3727\n",
      "Epoch 1333/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.4377 - accuracy: 0.8576 - val_loss: 5.5240 - val_accuracy: 0.3545\n",
      "Epoch 1334/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.1708 - accuracy: 0.9515 - val_loss: 5.3748 - val_accuracy: 0.4136\n",
      "Epoch 1335/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.1728 - accuracy: 0.9530 - val_loss: 5.4775 - val_accuracy: 0.3682\n",
      "Epoch 1336/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.1558 - accuracy: 0.9530 - val_loss: 5.2893 - val_accuracy: 0.3955\n",
      "Epoch 1337/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.2548 - accuracy: 0.9076 - val_loss: 5.8247 - val_accuracy: 0.3182\n",
      "Epoch 1338/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.2732 - accuracy: 0.8894 - val_loss: 5.7134 - val_accuracy: 0.3364\n",
      "Epoch 1339/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.4165 - accuracy: 0.8394 - val_loss: 5.3607 - val_accuracy: 0.3636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1340/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.1467 - accuracy: 0.9606 - val_loss: 5.3043 - val_accuracy: 0.4136\n",
      "Epoch 1341/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.2751 - accuracy: 0.9015 - val_loss: 5.3595 - val_accuracy: 0.3773\n",
      "Epoch 1342/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.3322 - accuracy: 0.8894 - val_loss: 6.0055 - val_accuracy: 0.3455\n",
      "Epoch 1343/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.7072 - accuracy: 0.7652 - val_loss: 5.8879 - val_accuracy: 0.3318\n",
      "Epoch 1344/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.3500 - accuracy: 0.8591 - val_loss: 5.3448 - val_accuracy: 0.3818\n",
      "Epoch 1345/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.2925 - accuracy: 0.8955 - val_loss: 5.6166 - val_accuracy: 0.3591\n",
      "Epoch 1346/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.1877 - accuracy: 0.9379 - val_loss: 5.3953 - val_accuracy: 0.3909\n",
      "Epoch 1347/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.1876 - accuracy: 0.9485 - val_loss: 5.4012 - val_accuracy: 0.3818\n",
      "Epoch 1348/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.2174 - accuracy: 0.9152 - val_loss: 5.4597 - val_accuracy: 0.3773\n",
      "Epoch 1349/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.4181 - accuracy: 0.8470 - val_loss: 5.8958 - val_accuracy: 0.3227\n",
      "Epoch 1350/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 1.2748 - accuracy: 0.6303 - val_loss: 6.8441 - val_accuracy: 0.2545\n",
      "Epoch 1351/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.7721 - accuracy: 0.7303 - val_loss: 5.5759 - val_accuracy: 0.3091\n",
      "Epoch 1352/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.3432 - accuracy: 0.8818 - val_loss: 5.3478 - val_accuracy: 0.3818\n",
      "Epoch 1353/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.1591 - accuracy: 0.9561 - val_loss: 5.2555 - val_accuracy: 0.3955\n",
      "Epoch 1354/1500\n",
      "83/83 [==============================] - 0s 676us/step - loss: 0.1450 - accuracy: 0.9636 - val_loss: 5.0341 - val_accuracy: 0.4000\n",
      "Epoch 1355/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.1484 - accuracy: 0.9576 - val_loss: 5.1502 - val_accuracy: 0.4045\n",
      "Epoch 1356/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.1713 - accuracy: 0.9515 - val_loss: 5.3231 - val_accuracy: 0.3818\n",
      "Epoch 1357/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.3032 - accuracy: 0.8864 - val_loss: 5.5467 - val_accuracy: 0.3273\n",
      "Epoch 1358/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.2654 - accuracy: 0.8909 - val_loss: 5.8968 - val_accuracy: 0.3182\n",
      "Epoch 1359/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.3302 - accuracy: 0.8727 - val_loss: 5.3396 - val_accuracy: 0.4000\n",
      "Epoch 1360/1500\n",
      "83/83 [==============================] - 0s 636us/step - loss: 0.6382 - accuracy: 0.8061 - val_loss: 7.1823 - val_accuracy: 0.2500\n",
      "Epoch 1361/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.8456 - accuracy: 0.7530 - val_loss: 5.4877 - val_accuracy: 0.3273\n",
      "Epoch 1362/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.1661 - accuracy: 0.9455 - val_loss: 5.2355 - val_accuracy: 0.3773\n",
      "Epoch 1363/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.1503 - accuracy: 0.9485 - val_loss: 5.4806 - val_accuracy: 0.3773\n",
      "Epoch 1364/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.1703 - accuracy: 0.9561 - val_loss: 5.2482 - val_accuracy: 0.4273\n",
      "Epoch 1365/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.1356 - accuracy: 0.9742 - val_loss: 5.3682 - val_accuracy: 0.4045\n",
      "Epoch 1366/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.4518 - accuracy: 0.8409 - val_loss: 5.7521 - val_accuracy: 0.3500\n",
      "Epoch 1367/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.2866 - accuracy: 0.8879 - val_loss: 5.2467 - val_accuracy: 0.3955\n",
      "Epoch 1368/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.3538 - accuracy: 0.8621 - val_loss: 5.5164 - val_accuracy: 0.3864\n",
      "Epoch 1369/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.2163 - accuracy: 0.9288 - val_loss: 5.3479 - val_accuracy: 0.3591\n",
      "Epoch 1370/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.2404 - accuracy: 0.9303 - val_loss: 5.5455 - val_accuracy: 0.3636\n",
      "Epoch 1371/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.3904 - accuracy: 0.8576 - val_loss: 5.5223 - val_accuracy: 0.3636\n",
      "Epoch 1372/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.3228 - accuracy: 0.8909 - val_loss: 5.4572 - val_accuracy: 0.4273\n",
      "Epoch 1373/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.6645 - accuracy: 0.7924 - val_loss: 5.9092 - val_accuracy: 0.2818\n",
      "Epoch 1374/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.5223 - accuracy: 0.8227 - val_loss: 5.1539 - val_accuracy: 0.3455\n",
      "Epoch 1375/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.1840 - accuracy: 0.9485 - val_loss: 5.3694 - val_accuracy: 0.4136\n",
      "Epoch 1376/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.1425 - accuracy: 0.9712 - val_loss: 5.4146 - val_accuracy: 0.3955\n",
      "Epoch 1377/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.1938 - accuracy: 0.9394 - val_loss: 5.3023 - val_accuracy: 0.3818\n",
      "Epoch 1378/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.2145 - accuracy: 0.9318 - val_loss: 5.2486 - val_accuracy: 0.3682\n",
      "Epoch 1379/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.2350 - accuracy: 0.9182 - val_loss: 5.3323 - val_accuracy: 0.3909\n",
      "Epoch 1380/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.8240 - accuracy: 0.7333 - val_loss: 5.6832 - val_accuracy: 0.3455\n",
      "Epoch 1381/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.3598 - accuracy: 0.8742 - val_loss: 5.1821 - val_accuracy: 0.3136\n",
      "Epoch 1382/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.2120 - accuracy: 0.9167 - val_loss: 4.9526 - val_accuracy: 0.3500\n",
      "Epoch 1383/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.2671 - accuracy: 0.9000 - val_loss: 5.1624 - val_accuracy: 0.3500\n",
      "Epoch 1384/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.1963 - accuracy: 0.9485 - val_loss: 5.5101 - val_accuracy: 0.3864\n",
      "Epoch 1385/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.1951 - accuracy: 0.9318 - val_loss: 5.0974 - val_accuracy: 0.3773\n",
      "Epoch 1386/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.5921 - accuracy: 0.7939 - val_loss: 5.5023 - val_accuracy: 0.3727\n",
      "Epoch 1387/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.3752 - accuracy: 0.8682 - val_loss: 5.5626 - val_accuracy: 0.3909\n",
      "Epoch 1388/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.2411 - accuracy: 0.9061 - val_loss: 5.7301 - val_accuracy: 0.3909\n",
      "Epoch 1389/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.3394 - accuracy: 0.8742 - val_loss: 5.3246 - val_accuracy: 0.3773\n",
      "Epoch 1390/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.2381 - accuracy: 0.9227 - val_loss: 5.2658 - val_accuracy: 0.3909\n",
      "Epoch 1391/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.1780 - accuracy: 0.9500 - val_loss: 5.5858 - val_accuracy: 0.3182\n",
      "Epoch 1392/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.2463 - accuracy: 0.9015 - val_loss: 5.1847 - val_accuracy: 0.3818\n",
      "Epoch 1393/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.2623 - accuracy: 0.9045 - val_loss: 5.7575 - val_accuracy: 0.3545\n",
      "Epoch 1394/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.1866 - accuracy: 0.9485 - val_loss: 5.3680 - val_accuracy: 0.4091\n",
      "Epoch 1395/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.2369 - accuracy: 0.9136 - val_loss: 5.7109 - val_accuracy: 0.3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1396/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.1889 - accuracy: 0.9379 - val_loss: 5.7425 - val_accuracy: 0.3818\n",
      "Epoch 1397/1500\n",
      "83/83 [==============================] - 0s 597us/step - loss: 0.2823 - accuracy: 0.9030 - val_loss: 5.4149 - val_accuracy: 0.3864\n",
      "Epoch 1398/1500\n",
      "83/83 [==============================] - 0s 621us/step - loss: 0.2917 - accuracy: 0.8909 - val_loss: 6.1586 - val_accuracy: 0.2455\n",
      "Epoch 1399/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 1.0751 - accuracy: 0.6879 - val_loss: 6.0846 - val_accuracy: 0.2636\n",
      "Epoch 1400/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 1.0267 - accuracy: 0.6697 - val_loss: 5.4451 - val_accuracy: 0.2682\n",
      "Epoch 1401/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.6968 - accuracy: 0.7545 - val_loss: 5.6703 - val_accuracy: 0.3636\n",
      "Epoch 1402/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.3373 - accuracy: 0.8621 - val_loss: 5.4772 - val_accuracy: 0.3545\n",
      "Epoch 1403/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.2088 - accuracy: 0.9333 - val_loss: 5.7562 - val_accuracy: 0.3273\n",
      "Epoch 1404/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.2456 - accuracy: 0.9136 - val_loss: 5.6917 - val_accuracy: 0.3409\n",
      "Epoch 1405/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.1560 - accuracy: 0.9576 - val_loss: 5.3069 - val_accuracy: 0.3591\n",
      "Epoch 1406/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.1605 - accuracy: 0.9530 - val_loss: 5.3011 - val_accuracy: 0.3864\n",
      "Epoch 1407/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.2664 - accuracy: 0.9030 - val_loss: 5.5567 - val_accuracy: 0.3318\n",
      "Epoch 1408/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.1862 - accuracy: 0.9364 - val_loss: 5.2251 - val_accuracy: 0.3682\n",
      "Epoch 1409/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.1386 - accuracy: 0.9682 - val_loss: 5.4782 - val_accuracy: 0.3636\n",
      "Epoch 1410/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.1554 - accuracy: 0.9530 - val_loss: 5.4852 - val_accuracy: 0.3682\n",
      "Epoch 1411/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.2521 - accuracy: 0.9136 - val_loss: 5.2344 - val_accuracy: 0.3682\n",
      "Epoch 1412/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.4891 - accuracy: 0.8152 - val_loss: 6.1599 - val_accuracy: 0.2409\n",
      "Epoch 1413/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.4603 - accuracy: 0.8258 - val_loss: 5.5348 - val_accuracy: 0.3591\n",
      "Epoch 1414/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.2900 - accuracy: 0.8894 - val_loss: 5.5913 - val_accuracy: 0.3909\n",
      "Epoch 1415/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.3877 - accuracy: 0.8591 - val_loss: 5.3844 - val_accuracy: 0.3682\n",
      "Epoch 1416/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.1601 - accuracy: 0.9545 - val_loss: 5.9260 - val_accuracy: 0.3591\n",
      "Epoch 1417/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.2293 - accuracy: 0.9182 - val_loss: 5.4613 - val_accuracy: 0.4091\n",
      "Epoch 1418/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.1621 - accuracy: 0.9515 - val_loss: 5.7496 - val_accuracy: 0.3455\n",
      "Epoch 1419/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.2459 - accuracy: 0.9045 - val_loss: 6.1084 - val_accuracy: 0.2818\n",
      "Epoch 1420/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.2967 - accuracy: 0.8924 - val_loss: 5.7770 - val_accuracy: 0.3682\n",
      "Epoch 1421/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.3572 - accuracy: 0.8773 - val_loss: 5.3038 - val_accuracy: 0.4136\n",
      "Epoch 1422/1500\n",
      "83/83 [==============================] - 0s 620us/step - loss: 0.3867 - accuracy: 0.8561 - val_loss: 6.0166 - val_accuracy: 0.2727\n",
      "Epoch 1423/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.7243 - accuracy: 0.7500 - val_loss: 5.6155 - val_accuracy: 0.3182\n",
      "Epoch 1424/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.7729 - accuracy: 0.7561 - val_loss: 5.7160 - val_accuracy: 0.3045\n",
      "Epoch 1425/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.3409 - accuracy: 0.8909 - val_loss: 5.8608 - val_accuracy: 0.3727\n",
      "Epoch 1426/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.2008 - accuracy: 0.9318 - val_loss: 5.6501 - val_accuracy: 0.3727\n",
      "Epoch 1427/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.1667 - accuracy: 0.9500 - val_loss: 5.5951 - val_accuracy: 0.3591\n",
      "Epoch 1428/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.2761 - accuracy: 0.9061 - val_loss: 6.1515 - val_accuracy: 0.3091\n",
      "Epoch 1429/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.4866 - accuracy: 0.8303 - val_loss: 5.7264 - val_accuracy: 0.3455\n",
      "Epoch 1430/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.2440 - accuracy: 0.9076 - val_loss: 5.5172 - val_accuracy: 0.3591\n",
      "Epoch 1431/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.2284 - accuracy: 0.9273 - val_loss: 5.3200 - val_accuracy: 0.3682\n",
      "Epoch 1432/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.1512 - accuracy: 0.9591 - val_loss: 5.3483 - val_accuracy: 0.4091\n",
      "Epoch 1433/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.1778 - accuracy: 0.9530 - val_loss: 5.4591 - val_accuracy: 0.3591\n",
      "Epoch 1434/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.1756 - accuracy: 0.9409 - val_loss: 5.4185 - val_accuracy: 0.3591\n",
      "Epoch 1435/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 0.1900 - accuracy: 0.9439 - val_loss: 5.5347 - val_accuracy: 0.3591\n",
      "Epoch 1436/1500\n",
      "83/83 [==============================] - 0s 617us/step - loss: 0.5170 - accuracy: 0.7939 - val_loss: 5.8725 - val_accuracy: 0.3227\n",
      "Epoch 1437/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.5664 - accuracy: 0.7864 - val_loss: 6.5791 - val_accuracy: 0.2864\n",
      "Epoch 1438/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.7880 - accuracy: 0.7485 - val_loss: 5.6687 - val_accuracy: 0.4045\n",
      "Epoch 1439/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.2249 - accuracy: 0.9318 - val_loss: 5.8171 - val_accuracy: 0.3682\n",
      "Epoch 1440/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.1416 - accuracy: 0.9667 - val_loss: 5.3586 - val_accuracy: 0.4091\n",
      "Epoch 1441/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 0.1667 - accuracy: 0.9530 - val_loss: 5.6253 - val_accuracy: 0.3727\n",
      "Epoch 1442/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.1964 - accuracy: 0.9470 - val_loss: 5.5525 - val_accuracy: 0.3864\n",
      "Epoch 1443/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.2096 - accuracy: 0.9303 - val_loss: 5.9007 - val_accuracy: 0.3227\n",
      "Epoch 1444/1500\n",
      "83/83 [==============================] - 0s 687us/step - loss: 0.3848 - accuracy: 0.8652 - val_loss: 5.5715 - val_accuracy: 0.3909\n",
      "Epoch 1445/1500\n",
      "83/83 [==============================] - 0s 605us/step - loss: 0.4234 - accuracy: 0.8409 - val_loss: 5.6759 - val_accuracy: 0.3727\n",
      "Epoch 1446/1500\n",
      "83/83 [==============================] - 0s 629us/step - loss: 0.3299 - accuracy: 0.8818 - val_loss: 6.0261 - val_accuracy: 0.3500\n",
      "Epoch 1447/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.4080 - accuracy: 0.8621 - val_loss: 5.4426 - val_accuracy: 0.4227\n",
      "Epoch 1448/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.4066 - accuracy: 0.8470 - val_loss: 5.5117 - val_accuracy: 0.3727\n",
      "Epoch 1449/1500\n",
      "83/83 [==============================] - 0s 648us/step - loss: 0.5559 - accuracy: 0.8106 - val_loss: 6.6116 - val_accuracy: 0.3636\n",
      "Epoch 1450/1500\n",
      "83/83 [==============================] - 0s 634us/step - loss: 0.7208 - accuracy: 0.7576 - val_loss: 6.2885 - val_accuracy: 0.3136\n",
      "Epoch 1451/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.4928 - accuracy: 0.8303 - val_loss: 5.8530 - val_accuracy: 0.3409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1452/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.1959 - accuracy: 0.9333 - val_loss: 5.8601 - val_accuracy: 0.3455\n",
      "Epoch 1453/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.1412 - accuracy: 0.9682 - val_loss: 5.5703 - val_accuracy: 0.3500\n",
      "Epoch 1454/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.1404 - accuracy: 0.9606 - val_loss: 5.3960 - val_accuracy: 0.4136\n",
      "Epoch 1455/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.1285 - accuracy: 0.9742 - val_loss: 5.4576 - val_accuracy: 0.3727\n",
      "Epoch 1456/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.1119 - accuracy: 0.9773 - val_loss: 5.5663 - val_accuracy: 0.3909\n",
      "Epoch 1457/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.1647 - accuracy: 0.9470 - val_loss: 5.5044 - val_accuracy: 0.3591\n",
      "Epoch 1458/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.3443 - accuracy: 0.8621 - val_loss: 6.3080 - val_accuracy: 0.3500\n",
      "Epoch 1459/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.6465 - accuracy: 0.7742 - val_loss: 6.5965 - val_accuracy: 0.3091\n",
      "Epoch 1460/1500\n",
      "83/83 [==============================] - 0s 609us/step - loss: 1.3136 - accuracy: 0.6364 - val_loss: 5.5212 - val_accuracy: 0.4136\n",
      "Epoch 1461/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.3682 - accuracy: 0.8621 - val_loss: 5.5570 - val_accuracy: 0.3955\n",
      "Epoch 1462/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.1756 - accuracy: 0.9500 - val_loss: 5.4020 - val_accuracy: 0.4000\n",
      "Epoch 1463/1500\n",
      "83/83 [==============================] - 0s 606us/step - loss: 0.1213 - accuracy: 0.9727 - val_loss: 5.4753 - val_accuracy: 0.4136\n",
      "Epoch 1464/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.1647 - accuracy: 0.9455 - val_loss: 5.6210 - val_accuracy: 0.3455\n",
      "Epoch 1465/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.1511 - accuracy: 0.9576 - val_loss: 5.5086 - val_accuracy: 0.4045\n",
      "Epoch 1466/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.1710 - accuracy: 0.9530 - val_loss: 5.3542 - val_accuracy: 0.4045\n",
      "Epoch 1467/1500\n",
      "83/83 [==============================] - 0s 610us/step - loss: 0.1369 - accuracy: 0.9652 - val_loss: 5.7105 - val_accuracy: 0.3545\n",
      "Epoch 1468/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 1.2143 - accuracy: 0.6636 - val_loss: 5.9499 - val_accuracy: 0.3545\n",
      "Epoch 1469/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.9516 - accuracy: 0.7045 - val_loss: 5.7632 - val_accuracy: 0.4091\n",
      "Epoch 1470/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.4887 - accuracy: 0.8273 - val_loss: 5.9093 - val_accuracy: 0.3409\n",
      "Epoch 1471/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 0.3218 - accuracy: 0.8833 - val_loss: 5.4737 - val_accuracy: 0.3909\n",
      "Epoch 1472/1500\n",
      "83/83 [==============================] - 0s 607us/step - loss: 0.1406 - accuracy: 0.9652 - val_loss: 5.5354 - val_accuracy: 0.3636\n",
      "Epoch 1473/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.1225 - accuracy: 0.9742 - val_loss: 5.6395 - val_accuracy: 0.3500\n",
      "Epoch 1474/1500\n",
      "83/83 [==============================] - 0s 626us/step - loss: 0.1572 - accuracy: 0.9621 - val_loss: 5.7423 - val_accuracy: 0.3364\n",
      "Epoch 1475/1500\n",
      "83/83 [==============================] - 0s 604us/step - loss: 0.1436 - accuracy: 0.9545 - val_loss: 5.6502 - val_accuracy: 0.3909\n",
      "Epoch 1476/1500\n",
      "83/83 [==============================] - 0s 616us/step - loss: 0.4185 - accuracy: 0.8485 - val_loss: 5.6481 - val_accuracy: 0.3500\n",
      "Epoch 1477/1500\n",
      "83/83 [==============================] - 0s 613us/step - loss: 0.3991 - accuracy: 0.8515 - val_loss: 5.5851 - val_accuracy: 0.4136\n",
      "Epoch 1478/1500\n",
      "83/83 [==============================] - 0s 619us/step - loss: 0.1506 - accuracy: 0.9621 - val_loss: 5.3788 - val_accuracy: 0.3909\n",
      "Epoch 1479/1500\n",
      "83/83 [==============================] - 0s 600us/step - loss: 0.1436 - accuracy: 0.9621 - val_loss: 5.3208 - val_accuracy: 0.4227\n",
      "Epoch 1480/1500\n",
      "83/83 [==============================] - 0s 618us/step - loss: 0.1675 - accuracy: 0.9424 - val_loss: 5.3152 - val_accuracy: 0.3955\n",
      "Epoch 1481/1500\n",
      "83/83 [==============================] - 0s 633us/step - loss: 0.1925 - accuracy: 0.9333 - val_loss: 5.5857 - val_accuracy: 0.3955\n",
      "Epoch 1482/1500\n",
      "83/83 [==============================] - 0s 601us/step - loss: 0.1317 - accuracy: 0.9697 - val_loss: 5.6378 - val_accuracy: 0.4045\n",
      "Epoch 1483/1500\n",
      "83/83 [==============================] - 0s 596us/step - loss: 0.1960 - accuracy: 0.9227 - val_loss: 5.4620 - val_accuracy: 0.4045\n",
      "Epoch 1484/1500\n",
      "83/83 [==============================] - 0s 614us/step - loss: 0.1544 - accuracy: 0.9636 - val_loss: 5.6002 - val_accuracy: 0.3682\n",
      "Epoch 1485/1500\n",
      "83/83 [==============================] - 0s 608us/step - loss: 0.2103 - accuracy: 0.9318 - val_loss: 5.7060 - val_accuracy: 0.3409\n",
      "Epoch 1486/1500\n",
      "83/83 [==============================] - 0s 598us/step - loss: 0.4296 - accuracy: 0.8485 - val_loss: 5.9158 - val_accuracy: 0.3909\n",
      "Epoch 1487/1500\n",
      "83/83 [==============================] - 0s 631us/step - loss: 1.9889 - accuracy: 0.5682 - val_loss: 6.2841 - val_accuracy: 0.3455\n",
      "Epoch 1488/1500\n",
      "83/83 [==============================] - 0s 624us/step - loss: 1.0402 - accuracy: 0.7121 - val_loss: 5.9247 - val_accuracy: 0.2864\n",
      "Epoch 1489/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.2050 - accuracy: 0.9364 - val_loss: 5.5665 - val_accuracy: 0.4091\n",
      "Epoch 1490/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 0.1419 - accuracy: 0.9667 - val_loss: 5.6395 - val_accuracy: 0.3455\n",
      "Epoch 1491/1500\n",
      "83/83 [==============================] - 0s 599us/step - loss: 0.1377 - accuracy: 0.9561 - val_loss: 5.6502 - val_accuracy: 0.3727\n",
      "Epoch 1492/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.1715 - accuracy: 0.9500 - val_loss: 5.6340 - val_accuracy: 0.3636\n",
      "Epoch 1493/1500\n",
      "83/83 [==============================] - 0s 615us/step - loss: 0.2241 - accuracy: 0.9348 - val_loss: 5.5062 - val_accuracy: 0.3909\n",
      "Epoch 1494/1500\n",
      "83/83 [==============================] - 0s 603us/step - loss: 0.4846 - accuracy: 0.8652 - val_loss: 6.1208 - val_accuracy: 0.2409\n",
      "Epoch 1495/1500\n",
      "83/83 [==============================] - 0s 612us/step - loss: 4.0237 - accuracy: 0.3455 - val_loss: 7.5733 - val_accuracy: 0.2591\n",
      "Epoch 1496/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 1.9026 - accuracy: 0.5303 - val_loss: 5.6194 - val_accuracy: 0.3136\n",
      "Epoch 1497/1500\n",
      "83/83 [==============================] - 0s 622us/step - loss: 1.0122 - accuracy: 0.6879 - val_loss: 5.4974 - val_accuracy: 0.3091\n",
      "Epoch 1498/1500\n",
      "83/83 [==============================] - 0s 611us/step - loss: 0.9956 - accuracy: 0.6652 - val_loss: 4.9924 - val_accuracy: 0.3864\n",
      "Epoch 1499/1500\n",
      "83/83 [==============================] - 0s 602us/step - loss: 0.8044 - accuracy: 0.7152 - val_loss: 5.1783 - val_accuracy: 0.3409\n",
      "Epoch 1500/1500\n",
      "83/83 [==============================] - 0s 623us/step - loss: 0.4928 - accuracy: 0.8303 - val_loss: 4.9654 - val_accuracy: 0.3591\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.35      0.35        20\n",
      "           1       0.68      0.75      0.71        20\n",
      "           2       0.06      0.05      0.05        20\n",
      "           3       0.68      0.75      0.71        20\n",
      "           4       0.14      0.15      0.15        20\n",
      "           5       0.54      0.75      0.63        20\n",
      "           6       0.53      0.45      0.49        20\n",
      "           7       0.39      0.35      0.37        20\n",
      "           8       0.12      0.15      0.13        20\n",
      "           9       0.35      0.35      0.35        20\n",
      "          10       0.22      0.10      0.14        20\n",
      "\n",
      "    accuracy                           0.38       220\n",
      "   macro avg       0.37      0.38      0.37       220\n",
      "weighted avg       0.37      0.38      0.37       220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 500 + (500 * choosenIndex)\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "model = getNetwork()\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.25)\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "report = classification_report(y_test, pred)\n",
    "classification_report_csv(report, \"NN\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Models in C code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpwxpktlh6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpwxpktlh6/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Neural network with TinyMLGen\n",
    "with open(tasks[choosenIndex] + '/exportedModels/' + 'NNmodel.h', 'w') as f:\n",
    "    f.write(tiny.port(model, optimize=False))\n",
    "\n",
    "# Classifiers with MicroMLGen\n",
    "for name, model in models:\n",
    "    prepath = tasks[choosenIndex] + '/exportedModels/'\n",
    "    path = prepath + name + '.h'\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(port(model, optimize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
